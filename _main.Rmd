---
knit: "bookdown::render_book"
title: "The Epidemiologist R Handbook"  
description: "The Epi R Handbook est un manuel de référence sur l'utilisation de R en épidémiologie appliquée et santé publique."
author: "the handbook team"
date: "`r Sys.Date()`"
#url: 'https://github.com/appliedepi/epiRhandbook_fr'
#twitter-handle: 
#cover-image: images/R_Handbook_Logo.png
site: bookdown::bookdown_site
# output: bookdown::gitbook:
#      config:
#           sharing:
#                twitter: yes
#                facebook: yes
#                whatsapp: yes
#                github: yes
documentclass: book
---





#  {-}

```{r, out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "Epi R Handbook banner beige 1500x500.png"))
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<meta name="description" content="The Epi R Handbook est un manuel de référence sur l'utilisation de R en épidémiologie appliquée et santé publique.">

<meta http-equiv="Content-Type" content="text/html; charset=utf-8">

<!-- <span style="color: red;">**THIS IS A DRAFT.  REVIEWERS GIVE FEEDBACK AT THIS [LINK](https://forms.gle/4RNdRRLGx67xW9yq9)**.</span> -->

<!-- <span style="color: darkgreen;">**DO YOU LIKE THIS HANDBOOK? SHOULD SOMETHING BE CHANGED? PLEASE TELL US!**</span> -->

<!-- <form target="_blank" action="https://forms.gle/A5SnRVws7tPD15Js9"> -->
<!--     <input type="submit" value="FEEDBACK" /> -->
<!-- </form> -->

<!-- ======================================================= -->
<!-- ## An R reference manual for applied epidemiology and public health {.unnumbered} -->


<!-- <span style="color: brown;">**The Epi R Handbook is an R reference manual for applied epidemiology and public health.**</span> -->

<!-- ## About this handbook   -->

## R pour l'épidémiologie appliquée et la santé publique {-}  

**Ce manuel a les objectifs suivants :**  

* Établir une base de code de référence
* Illustrer comment utiliser R pour effectuer des tâches classiques et récurrentes en épidémiologie
* Aider les épidémiologistes à faire la transition vers R 
* Être facilement accessible, y compris dans les endroits ayant une mauvaise connexion internet, grâce à une **[version hors-ligne](#download_offline)**  


<!-- * Use practical epi examples - cleaning case linelists, making transmission chains and epidemic curves, automated reports and dashboards, modeling incidence and making projections, demographic pyramids and rate standardization, record matching, outbreak detection, survey analysis, survival analysis, GIS basics, contact tracing, phylogenetic trees...   -->



<!-- **How is this different than other R books?**   -->

<!-- * It is community-driven - *written for epidemiologists by epidemiologists* in their spare time and leveraging experience in local, national, academic, and emergency settings   -->

<!-- Dual-column created based on the rmarkdown cookbook here: https://bookdown.org/yihui/rmarkdown-cookbook/multi-column.html -->



<br>


:::: {style="display: flex;"}

::: {}
```{r, out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "epiRhandbook_HexSticker_500x500.png"))
```
:::


::: {.col data-latex="{0.05\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::

::: {}
<span style="color: black;">**Écrit par des épidémiologistes, pour les épidémiologistes**</span>

Nous sommes une équipe d'épidémiologistes venant du monde entier. Nous travaillons bénévolement pour  offrir cette ressource à la communauté. Vos encouragements et vos retours sont les bienvenus :

* Retours structurés via un **[formulaire](https://forms.gle/A5SnRVws7tPD15Js9)**  
* Courriel **epiRhandbook@gmail.com** or tweet **[\@epiRhandbook](https://twitter.com/epirhandbook)** * Soumettre un ticket sur notre **[répertoire Github](https://github.com/appliedepi/epiRhandbook_fr)**  

:::

::::




<!-- ======================================================= -->
## Comment utiliser ce mannuel ? {-} 

* Naviguez dans la table des matières ou utilisez le formulaire de recherche  
* Cliquez sur le bouton "Copy" pour copier le code
* Utilisez les [données fournies avec le manuel](#download_book_data) pour exécuter le code sur votre ordinateur
* utilisez la section "Ressources" de chaque page pour aller plus loin
 

**Version hors-ligne**  

Voir les instructions dans le chapitre [Téléchargement du livre et données](#download_book_data) pour télécharger le livre.  

**Langues**  

Ce livre a été originellement publié en Anglais puis traduit bénévolement par une équipe. Si vous voulez nous aider pour la traduction des prochaines éditions, n'hésitez pas à nous contacter.


<!-- ======================================================= -->
## Remmerciements {-}  

Cet ouvrage est le fruit du travail d'une équipe internationale d'épidémiologistes, qui se sont appuyés sur leur expérience auprès d'organisations telles que les agences sanitaires locales, régionales, provinciales et nationales de divers pays, l'Organisation mondiale de la santé (OMS), Médecins Sans Frontières (MSF), les systèmes hospitaliers et les institutions universitaires.

Ce guide n'est **pas** un produit approuvé par une organisation spécifique. Bien que nous nous soyons efforcés à être précis, nous ne pouvons fournir aucune garantie quant au contenu de ce livre.  


### Auteurs et contributeurs {-}  

**Editeur:** [Neale Batra](https://www.linkedin.com/in/neale-batra/) 

**Commité éditorial** [Neale Batra](https://www.linkedin.com/in/neale-batra/), [Alex Spina](https://github.com/aspina7), [Amrish Baidjoe](https://twitter.com/Ammer_B), Pat Keating, [Henry Laurenson-Schafer](https://github.com/henryls1), [Finlay Campbell](https://github.com/finlaycampbell)  

**Auteurs et autrices**: [Neale Batra](https://www.linkedin.com/in/neale-batra/), [Alex Spina](https://github.com/aspina7), [Paula Blomquist](https://www.linkedin.com/in/paula-bianca-blomquist-53188186/), [Finlay Campbell](https://github.com/finlaycampbell), [Henry Laurenson-Schafer](https://github.com/henryls1), [Isaac Florence](www.Twitter.com/isaacatflorence), [Natalie Fischer](https://www.linkedin.com/in/nataliefischer211/), [Aminata Ndiaye](https://twitter.com/aminata_fadl), [Liza Coyer]( https://www.linkedin.com/in/liza-coyer-86022040/), [Jonathan Polonsky](https://twitter.com/jonny_polonsky), [Yurie Izawa](https://ch.linkedin.com/in/yurie-izawa-a1590319), [Chris Bailey](https://twitter.com/cbailey_58?lang=en), [Daniel Molling](https://www.linkedin.com/in/daniel-molling-4005716a/), [Isha Berry](https://twitter.com/ishaberry2), [Emma Buajitti](https://twitter.com/buajitti), [Mathilde Mousset](https://mathildemousset.wordpress.com/research/), [Sara Hollis](https://www.linkedin.com/in/saramhollis/), Wen Lin  

**Relecture**: Pat Keating, Annick Lenglet, Margot Charette, Danielly Xavier, Esther Kukielka, Michelle Sloan, Aybüke Koyuncu, Rachel Burke, Kate Kelsey, [Berhe Etsay](https://www.linkedin.com/in/berhe-etsay-5752b1154/), John Rossow, Mackenzie Zendt, James Wright, Laura Haskins, [Flavio Finger](ffinger.github.io), Tim Taylor, [Jae Hyoung Tim Lee](https://www.linkedin.com/in/jaehyoungtlee/), [Brianna Bradley](https://www.linkedin.com/in/brianna-bradley-bb8658155), [Wayne Enanoria](https://www.linkedin.com/in/wenanoria), Manual Albela Miranda, [Molly Mantus](https://www.linkedin.com/in/molly-mantus-174550150/), Pattama Ulrich, Joseph Timothy, Adam Vaughan, Olivia Varsaneux, Lionel Monteiro, Joao Muianga  

**Illustrations**: Calder Fong  

**Traduction**: 

<!-- **Editor-in-Chief:** Neale Batra  -->

<!-- **Project core team:** Neale Batra, Alex Spina, Amrish Baidjoe, Pat Keating, Henry Laurenson-Schafer, Finlay Campbell   -->

<!-- **Authors**: Neale Batra, Alex Spina, Paula Blomquist, Finlay Campbell, Henry Laurenson-Schafer, [Isaac Florence](www.Twitter.com/isaacatflorence), Natalie Fischer, Aminata Ndiaye, Liza Coyer, Jonathan Polonsky, Yurie Izawa, Chris Bailey, Daniel Molling, Isha Berry, Emma Buajitti, Mathilde Mousset, Sara Hollis, Wen Lin   -->

<!-- **Reviewers**: Pat Keating, Mathilde Mousset, Annick Lenglet, Margot Charette, Isha Berry, Paula Blomquist, Natalie Fischer, Daniely Xavier, Esther Kukielka, Michelle Sloan, Aybüke Koyuncu, Rachel Burke, Daniel Molling, Kate Kelsey, Berhe Etsay, John Rossow, Mackenzie Zendt, James Wright, Wayne Enanoria, Laura Haskins, Flavio Finger, Tim Taylor, Jae Hyoung Tim Lee, Brianna Bradley, Manual Albela Miranda, Molly Mantus, Priscilla Spencer, Pattama Ulrich, Joseph Timothy, Adam Vaughan, Olivia Varsaneux, Lionel Monteiro, Joao Muianga   -->


### Financements {-}  

Le manuel a reçu un financement de soutien via une subvention d'urgence COVID-19 pour le renforcement des capacités de la part de [TEPHINET] (https://www.tephinet.org/), le réseau mondial des programmes de formation en épidémiologie de terrain (FETP).  

Le réseau des anciens d'EPIET ([EAN](https://epietalumni.net/)) a fourni un soutien administratif (Annika Wendland en particulier). EPIET est le programme européen de formation en épidémiologie d'intervention.

Nous remercions tout particulièrement le Centre Opérationnel d'Amsterdam de Médecins Sans Frontières  (MSF OCA) pour son soutien lors de l'élaboration de ce manuel.  

**Cette publication a été soutenue par l'accord de coopération numéro NU2GGH001873, financé par les Centers for Disease Control and Prevention par le biais de TEPHINET, un programme de "The Task Force for Global Health". Son contenu relève de la seule responsabilité des auteurs et ne reflète pas les opinions officielles des Centers for Disease Control and Prevention, du Department of Health and Human Services, de The Task Force for Global Health, Inc. ou de TEPHINET.**


### Inspirations {-}  

Nous nous sommes inspiré de multiples tutoriels, livres et vignettes développés par la communauté pour développer ce manuel. Ces ressources, sont crédités dans les chapitres respectifs, mais nous souhaitons citer quelques sources d'inspiration générales que nous utilisons de manière récurrente :  

[The "R4Epis" project](https://r4epis.netlify.app/) (une collaboration entre MSF et RECON)  
[R Epidemics Consortium (RECON)](https://www.repidemicsconsortium.org/)  
[R for Data Science book (R4DS)](https://r4ds.had.co.nz/)  
[bookdown: Authoring Books and Technical Documents with R Markdown](https://bookdown.org/yihui/bookdown/)  
[Netlify](https://www.netlify.com) qui héberge ce site 


<!-- ### Image credits {-}   -->

<!-- Images in logo from US CDC Public Health Image Library) include [2013 Yemen looking for mosquito breeding sites](https://phil.cdc.gov/Details.aspx?pid=19623), [Ebola virus](https://phil.cdc.gov/Details.aspx?pid=23186), and [Survey in Rajasthan](https://phil.cdc.gov/Details.aspx?pid=19838).   -->


## Conditions d'utilisation et contribution {-}  

### License {.unnumbered} 

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />Ce document est mis à disposition selon les termes de lalicence <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.fr">Creative Commons Attribution - Pas d’Utilisation Commerciale - Partage dans les Mêmes Conditions 4.0 International (CC BY-NC-SA 4.0) </a>.


N'hésitez pas à utiliser les contenus de ce manuel dans vos cours et formations en épidémiologie, ou à le conseiller à vos étudiants. Si vous avez des questions sur l'utilisation que vous souhaitez en faire, envoyez un courriel à **epiRhandbook@gmail.com**.  

### Citation {.unnumbered}

Batra, Neale, et al. The Epidemiologist R Handbook. 2021.  <a rel="license" href="https://zenodo.org/badge/231610102.svg"><img alt="DOI" style="border-width:0" src="https://zenodo.org/badge/231610102.svg" /></a><br />

### Contribuer {.unnumbered}  

Si vous souhaitez contribuer à cet ouvrage, veuillez d'abord nous contacter via les tickets (_issues_) Github ou par courriel. Nous sommes en train de développer un calendrier de mise à jour et un guide du contributeur.  

Veuillez noter que le projet epiRhandbook est publié avec un [code de conduite du contributeur](https://contributor-covenant.org/version/2/0/CODE_OF_CONDUCT.html). En contribuant à ce projet, vous acceptez de vous conformer à ses conditions.


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:index.Rmd-->

# (PART) About this book {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_about_book.Rmd-->

# Notes techniques et choix éditoriaux {#editorial_notes}

Nous décrivons ici les choix pédagogiques, le style et les décisions éditoriales spécifiques prises lors de l'écriture de ce guide.  


## Approche et style

Le public visé par ce manuel est large. Nous espérons qu'il sera utile aux épidémiologistes novices en R, mais aussi aux utilisateurs expérimentés à la recherche de bonnes pratiques et d'astuces. L'ouvrage doit donc être à la fois accessible et succinct. Notre cherchons à fournir *juste assez* d'explications textuelles pour qu'une personne débutante en R puisse appliquer le code et comprendre ce qu'il fait.  


En conséquences de quoi, ce guide est :  

* un ouvrage de référence de code, accompagné d'exemples relativement brefs, *et non* un manuel complet sur R ou la science des données  
* un *guide R* à utiliser dans le cadre de l'épidémiologie appliquée, *et non* un manuel sur les méthodes ou la science de l'épidémiologie appliquée  
* un document évolutif : les paquets R optimaux pour une tâche donnée changent souvent et nous sommes ouverts à toute discussion sur les paquets à privilégier dans ce manuel.  



### Paquets R {.unnumbered}

**Tellement de possibilités...**  

Un aspect difficile de l'apprentissage de R est de savoir quel paquet R utiliser pour une tâche donnée. Il n'est pas rare que l'on se décarcasse à écrire vingt (cent ?) lignes de code, pour se rendre compte plus tard qu'il existe un paquet R qui donne le même résultat recheré en une seule ligne de commande !  


Dans ce guide, nous essayons de vous proposer au moins deux façons de réaliser chaque tâche : une méthode éprouvée (probablement dans R de **base** ou utilisant le **tidyverse**) et un paquet R spécialement conçu à cet effet. Nous voulons que vous ayez les deux options, au cas où vous ne pourriez pas télécharger un paquet donné ou si celui-ci ne vous convient pas.  
 
Pour choisir les paquets à utiliser, nous avons donné la priorité aux paquets R et aux approches qui ont été testés et approuvés par la communauté, qui minimisent le nombre de paquets utilisés dans une session de travail typique, qui sont stables (ne changent pas très souvent) et qui accomplissent la tâche simplement et proprement.  


Ce manuel donne généralement la priorité aux paquets et fonctions R du méta-paquet **tidyverse**. Tidyverse est une *collection de paquets R* conçus pour la science des données, et qui partagent une grammaire et des structures de données sous-jacentes. Tous les paquets du Tidyverse peuvent être installés ou chargés séparément, ou en masse via le paquet **tidyverse**. Pour en savoir plus, consultez le [site Web du tidyverse](https://www.tidyverse.org/). 


Nous proposons également souvent des options de code utilisant R de **base** (les paquets et fonctions fournis avec R à l'installation). En effet, nous sommes conscients que certains lecteurs de ce livre ne disposent pas d'un accès Internet fiable pour télécharger des paquets supplémentaires. 


**Expliciter quelle fonction appartient à quel paquet**

Il est souvent frustrant lorsque l'on suit un tutoriel R de ne pas savoir de quel paquet provient une fonction (et donc de ne pas pouvoir l'utiliser immédiatement dans notre code) ! 

Dans ce guide, les noms des paquets seront écrits en gras (par exemple **dplyr**) et les fonctions sont écrites comme ceci : `mutate()`. Nous nous efforçons d'être explicites quant au paquet dont provient une fonction, soit en faisant référence au paquet dans le texte voisin, soit en spécifiant le paquet explicitement dans le code, comme ceci : `dplyr::mutate()`. Cela alourdit un petit peu le code, mais rend plus facile la réutilisation du code chez vous.  

Consultez la page sur les [Bases de R](#rbasics) pour en savoir plus sur les paquets et les fonctions.



### Choix d'yn style de code {.unnumbered}

Dans le manuel, nous allons fréquemment à la ligne, ce qui rend notre code "long". Nous faisons cela pour plusieurs raisons :  

* cela permet d'écrire des commentaires explicatifs avec `#` adjacents à la commande qu'ils décrivent,  
* généralement, un code plus long (vertical) est plus facile à lire,   
* il est plus facile à lire sur un écran étroit (pas de défilement latéral nécessaire),  
* il est plus facile de savoir quels arguments appartiennent à quelle fonction grâce aux indentations.  

Par conséquent, un bout de code code qui *pourrait* être écrit comme ceci : 

```{r, eval=F}
linelist %>% group_by(hospital) %>%  # groupe les lignes par hopital
  slice_max(date, n = 1, with_ties = F) # s'il y a égalité de date, prendre la première
```

...est écrit comme cela :  

```{r, eval=F}
linelist %>% 
  group_by(hospital) %>% # groupe les lignes par hopital
  slice_max(
    date,                # Garde les lignes avec la date maximun à l'intérieur de chaque groupe
    n = 1,               # Ne garder que la date maximum
    with_ties = F)       # S'il y a égalité de date, prendre la première
```

Le code R n'est généralement pas affecté par les nouvelles lignes ou les indentations. Lorsque vous écrivez dans Rstudio (ou un éditeur décent), l'indentation se fera automatiquement lorsque vous allez à la ligne après une virgule. 

Nous utilisons beaucoup d'espaces (par exemple `n = 1` au lieu de `n=1`) parce que c'est plus facile à lire pour beaucoup de personnes. Pensez aux gens qui lisent votre code !  



### Nomenclature {.unnumbered}  

Dans ce manuel, nous faisons généralement référence aux "colonnes" et aux "lignes" plutôt qu'aux "variables" et "observations". Comme l'explique cette introduction aux ["données ordonnées"](https://tidyr.tidyverse.org/articles/tidy-data.html), la plupart des jeux de données statistiques épidémiologiques se composent structurellement de lignes, de colonnes et de valeurs.  

Les *variables* contiennent les valeurs qui mesurent le même attribut sous-jacent (comme le groupe d'âge, le résultat ou la date d'apparition des symptomes). Les *observations* contiennent toutes les valeurs mesurées sur la même unité (par exemple, une personne, un site ou un échantillon de laboratoire). Ces aspects peuvent donc être plus difficiles à définir de manière tangible.  

Dans les ensembles de données "ordonnés" (*tidy data* en anglais), chaque colonne est une variable, chaque ligne est une observation et chaque cellule est une valeur unique. Cependant, certains jeux de données que vous rencontrerez ne correspondront pas à ce modèle - un ensemble de données au format "large" peut avoir une variable répartie sur plusieurs colonnes (voir un exemple à la page  [Transformation long-large](#pivoting_data)). De même, les observations peuvent être réparties sur plusieurs lignes.  

La majeure partie de ce manuel porte sur le nettoyage et la transformation des données, et il est donc plus pertinent de se référer aux structures de données concrètes que sont les lignes et les colonnes qu'aux observations et aux variables plus abstraites. Les exceptions se produisent principalement dans les pages sur l'analyse des données, où vous verrez davantage de références aux "variables" et aux "observations".  


### Notes {.unnumbered} 

Voici les types de notations utilisées dans le guide :  

<span style="color: black;">**_NOTE:_** Ceci est une note</span>  
<span style="color: darkgreen;">**_TIP:_** Ceci est un conseil ou une astuce.</span>  
<span style="color: orange;">**_CAUTION:_** Ceci vous invite à bien prêter attention.</span>  
<span style="color: red;">**_DANGER:_** Ceci est un avertissement.</span>  



## Choix techniques

Ci-dessous, nous décrivons les principales décisions concernant le choix des paquets et des fonctions. Si vous n'êtes pas d'accord ou si vous souhaitez proposer un nouvel outil à examiner, veuillez rejoindre/démarrer une conversation sur notre [page Github](https://github.com/epirhandbook/Epi_R_handbook).    




**Tableau des paquets, fonction et autres choix techniques**  


Sujet           |     Considéré      |   Choisi              |    Explication brève   
----------------- | --------------------|------------------------|-----------------------------------------------

Approche générale|**tidyverse**, **data.table**, **base**|**tidyverse**, avec un chapitre sur **data.table**, et mentions d'alternatives en R de **base** pour les lecteurs avec une connexion Internet faible|lisibilité accrue, universel, paquets très répandus  
Importation des paquets|`library()`,`install.packages()`, `require()`, **pacman**|**pacman**|Simplification et code plus court pour les cas avec de nombreux paquets à installer puis importer
Import et export de données|**rio**, de nombreux paquets spécialisés|**rio**|Gère un grand nombre de format de jeux de données
Résumer des données agrégées|**dplyr** `group_by()`, **stats** `aggregate()`|**dplyr** `group_by()`|Reste cohérent avec nos choix d'utiliser le **tidyverse**
Transformation long-large|**tidyr** (fonctions `pivot_XXX`), **reshape2** (melt/cast), **tidyr** (spread/gather)|**tidyr** (fonctions `pivot_XXX`)|**reshape2** est en fin de vie (recommandations officielles d'utiliser **tidyr**), **tidyr** utilise les fonctions `pivot_XXX` dès la versions v1.0.0
Nettoyer les noms des colonnes|**matchmaker**, **janitor**|**janitor**|Janitor est utilisé pour plusieus tâches dans le guide (optimisation des paquets)
Semaines epi |**lubridate**, **aweek**, **tsibble**, **zoo**|**lubridate** en général, avec utilisation ponctuelle d'autres paquets pour des cas spécifiques|La grande flexibilité de **lubridate**, la cohérence avec le **tidyverse**, une meilleure maintenance future (?)  
Labels ggplot |`labs()`, `ggtitle()`/`ylab()`/`xlab()` |`labs()` |Simplicité, tous les labels dans la même commande  
Conversion en facteur |`factor()`, **forcats**|**forcats**|différentes fonctions pour transformer les facteurs dans la même commande
Courbes épidémiques|**incidence**, **ggplot2**, **EpiCurve**|**incidence2** pour le plus rapide, **ggplot2** pour les détails|fiabilité
Concaténation|`paste()`, `paste0()`, `str_glue()`, `glue()`|`str_glue()`|syntaxe plus simple que `paste()`; dans **stringr**


## Révisions majeures


Date           |Changements majeurs     
---------------| ------------------------------------------    
10 Mai 2021    |Publication de la version 1.0.0 
20 Nov 2022    |Publication de la version 1.0.1

**NEWS**
Avec la version 1.0.1, les changements suivants ont été mis en œuvre :  

* Mise à jour vers la version 4.2 de R  
* Nettoyage des données : remplacement de {linelist} par {matchmaker}, suppression d'une ligne inutile dans l'exemple `case_when()`.  
* Dates : remplacement de {linelist} `guess_date()` par {parsedate} `parse_date()`.
* Pivot : légère mise à jour de `pivot_wider()`id_cols=`.  
* Analyse d'enquête : remplacement de `plot_age_pyramid()` par `age_pyramid()`, légère modification du code du tracé alluvial.  
* Graphiques de chaleur : ajout de `ungroup()` au chunk `agg_weeks`.  
* Graphiques interactifs : ajout de `ungroup()` au chunk qui fait `agg_weeks` pour que `expand()` fonctionne comme prévu.  
* Séries temporelles : ajout de `data.frame()` autour des objets dans toutes les commandes `trending::fit()` et `predict()`.  
* Analyse des combinaisons : Remplacer `case_when()` par `ifelse()` et ajouter le code optionnel `across()` pour préparer les données. 


## Information de session (R, RStudio, paquets)  

Vous trouverez ci-dessous les informations sur les versions de R, RStudio et les paquets R utilisés lors de la compilation du guide.  

```{r}
sessioninfo::session_info()
```




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/editorial_style.Rmd-->

# Télécharger le manuel et les données  {#download_book_data}


<!-- Note to self: If you want to create a download link to Github, right-click the "View Raw" button on Github, copy the address, and use that in the HTML below. -->




## Télécharger le manuel hors-ligne  {#download_offline}

Vous pouvez télécharger la version hors-ligne de ce manuel en tant que fichier HTML afin de pouvoir le visualiser dans votre navigateur Web même si vous n'avez plus accès à Internet. Si vous envisagez d'utiliser le manuel Epi R hors ligne, voici quelques éléments à prendre en compte :  

* Lorsque vous ouvrez le fichier, le chargement des images et de la table des matières peut prendre une minute ou deux.  
* Le manuel hors ligne a une mise en page légèrement différente : une très longue page avec la table des matières à gauche. Pour rechercher des termes spécifiques, utilisez Ctrl + F (Cmd + F).  
* Consultez la page [Paquets conseillés](#suggested_packages) pour vous aider à installer les paquets R appropriés avant de perdre votre connexion à Internet.  
* Installez notre paquet R **epirhandbook** qui contient toutes les données utilisées dans les exemples (le processus d'installation est décrit ci-dessous).  

**Il y a deux façons de télécharger le manuel :**  



### Utiliser le lien de téléchargement {.unnumbered}  

Pour un accès rapide, **cliquez à droite** [ce lien](https://github.com/appliedepi/epiRhandbook_fr/blob/master/offline_long/Epi_R_Handbook_offline.html) **et sélectionnez "Enregistrer le lien sous"**.  

Si vous êtes sur un Mac, utilisez Cmd + clic. Si vous êtes sur un téléphone portable, appuyez sur le lien et maintenez-le enfoncé, puis sélectionnez "Enregistrer le lien". Le manuel sera téléchargé sur votre appareil. Si un écran contenant un code HTML brut apparaît, assurez-vous d'avoir suivi les instructions ci-dessus ou essayez l'option 2.  



### Utiliser notre paquet R {.unnumbered}  

Nous avons développé un paquet R appelé **epirhandbook**. Il comprend une fonction `download_book()` qui télécharge le fichier du guide depuis notre dépôt Github sur votre ordinateur.  


Ce package contient également une fonction `get_data()` qui télécharge toutes les données utilisées dans les chapitres sur votre ordinateur.  


Exécutez le code suivant pour installer notre paquet R **epirhandbook** à partir du [dépôt Github *appliedepi*](https://github.com/appliedepi/epirhandbook). Ce paquet n'est pas sur le CRAN, donc utilisez la fonction spéciale `p_install_gh()` du paquet **pacman** pour l'installer depuis Github.  


```{r, eval=F}
# installer la dernière version du paquet epirhandbook
pacman::p_install_gh("appliedepi/epirhandbook")
```

Maintenant, importer le paquet pour l'utiliser dans votre session R actuelle :  

```{r, eval=F}
# Importer le paquet pour pouvoir l'utiliser dans la session ouverte
pacman::p_load(epirhandbook)
```

Ensuite, exécutez la fonction du paquet `download_book()` (avec des parenthèses vides) pour télécharger le manuel sur votre ordinateur. En supposant que vous êtes dans RStudio, une fenêtre apparaîtra pour vous permettre de sélectionner un emplacement de sauvegarde.  


```{r, eval=F}
# télécharger la version html du manuel localement
download_book()
```



## Télécharger les données

Pour pouvoir reproduire les exemples au fur et à mesure sur votre ordinateur, vous pouvez télécharger les données et les fichiers générés. 

### Utiliser notre paquet R {.unnumbered}  

Une fois le paquet téléchargé et importé dans votre session R (voir section au-dessus) utilisez la fonction `get_data()` du paquet pour obtenir les données d'exemple sur votre ordinateur. Exécutez `get_data("all")` pour obtenir *toutes* les données d'exemple, ou fournissez un nom de fichier spécifique et une extension entre guillemets pour récupérer un seul fichier.  

Techniquement, toutes les données ont déjà été téléchargées avec le paquet, et doivent simplement être transférées dans un dossier de votre ordinateur. Une fenêtre pop-up apparaîtra, vous permettant de sélectionner un emplacement de dossier de sauvegarde. Nous vous suggérons de créer un nouveau dossier "data" car il y a environ 30 fichiers (y compris les données d'exemple et les sorties générées par les exemples).  


```{r, eval=F}
# enregistrer toutes les données dans un dossier sur votre ordinateur
get_data("all")

# enregistrer les données linelist dans un dossiet sur votre ordinateur
get_data(file = "linelist_cleaned.rds")

```


Une fois que vous avez utilisé `get_data()` pour enregistrer un fichier sur votre ordinateur, vous devrez encore l'importer dans R. Voir la page [Importer et exporter des données](#import_export) pour plus de détails.  

Si vous le souhaitez, vous pouvez consulter toutes les données utilisées dans ce manuel dans le **["dossier données"](https://github.com/epirhandbook/Epi_R_handbook/tree/master/data)** de notre dépôt Github.  


### Téléchargement manuel {.unnumbered}  

Vous pouvez télécharger les données fichier par fichier à partir de notre dépôt Github via un lien ou une commande R spécifique au fichier. Certains types de fichiers ont un bouton de téléchargement, tandis que d'autres peuvent être téléchargés via une commande R. 


#### Liste de cas (linelist) {.unnumbered}

Il s'agit d'une linelist pour une épidémie d'Ebola fictive, développée par notre équipe à partir du jeu de données d'exemple `ebola_sim` du paquet **outbreaks**.  

* <a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/case_linelists/linelist_raw.xlsx' class='download-button'>Cliquer pour télécharger les données brutes (.xlsx)</span></a>. La liste de cas "brute" est une feuille de calcul Excel contenant des données désordonnées. Utilisez-la pour suivre la page [Nettoyer les données et fonctions essentielles](#cleaning_data).  

* <a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>Cliquer pour télécharger la linelist nettoyée (.rds)</a>. Utilisez ce fichier pour toutes les autres pages de ce manuel qui utilisent la linelist. Un fichier .rds est un type de fichier spécifique à R qui préserve les classes de colonnes. Cela garantit que vous n'aurez qu'un nettoyage minimal à faire après avoir importé les données dans R.  

*Autres fichiers linelist :*  

* <a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/case_linelists/linelist_cleaned.xlsx' class='download-button'>Cliquer pour télécharger la version nettoyée de la linelist sous format Excel</a>

* Une partie de la page sur le nettoyage des données utilise un "dictionnaire de nettoyage" (fichier .csv). Vous pouvez le charger directement dans R en exécutant les commandes suivantes :  

```{r, eval=F}
pacman::p_load(rio) # installer/importer le paquet **rio**

# importer le fichier directement depuis github
cleaning_dict <- import("https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/case_linelists/cleaning_dict.csv")
```


#### Cas de paludisme {#data_malaria .unnumbered}  

Ces données sont des comptages fictifs de cas de paludisme par groupe d'âge, établissement et jour. Un fichier .rds est un type de fichier spécifique à R qui préserve les classes de colonnes. Cela garantit que vous n'aurez qu'un nettoyage minimal à faire après avoir importé les données dans R.  

<a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/malaria_facility_count_data.rds' class='download-button'>
	Click to download
	<span>les comptages de de cas de paludisme (.rds file)</span>
</a>


#### Données sur l'échelle de Likert {.unnumbered}  

Il s'agit de données fictives issues d'une enquête de type Likert, utilisées dans la page [Pyramides démographiques et échelles de Likert](#age_pyramid). Vous pouvez charger ces données directement dans R en exécutant les commandes suivantes :    


```{r, eval=F}
pacman::p_load(rio)  # installer/importer le paquet **rio**

# importer le fichier directement depuis github
likert_data <- import("https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/likert_data.csv")
```


#### Flexdashboard {.unnumbered}  

Vous trouverez ci-dessous des liens vers le fichier associé à la page [Tableaux de bord avec R Markdown](#dashboards): 

* Pour télécharger le fichier  RMarkdown (.Rmd) du tableau de bord sur les épidémies, faites un clic droit sur ce [lien](https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/flexdashboard/outbreak_dashboard.Rmd) (Cmd+clic pour Mac) et sélectionnez "Enregistrer le lien sous".  
* Pour télécharger le tableau de bord HTML, cliquez avec le bouton droit de la souris sur ce [lien](https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/flexdashboard/outbreak_dashboard_test.html). (Cmd + clic pour Mac) et sélectionnez "Enregistrer le lien sous".  


#### recherche des contacts {.unnumbered} 

La page [Recherche des contacts](#contact_tracing) présente une analyse des données de recherche des contacts, à l'aide d'exemples de données provenant de [Go.Data](https://github.com/WorldHealthOrganization/godata/tree/master/analytics/r-reporting). Les données utilisées dans cette page peuvent être téléchargées sous forme de fichiers .rds en cliquant sur les liens suivants :  

<a href='https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/cases_clean.rds?raw=true' class='download-button'>
	Cliquer pour télécharger
	<span>les données d'investigation des cas (.rds file)</span>
</a>

<a href='https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/contacts_clean.rds?raw=true' class='download-button'>
	Cliquer pour télécharger
	<span>les données d'enregistrement des contacts (.rds file)</span>
</a>

<a href='https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/followups_clean.rds?raw=true' class='download-button'>
	Cliquer pour télécharger
	<span>les données de suivi des contacts (.rds file)</span>
</a>



<span style="color: black;">**_NOTE:_** Les données structurées de recherche des contacts provenant d'autres logiciels (par exemple KoBo, DHIS2 Tracker, CommCare) peuvent être organisées differement. Si vous souhaitez contribuer à l'élaboration d'un échantillon de données ou d'un contenu alternatif pour cette page, veuillez [nous contacter](#contact_us).</span> 

<span style="color: darkgreen;">**_TIP:_** Si vous déployez Go.Data et souhaitez vous connecter à l'API de votre instance, consultez la page Importation et exportation, [(section API)](#import_api) et la [Communauté de pratique Go.Data](https://community-godata.who.int/).</span>



#### SIG {.unnumbered}  

Les fichiers Shapefiles comportent de nombreux sous-fichiers, chacun avec une extension de fichier différente. Un fichier aura l'extension ".shp", mais d'autres peuvent avoir ".dbf", ".prj", etc.  

La page [Notions de base sur les SIG](#gis) fournit des liens vers le site Web *Humanitarian Data Exchange* où vous pouvez télécharger les fichiers de forme directement sous forme de fichiers zippés.  

Par exemple, les données des locations des établissements de santé peuvent être téléchargées [ici](https://data.humdata.org/dataset/hotosm_sierra_leone_health_facilities). Téléchargez "hotosm_sierra_leone_health_facilities_points_shp.zip". Une fois enregistré sur votre ordinateur, décompressez le dossier. Vous verrez plusieurs fichiers avec des extensions différentes (par exemple, ".shp", ".prj", ".shx"); tous ces fichiers doivent être enregistrés dans le même dossier sur votre ordinateur. Ensuite, pour importer dans R, fournissez le chemin et le nom du fichier ".shp" à `st_read()` du paquet **sf** (comme décrit dans la page [Notions de base sur les SIG](#gis)).  

Si vous suivez l'option 1 pour télécharger toutes les données de l'exemple (via notre paquet R **epirhandbook**), tous les shapefiles sont inclus.  

Vous pouvez également télécharger les fichiers Shapefile à partir du dossier "data" du manuel R sur Github (voir le sous-dossier "gis"). Cependant, sachez que vous devrez télécharger *chaque* sous-fichier individuellement sur votre ordinateur. Dans Github, cliquez sur chaque fichier et téléchargez-les en cliquant sur le bouton "Télécharger". Ci-dessous, vous pouvez voir comment le fichier de forme "sle_adm3" se compose de plusieurs fichiers, chacun devant être téléchargé depuis Github. 

```{r out.height = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "download_shp.png"))
```


#### Arbres phylogénétiques {.unnumbered}  

La page sur les [arbres phylogénétiques](phylogenetic_trees) utilise un fichier Newick pour l'arbre phylogénétique construit à partir du séquençage du génome entier de 299 échantillons de Shigella sonnei et des données d'échantillons correspondantes (converties en fichier texte). Les échantillons belges et les données résultantes sont aimablement fournis par le CNR belge pour Salmonella et Shigella dans le cadre d'un projet mené par un boursier EUPHEM de l'ECDC, et seront également publiés dans un manuscrit. Les données internationales sont disponibles sur des bases de données publiques (ncbi) et ont déjà été publiées.  


* Pour télécharger le fichier de l'arbre phylogénétique "Shigella_tree.txt", cliquez avec le bouton droit de la souris sur ce [lien](https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/phylo/Shigella_tree.txt) (Cmd+click for Mac) et sélectionnez "Enregistrer le lien sous". 
* Pour télécharger le fichier "sample_data_Shigella_tree.csv" contenant des informations supplémentaires sur chaque échantillon, cliquez avec le bouton droit de la souris sur ce [lien](https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/phylo/sample_data_Shigella_tree.csv) (Cmd+clic pour Mac) et sélectionnez "Enregistrer le lien sous".  
* Pour voir le nouveau sous-arbre créé, cliquez avec le bouton droit de la souris sur ce [lien](https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/phylo/Shigella_subtree_2.txt) (Cmd+clic pour Mac) et sélectionnez "Enregistrer le lien sous". Le fichier .txt sera téléchargé sur votre ordinateur.  

Vous pouvez ensuite importer les fichiers .txt avec `read.tree()` du paquet **ape**, comme expliqué dans le chapitre concerné.


```{r, eval=F}
ape::read.tree("Shigella_tree.txt")
```


#### Standardization {.unnumbered} 

Pour la page sur la [standardisation des données](#standardisation), vous pouvez charger les données directement depuis notre dépôt Github sur internet dans votre session R avec les commandes suivantes :  


```{r, eval=F}
# installer/importer le paquet **rio**
pacman::p_load(rio) 

##############
# Pays A
##############
# import des données démographiques du pays depuis github
A_demo <- import("https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/standardization/country_demographics.csv")

# import des données de mortalité du pays depuis github
A_deaths <- import("https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/standardization/deaths_countryA.csv")



##############
# Pays B
##############
# import des données démographiques du pays depuis github
B_demo <- import("https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/standardization/country_demographics_2.csv")

# import des données de mortalité du pays depuis github
B_deaths <- import("https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/standardization/deaths_countryB.csv")


###############
# Population de référence
###############
# import depuis Github
standard_pop_data <- import("https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/standardization/world_standard_population_by_sex.csv")
```



#### Séries temporelles et détection des épidémies {#data_outbreak .unnumbered}  

Voir la page sur [les séries temporelles et la détection des épidémies](#time_series). Nous utilisons les cas de campylobacter rapportés en Allemagne de 2002 à 2011, tels que disponibles dans le paquet R **surveillance**. (*note* cet ensemble de données a été adapté de l'original, en ce sens que 3 mois de données ont été supprimés à partir de la fin de 2011 à des fins de démonstration).


<a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/time_series/campylobacter_germany.xlsx' class='download-button'>
	Cliquer pour télécharger
	<span> Campylobacter en Allemagne (.xlsx)</span>
</a>

Nous utilisons également les données climatiques de l'Allemagne entre 2002 et 2011 (température en degrés Celsius et précipitations en millimètres). Ces données ont été téléchargées à partir d'un jeu de données dérivé des données produites par le satellite Copernicus (UE) à l'aide du paquet **ecmwfr**. Vous devrez télécharger toutes ces données et les importer avec `stars::read_stars()` comme expliqué dans la page sur les séries temporelles.  


<a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/time_series/weather/germany_weather2002.nc' class='download-button'>
	Cliquer pour télécharger
	<span> Climat Allemagne 2002 (.nc file)</span>
</a> 

<a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/time_series/weather/germany_weather2003.nc' class='download-button'>
	Cliquer pour télécharger
	<span> Climat Allemagne 2003 (.nc file)</span>
</a> 

<a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/time_series/weather/germany_weather2004.nc' class='download-button'>
	Cliquer pour télécharger
	<span> Climat Allemagne 2004 (.nc file)</span>
</a> 

<a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/time_series/weather/germany_weather2005.nc' class='download-button'>
	Cliquer pour télécharger
	<span> Climat Allemagne 2005 (.nc file)</span>
</a> 

<a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/time_series/weather/germany_weather2006.nc' class='download-button'>
	Cliquer pour télécharger
	<span> Climat Allemagne 2006 (.nc file)</span>
</a> 

<a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/time_series/weather/germany_weather2007.nc' class='download-button'>
	Cliquer pour télécharger
	<span> Climat Allemagne 2007 (.nc file)</span>
</a> 

<a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/time_series/weather/germany_weather2008.nc' class='download-button'>
	Cliquer pour télécharger
	<span> Climat Allemagne 2008 (.nc file)</span>
</a> 

<a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/time_series/weather/germany_weather2009.nc' class='download-button'>
	Cliquer pour télécharger
	<span> Climat Allemagne 2009 (.nc file)</span>
</a> 

<a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/time_series/weather/germany_weather2010.nc' class='download-button'>
	Cliquer pour télécharger
	<span> Climat Allemagne 2010 (.nc file)</span>
</a> 

<a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/time_series/weather/germany_weather2011.nc' class='download-button'>
	Cliquer pour télécharger
	<span> Climat Allemagne 2011 (.nc file)</span>
</a>



#### Analyse d'enquêtes {#data_survey .unnumbered}  

Pour la page [analyse d'enquête](#survey_analysis), nous utilisons des données d'enquêtes de mortalité fictives basées sur les modèles d'enquête MSF OCA. Ces données fictives ont été générées dans le cadre du projet ["R4Epis"](https://r4epis.netlify.app/).

<a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/surveys/survey_data.xlsx' class='download-button'>
	Cliquer pour télécharger
	<span> Données d'enquête fictives (.xlsx)</span>
</a>

<a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/surveys/survey_dict.xlsx' class='download-button'>
	Cliquer pour télécharger
	<span> Données d'enquête fictives (dictionnaire) (.xlsx)</span>
</a>

<a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/surveys/population.xlsx' class='download-button'>
	Cliquer pour télécharger
	<span> Données d'enquête fictives (données de population) (.xlsx)</span>
</a>




#### Shiny {#data_shiny .unnumbered}  

La page sur les [tableaux de bord avec Shiny](#shiny) illustre la construction d'une application simple pour afficher les données sur le paludisme.  

Pour télécharger les fichiers R qui produisent l'app Shiny :  

Vous pouvez <a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/malaria_app/app.R' class='download-button'>
	cliquer ici pour télécharger le fichier app.R<span> qui contient à la fois le code de l'interface utilisateur et du serveur pour l'application Shiny.</span></a>.

Vous pouvez <a href='https://github.com/epirhandbook/Epi_R_handbook/blob/master/data/malaria_app/data/facility_count_data.rds' class='download-button'>
	cliquer ici pour télécharger le fichier facility_count_data.rds<span></a> qui contient les données sur le paludisme pour l'application Shiny. Notez que vous devrez peut-être l'enregistrer dans un dossier "data" pour que les chemins de fichier here() fonctionnent correctement.  

Vous pouvez <a href='https://github.com/epirhandbook/Epi_R_handbook/blob/master/data/malaria_app/global.R' class='download-button'>
	cliquer ici pour télécharger le fichier global.R<span></a> qui doit être exécuté avant l'ouverture de l'app, comme expliqué dans la page.
	
Vous pouvez <a href='https://github.com/appliedepi/epiRhandbook_fr/tree/master/data/malaria_app/funcs/plot_epicurve.R' class='download-button'>
	cliquer ici pour télécharger le fichier plot_epicurve.R<span></a> dont l'exécution est lancée par le script global.R. Notez que vous devrez peut-être le stocker dans un dossier "funcs" pour que les chemins de fichier here() fonctionnent correctement.



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/data_used.Rmd-->

# (PART) Basics {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_basics.Rmd-->

# R - les bases {#rbasics}

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "basics_header_close.png"))
```

Bienvenue !

Cette page passe en revue les éléments essentiels de R. Elle n'a pas pour but d'être un tutoriel complet, mais elle fournit les bases et peut être utile pour rafraîchir votre mémoire. La section [Ressources pour l'apprentissage](#learning) renvoie à des didacticiels plus complets.

Certaines parties de cette page ont été adaptées avec l'autorisation du [projet R4Epis](https://r4epis.netlify.app/).

Voir la page sur la [transition a R](#transition_to_r) pour des conseils sur le passage de STATA, SAS ou Excel à R.

```{r, echo=F}
# Importer la liste linéaire nettoyée d'ebola:
linelist <- rio::import(here::here("data", 
                                   "case_linelists", 
                                   "linelist_cleaned.rds"))

# Chargez le paquet apyramid (contient tableaux de données d'exemple):
pacman::p_load(apyramid)
```

<!-- ======================================================= -->

## Pourquoi utiliser R ?

Comme indiqué sur le [site Web du projet R](https://www.r-project.org/about.html), R est un langage de programmation et un environnement pour le calcul statistique et les graphiques. Il est très polyvalent, extensible et axé sur la communauté.

**Coût**

L'utilisation de R est gratuite ! Il existe une forte éthique dans la communauté du matériel gratuit et open-source.

**Reproductibilité**

La gestion et l'analyse de vos données par le biais d'un langage de programmation (par rapport à Excel ou à un autre outil essentiellement manuel) améliore la reproductibilité, facilite la détection des erreurs et allège votre charge de travail.

**Communauté**

La communauté des utilisateurs de R est énorme et collaborative. De nouveaux paquets et outils destinés à résoudre des problèmes concrets sont développés quotidiennement et approuvés par la communauté des utilisateurs. À titre d'exemple, [R-Ladies](https://rladies.org/) est une organisation mondiale dont la mission est de promouvoir la diversité des genres dans la communauté R, et c'est l'une des plus grandes organisations d'utilisateurs de R. Elle a probablement un chapitre près de chez vous !

## Termes clés

**RStudio** - RStudio est une interface utilisateur graphique (GUI) qui facilite l'utilisation de **R**. Pour en savoir plus, consultez la section [RStudio](#rstudio).

**Objets** - Tout ce que vous stockez dans R - les jeu de données, les variables, une liste de noms de villages, un population total d'habitants, et même les résultats tels que les graphiques - sont des *objets* auxquels on *attribue un nom* et qui *peuvent être référencés* dans des commandes ultérieures. Pour en savoir plus, consultez la section [Objets](#objects).

**Fonctions** - Une fonction est une opération de code qui accepte des entrées et renvoie une sortie transformée. Pour en savoir plus, consultez la section [Fonctions](#functions).

**Paquets** - Un paquet R est un ensemble de fonctions partageables. Pour en savoir plus, consultez la section [Packages](#packages).

**Scripts** - Un script est le fichier document qui contient vos commandes. Pour en savoir plus, consultez la section [Scripts](#scripts)

## Ressources pour l'apprentissage {#learning}

### Ressources au sein de RStudio {.unnumbered}

**Documentation d'aide**

Recherchez dans l'onglet "Aide" de RStudio la documentation sur les paquets R et les fonctions spécifiques. Cet onglet se trouve dans le volet qui contient également les fichiers, les graphiques et les paquets (généralement dans le volet inférieur à droit). Comme raccourci, vous pouvez également taper le nom d'un paquet ou d'une fonction dans la console R après un point d'interrogation pour ouvrir la page d'aide correspondante. N'incluez pas les parenthèses.

Par exemple : `?filter` ou `?diagrammeR`.

**Tutoriels interactifs**

Il existe plusieurs façons d'apprendre R de manière interactive *dans* RStudio.

RStudio lui-même offre un volet Tutoriel qui est alimenté par le paquet R [**learnr**](https://blog.rstudio.com/2020/02/25/rstudio-1-3-integrated-tutorials/). Il suffit d'installer ce paquet et d'ouvrir un tutoriel via le nouvel onglet "Tutorial" dans le volet supérieur droit de RStudio (qui contient également les onglets Environnement et Historique).

Le paquet R [**swirl**](https://swirlstats.com/) propose des cours interactifs dans la console R. Installez et chargez ce paquet, puis lancez la commande `swirl()` (parenthèses vides) dans la console R. Vous verrez apparaître des invites dans la console. Répondez en tapant dans la console. Elle vous guidera à travers un cours de votre choix.

### Fiches d'aide-mémoire {.unnumbered}

Il existe de nombreuses fiches d'aide-mémoire au format PDF disponibles sur le [site Web de RStudio](https://rstudio.com/resources/cheatsheets/), par exemple :

-   Facteurs avec le paquet **forcats**\
-   Dates et heures avec le paquet **lubridate**\
-   Chaînes de caractères avec le paquet **stringr**\
-   Opérations itératives avec le paquet **purrr**\
-   Importation de données\
-   Aide-mémoire pour la transformation des données avec le paquet **dplyr**\
-   R Markdown (pour créer des documents comme PDF, Word, Powerpoint...)\
-   Shiny (pour créer des applications Web interactives)\
-   Visualisation de données avec le paquet **ggplot2**\
-   Cartographie (SIG)\
-   Paquet **leaflet** (cartes interactives)\
-   Python avec R (paquet **reticulate**)

Il existe également une ressource R en ligne spécialement destinée aux [utilisateurs d'Excel](https://jules32.github.io/r-for-excel-users/).

### Twitter {.unnumbered}

R possède une communauté Twitter dynamique où vous pouvez apprendre des astuces, des raccourcis et des nouvelles - suivez ces comptes :

-   Suivez-nous ! [\@epiRhandbook](https://twitter.com/epirhandbook)\
-   R Function A Day [\@rfuntionaday](https://twitter.com/rfunctionaday) est une ressource *incroyable*\
-   R pour la science des données [\@rstats4ds](https://twitter.com/rstats4ds?lang=en)\
-   RStudio [\@RStudio](https://twitter.com/rstudio?lang=en)\
-   Conseils sur RStudio [\@rstudiotips](https://twitter.com/rstudiotips)\
-   R-Bloggers [\@Rbloggers](https://twitter.com/Rbloggers)\
-   R-ladies [\@RLadiesGlobal](https://twitter.com/RLadiesGlobal)\
-   Hadley Wickham [\@hadleywickham](https://twitter.com/hadleywickham?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)

Aussi :

**#epitwitter** et **#rstats**

### Ressources gratuites en ligne {.unnumbered}

Un texte définitif est le livre [R for Data Science](https://r4ds.had.co.nz/) de Garrett Grolemund et Hadley Wickham.

Le site Web du projet [R4Epis](https://r4epis.netlify.app/) vise à "développer des outils standardisés de nettoyage, d'analyse et de rapport des données pour couvrir les types courants d'épidémies et d'enquêtes auprès de la population qui seraient menées dans le cadre d'une réponse d'urgence de MSF". Vous y trouverez des supports de formation aux bases de R, des modèles de rapports RMarkdown sur les épidémies et les enquêtes, ainsi que des tutoriels pour vous aider à les configurer.

### Langues autres que l'anglais {.unnumbered}

[Materiales de RStudio en Español](https://www.rstudio.com/collections/espanol/)

[Introduction à R et au tidyverse (Francais)](https://juba.github.io/tidyverse/index.html)

<!-- ======================================================= -->

## Installation

### R et RStudio {.unnumbered}

**Comment installer R**

Visitez ce site Web <https://www.r-project.org/> et téléchargez la dernière version de R adaptée à votre ordinateur.

**Comment installer RStudio**

Visitez ce site Web <https://rstudio.com/products/rstudio/download/> et téléchargez la dernière version de bureau gratuite de RStudio adaptée à votre ordinateur.

**Autorisations requises**\

Notez que vous devez installer R et RStudio sur un lecteur sur lequel vous avez des droits de lecture et d'écriture. Sinon, votre capacité à installer des paquets R (ce qui arrive fréquemment) sera affectée. Si vous rencontrez des problèmes, essayez d'ouvrir RStudio en faisant un clic droit sur l'icône et en sélectionnant "Exécuter en tant qu'administrateur". Vous trouverez d'autres conseils sur la page [R sur les lecteurs réseau](#network_drives).

**Comment mettre à jour R et RStudio**

Votre version de R est imprimée dans la Console R au démarrage. Vous pouvez également exécuter `sessionInfo()`.

Pour mettre à jour R, allez sur le site web mentionné ci-dessus et réinstallez R. Alternativement, vous pouvez utiliser le paquet **installr** (sous Windows) en exécutant `installr::updateR()`. Cela ouvrira des boîtes de dialogue pour vous aider à télécharger la dernière version de R et à mettre à jour vos paquets vers la nouvelle version de R. Plus de détails peuvent être trouvés dans la [documentation de **installr**](https://www.r-project.org/nosvn/pandoc/installr.html).

Sachez que l'ancienne version de R existera toujours sur votre ordinateur. Vous pouvez temporairement exécuter une ancienne version (ancienne "installation") de R en cliquant sur "Outils" -\> "Options globales" dans RStudio et en choisissant une version de R. Cela peut être utile si vous voulez utiliser un paquet qui n'a pas été mis à jour pour fonctionner sur la version la plus récente de R.

Pour mettre à jour RStudio, vous pouvez aller sur le site Web ci-dessus et retélécharger RStudio. Une autre option consiste à cliquer sur "Aide" -\> "Vérifier les mises à jour" dans RStudio, mais cela peut ne pas montrer les toutes dernières mises à jour.

Pour savoir quelles versions de R, RStudio ou des paquets ont été utilisées lors de la réalisation de ce manuel, consultez la page sur [Notes techniques et choix éditoriaux](#editorial_notes).

### Autres logiciels que vous *pourriez* avoir besoin d'installer {.unnumbered}

-   TinyTeX (*pour la compilation d'un document RMarkdown au format PDF*)\
-   Pandoc (*pour compiler des documents RMarkdown*)\
-   RTools (*pour construire des paquets pour R*)\
-   phantomjs (*pour enregistrer des images fixes de réseaux animés, tels que des chaînes de transmission*)

#### TinyTex {.unnumbered}

TinyTex est une distribution LaTeX personnalisée, utile lorsqu'on essaie de produire des PDF à partir de R.\
Voir <https://yihui.org/tinytex/> pour plus d'informations.

Pour installer TinyTex à partir de R :

```{r, eval=F}

install.packages('tinytex')
tinytex::install_tinytex()

# pour désinstaller TinyTeX, lancez tinytex::uninstall_tinytex()
```

#### Pandoc {.unnumbered}

Pandoc est un convertisseur de document, un logiciel séparé de R. **Il est fourni avec RStudio et ne devrait pas avoir besoin d'être téléchargé.** Il aide le processus de conversion de documents Rmarkdown à des formats comme .pdf et ajoute des fonctionnalités complexes.

#### RTools {.unnumbered}

RTools est une collection de logiciels permettant de construire des paquets pour R.

Installer à partir de ce site web : <https://cran.r-project.org/bin/windows/Rtools/>

#### phantomjs {.unnumbered}

Cet outil est souvent utilisé pour faire des "captures d'écran" des pages web. Par exemple, lorsque vous faites une chaîne de transmission avec le paquet **epicontacts**, un fichier HTML interactif et dynamique est produit. Si vous voulez une image statique, il peut être utile d'utiliser le paquet [**webshot**](https://wch.github.io/webshot/articles/intro.html) pour automatiser ce processus. Cela nécessite le programme externe "phantomjs". Vous pouvez installer phantomjs via le paquet **webshot** avec la commande `webshot::install_phantomjs()`.

<!-- ======================================================= -->

### RStudio {#rstudio}

### Orientation de RStudio {.unnumbered}

**D'abord, ouvrez RStudio.** Comme leurs icônes peuvent être très similaires, assurez-vous que vous ouvrez bien *RStudio* et non pas R.

Pour que RStudio fonctionne, vous devez également avoir R installé sur l'ordinateur (voir ci-dessus pour les instructions d'installation).

**RStudio** est une interface (GUI) pour une utilisation plus facile de **R**. Vous pouvez considérer R comme le moteur d'un véhicule, qui effectue le travail crucial, et RStudio comme le corps du véhicule (avec les sièges, les accessoires, etc.) qui vous aide à utiliser le moteur pour avancer ! Vous pouvez consulter la fiche technique complète de l'interface utilisateur de RStudio (PDF) [ici](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf)

Par défaut, RStudio affiche quatre volets rectangulaires.

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "RStudio_overview.png"))
```

[***TIP:*** Si votre RStudio n'affiche qu'un seul volet gauche, c'est parce que vous n'avez pas encore de scripts ouverts.]{style="color: black;"}

**Le volet source**

Ce volet, par défaut en haut à gauche, est un espace pour éditer, exécuter et enregistrer vos [scripts](#scripts). Les scripts contiennent les commandes que vous souhaitez exécuter. Ce volet peut également afficher des ensembles de données (cadres de données) pour les visualiser.

Pour les utilisateurs de Stata, ce volet est similaire aux fenêtres Do-file et Data Editor.

**Le volet Console R**

La console R, qui est par défaut le volet gauche ou inférieur gauche de R Studio, est le siège du "moteur" R. C'est là que les commandes sont réellement exécutées et que les sorties non graphiques et les messages d'erreur/d'avertissement apparaissent. Vous pouvez saisir et exécuter directement des commandes dans la console R, mais sachez que ces commandes ne sont pas enregistrées comme c'est le cas lorsque vous exécutez des commandes à partir d'un script.

Si vous êtes familier avec Stata, la console R ressemble à la fenêtre de commande et à la fenêtre des résultats.

**Le volet Environnement**

Ce volet, situé par défaut en haut à droite, est le plus souvent utilisé pour afficher de brefs résumés des [objets](#objets) de l'environnement R dans la session en cours. Ces objets peuvent inclure des ensembles de données importés, modifiés ou créés, des paramètres que vous avez définis (par exemple, une semaine épi spécifique pour l'analyse), ou des vecteurs ou des listes que vous avez définis pendant l'analyse (par exemple, les noms des régions). Vous pouvez cliquer sur la flèche à côté du nom d'un cadre de données pour voir ses variables.

Dans Stata, cette fenêtre est très similaire à celle du gestionnaire de variables.

Ce volet contient également l'onglet "Historique" où vous pouvez voir les commandes que vous avez exécutées précédemment. Il comporte également un onglet "Tutoriel" où vous pouvez suivre des tutoriels R interactifs si vous avez installé le paquet **learnr**. En outre, il existe un volet "Connexions" pour les connexions aux bases de données externes. Si vous avez lié le répertoire actif à un dépôt sur Github, il y aura également un volet "Git".

**Volets Graphiques, visionneuse, paquets et aide**

Le volet inférieur droit comprend plusieurs onglets importants. Les graphiques de tracé typiques, y compris les cartes, s'affichent dans le volet Tracé. Les sorties interactives ou HTML s'affichent dans le volet Visionneuse. Le volet Aide permet d'afficher la documentation et les fichiers d'aide. Le volet Fichiers est un navigateur qui peut être utilisé pour ouvrir ou supprimer des fichiers. Le volet Paquets vous permet de voir, d'installer, de mettre à jour, de supprimer, de charger/décharger des paquets R et de voir quelle version du paquet vous avez. Pour en savoir plus sur les paquets, consultez la [section paquets](#packages) ci-dessous.

Ce volet contient les équivalents Stata des fenêtres Plots Manager et Project Manager.

### Paramètres RStudio {.unnumbered}

Modifiez les paramètres et l'apparence de RStudio dans le menu déroulant *Outiles*, en sélectionnant *Options globales*. Vous pouvez y modifier les paramètres par défaut, y compris l'apparence/couleur de fond.

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "RStudio_tools_options_1.png"))

knitr::include_graphics(here::here("images", "RStudio_tools_options.png"))
```

**Redémarrage**

Si votre R se fige, vous pouvez redémarrer R en allant dans le menu Session et en cliquant sur "Redémarrer R". Cela vous évite de devoir fermer et ouvrir RStudio. Tout ce qui se trouve dans votre environnement R sera supprimé lorsque vous ferez cela.

### Raccourcis clavier {.unnumbered}

Vous trouverez ci-dessous quelques raccourcis clavier très utiles. Vous trouverez tous les raccourcis clavier pour Windows, Max et Linux sur la deuxième page de ce [fichier technique](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf) par RStudio.

+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Windows/Linux                           | Mac                    | Action                                                                                                                                                              |
+=========================================+========================+=====================================================================================================================================================================+
| Esc                                     | Esc                    | Interrompre la commande en cours (utile si vous avez accidentellement lancé une commande incomplète et que vous ne pouvez pas éviter de voir "+" dans la console R) |
+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl+s                                  | Cmd+s                  | Sauvegarder (script)                                                                                                                                                |
+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Tab                                     | Tab                    | Autocomplétion                                                                                                                                                      |
+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Enter                            | Cmd + Enter            | Exécuter la ou les ligne(s) courante(s)/sélection(s) de code                                                                                                        |
+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Shift + C                        | Cmd + Shift + c        | commenter/dé-commenter les lignes souslignées                                                                                                                       |
+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Alt + -                                 | Option + -             | Insérer `<-`                                                                                                                                                        |
+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Shift + m                        | Cmd + Shift + m        | Insérer `%>%`                                                                                                                                                       |
+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + l                                | Cmd + l                | Effacer le contenu de la console R                                                                                                                                  |
+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Alt + b                          | Cmd + Option + b       | Exécuter du début à la ligne courante                                                                                                                               |
+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Alt + t                          | Cmd + Option + t       | Exécuter la section de code actuelle (R Markdown)                                                                                                                   |
+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Alt + i                          | Cmd + Shift + r        | Insérer un morceau de code (en R Markdown)                                                                                                                          |
+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Alt + c                          | Cmd + Option + c       | Exécuter le morceau de code actuel (en R Markdown)                                                                                                                  |
+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Flèches haut/bas dans la console R      |     Idem               |     Basculer entre les commandes récemment exécutées                                                                                                                |
+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Shift + flèches haut/bas dans le script | Idem                   |     Sélectionner plusieurs lignes de code                                                                                                                           |
+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + f                                | Cmd + f                | Rechercher et remplacer dans le script actuel                                                                                                                       |
+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Shift + f                        | Cmd + Shift + f        | Rechercher dans les dossiers (rechercher/remplacer dans plusieurs scripts)                                                                                          |
+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Alt + l                                 | Cmd + Option + l       | Plier le code sélectionné                                                                                                                                           |
+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Shift + Alt + l                         | Cmd + Shift + Option+l | Déplier le code sélectionné                                                                                                                                         |
+-----------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[***TIP:*** Utilisez votre touche de tabulation lorsque vous tapez pour activer la fonctionnalité de complétion automatique de RStudio. Cela peut éviter les fautes d'orthographe. Appuyez sur la touche Tab pendant la saisie pour produire un menu déroulant de fonctions et d'objets probables, en fonction de ce que vous avez tapé jusqu'à présent.]{style="color: darkgreen;"}

<!-- ======================================================= -->

## Fonctions {#functions}

Les fonctions sont au cœur de l'utilisation de R. Les fonctions vous permettent d'effectuer des tâches et des opérations. De nombreuses fonctions sont installées avec R, beaucoup d'autres sont disponibles à télécharger dans des *paquets* (expliqués dans la section [paquets](#packages)), et vous pouvez même écrire vos propres fonctions personnalisées !

Cette section de base sur les fonctions explique :

-   Ce qu'est une fonction et comment elle fonctionne\
-   Ce que sont les *paramètres* des fonctions\
-   Comment obtenir de l'aide pour comprendre une fonction

*Une note rapide sur la syntaxe :* Dans ce manuel, les fonctions sont écrites en code-texte avec des parenthèses vides, comme ceci : `filter()`. Comme expliqué dans la section [paquets](#packages), les fonctions sont téléchargées dans des *paquets*. Dans ce manuel, les noms de paquets sont écrits en **gras**, comme **dplyr**. Parfois, dans le code d'exemple, vous pouvez voir le nom de la fonction lié explicitement au nom de son paquet avec deux points de suspension (`::`) comme ceci : `dplyr::filter()`. Le but de ce lien est expliqué dans la section sur les paquets.

<!-- ======================================================= -->

### Fonctions simples {.unnumbered}

**Une fonction est comme une machine qui reçoit des entrées, effectue une action avec ces entrées, et produit une sortie.** La nature de la sortie dépend de la fonction.

**Les fonctions opèrent généralement sur un objet placé entre les parenthèses de la fonction**. Par exemple, la fonction `sqrt()` calcule la racine carrée d'un nombre :

```{r basics_function_sqrt}
sqrt(49)
```

L'objet fourni à une fonction peut également être une colonne dans un jeu de données (voir la section [Objets](#objects) pour plus de détails sur tous les types d'objets). Comme R peut stocker plusieurs jeux de données, vous devrez spécifier à la fois le jeu de données et la colonne. Une façon de le faire est d'utiliser la notation `$` pour lier le nom du jeu de données et le nom de la colonne (`dataset$column`). Dans l'exemple ci-dessous, la fonction `summary()` est appliquée à la colonne numérique `age` du jeu de données `linelist`, et la sortie est un résumé des valeurs numériques et manquantes de la colonne.

```{r basics_functions_summary}
# Imprimez les statistiques sommaires de la colonne 'age' dans le jeu de données 'linelist'.
summary(linelist$age)
```

[***NOTE:*** En coulisses, une fonction représente un code supplémentaire complexe qui a été regroupé pour l'utilisateur dans une seule commande simple.]{style="color: black;"}

<!-- ======================================================= -->

### Fonctions à paramètres multiples {.unnumbered}

Les fonctions demandent souvent plusieurs entrées, appelées ***paramètres***, situées entre les parenthèses de la fonction, généralement séparées par des virgules.

-   Certains paramètres sont obligatoires pour que la fonction fonctionne correctement, d'autres sont facultatifs\
-   Les paramètres facultatifs ont des valeurs par défaut\
-   Les paramètres peuvent prendre des entrées de type caractère, numérique, logique (VRAI/FAUX) et autres.

Voici une fonction fictive amusante, appelée `oven_bake()` (cuisson au four), comme exemple d'une fonction typique. Elle prend un objet comme entrée (par exemple un jeu de données, ou dans cet exemple "pâte") et effectue des opérations sur celui-ci comme spécifié par des paramètres supplémentaires (`minutes =` et `température =`). La sortie peut être imprimée sur la console, ou sauvegardée comme un objet en utilisant l'opérateur d'affectation `<-`.

```{r basics_functions_image, echo=F, out.width = "75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Function_Bread_Example.png"))
```

**Dans un exemple plus réaliste**, la commande `age_pyramid()` ci-dessous produit un graphique de pyramide des âges basé sur des groupes d'âge définis et une colonne de division binaire, comme le genre `gender`. La fonction reçoit trois paramètres entre parenthèses, séparés par des virgules. Les valeurs fournies aux paramètres établissent `linelist` comme le cadre de données à utiliser, `age_cat5` comme la colonne à compter, et `gender` comme la colonne binaire à utiliser pour diviser la pyramide par couleur.

```{r basics_functions_arguments, include=FALSE, results='hide', message=FALSE, warning=FALSE,}
## Créer une variable de groupe d'âge en spécifiant des ruptures catégorielles
linelist$age_group <- cut(linelist$age, breaks = c(0, 5, 10, 15, 20, 30, 45, 60))
```

```{r message=FALSE, warning=FALSE,  out.width = "75%", out.height="75%"}
# Créer une pyramide des âges
age_pyramid(data = linelist, age_group = "age_cat5", split_by = "gender")
```

La commande ci-dessus peut être écrite de manière équivalente comme ci-dessous, dans un style plus long avec une nouvelle ligne pour chaque argument. Ce style peut être plus facile à lire, et plus facile d'écrire des "commentaires" avec `#` pour expliquer chaque partie (commenter abondamment est une bonne pratique !). Pour exécuter cette commande plus longue, vous pouvez souligner la commande entière et cliquer sur "Run", ou simplement placer votre curseur sur la première ligne et appuyer simultanément sur les touches `Ctrl` et `Enter`.

```{r message=FALSE, warning=FALSE,  out.width = "75%", out.height="75%"}
# Créer une pyramide des âges
age_pyramid(
  data = linelist,        # utiliser la liste linéaire des cas
  age_group = "age_cat5", # fournir une colonne de groupe d'âge
  split_by = "gender"     # utiliser la colonne genre pour les deux côtés de la pyramide
  )
```

La première moitié d'une affectation de paramètre (par exemple `data =`) n'a pas besoin d'être spécifiée si les paramètres sont écrits dans un ordre spécifique (spécifié dans la documentation de la fonction). Le code ci-dessous produit exactement la même pyramide que ci-dessus, parce que la fonction attend l'ordre des paramètres : cadre de données, le variable `age_group`, puis le variable `split_by`.

```{r, basics_functions_pyramid2, eval = FALSE, warning=FALSE, message=FALSE, , out.width = "75%", out.height="75%", eval=F}
# Cette commande produira exactement le même graphique que ci-dessus
age_pyramid(linelist, "age_cat5", "gender")
```

**Une commande `age_pyramid()` plus complexe pourrait inclure les paramètres *optionnels* pour :**

-   Afficher les proportions au lieu des nombres (définissez `proportional = TRUE` (vrai) quand la valeur par défaut est `FALSE` (faux))\`\
-   Spécifier les deux couleurs à utiliser (`pal =` est l'abréviation de "palette" et est fourni avec un vecteur de deux noms de couleurs. Voir la page [objets](#objectstructure) pour savoir comment la fonction `c()` fabrique un vecteur).

[***NOTE:*** Pour les paramètres que vous spécifiez avec les deux parties du paramètre (par exemple `proportional = TRUE`), leur ordre parmi tous les paramètres n'a pas d'importance.]{style="color: black;"}

```{r message=FALSE, warning=FALSE, out.width = "75%", out.height="75%"}
age_pyramid(
  linelist,                    # utiliser la liste linéaire des cas
  "age_cat5",                  # colonne de groupe d'âge
  "gender",                    # répartition par genre
  proportional = TRUE,         # pourcentage au lieu du nombre
  pal = c("orange", "purple")  # couleurs
  )
```

<!-- ======================================================= -->

### Ecrire des fonctions {.unnumbered}

R est un langage orienté autour des fonctions, vous devez donc vous sentir capable d'écrire vos propres fonctions. La création de fonctions présente plusieurs avantages :

-   Faciliter la programmation modulaire - la séparation du code en morceaux indépendants et gérables\
-   Remplacer le copier-coller répétitif, qui peut être source d'erreurs\
-   Donner des noms mémorisables aux morceaux de code

L'écriture d'une fonction est traitée en détail à la page [Écriture de fonctions](#writing_functions).

<!-- ======================================================= -->

## Paquets {#packages}

**Les paquets contiennent des fonctions.**

Un paquet en R est un ensemble partageable de code et de documentation qui contient des fonctions prédéfinies. Les utilisateurs de la communauté R développent en permanence des packages répondant à des problèmes spécifiques; donc il est probable que l'un d'entre eux puisse vous aider dans votre travail ! Vous allez installer et utiliser des centaines de paquets dans votre utilisation de R.

À l'installation, R contient des paquets et des fonctions **"de base "** qui effectuent des tâches élémentaires communes. Mais de nombreux utilisateurs de R créent des fonctions spécialisées, qui sont vérifiées par la communauté R et que vous pouvez télécharger en tant que **paquet** pour votre propre usage. Dans ce manuel, les noms des paquets sont écrits en **gras**. L'un des aspects les plus difficiles de R est qu'il existe souvent de nombreuses fonctions ou paquets parmi lesquels on peut choisir pour effectuer une tâche donnée.

### Installer et charger {.unnumbered}

Les *fonctions* sont contenues dans des **paquets** qui peuvent être téléchargés ("installés") sur votre ordinateur à partir d'Internet. Une fois qu'un paquet est téléchargé, il est stocké dans votre "bibliothèque". Vous pouvez alors accéder aux fonctions qu'il contient pendant votre séance R actuelle en "chargeant" le paquet.

*Pensez à R comme votre bibliothèque personnelle* : Lorsque vous téléchargez un paquet, votre bibliothèque gagne un nouveau livre de fonctions, mais chaque fois que vous voulez utiliser une fonction de ce livre, vous devez emprunter ("charger") ce livre dans votre bibliothèque.

En résumé : pour utiliser les fonctions disponibles dans un paquet R, deux étapes doivent être mises en œuvre :

1)  Le paquet doit être **installé** (une fois), *et*\
2)  Le paquet doit être **chargé** (à chaque séance R)

#### Votre bibliothèque {.unnumbered}

Votre "bibliothèque" est en fait un dossier sur votre ordinateur, contenant un dossier pour chaque paquet qui a été installé. Déterminez où R est installé sur votre ordinateur, et cherchez un dossier appelé "win-library". Par exemple : `R\win-library\4.0` (4.0 est la version de R). Notez que vous aurez une bibliothèque différente pour chaque version de R que vous avez téléchargée.

Vous pouvez imprimer le chemin d'accès à votre bibliothèque en entrant `.libPaths()` (parenthèses vides). Ceci devient particulièrement important si vous travaillez avec [R sur des lecteurs réseau](#network_drives).

#### Installer à partir du CRAN {.unnumbered}

Le plus souvent, les utilisateurs de R téléchargent des paquets depuis CRAN. CRAN (Comprehensive R Archive Network) est un entrepôt public en ligne de paquets R qui ont été publiés par des membres de la communauté R.

Vous vous inquiétez des virus et de la sécurité lorsque vous téléchargez un paquet depuis CRAN ? Lisez [cet article](https://support.rstudio.com/hc/en-us/articles/360042593974-R-and-R-Package-Security) à ce sujet.

#### Comment installer et charger {.unnumbered}

Dans ce manuel, nous suggérons d'utiliser le paquet **pacman** (abréviation de "package manager" en anglais). Il offre une fonction pratique `p_load()` qui installera un paquet si nécessaire *et* le chargera pour l'utiliser dans la séance R actuelle.

La syntaxe est assez simple. Il suffit de lister les noms des paquets entre les parenthèses de `p_load()`, séparés par des virgules.

La commande ci-dessous installera les paquets **rio**, **tidyverse**, et **here** s'ils ne sont pas encore installés, et les chargera pour les utiliser. Cela rend l'approche `p_load()` pratique et concise si vous partagez des scripts avec d'autres personnes. Notez que les noms des paquets sont sensibles à la casse.

```{r}
# Installer (si nécessaire) et charger les paquets pour l'utilisation
pacman::p_load(rio, tidyverse, here)
```

Notez que nous avons utilisé la syntaxe `pacman::p_load()` qui écrit explicitement le nom du paquet (**pacman**) avant le nom de la fonction (`p_load()`), reliés par deux deux points `::`. Cette syntaxe est utile car elle charge également le paquet **pacman** (en supposant qu'il soit déjà installé).

Il existe d'autres fonctions R **de base** que vous verrez souvent. La fonction R **de base** pour installer un paquet est `install.packages()`. Le nom du paquet à installer doit être fourni entre les parenthèses et *entre guillemets*. Si vous voulez installer plusieurs paquets en une seule commande, ils doivent être listés dans un vecteur de caractères `c()`.

Remarque : cette commande *installe* un paquet, mais ne le charge *pas* pour l'utiliser dans la séance en cours.

```{r, eval=F}
# Installer un seul paquet avec la base R
install.packages("tidyverse")

# Installer plusieurs paquets avec la base R
install.packages(c("tidyverse", "rio", "here"))
```

L'installation peut également être effectuée par pointer-cliquer en allant dans le panneau "Packages" de RStudio, en cliquant sur "Installer" et en recherchant le nom du paquet souhaité.

La fonction **base** de R pour **charger** un paquet à utiliser (après qu'il ait été installé) est `library()`. Elle ne peut charger qu'un seul paquet à la fois (une autre raison d'utiliser `p_load()`). Vous pouvez fournir le nom du paquet avec ou sans guillemets.

```{r, eval=F}
# Charger des paquets à utiliser, avec la base R
library(tidyverse)
library(rio)
library(here)
```

Pour vérifier si un paquet est installé et/ou chargé, vous pouvez afficher le panneau des paquets dans RStudio. Si le paquet est installé, il est affiché avec son numéro de version. Si sa case est cochée, il est chargé pour la séance en cours.

**Installation depuis Github**

Parfois, vous avez besoin d'installer un paquet qui n'est pas encore disponible sur CRAN. Ou peut-être que le paquet est disponible sur CRAN mais que vous voulez la *version de développement* avec de nouvelles fonctionnalités qui ne sont pas encore proposées dans la version CRAN publiée, plus stable. Ces versions sont souvent hébergées sur le site Web [github.com](https://github.com/) dans un "dépôt" de code libre et public. Pour en savoir plus sur Github, consultez la page du manuel intitulée [Version control et collaboration avec GitHub](#collaboration).

Pour télécharger des paquets R depuis Github, vous pouvez utiliser la fonction `p_load_gh()` de **pacman**, qui installera le paquet si nécessaire, et le chargera pour l'utiliser dans votre séance R actuelle. Les alternatives à l'installation incluent l'utilisation des paquets **remotes** ou **devtools**. Pour en savoir plus sur toutes les fonctions de **pacman**, consultez la [documentation du paquet](https://cran.r-project.org/web/packages/pacman/pacman.pdf).

Pour installer à partir de Github, vous devez fournir plus d'informations. Vous devez fournir :

1)  L'ID Github (nom d'utilisateur) du propriétaire du dépôt.
2)  Le nom du dépôt qui contient le paquet.
3)  *(facultatif) Le nom de la "branche" (version de développement spécifique) que vous souhaitez télécharger*.

Dans les exemples ci-dessous, le premier mot entre guillemets est l'ID Github du propriétaire du dépôt. Après la barre oblique est le nom du dépôt (typiquement le nom du paquet).

```{r, eval=F}
# Installer/charger le paquet epicontacts depuis son dépôt Github
p_load_gh("reconhub/epicontacts")
```

Si vous voulez installer à partir d'une "branche" (version) autre que la branche principale, ajoutez le nom de la branche après un "\@", après le nom du dépôt.

```{r, eval=F}
# Installer la branche "timeline" du paquet epicontacts depuis Github
p_load_gh("reconhub/epicontacts@timeline")
```

S'il n'y a pas de différence entre la version Github et la version sur votre ordinateur, aucune action ne sera entreprise. Vous pouvez "forcer" une réinstallation en utilisant `p_load_current_gh()` avec le paramètre `update = TRUE`. Lisez plus sur **pacman** dans cette [vignette en ligne](http://trinker.github.io/pacman/vignettes/Introduction_to_pacman.html)

**Installation à partir d'un ZIP ou d'un TAR**

Vous pouvez installer le paquet à partir d'une URL :

```{r, eval=F}
packageurl <- "https://cran.r-project.org/src/contrib/Archive/dsr/dsr_0.2.2.tar.gz"
install.packages(packageurl, repos = NULL, type = "source")
```

Ou bien, le télécharger sur votre ordinateur dans un fichier zippé :

Option 1 : utiliser `install_local()` du paquet **remotes**.

```{r, eval=F}
remotes::install_local("~/Downloads/dplyr-master.zip")
```

Option 2 : en utilisant `install.packages()` du R de **base**, en fournissant le chemin d'accès au fichier ZIP et en définissant `type = "source"` et `repos = NULL`.

```{r, eval=F}
install.packages("~/Downloads/dplyr-master.zip", 
                 type = "source", 
                 repos = NULL)
```

### Syntaxe du code {.unnumbered}

Pour plus de clarté dans ce manuel, les fonctions sont parfois précédées du nom de leur paquet en utilisant le symbole `::` de la manière suivante : `nom_du_paquet::nom_de_la_fonction()`.

Une fois qu'un paquet est chargé pour une séance, ce style explicite n'est plus nécessaire. On peut simplement utiliser `nom_de_la_fonction()`. Cependant, écrire le nom du paquet est utile lorsqu'un nom de fonction est commun et peut exister dans plusieurs paquets (par exemple, `plot()`). L'écriture du nom du paquet chargera également le paquet s'il n'est pas déjà chargé.

```{r eval=FALSE}
# Cette commande utilise le paquet "rio" et sa fonction "import()" pour importer un jeu de données
linelist <- rio::import("linelist.xlsx", which = "Sheet1")
```

### Aide sur les fonctions {.unnumbered}

Pour en savoir plus sur une fonction, vous pouvez la rechercher dans l'onglet Aide du RStudio en bas à droite. Vous pouvez également lancer une commande comme `?thefunctionname` (mettez le nom de la fonction après un point d'interrogation) et la page d'aide apparaîtra dans le volet d'aide. Enfin, essayez de rechercher des ressources en ligne.

### Mettre à jour les paquets {.unnumbered}

Vous pouvez mettre à jour les paquets en les réinstallant. Vous pouvez également cliquer sur le bouton vert "Update" dans votre panneau "RStudio Packages" pour voir quels paquets ont de nouvelles versions à installer. Sachez que votre ancien code peut avoir besoin d'être mis à jour s'il y a une révision majeure du fonctionnement d'une fonction !

### Supprimer des paquets {.unnumbered}

Utilisez `p_delete()` de **pacman**, ou `remove.packages()` de **base** R. Alternativement, allez chercher le dossier qui contient votre bibliothèque et supprimez manuellement le dossier.

### Dépendances {.unnumbered}

Les paquets dépendent souvent d'autres paquets pour fonctionner. Ceux-ci sont appelés dépendances. Si une dépendance ne s'installe pas, le paquet qui en dépend peut également ne pas s'installer.

Voir les dépendances d'un paquet avec `p_depends()`, et voir quels paquets en dépendent avec `p_depends_reverse()`.

### Fonctions masquées {.unnumbered}

Il n'est pas rare que deux paquets ou plus contiennent le même nom de fonction. Par exemple, le paquet **dplyr** possède une fonction `filter()`, mais le paquet **stats** aussi. La fonction `filter()` par défaut dépend de l'ordre dans lequel ces paquets sont chargés pour la première fois dans la séance R - le dernier sera la fonction par défaut de la commande `filter()`.

Vous pouvez vérifier l'ordre dans votre panneau Environnement de R Studio - cliquez sur la liste déroulante pour "Global Environment" et voyez l'ordre des paquets. Les fonctions des paquets *inférieurs* dans cette liste déroulante masqueront les fonctions du même nom dans les paquets qui apparaissent plus haut dans la liste déroulante. Lors du premier chargement d'un paquet, R vous avertira dans la console si le masquage se produit, mais il est facile de ne pas le voir.

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "masking_functions.png"))
```

Voici comment vous pouvez corriger le masquage :

1)  Spécifiez le nom du paquet dans la commande. Par exemple, utilisez `dplyr::filter()`\
2)  Réorganisez l'ordre dans lequel les paquets sont chargés (par exemple, dans `p_load()`), et **démarrez une nouvelle séance R**.

### Détacher / décharger {.unnumbered}

Pour détacher (décharger) un paquet, utilisez cette commande, avec le nom correct du paquet et un seul deux-points. Notez que cela peut ne pas résoudre le masquage.

```{r, eval=F}
detach(package:NOM_DU_PAQUET_ICI, unload = TRUE)
```

### Installer une ancienne version {.unnumbered}

Consultez ce [guide](https://support.rstudio.com/hc/en-us/articles/219949047-Installing-older-versions-of-packages) pour installer une ancienne version d'un paquet particulier.

### Paquets suggérés {.unnumbered}

Voir la page [Paquets suggérés](#suggested_packages) pour une liste de paquets que nous recommandons pour l'épidémiologie quotidienne.

<!-- ======================================================= -->

## Scripts {#scripts}

Les scripts sont une partie fondamentale de la programmation. Ce sont des documents qui contiennent vos commandes (par exemple, des fonctions pour créer et modifier des jeux de données, imprimer des visualisations, etc). Vous pouvez sauvegarder un script et l'exécuter à nouveau ultérieurement. Le stockage et l'exécution de vos commandes à partir d'un script présentent de nombreux avantages (par rapport à la saisie des commandes une par une dans la "ligne de commande" de la console R) :

-   Portabilité : vous pouvez partager votre travail avec d'autres personnes en leur envoyant vos scripts\
-   Reproductibilité : pour que vous et les autres sachiez exactement ce que vous avez fait\
-   Contrôle de version : pour que vous puissiez suivre les modifications apportées par vous-même ou par vos collègues\
-   Commentaire/annotation : pour expliquer à vos collègues ce que vous avez fait

### Commentaire {.unnumbered}

Dans un script, vous pouvez également annoter ("commenter") votre code R. Les commentaires sont utiles pour expliquer à vous-même et aux autres lecteurs ce que vous faites. Vous pouvez ajouter un commentaire en tapant le symbole dièse (\#) et en écrivant votre commentaire après. Le texte commenté apparaîtra dans une couleur différente de celle du code R.

Tout code écrit après le \# ne sera pas exécuté. Par conséquent, placer un \# avant le code est également un moyen utile de bloquer temporairement une ligne de code ("commenter") si vous ne souhaitez pas la supprimer). Vous pouvez mettre en commentaire plusieurs lignes à la fois en les soulignant et en appuyant sur Ctrl+Shift+c (Cmd+Shift+c sur Mac).

```{r, eval = F}

# Un commentaire peut être sur une ligne par lui-même, ex.:
# Importer des données:
linelist <- import("linelist_raw.xlsx") %>% # un commentaire peut aussi venir après le code
     # filter(age > 50)
     # Il peut aussi être utilisé pour désactiver une ligne de code
count()

```

Vous trouverez ci-dessous quelques conseils essentiels pour commenter et annoter votre code :

-   Commentez *ce que vous faites* et *pourquoi* vous le faites\
-   Découpez votre code en sections logiques\
-   Accompagnez votre code d'une description textuelle étape par étape de ce que vous faites (par exemple, des étapes numérotées).

### Style {.unnumbered}

Il est important d'être conscient de votre style de codage, surtout si vous travaillez en équipe. Nous préconisons le **tidyverse** [guide de style](https://style.tidyverse.org/). Il existe également des paquets tels que **styler** et **lintr** qui vous aident à vous conformer à ce style.

Quelques points très basiques pour rendre votre code lisible pour les autres:\
\* Lorsque vous nommez des objets, n'utilisez que des lettres minuscules, des chiffres et des traits de soulignement `_`, par exemple `mes_donnees`\
\* Utilisez fréquemment des espaces, y compris autour des opérateurs, par exemple `n = 1` et `age_nouveau <- age_vieillesse + 3`.

### Exemple de script {.unnumbered}

Vous trouverez ci-dessous un exemple d'un court script R. N'oubliez pas que plus vous expliquerez succinctement votre code dans les commentaires, plus vos collègues vous apprécieront !

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "example_script.png"))
```

<!-- ======================================================= -->

### R markdown {.unnumbered}

Un script R markdown est un type de script R dans lequel le script lui-même *devient* un document de sortie (PDF, Word, HTML, Powerpoint, etc.). Ce sont des outils incroyablement utiles et polyvalents, souvent utilisés pour créer des rapports dynamiques et automatisés. Même ce site Web et ce manuel sont produits à l'aide de scripts R markdown !

Il convient de noter que les utilisateurs débutants de R peuvent également utiliser R Markdown - ne vous laissez pas intimider !Pour en savoir plus, consultez la page du manuel consacrée aux [rapports avec R Markdown](#rmarkdown).

<!-- ======================================================= -->

### Carnets de notes R {.unnumbered}

Il n'y a pas de différence entre écrire dans un Rmarkdown et un R notebook. Cependant, l'exécution du document diffère légèrement. Voir ce [site](http://uc-r.github.io/r_notebook) pour plus de détails.

<!-- ======================================================= -->

### Shiny {.unnumbered}

Les applications/sites web Shiny sont contenus dans un script, qui doit être nommé `app.R`. Ce fichier comporte trois éléments :

1)  Une interface utilisateur (ui)\
2)  Une fonction serveur\
3)  Un appel à la fonction `shinyApp`

Consultez la page du manuel sur [les teableaux de bord avec Shiny](#shiny), ou ce tutoriel en ligne : [Tutoriel Shiny](https://shiny.rstudio.com/tutorial/written-tutorial/lesson1/)

*Auparavant, le fichier ci-dessus était divisé en deux fichiers (`ui.R` et `server.R`)*.

### Repli du code {.unnumbered}

Vous pouvez replier des portions de code pour rendre votre script plus facile à lire.

Pour ce faire, créez un en-tête de texte avec #, écrivez votre en-tête, et faites-le suivre d'au moins 4 tirets (-), hachages (\#) ou égaux (=). Lorsque vous aurez fait cela, une petite flèche apparaîtra dans la "gouttière" à gauche (près du numéro de ligne). Vous pouvez cliquer sur cette flèche et sur le code situé en dessous jusqu'à ce que l'en-tête suivant se réduise et qu'une icône à double flèche apparaisse à sa place.

Pour développer le code, cliquez à nouveau sur la flèche dans la gouttière ou sur l'icône à double flèche. Il existe également des raccourcis clavier, comme expliqué dans la section [RStudio](#rstudio) de cette page.

En créant des en-têtes avec #, vous activerez également la table des matières au bas de votre script (voir ci-dessous) que vous pouvez utiliser pour naviguer dans votre script. Vous pouvez créer des sous-titres en ajoutant d'autres symboles, par exemple \# pour les titres primaires, \## pour les titres secondaires et \### pour les titres tertiaires.

Vous trouverez ci-dessous deux versions d'un exemple de script. À gauche, l'original avec des en-têtes commentés. À droite, quatre tirets ont été écrits après chaque en-tête, les rendant ainsi repliables. Deux d'entre eux ont été réduits, et vous pouvez voir que la table des matières en bas de page affiche maintenant chaque section.

```{r, out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "code_folding1.png"))
knitr::include_graphics(here::here("images", "code_folding2.png"))
```

D'autres zones de code qui sont automatiquement éligibles pour le pliage incluent les régions "accolées" avec des parenthèses `{ }` telles que les définitions de fonctions ou les blocs conditionnels (instructions "if else"). Vous pouvez en savoir plus sur le pliage du code sur le [site RStudio](https://support.rstudio.com/hc/en-us/articles/200484568-Code-Folding-and-Sections).

<!-- ======================================================= -->

<!-- ======================================================= -->

<!-- ======================================================= -->

## Répertoire de travail

Le répertoire de travail est l'emplacement du dossier racine utilisé par R pour votre travail - où R recherche et enregistre les fichiers par défaut. Par défaut, il enregistrera de nouveaux fichiers et sorties à cet emplacement et recherchera ici des fichiers (par exemple, des ensembles de données).

Le répertoire de travail apparaît dans le texte gris en haut du volet de la console RStudio. Vous pouvez également imprimer le répertoire de travail actuel en exécutant `getwd()` (laissez les parenthèses vides).

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "working_directory_1.png"))
```

### Approche recommandée {.unnumbered}

**Voir la page sur** [projets R](#r_projects) pour plus de détails sur notre approche recommandée pour gérer votre répertoire de travail.\

Un moyen commun, efficace et sans problème de gérer votre répertoire de travail et vos chemins de fichier consiste à combiner ces trois éléments dans un flux de travail du [projets R](#r_projects) orienté comme expliqué ci-dessous:

1.  Un projet R pour stocker tous vos fichiers (voir page sur [projets R](#r_projects))
2.  Le paquet **here** pour localiser les fichiers (voir page sur [importer et exporter](#import_export))
3.  Le paquet **rio** pour importer ou exporter des fichiers (voir page sur [importer et exporter](#import_export))

<!-- ======================================================= -->

### Définir le répertoire de travail par commande {.unnumbered}

Jusqu'à récemment, de nombreuses personnes apprenant R ont appris à commencer leurs scripts avec une commande `setwd()`. Veuillez plutôt envisager d'utiliser un flux de travail orienté par [projets R](#r_projects) et lire les [raisons de ne pas utiliser `setwd()`](https://www.tidyverse.org/blog/2017/12/workflow-vs-script/).

En bref, votre travail devient spécifique à votre ordinateur, les chemins de fichier utilisés pour importer et exporter des fichiers deviennent "cassants", ce qui entrave gravement la collaboration et l'utilisation de votre code sur tout autre ordinateur. Heureusement il existe des alternatives faciles!

Comme indiqué ci-dessus, bien que nous ne recommandons pas cette approche dans la plupart des cas, vous pouvez utiliser la commande `setwd()` avec le chemin du fichier de dossier souhaité dans les citations, par exemple:

```{r, eval=F}
setwd("C:/Documents/R Files/My analysis")
```

[***DANGER:*** Définition d'un répertoire de travail avec `setwd()` *peut* être "cassant" si le chemin de fichier est spécifique à un ordinateur. Au lieu de cela, utilisez des chemins de fichier par rapport à un répertoire racine du projet R (avec le paquet **here**).]{style="color:red;"}

<!-- ======================================================= -->

### Définir manuellement le répertoire de travail {.unnumbered}

Pour définir le répertoire de travail manuellement (l'équivalent graphique du `setwd()`), cliquez sur le menu déroulant "Session" et accédez à "Set Working Directory", puis "Choose Directory". Cela définira le répertoire de travail pour cette scéance spécifique de R. Remarque: Si vous utilisez cette approche, vous devrez le faire manuellement chaque fois que vous ouvrez Rstudio.

<!-- ======================================================= -->

### Définir le répertoire de travail dans un projet R {.unnumbered}

Si vous utilisez un projet R, le répertoire de travail sera par défaut dans le dossier racine du projet R qui contient le fichier `.rproj`. Cela s'appliquera si vous ouvrez RStudio en cliquant sur le projet R (le fichier avec l'extension `.rproj`).

<!-- ======================================================= -->

### Répertoire de travail dans un script R Markdown {.Unnumbered}

Dans un script R Markdown, le répertoire de travail par défaut est le dossier ou le fichier RMarkdown (`.rmd`) est enregistré. Si vous utilisez un projet R et le paquet **here**, cela ne s'applique pas et le répertoire de travail sera `here()`, comme expliqué dans la page [projets R](#r_projects).

Si vous souhaitez modifier le répertoire de travail d'une dossier RMarkdown autonome (qui ne fait pas partie d'un projet R), et vous utilisez `setwd()`, cela ne s'appliquera qu'à ce morceau de code spécifique. Pour modifier tous les morceaux de code dans une dossier RMarkdown, modifiez le morceau de configuration pour ajouter le paramètre `root.dir =`, comme ci-dessous:

```{r, eval=F}
knitr::opts_knit$set(root.dir = 'desired/directorypath')
```

Il est beaucoup plus facile d'utiliser simplement le script RMarkdown dans un projet R et d'utiliser le paquet **here**.

<!-- ======================================================= -->

### Fournir des chemins de fichier {.unnumbered}

La source de frustration la plus commune pour un débutant R (au moins sur un ordinateur avec Windows) est de saisir un chemin de fichier pour importer ou exporter des données. Il existe une explication approfondie sur la meilleure façon de saisir les chemins de fichier de saisie dans la page [importer et exporter](#import_export), mais voici quelques points clés:

**Chemins cassés**

Vous trouverez ci-dessous un exemple de chemin de fichier "absolute" avec un "adresse complète". Ceux-ci se casseront probablement s'ils sont utilisés par un autre ordinateur. Une exception est si vous utilisez un dossier sur un réseau partagé.

    C:/Utilisateurs/Nom/Document/Logiciels analytiques/R/Projets/Analyse2019/data/mars2019.csv

**Direction de la barre oblique**

*Si vous saisissez un chemin de fichier, soyez conscient de la direction des barres obliques.* Utilisez *des barres obliques vers l'avant* (`/`) pour séparer les composants, par exemple `Data/Provincial.csv`. Le défaut pour les ordinateurs avec Windows est de séparer les composants du chemin avec *des barres obliques en arrière* (`\\`). Vous devrez donc modifier la direction de chaque barre oblique. Si vous utilisez le paquet **here** comme décrit dans la page [projets R](#r_projects), la direction des barres obliques n'est pas un problème.

**Chemins de fichiers relatifs**

Nous recommandons généralement de utiliser des fichiers avec chemins "relatifs" - c'est-à-dire le chemin *par rapport à* la racine de votre projet R. Vous pouvez le faire en utilisant le paquet **here** comme expliqué dans la page [projets R](#r_projects). Un chemin de fichiers relatif peut ressembler à ceci:

```{r, eval=F}

# Importer csv Linelist à partir de données/listes linéare/propres/sous-dossiers d'un projet R

linelist <- import(here("data", "clean", "linelists", "marin_country.csv"))
```

Même si vous utilisez des chemins de fichiers relatifs dans un projet R, vous pouvez toujours utiliser des chemins absolus pour importer/exporter des données en dehors de votre projet R.

<!-- ======================================================= -->

## Objets {#objets}

Tout dans R est un objet, et R est une langue "orienté sur l'objet". Les sections suivantes expliquent:

-   Comment créer des objets (`<-`)
-   Types d'objets (par exemple, trames de données, vecteurs ..)\
-   Comment accéder à des sous-parties d'objets (par exemple, des variables dans un jeu de données)\
-   Classes d'objets (ex. numérique, logique, nombres entieres, double, caractère, facteur)

<!-- ======================================================= -->

### Tout est un objet {.unnumbered}

*Cette section est adaptée du [projet R4Epis](https://r4epis.netlify.app/training/r_basics/objects/).*\
Tout ce que vous stockez dans R - des ensembles de données, des variables, une liste de noms de villages, un nombre total de population, même des sorties telles que des graphiques - sont des **objets** qui sont **attribués à un nom** et **peuvent être référencés** dans les commandes ultérieures.

Un objet existe lorsque vous lui avez attribué une valeur (voir la section d'attribution ci-dessous). Lorsqu'une valeur lui est attribuée, l'objet apparaît dans l'environnement (voir le volet supérieur droit de RStudio). Il peut alors être exploité, manipulé, modifié et redéfini.

<!-- ======================================================= -->

### Définir des objets (`<-`) {.unnumbered}

**Créez des objets *en leur attribuant une valeur* avec l'opérateur `<-`.** Vous pouvez considérer l'opérateur d'affectation`<-` comme les mots "est défini comme". Les commandes d'affectation suivent généralement un ordre standard:

**`nom_objet <- valeur`** (ou processus/calcul qui produit une valeur)

Par exemple, vous souhaiterez peut-être enregistrer la semaine de rapport épidémiologique en cours en tant qu'objet de référence dans le code ultérieur. Dans cet exemple, l'objet `semaine_en_cours` est créé lorsqu'il reçoit la valeur `"2018-W10"` (les guillemets en font une valeur de caractère). L'objet `semaine_en_cours` apparaîtra alors dans le volet Environnement de RStudio (en haut à droite) et pourra être référencé dans les commandes ultérieures.

Voir les commandes R et leur sortie dans les cases ci-dessous.

```{r basics_objects_assignment}

# Créer l'objet semaine_en_cours en lui attribuant une valeur:
semaine_en_cours <- "2018-W10"   

# Imprime la valeur actuelle de l'objet semaine_en_cours dans la console:
semaine_en_cours

```

[***NOTE:*** Notez que le `[1]` dans la sortie de la console R indique simplement que vous visualisez le premier élément de la sortie]{style="color: black;"}

[***ATTENTION:*** **La valeur d'un objet peut être écrasée** à tout moment en exécutant une commande d'affectation pour redéfinir sa valeur. Ainsi, **l'ordre d'exécution des commandes est très important**.]{style="color: orange;"}

La commande suivante redéfinira la valeur de `semaine_en_cours`:

```{r basics_objects_reassignment}

# Attribuer une NOUVELLE valeur à l'objet semaine_en_cours:
semaine_en_cours <- "2018-W51"

# Afficher la valeur actuelle de semaine_en_cours dans la console:
semaine_en_cours

```

**Signe égal `=`**

Vous verrez également des signes égal dans le code R:

-   Un double signe égal `==` entre deux objets ou valeurs pose une *question* logique: "est-ce égal à cela?".
-   Vous verrez également des signes égal dans les fonctions utilisées pour spécifier les valeurs des arguments d'un fonction (lisez-les dans les sections ci-dessous), par exemple `max(age, na.rm = TRUE)`.
-   Vous *pouvez* utiliser un seul signe égal `=` à la place de `<-` pour créer et définir des objets, mais cela est déconseillé. Vous pouvez lire pourquoi cela est déconseillé [ici](https://renkun.me/2014/01/28/difference-between-assignment-operators-in-r/).

**Ensembles de données**

Les ensembles de données sont également des objets (généralement des « dataframes ») et doivent recevoir des noms lors de leur importation. Dans le code ci-dessous, l'objet `linelist` est créé et reçoit la valeur d'un fichier CSV importé avec le paquet **rio** et sa fonction `import()`.

```{r basics_objects_dataframes, eval=FALSE}

# <<linelist>> est créée et reçoit la valeur du fichier CSV importé:
linelist <- import("my_linelist.csv")

```

Vous pouvez en savoir plus sur l'importation et l'exportation d'ensembles de données dans la section sur [importer et exporter](#import_export).

[***ATTENTION:*** Une note rapide sur la dénomination des objets:]{style="color: orange;"}

-   Les noms d'objets ne doivent pas contenir d'espaces, mais vous devez utiliser un trait de soulignement (\_) ou un point (.) au lieu d'un espace.
-   Les noms d'objets sont sensibles à la casse (lettres majuscules et minuscules; ce qui signifie que **D**ataset_A est différent de **d**ataset_A).
-   Les noms d'objets doivent commencer par une lettre (ne peuvent pas commencer par un chiffre comme 1, 2 ou 3).

**Les sorties**

Les sorties telles que les tableaux et les tracés fournissent un exemple de la façon dont les sorties peuvent être enregistrées en tant qu'objets ou simplement imprimées sans être enregistrées. Un tableau croisé du sexe et du résultat à l'aide de la fonction R **base** `table()` peut être imprimé directement sur la console R (*sans* être enregistré).

```{r}

# Imprimé sur la console R uniquement:
table(linelist$gender, linelist$outcome)

```

La même table peut également être enregistrée en tant qu'objet nommé. Ensuite, éventuellement, il peut être imprimé.

```{r}

# Enregistrer:
gen_out_table <- table(linelist$gender, linelist$outcome)

# Imprimer:
gen_out_table

```

**Colonnes**

Les colonnes d'un ensemble de données sont également des objets et peuvent être définies, écrasées et créées comme décrit ci-dessous dans la section sur les colonnes.

Vous pouvez utiliser l'opérateur d'affectation de **base** R pour créer une nouvelle colonne. Ci-dessous, la nouvelle colonne `bmi` (indice de masse corporelle) est créée, et pour chaque ligne la nouvelle valeur est le résultat d'une opération mathématique sur la valeur de la ligne dans les colonnes `wt_kg` et `ht_cm`.

```{r, eval=F}

# Créer une nouvelle colonne "bmi" en utilisant la syntaxe de base R:
linelist$bmi <- linelist$wt_kg / (linelist$ht_cm/100)^2

```

Cependant, dans ce manuel, nous mettons l'accent sur une approche différente de la définition des colonnes, qui utilise la fonction `mutate()` du package **dplyr** et *piping* avec l'opérateur pipe (`%>%`). La syntaxe est plus facile à lire et il y a d'autres avantages expliqués dans la page [nettoyage de donnees et fonctions essentielles](#cleaning_data).

Vous pouvez lire plus sur *piping* dans la section "Piping" ci-dessous.

```{r, eval=F}

# Créer une nouvelle colonne "bmi" en utilisant la syntaxe dplyr:
linelist <- linelist %>% 
  mutate(bmi = wt_kg / (ht_cm/100)^2)

```

<!-- ======================================================= -->

### Structure d'objet {.unnumbered}

**Les objets peuvent être une seule donnée (par exemple, "mon_numéro \<- 24"), ou ils peuvent être constitués de données structurées.**

Le graphique ci-dessous est emprunté à [ce tutoriel R en ligne](http://venus.ifca.unican.es/Rintro/dataStruct.html). Il montre certaines structures de données courantes et leurs noms. Les données spatiales ne sont pas incluses dans cette image, qui sont abordées dans la page [bases de GIS](#gis).

```{r basics_objects_structures, echo=F, out.width = "75%", out.height="50%", fig.align = "center"}
knitr::include_graphics(here::here("images", "R_data_structures.png"))
```

En épidémiologie (et en particulier en épidémiologie de terrain), vous rencontrerez *le plus souvent* des trames de données et des vecteurs:

+-------------------+----------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+
| Structure commune | Explication                                                                                                    | Exemple                                                                                                           |
+===================+================================================================================================================+===================================================================================================================+
| Vecteurs          | Un conteneur pour une séquence d'objets singuliers, tous de la même classe (par exemple numérique, caractère). | **Les "variables" (colonnes) dans les blocs de données sont des vecteurs** (par exemple, la colonne `age_years`). |
+-------------------+----------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+
| Trames de données | Vecteurs (par exemple, des colonnes) qui sont liés ensemble et qui ont tous le même nombre de lignes.          | `linelist` est une trame de données.                                                                              |
+-------------------+----------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+

Notez que pour créer un vecteur "autonome" (ne faisant pas partie d'un bloc de données), la fonction `c()` est utilisée pour combiner les différents éléments. Par exemple, si vous créez un vecteur de couleurs à appliquer à l'échelle de couleurs d'un tracé:

`vector_of_colors <- c("blue", "red2", "orange", "grey")`

<!-- ======================================================= -->

### Classes d'objets {.unnumbered}

Tous les objets stockés dans R ont une *classe* qui indique à R comment gérer l'objet. Il existe de nombreuses classes possibles, mais les plus courantes incluent:

+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+
| Classe     | Explication                         | Exemples                                                                                                           |
+============+==================================================================================================================================================================================================================================+====================================================================================================================+
| Caractère  | Ce sont des textes/mots/phrases **"entre guillemets"**. Les mathématiques ne peuvent pas être effectuées sur ces objets.                                                                                                         | "Les objets caractères sont entre guillemets"                                                                      |
+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+
| Entier     | Nombres **entiers uniquement** (pas de décimales)                                                                                                                                                                                | -5, 14 ou 2000                                                                                                     |
+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+
| Numérique  | Ce sont des nombres et **peuvent inclure des décimales**. S'ils sont entre guillemets, ils seront considérés comme une classe de caractères.                                                                                     | 23.1 ou 14                                                                                                         |
+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+
| Facteur    | Ce sont des vecteurs qui ont un **ordre spécifié** ou une hiérarchie de valeurs                                                                                                                                                  | Une variable de statut économique à valeurs ordonnées                                                              |
+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+
| Des dates  | **Une fois que R est informé que certaines données sont des dates**, ces données peuvent être manipulées et affichées de manière spéciale. Voir la page sur [Manipuler les dates](#working_dates) pour plus d'informations.                       | 2018-04-12 ou                                                                                                      |
|            |                                                                                                                                                                                                                                  |                                                                                                                    |
|            |                                                                                                                                                                                                                                  | 15/3/1954 ou                                                                                                       |
|            |                                                                                                                                                                                                                                  |                                                                                                                    |
|            |                                                                                                                                                                                                                                  | mer 4 janv 1980                                                                                                    |
+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+
| Logique    | Les valeurs doivent être l'une des deux valeurs spéciales `TRUE` ou `FALSE` (notez qu'elles ne sont **pas** "TRUE" et "FALSE" entre guillemets)                                                                                  | TRUE ou FALSE                                                                                                      |
+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+
| data.frame | Une trame de données est la façon dont R stocke un **ensemble de données typique**. Il se compose de vecteurs (colonnes) de données liés entre eux, qui ont tous le même nombre d'observations (lignes).                         | L'exemple de jeu de données AJS nommé `linelist_raw` contient 68 variables avec 300 observations (lignes) chacune. |
+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+
| tibble     | Les tibbles sont une variante du cadre de données; la principale différence opérationnelle étant qu'ils s'impriment mieux sur la console (affichent les 10 premières lignes et uniquement les colonnes qui tiennent sur l'écran) | Tout cadre de données, liste ou matrice peut être converti en tibble avec `as_tibble()`                            |
+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+
| liste      | Une liste est comme un vecteur, mais contient d'autres objets qui peuvent être d'autres classes différentes                                                                                                                      | Une liste peut contenir un seul nombre, une trame de données, un vecteur et même une autre liste à l'intérieur!    |
+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+

**Vous pouvez tester la classe d'un objet en fournissant son nom à la fonction `class()`**. Remarque : vous pouvez référencer une colonne spécifique dans un jeu de données en utilisant la notation «\$» pour séparer le nom du jeu de données et le nom de la colonne.

```{r, echo=TRUE,}

# La classe doit être une trame de données ou un tibble:
class(linelist)         

# La classe doit être numérique:
class(linelist$age)

# La classe doit être caractère:
class(linelist$gender)

```

Parfois, une colonne sera automatiquement convertie dans une classe différente par R. Attention à cela ! Par exemple, si vous avez un vecteur ou une colonne de nombres, mais qu'une valeur de caractère est insérée; toute la colonne deviendra un caractère de classe.

```{r}

# Définir le vecteur avec des numéros:
num_vector <- c(1,2,3,4,5) 

# Le vecteur est de classe "numérique":
class(num_vector)          

# Convertir le troisième élément en caractère:
num_vector[3] <- "three"   

# Le vecteur est maintenant de classe "caractère"
class(num_vector)          

```

Un exemple courant de ceci est lors de la manipulation d'un bloc de données afin d'imprimer un tableau. Si vous faites une ligne totale et essayez de coller/coller ensemble des pourcentages dans la même cellule que des nombres (par exemple `23 (40%)`), le toute la colonne numérique ci-dessus sera convertie en caractère et ne pourra plus être utilisée pour des calculs mathématiques. **Parfois, vous devrez convertir des objets ou des colonnes dans une autre classe.**

+------------------+------------------------------------------------------------------------------------------------------+
| Fonction         | Action                                                                                               |
+==================+======================================================================================================+
| `as.character()` | Convertit en classe "caractère"                                                                      |
+------------------+------------------------------------------------------------------------------------------------------+
| `as.numeric()`   | Convertit en classe "numérique"                                                                      |
+------------------+------------------------------------------------------------------------------------------------------+
| `as.integer()`   | Convertit en classe "entière"                                                                        |
+------------------+------------------------------------------------------------------------------------------------------+
| `as.Date()`      | Convertit en classe "Date"                                                                           |
|                  |                                                                                                      |
|                  | *Remarque:* voir le chapitre sur les [dates](#working_dates) pour plus de détails                                   |
+------------------+------------------------------------------------------------------------------------------------------+
| `factor()`       | Convertit en classe "facteur"                                                                        |
|                  |                                                                                                      |
|                  | *Remarque:* la redéfinition de l'ordre des niveaux de valeur nécessite des arguments supplémentaires |
+------------------+------------------------------------------------------------------------------------------------------+

De même, il existe des fonctions **base** R pour vérifier si un objet EST d'une classe spécifique, comme `is.numeric()`, `is.character()`, `is.double()`, `is .facteur()`, `is.integer()`

Voici [plus de matériel en ligne sur les classes et les structures de données dans R](https://swcarpentry.github.io/r-novice-inflammation/13-supp-data-structures/).

<!-- ======================================================= -->

### Colonnes/Variables (`$`) {.unnumbered}

**Une colonne dans un bloc de données est techniquement un "vecteur" (voir tableau ci-dessus)** - une série de valeurs qui doivent toutes être de la même classe (caractère, numérique, logique, etc.).

Un vecteur peut exister indépendamment d'un bloc de données, par exemple un vecteur de noms de colonnes que vous souhaitez inclure en tant que variables explicatives dans un modèle. Pour créer un vecteur "autonome", utilisez la fonction `c()` comme ci-dessous:

```{r, warning=F, message=F}

# Définir le vecteur autonome des valeurs de classe caractère:
var_explicatives <- c("gender", "fever", "chills", "cough", "aches", "vomit")

# Affiche les valeurs dans ce vecteur nommé:
var_explicatives

```

**Les colonnes d'un bloc de données sont également des vecteurs et peuvent être appelées, référencées, extraites ou créées à l'aide du symbole `$`.** Le symbole `$` relie le nom de la colonne au nom de son bloc de données. Dans ce manuel, nous essayons d'utiliser le mot "colonne" au lieu de "variable".

```{r basics_objects_call, eval=F}

# Récupérer la longueur du vecteur age:
length(linelist$age) # (l'âge est une colonne dans le bloc de données nomé "linelist")

```

En tapant le nom de la trame de données suivi de `$`, vous verrez également un menu déroulant de toutes les colonnes de la trame de données. Vous pouvez les faire défiler à l'aide de votre touche fléchée, en sélectionner une avec votre touche Entrée et éviter les fautes d'orthographe !

```{r echo=F, out.width = "100%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Calling_Names.gif"))
```

[***CONSEIL AVANCÉ:*** Certains objets plus complexes (par exemple, une liste ou un objet `epicontacts`) peuvent avoir plusieurs niveaux accessibles via plusieurs signes dollar. Par exemple `epicontacts$linelist$date_onset`]{style="color: darkgreen;"}

<!-- ======================================================= -->

### Accès/index avec crochets (`[ ]`) {.unnumbered}

Vous devrez peut-être afficher des parties d'objets, également appelées "indexation", ce qui se fait souvent à l'aide des crochets `[ ]`. L'utilisation de `$` sur une trame de données pour accéder à une colonne est également un type d'indexation.

```{r}

# Définir le vecteur:
mon_vecteur <- c("a", "b", "c", "d", "e", "f")

# Imprimer le 5ème élément:
mon_vecteur[5]

```

Les crochets fonctionnent également pour renvoyer des parties spécifiques d'une sortie renvoyée, comme la sortie d'une fonction `summary()`:

```{r}

# Tout le résumé
summary(linelist$age)

# Juste le deuxième élément du résumé, avec le nom (en utilisant uniquement des crochets simples)
summary(linelist$age)[2]

# Juste le deuxième élément, sans nom (en utilisant des doubles crochets)
summary(linelist$age)[[2]]

# Extraire un élément par son nom, sans afficher le nom
summary(linelist$age)[["Median"]]

```

Les crochets fonctionnent également sur les blocs de données pour afficher des lignes et des colonnes spécifiques. Vous pouvez le faire en utilisant la syntaxe `dataframe[lignes, colonnes]`:

```{r basics_objects_access, eval=F}

# Afficher une ligne spécifique (2) du jeu de données, avec toutes les colonnes 
# (n'oubliez pas la virgule!)
linelist[2,]

# Afficher toutes les lignes, mais une seule colonne:
linelist[, "date_onset"]

# Afficher les valeurs de la ligne 2 et des colonnes 5 à 10:
linelist[2, 5:10]

# Afficher les valeurs de la ligne 2 et des colonnes 5 à 10 et 18:
linelist[2, c(5:10, 18)]

# Afficher les lignes 2 à 20 et des colonnes spécifiques:
linelist[2:20, c("date_onset", "outcome", "age")]

# Afficher les lignes et les colonnes en fonction de critères
# *** Notez que le dataframe doit toujours être nommé dans les critères!
linelist[linelist$age > 25 , c("date_onset", "outcome", "age")]

# Utilisez View() pour voir les sorties dans le volet RStudio Viewer (plus facile à lire)
# *** Notez le "V" majuscule dans la fonction View()
View(linelist[2:20, "date_onset"])

# Enregistrer en tant que nouvel objet:
new_table <- linelist[2:20, c("date_onset")]

```

Notez que vous pouvez également réaliser l'indexation des lignes/colonnes ci-dessus sur les blocs de données et les tibbles en utilisant la syntaxe **dplyr** (fonctions `filter()` pour les lignes et `select()` pour les colonnes). Pour en savoir plus sur ces fonctions principales, [consultez la page sur le nettoyage de deonnees et fonctions essentielles](#cleaning_data).

Pour filtrer en fonction du "numéro de ligne", vous pouvez utiliser la fonction **dplyr** `row_number()` avec des parenthèses ouvertes dans le cadre d'une instruction de filtrage logique. Vous utiliserez souvent l'opérateur `%in%` et une plage de nombres dans le cadre de cette instruction logique, comme indiqué ci-dessous. Pour voir les *premières* N lignes, vous pouvez également utiliser la fonction spéciale **dplyr** `head()`.

```{r, eval=F}

# Afficher les 100 premières lignes:
linelist %>% 
     head(100)

# Afficher la ligne 5 uniquement:
linelist %>% 
     filter(row_number() == 5)

# Afficher les lignes 2 à 20 et trois colonnes spécifiques 
# (notez qu'aucun guillemet n'est nécessaire sur les noms de colonne)
linelist %>% 
     filter(row_number() %in% 2:20) %>% 
     select(date_onset, issue, age)

```

Lors de l'indexation d'un objet de classe **list**, les crochets simples retournent toujours avec la classe list, même si un seul objet est retourné. Les crochets doubles, cependant, peuvent être utilisés pour accéder à un seul élément et renvoyer une classe différente de la liste.\
Les parenthèses peuvent également être écrites les unes après les autres, comme illustré ci-dessous.

Cette [explication visuelle de l'indexation des listes, avec des poivrières](https://r4ds.had.co.nz/vectors.html#lists-of-condiments) est humoristique et utile.

```{r}

# définir la liste des démos
ma_liste <- list(
   # Le premier élément de la liste est un vecteur de caractères:
   hopitaux = c("Central", "Empire", "Santa Anna"),
  
   # Le deuxième élément de la liste est une trame de données d'adresses:
   adresses = data.frame(
     rue = c("145 Medical Way", "1048 Brown Ave", "999 El Camino"),
     ville = c("Andover", "Hamilton", "El Paso")
     )
   )

```

Voici à quoi ressemble la liste lorsqu'elle est imprimée sur la console. Voyez comment il y a deux éléments nommés:

-   `hôpitaux`, un vecteur de caractères\
-   `adresses`, une trame de données d'adresses

```{r}
ma_liste
```

Maintenant, nous extrayons, en utilisant diverses méthodes:

```{r}

# Cela renvoie l'élément dans la classe "list" - le nom de l'élément est toujours affiché:
ma_liste[1] 

# Cela ne renvoie que le vecteur de caractères (sans nom):
ma_liste[[1]]

# Vous pouvez également indexer par le nom de l'élément de la liste:
ma_liste[["hopitaux"]]

# Cela renvoie le troisième élément du vecteur de caractères "hôpitaux":
ma_liste[[1]][3] 

# Cela renvoie la première colonne ("rue") de la trame de données d'adresse:
ma_liste[[2]][1]

```

<!-- ======================================================= -->

### Supprimer des objets {.unnumbered}

Vous pouvez supprimer des objets individuels de votre environnement R en mettant le nom dans la fonction `rm()` (sans guillemets):

```{r, eval=F}
rm(nom_objet)
```

Vous pouvez supprimer tous les objets (vider votre espace de travail) en exécutant:

```{r, eval=F}
rm(list = ls(all = TRUE))
```

<!-- ======================================================= -->

<!-- ======================================================= -->

<!-- ======================================================= -->

## Tuyauterie / "Piping" (`%>%`)

**Deux approches générales pour travailler avec des objets sont:**

1)  **Pipes/tidyverse** - les tuyaux envoient un objet d'une fonction à l'autre - l'accent est mis sur *l'action*, pas sur l'objet\
2)  **Définir les objets intermédiaires** - un objet est redéfini encore et encore - l'accent est mis sur l'objet

<!-- ======================================================= -->

### **Tuyaux / Pipes** {.unnumbered}

\*\* Expliqué simplement, l'opérateur pipe (`%>%`) passe une sortie intermédiaire d'une fonction à la suivante. \*\*\
Vous pouvez penser que cela signifie "alors". De nombreuses fonctions peuvent être liées avec `%>%`.

-   **Le tuyau met l'accent sur une séquence d'actions, et non sur l'objet sur lequel les actions sont effectuées**\
-   Les tuyaux sont plus efficaces lorsqu'une séquence d'actions doit être effectuée sur un objet\
-   Les tuyaux proviennent du paquet **magrittr**, qui est automatiquement inclus dans les paquets **dplyr** et **tidyverse**
-   Les tuyaux peuvent rendre le code plus propre et plus facile à lire, plus intuitif

En savoir plus sur cette approche dans le tidyverse [guide de style](https://style.tidyverse.org/pipes.html)

Voici un faux exemple de comparaison, utilisant des fonctions fictives pour "faire un gâteau". Tout d'abord, la méthode du tuyau:

```{r piping_example_pipe, eval=F}

# Un faux exemple de comment faire cuire un gâteau en utilisant la syntaxe de tuyauterie:

gateau <- farine %>% # pour définir le gâteau, commencez par la farine, puis...
     # ajouter des oeufs
     add(oeufs) %>% 
     # ajouter de l'huile
     add(huile) %>% 
     # ajouter de l'eau
     add(eau) %>% 
     # mélanger ensemble avec cuillère pour 2 minutes:
     mix_together(
          ustensil = "spoon",
          minutes = 2) %>%
     # cuire à 200 degrés centigrade pour 35 minutes:
     bake(
          degrees = 200, 
          system = "centigrade",
          minute = 35) %>%
     # laissez-le refroidir
     let_cool() 


```

Voici un autre [lien](https://cfss.uchicago.edu/notes/pipes/#:~:text=Pipes%20are%20an%20extremely%20useful,code%20and%20combine%20multiple%20operations) décrivant l'utilitaire de tuyaux.

La tuyauterie n'est pas une fonction de **base** en R. Pour utiliser la tuyauterie, le paquet **magrittr** doit être installé et chargé (cela se fait généralement en chargeant le paquet **tidyverse** ou **dplyr** qui l'inclut). Vous pouvez [en savoir plus sur la tuyauterie dans la documentation de magrittr](https://magrittr.tidyverse.org/).

Notez que, tout comme les autres commandes R, les tuyaux peuvent être utilisés pour afficher simplement le résultat ou pour enregistrer/réenregistrer un objet, selon que l'opérateur d'affectation `<-` est impliqué ou non. Voir les deux exemplaires ci-dessous:

```{r, eval=F}

# Créer ou écraser un objet, en le définissant sous 
# forme de nombres agrégés par catégorie d'âge (non imprimé)
linelist_summary <- linelist %>% 
  count(age_cat)

```

```{r}

# Imprimez le tableau des comptes dans la console, mais ne l'enregistrez pas:
linelist %>% 
  count(age_cat)

```

**`%<>%`**\
Il s'agit d'un "tuyau d'affectation" du paquet **magrittr**, qui *transmet un objet en avant et redéfinit également l'objet*. Il doit être le premier opérateur pipe de la chaîne. C'est un raccourci. Les deux commandes ci-dessous sont équivalentes:

```{r, eval=F}

# Utilisez l'opérateur d'affectation:
linelist <- linelist %>%
  filter(age > 50)

# Utilisez le tuyau d'affectation:
linelist %<>% filter(age > 50)

```

<!-- ======================================================= -->

### Définir les objets intermédiaires {.unnumbered}

Cette approche de modification des objets ou trammes de données peut être meilleure si:

-   Vous devez manipuler plusieurs objets\
-   Il y a des étapes intermédiaires qui sont significatives et méritent des noms d'objets séparés

**Des risques:**

-   Créer de nouveaux objets pour chaque étape signifie créer beaucoup d'objets. Si vous utilisez le mauvais, vous ne vous en rendrez peut-être pas compte!\
-   Nommer tous les objets peut prêter à confusion\
-   Les erreurs peuvent ne pas être facilement détectables

Soit nommer chaque objet intermédiaire, soit écraser l'original, soit combiner toutes les fonctions ensemble. Tous viennent avec leurs propres risques.

Vous trouverez ci-dessous le même exemple de faux "gâteau" que ci-dessus, mais en utilisant ce style:

```{r piping_example_redefine, eval=F}

# un faux exemple de comment faire un gâteau en utilisant cette méthode 
# (définissant des objets intermédiaires):

# Ajouter le farine et les oeufs:
pate_1 <- left_join(farine, oeufs)

# Ajouter l'huile:
pate_2 <- left_join (pate_1, huile)

# Ajouter l'eau:
pate_3 <- left_join(pate_2, eau)

# Melange tous ensemble:
pate_4 <- mix_together(object = pate_3, 
                       ustensil = "spoon", 
                       minutes = 2)

# Cuire le gâteau dans le four:
gateau <-bake(object = pate_4, 
              degrees = 200, 
              system = "centigrade", 
              minutes = 35)

# Laissez-le à refroidir:
gateau <- let_cool(gateau)

```

Combinez toutes les fonctions ensemble - c'est difficile à lire :

```{r eval=F}

# Un exemple de combinaison/imbrication de plusieurs fonctions - difficile à lire:
gateau <- let_cool(bake(mix_together(pate_3, 
                                     utensil = "spoon", 
                                     minutes = 2), 
                        degrees = 200, 
                        system = "centigrade",
                        minutes = 35))

```

<!-- ======================================================= -->

## Opérateurs clés et fonctions {#operators}

Cette section détaille les opérateurs dans R, tels que:

-   Opérateurs définitionnels\
-   Opérateurs relationnels (inférieur à, égal aussi..)\
-   Opérateurs logiques (et, ou...)\
-   Gestion des valeurs manquantes\
-   Opérateurs et fonctions mathématiques (+/-, \>, sum(), median(), ...)\
-   L'opérateur `%in%`

<!-- ======================================================= -->

### Opérateurs d'affectation {.unnumbered}

**`<-`**

L'opérateur d'affectation de base dans R est `<-`. Tel que `nom_objet <- valeur`.\
Cet opérateur d'affectation peut également être écrit comme `=`. Nous vous conseillons d'utiliser `<-` pour une utilisation générale de R.\
Nous conseillons également d'entourer ces opérateurs d'espaces, pour plus de lisibilité.

**`<<-`**

Si [Fonctions d'écriture](#writing_functions), ou si vous utilisez R de manière interactive avec des scripts sourcés, vous devrez peut-être utiliser cet opérateur d'affectation `<<-` (de **base** R). Cet opérateur est utilisé pour définir un objet dans un environnement R « parent » supérieur. Voir ceci [référence en ligne](https://stat.ethz.ch/R-manual/R-devel/library/base/html/assignOps.html).

**`%<>%`**

Il s'agit d'un "tuyau d'affectation" du paquet **magrittr**, qui dirige un objet vers l'avant et *redéfinit également l'objet*. Il doit être le premier opérateur pipe de la chaîne. Il s'agit d'un raccourci.

**`%<+%`**

Ceci est utilisé pour ajouter des données aux arbres phylogénétiques avec le package **ggtree**. Voir la page sur les [arbres phylogénétiques](#phylogenetic_trees) ou ce [livre de ressources en ligne](https://yulab-smu.top/treedata-book/).

<!-- ======================================================= -->

### Opérateurs relationnels et logiques {.unnumbered}

Les opérateurs relationnels comparent les valeurs et sont souvent utilisés lors de la définition de nouvelles variables et de sous-ensembles des blocs de données. Voici les opérateurs relationnels courants dans R:

+----------------------+------------+--------------+------------------------------------------------------------------------------------------------------------------+
| Sens                 | Opérateur  | Exemple      | Exemple de résultat                                                                                              |
+======================+============+==============+==================================================================================================================+
| Égal à               | `==`       | `"A" == "a"` | `FALSE` (parce que R est sensible à la casse)                                                                    |
|                      |            |              |                                                                                                                  |
|                      |            |              | *Notez que `==` (double égal) est différent de `=` (simple égal), qui agit comme l'opérateur d'affectation `<-`* |
+----------------------+------------+--------------+------------------------------------------------------------------------------------------------------------------+
| Non égal à           | `!=`       | `2 != 0`     | `TRUE`                                                                                                           |
+----------------------+------------+--------------+------------------------------------------------------------------------------------------------------------------+
| Supérieur à          | `>`        | `4 > 2`      | `TRUE`                                                                                                           |
+----------------------+------------+--------------+------------------------------------------------------------------------------------------------------------------+
| Moins de             | `<`        | `4 < 2`      | `FALSE`                                                                                                          |
+----------------------+------------+--------------+------------------------------------------------------------------------------------------------------------------+
| Supérieur ou égal à  | `>=`       | `6 >= 4`     | `TRUE`                                                                                                           |
+----------------------+------------+--------------+------------------------------------------------------------------------------------------------------------------+
| Inférieur ou égal à  | `<=`       | `6 <= 4`     | `FALSE`                                                                                                          |
+----------------------+------------+--------------+------------------------------------------------------------------------------------------------------------------+
| Valeur manquante     | `is.na()`  | `is.na(7)`   | `FALSE`                                                                                                          |
|                      |            |              |                                                                                                                  |
|                      |            |              | (voir page sur [valeur manquante](#missing_data))                                                                                 |
+----------------------+------------+--------------+------------------------------------------------------------------------------------------------------------------+
| Valeur ne manque pas | `!is.na()` | `!is.na(7)`  | `TRUE`                                                                                                           |
+----------------------+------------+--------------+------------------------------------------------------------------------------------------------------------------+

Les opérateurs logiques, tels que ET et OU, sont souvent utilisés pour connecter des opérateurs relationnels et créer des critères plus complexes. Les instructions complexes peuvent nécessiter des parenthèses ( ) pour le regroupement et l'ordre d'application.

+-------------+-------------------------------------------------------------------------------+
| Sens        | Opérateur                                                                     |
+=============+===============================================================================+
| ET          | `&`                                                                           |
+-------------+-------------------------------------------------------------------------------+
| OU          | `|` (barre verticale)                                                         |
+-------------+-------------------------------------------------------------------------------+
| Parenthèses | `( )` Utilisé pour regrouper les critères et clarifier l'ordre des opérations |
+-------------+-------------------------------------------------------------------------------+

Par exemple, ci-dessous, nous avons une liste linéaire avec deux variables que nous voulons utiliser pour créer notre définition de cas, `resultat_tdr`, un résultat d'un test rapide, et `autres_cas_menage`, qui nous dira s'il y a d'autres cas dans le ménage. La commande ci-dessous utilise la fonction `case_when()` pour créer la nouvelle variable `case_def` telle que:

```{r eval=FALSE}
linelist_propre <- linelist %>%
  mutate(case_def = case_when(
    is.na(resultat_tdr) & is.na(autres_cas_menage)            ~ NA_character_,
    resultat_tdr == "Positive"                                 ~ "Confirmé",
    resultat_tdr != "Positive" & other_cases_in_home == "Oui"  ~ "Probable",
    TRUE                                                     ~ "Suspect"
  ))
```

+--------------------------------------------------------------------------------+------------------------+
| Critères dans l'exemple ci-dessus                                              | Valeur dans "case_def" |
+================================================================================+========================+
| Si la valeur des variables `resultat_tdr` et `autres_cas_menage` est manquante | `NA` (manquante)       |
+--------------------------------------------------------------------------------+------------------------+
| Si la valeur dans `resultat_tdr` est "Positive"                                | "Confirmé"             |
+--------------------------------------------------------------------------------+------------------------+
| Si la valeur dans `resultat_tdr` n'est pas "Positive" **ET**                   | "Probable"             |
|                                                                                |                        |
| la valeur dans `autres_cas_menage` est "Oui"                                   |                        |
+--------------------------------------------------------------------------------+------------------------+
| Si l'un des critères ci-dessus n'est pas rempli                                | "Suspect"              |
+--------------------------------------------------------------------------------+------------------------+

*Notez que R est sensible à la casse, donc "Positif" est différent de "positif"...*

<!-- ======================================================= -->

### Valeurs manquantes {.unnumbered}

Dans R, les valeurs manquantes sont représentées par la valeur spéciale `NA` (une valeur "réservée") (lettres majuscules N et A - pas entre guillemets). Si vous importez des données qui enregistrent des données manquantes d'une autre manière (par exemple, 99, "Missing" ou .), vous pouvez recoder ces valeurs en "NA". La procédure à suivre est expliquée dans la page [importer et exporter](#import_export).

**Pour tester si une valeur est `NA`, utilisez la fonction spéciale `is.na()`**, qui renvoie `TRUE` ou `FALSE`.

```{r basics_operators_missing}

# 2 cas positives, un suspect et un inconnu:
resultat_tdr <- c("Positive", "Suspect", "Positive", NA)   

# Verifier si il y' a des valeurs manquantes:
is.na(resultat_tdr)

```

En savoir plus sur les valeurs manquantes, infinies, `NULL` et impossibles dans la page sur [les valeur manquantes](#missing_data). Découvrez comment convertir les valeurs manquantes lors de l'importation de données dans la page sur [importer et exporter](#import_export).

<!-- ======================================================= -->

### Mathématiques et statistiques {.unnumbered}

Tous les opérateurs et fonctions de cette page sont automatiquement disponibles en utilisant **base** R.

#### Opérateurs mathématiques {.unnumbered}

Ceux-ci sont souvent utilisés pour effectuer des additions, des divisions, pour créer de nouvelles colonnes, etc. Vous trouverez ci-dessous des opérateurs mathématiques courants dans R. Que vous mettiez des espaces autour des opérateurs n'est pas important.

| Objectif             | Exemple en R |
|----------------------|--------------|
| addition             | 2 + 3        |
| soustraction         | 2 - 3        |
| multiplication       | 2 \* 3       |
| division             | 30 / 5       |
| exposant             | 2\^3         |
| ordre des opérations | ( )          |

#### Fonctions mathématiques {.unnumbered}

+--------------------------------+---------------------------------------+
| Objectif                       | Fonction                              |
+================================+=======================================+
| arrondir                       | round(x, digits = n)                  |
+--------------------------------+---------------------------------------+
| arrondir                       | janitor::round_half_up(x, digits = n) |
+--------------------------------+---------------------------------------+
| plafond (arrondi)              | ceiling(x)                            |
+--------------------------------+---------------------------------------+
| étage (arrondir à l'inférieur) | floor(x)                              |
+--------------------------------+---------------------------------------+
| valeur absolue                 | abs(x)                                |
+--------------------------------+---------------------------------------+
| racine carrée                  | sqrt(x)                               |
+--------------------------------+---------------------------------------+
| exposant                       | exponent(x)                           |
+--------------------------------+---------------------------------------+
| un algorithme naturel          | log(x)                                |
+--------------------------------+---------------------------------------+
| log à la base 10               | log10(x)                              |
+--------------------------------+---------------------------------------+
| log à la base 2                | log2(x)                               |
+--------------------------------+---------------------------------------+

Remarque: pour `round()`, les `digits =` spécifient le nombre de décimales placées. Utilisez `signif()` pour arrondir à un nombre de chiffres significatifs.

#### Notation scientifique {.unnumbered}

La probabilité d'utilisation de la notation scientifique dépend de la valeur de l'option "scipen".

D'après la documentation de `?options`: scipen est une pénalité à appliquer lors de la décision d'imprimer des valeurs numériques en notation fixe ou exponentielle. Les valeurs positives tendent vers la notation fixe et négatives vers la notation scientifique: la notation fixe sera préférée à moins qu'elle ne soit plus large de plus de 'scipen'.

S'il est réglé sur un nombre faible (par exemple 0), il sera toujours "allumé". Pour "désactiver" la notation scientifique dans votre session R, définissez-la sur un nombre très élevé, par exemple:

```{r, eval=F}
# Désactiver la notation scientifique
options(scipen = 999)
```

#### Arrondi {.unnumbered}

[***DANGER:*** `round()` utilise "l'arrondi du banquier" qui arrondit à partir de 0,5 uniquement si le nombre supérieur est pair. Utilisez `round_half_up()` de **janitor** pour arrondir systématiquement les moitiés au nombre entier le plus proche. Voir [cette explication](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#explore-records-with-duplicated-values-for-specific-combinations-of-variables-%20with-get_dupes)]{style="color: rouge;"}

```{r}

# Fonction d'arrondi avec R de base:
round(c(2.5, 3.5))

# Fonction d'arrondi du paquet "janitor":
janitor::round_half_up(c(2.5, 3.5))

```

#### Fonctions statistiques {.unnumbered}

[***ATTENTION:*** Les fonctions ci-dessous incluront par défaut les valeurs manquantes dans les calculs. Les valeurs manquantes entraîneront une sortie de `NA`, sauf si l'argument `na.rm = TRUE` est spécifié. Cela peut être écrit en raccourci comme `na.rm = T`.]{style="color: orange;"}

| Objective                   | Fonction           |
|-----------------------------|--------------------|
| moyen                       | mean(x, na.rm=T)   |
| médian                      | median(x, na.rm=T) |
| écart-type                  | sd(x, na.rm=T)     |
| quantiles\*                 | quantile(x, probs) |
| somme                       | sum(x, na.rm=T)    |
| valeur minimum              | min(x, na.rm=T)    |
| valeur maximum              | max(x, na.rm=T)    |
| plage de valeurs numériques | range(x, na.rm=T)  |
| sommaire\*\*                | summary(x)         |

Remarques:

-   `*quantile()`: `x` est le vecteur numérique à examiner et `probs =` est un vecteur numérique avec des probabilités comprises entre 0 et 1,0, par exemple `c(0,5, 0,8, 0,85)`
-   `**summary()`: donne un résumé sur un vecteur numérique comprenant la moyenne, la médiane et les centiles communs

[***DANGER:*** Si vous fournissez un vecteur de nombres à l'une des fonctions ci-dessus, assurez-vous d'envelopper les nombres dans `c()`.]{style="color: red;"}

```{r}
# Si vous fournissez des nombres bruts à une fonction, 
# enveloppez-les dans c():

# !!! ERREUR !!!
mean(1, 6, 12, 10, 5, 0)      

# CORRECT
mean(c(1, 6, 12, 10, 5, 0)) 

```

#### Autres fonctions utiles {.unnumbered}

+----------------------------------+-------------------+-------------------------------------------------+
| Objectif                         | Fonction          | Exemple                                         |
+==================================+===================+=================================================+
| créer une séquence               | seq(from, to, by) | `seq(1, 10, 2)`                                 |
+----------------------------------+-------------------+-------------------------------------------------+
| répéter x, n fois                | rep(x, ntimes)    | `rep(1:3, 2)` or `rep(c("a", "b", "c"), 3)`     |
+----------------------------------+-------------------+-------------------------------------------------+
| subdiviser un vecteur numérique  | cut(x, n)         | `cut(linelist$age, 5)`                          |
+----------------------------------+-------------------+-------------------------------------------------+
| prendre un échantillon au hasard | sample(x, size)   | `sample(linelist$id, size = 5, replace = TRUE)` |
+----------------------------------+-------------------+-------------------------------------------------+

<!-- ======================================================= -->

### `%in%` {.unnumbered}

Un opérateur très utile pour faire correspondre les valeurs et pour évaluer rapidement si une valeur se trouve dans un vecteur ou une trame de données:

```{r}
mon_vecteur <- c("a", "b", "c", "d")
```

```{r}
"a" %in% mon_vecteur
"h" %in% mon_vecteur
```

Pour demander si une valeur n'est **pas** `%in%` un vecteur, placez un point d'exclamation (!) **devant** l'instruction logique:

```{r}
# Pour nier, mettre une exclamation devant:
!"a" %in% mon_vecteur
!"h" %in% mon_vecteur
```

`%in%` est très utile lors de l'utilisation de la fonction **dplyr** `case_when()`. Vous pouvez définir un vecteur précédemment, puis le référencer ultérieurement. Par exemple:

```{r eval=F}
affirmative <- c("1", "Yes", "YES", "yes", "y", "Y", "oui", "Oui", "Si")

linelist <- linelist %>% 
  mutate(enfant_hospitalise = case_when(
    hospitalise %in% affirmative & age < 18 ~ "Hospitalized Child",
    TRUE                                    ~ "Not"))
```

Remarque: Si vous souhaitez détecter une chaîne partielle, en utilisant peut-être `str_detect()` de **stringr**, il n'acceptera pas un vecteur de caractères tel que `c("1", "Oui", "oui", "y ")`. Au lieu de cela, il doit recevoir une *expression régulière* - une chaîne condensée avec des barres OU, telle que "1\|Oui\|oui\|y". Par exemple, `str_detect(hospitalisé, "1|Oui|oui|y")`. Voir la page sur [les caractères et les chaîne de caractères](#character_strings) pour plus d'informations.

Vous pouvez convertir un vecteur de caractères en une expression régulière nommée avec cette commande:

```{r}
affirmative <- c("1", "Yes", "YES", "yes", "y", "Y", "oui", "Oui", "Si")
affirmative

# Condenser à: 
affirmative_str_search <- paste0(affirmative, collapse = "|")  # option avec R de base
affirmative_str_search <- str_c(affirmative, collapse = "|")   # option avec le paquet stringr

affirmative_str_search
```

<!-- ======================================================= -->

<!-- ======================================================= -->

<!-- ======================================================= -->

## Erreurs et avertissements

Cette section explique :

-   La différence entre les erreurs et les avertissements\
-   Conseils généraux de syntaxe pour l'écriture de code R\
-   Aides au code

Les erreurs et avertissements courants ainsi que des conseils de dépannage sont disponibles sur la page [erreurs frequentes](#errors).

<!-- ======================================================= -->

### Erreur contre avertissement {.unnumbered}

Lorsqu'une commande est exécutée, la console R peut afficher des messages d'avertissement ou d'erreur en texte rouge.

-   Un **avertissement** signifie que R a terminé votre commande, mais a dû prendre des mesures supplémentaires ou a produit une sortie inhabituelle dont vous devez être conscient.

-   Une **erreur** signifie que R n'a pas pu terminer votre commande.

Cherchez des indices:

-   Le message d'erreur/d'avertissement inclura souvent un numéro de ligne pour le problème.

-   Si un objet "est inconnu" ou "introuvable", vous l'avez peut-être mal orthographié, vous avez oublié d'appeler un package avec library() ou vous avez oublié de relancer votre script après avoir apporté des modifications.

Si tout le reste échoue, copiez le message d'erreur dans Google avec quelques termes clés - il y a de fortes chances que quelqu'un d'autre ait déjà travaillé dessus!

<!-- ======================================================= -->

### Conseils généraux sur la syntaxe {.unnumbered}

Quelques points à retenir lors de l'écriture de commandes dans R, pour éviter les erreurs et les avertissements:

-   Fermez toujours les parenthèses - astuce: comptez le nombre de "(" et de parenthèses fermantes ")" pour chaque bloc de code
-   Évitez les espaces dans les noms de colonnes et d'objets. Utilisez le trait de soulignement ( \_ ) ou les points ( . ) à la place
-   Gardez une trace et n'oubliez pas de séparer les arguments d'une fonction par des virgules
-   R est sensible à la casse, ce qui signifie que `Variable_A` est *différent* de `Variable_a`

<!-- ======================================================= -->

### Aides au code {.unnumbered}

N'importe quel script (RMarkdown ou autre) donnera des indices lorsque vous avez fait une erreur. Par exemple, si vous avez oublié d'écrire une virgule là où c'est nécessaire, ou de fermer une parenthèse, RStudio lèvera un drapeau sur cette ligne, sur le côté droit du script, pour vous avertir.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/basics.Rmd-->


<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
# Transition vers R {#transition_to_r}  

Nous vous proposons ci-dessous quelques conseils et ressources si vous êtes en train de passer à R.  

R a été introduit à la fin des années 1990 et a depuis pris une ampleur considérable. Ses capacités sont si étendues que les alternatives commerciales ont réagi aux développements de R afin de rester compétitives! (lire [cet article comparant R, SPSS, SAS, STATA et Python](https://www.inwt-statistics.com/read-blog/comparison-of-r-python-sas-spss-and-stata.html)).  

En outre, R est beaucoup plus facile à apprendre qu'il y a dix ans. Auparavant, R avait la réputation d'être difficile pour les débutants. C'est désormais beaucoup plus facile grâce à des interfaces utilisateur conviviales comme RStudio, à un code intuitif comme le **tidyverse** et à de nombreuses ressources didactiques.  

<span style="color: darkgreen;">**Ne vous sentez pas intimidé - venez découvrir le monde de R!**</span>  

  

```{r, echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "transition_door.png"))
```




## De Excel  

Passer directement d'Excel à R est un objectif tout à fait réalisable. Cela peut sembler décourageant, mais vous pouvez le faire!  

Il est vrai qu'une personne ayant de solides compétences en Excel peut effectuer des activités très avancées dans Excel seul - même en utilisant des outils de script comme VBA. Excel est utilisé dans le monde entier et constitue un outil essentiel pour un épidémiologiste. Cependant, le compléter avec R peut améliorer et étendre considérablement vos méthodes de travail.  

### Bénéfices {.unnumbered}  

Vous constaterez que l'utilisation de R offre d'immenses avantages en termes de gain de temps, d'analyses plus cohérentes et plus précises, de reproductibilité, de partage et de correction plus rapide des erreurs. Comme tout nouveau logiciel, il y a une "courbe" d'apprentissage dans laquelle vous devez investir du temps pour vous familiariser. Les bénéfices seront significatifs et un immense champ de nouvelles possibilités s'ouvrira à vous avec R.

Excel est un logiciel bien connu qui peut être facile à utiliser pour un débutant afin de produire des analyses et des visualisations simples par "pointer-cliquer". En comparaison, il faut parfois quelques semaines pour se familiariser avec les fonctions et l'interface de R. Cependant, R a évolué au cours des dernières années pour devenir beaucoup plus facile à utiliser pour les débutants.  

De nombreux flux de travail Excel reposent sur la mémoire et la répétition - les risques d'erreur sont donc nombreux. En outre, le nettoyage des données, la méthodologie d'analyse et les équations utilisées sont généralement cachés. Un nouveau collègue peut avoir besoin de beaucoup de temps pour apprendre ce que fait un classeur Excel et comment le réparer. Avec R, toutes les étapes sont explicitement écrites dans le script et peuvent être facilement visualisées, modifiées, corrigées et appliquées à d'autres ensembles de données.


**Pour commencer votre transition d'Excel à R, vous devez adapter votre état d'esprit sur quelques points importants:**  


### Données ordonnées ("Tidy" data) {.unnumbered}  

Utilisez des données "ordonnées" lisibles par une machine plutôt que des données désordonnées "lisibles par l'homme". Il existe trois exigences principales pour les données "ordonnées", comme l'explique ce tutoriel ["tidy" data avec R](https://r4ds.had.co.nz/tidy-data.html):  

* Chaque variable doit avoir sa propre colonne  
* Chaque observation doit avoir sa propre ligne  
* Chaque valeur doit avoir sa propre cellule  

Aux utilisateurs d'Excel - pensez au rôle que [les tableaux Excel](https://exceljet.net/excel-tables) jouent pour normaliser les données et rendre le format plus compréhensible.  

Un exemple de données "ordonnées" serait la liste linéaire (linelist) utilisée dans ce manuel - chaque variable est contenue dans une colonne, chaque observation (un cas) a sa propre ligne, et chaque valeur est dans une seule cellule. Ci-dessous, vous pouvez visualiser les 50 premières lignes de la liste linéaire.:  

```{r, echo=F}
# importer la liste linéaire dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, message=FALSE, echo=F}
# afficher les données de la liste linéaire sous forme de tableau
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

*La principale raison pour laquelle on rencontre des données non ordonnées est que de nombreuses feuilles de calcul Excel sont conçues pour être lues facilement par des humains et non par des machines/logiciels.*  

Pour vous aider à voir la différence, voici quelques exemples fictifs de **données non ordonnées** qui privilégient la lisibilité *humaine* à la lisibilité *machine*.:  

```{r, echo=F, out.width = "100%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Excel_nonTidy_1.png"))
```


*Problèmes:* Dans la feuille de calcul ci-dessus, il y a des cellules *fusionnées* qui ne sont pas facilement digérées par R. La ligne qui doit être considérée comme "l'en-tête" n'est pas claire. Un dictionnaire basé sur les couleurs se trouve à droite et les valeurs des cellules sont représentées par des couleurs - ce qui n'est pas non plus facilement interprété par R (ni par les humains atteints de daltonisme !). En outre, différents éléments d'information sont combinés dans une seule cellule (plusieurs organisations partenaires travaillant dans un même domaine, ou le statut " à confirmer " dans la même cellule que " partenaire D ").  


```{r, echo=F, out.width = "100%", out.height="100%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Excel_nonTidy_2.png"))
```


*Problèmes:* Dans la feuille de calcul ci-dessus, il y a de nombreuses lignes et colonnes vides supplémentaires dans l'ensemble de données - cela causera des problèmes de nettoyage dans R. De plus, les coordonnées GPS sont réparties sur deux lignes pour un centre de traitement donné. Par ailleurs, les coordonnées GPS sont dans deux formats différents!  

Les ensembles de données "ordonnées" ne sont peut-être pas aussi lisibles à l'œil nu, mais ils facilitent grandement le nettoyage et l'analyse des données ! Les données ordonnées peuvent être stockées sous différents formats, par exemple "long" ou "large" (voir la page sur les [Données pivotées](#pivoting_data)), mais les principes ci-dessus sont toujours respectés.


### Functions {.unnumbered}  

Le mot "fonction" en R est peut-être nouveau, mais le concept existe aussi dans Excel sous la forme de *formules*. Les formules dans Excel requièrent également une syntaxe précise (par exemple, le placement des points-virgules et des parenthèses). Tout ce que vous avez à faire est d'apprendre quelques nouvelles fonctions et comment elles fonctionnent ensemble dans R.  



### Scripts {.unnumbered}  

Au lieu de cliquer sur des boutons et de faire glisser des cellules, vous allez écrire *chaque* étape et procédure dans un "script". 
Les utilisateurs d'Excel connaissent peut-être les "macros VBA" qui utilisent également une approche de script.  

*Le script R est constitué d'instructions étape par étape*, ce qui permet à tout collègue de lire le script et de voir facilement les étapes que vous avez suivies. Cela permet également de corriger les erreurs ou les calculs imprécis. Voir la section [Bases de R](#rbasics) sur les scripts pour des exemples.  

Voici un exemple de script R:  

```{r, echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "example_script.png"))
```







### Ressources Excel à R {.unnumbered}

Voici quelques liens vers des tutoriels pour vous aider à passer d'Excel à R:  

* .[R vs. Excel](https://www.northeastern.edu/graduate/blog/r-vs-excel/)  
* .[Cours RStudio en R pour les utilisateurs d'Excel](https://rstudio-conf-2020.github.io/r-for-excel/)  


### Intéraction R-Excel {.unnumbered}  

R dispose de moyens robustes pour importer des classeurs Excel, travailler avec les données, exporter/enregistrer des fichiers Excel et travailler avec les nuances des feuilles Excel.  

Il est vrai que certaines des mises en forme Excel les plus esthétiques peuvent se perdre dans la traduction (par exemple, l'italique, le texte latéral, etc.). Si votre flux de travail nécessite le passage de documents entre R et Excel tout en conservant le formatage Excel original, essayez des packages comme **openxlsx**.  







## De Stata  
<!-- ======================================================= -->

**Passer de Stata à R**  

De nombreux épidémiologistes apprennent d'abord à utiliser Stata, et le passage à R peut sembler intimidant. Cependant, si vous êtes un utilisateur de Stata à l'aise, le passage à R est certainement plus facile à gérer que vous ne le pensez. Bien qu'il existe quelques différences essentielles entre Stata et R dans la façon dont les données peuvent être créées et modifiées, ainsi que dans la façon dont les fonctions d'analyse sont mises en œuvre - après avoir appris ces différences clés, vous serez en mesure de traduire vos compétences.

Vous trouverez ci-dessous quelques traductions clés entre Stata et R, qui pourront vous être utiles lors de la lecture de ce guide.


**Notes générales**

**STATA**                    | **R**  
---------------------------- | ---------------------------------------------    
Vous ne pouvez visualiser et manipuler qu'un seul ensemble de données à la fois | Vous pouvez visualiser et manipuler plusieurs ensembles de données en même temps, vous devrez donc fréquemment spécifier votre ensemble de données dans le code
Communauté en ligne disponible via [https://www.statalist.org/](https://www.statalist.org/) | Communauté en ligne disponible via [RStudio](https://community.rstudio.com/), [StackOverFlow](https://stackoverflow.com/questions/tagged/r), et [R-bloggers](https://www.r-bloggers.com/)
Fonctionnalité "pointer et cliquer" en option | Fonctionnalité minimale de type "pointer-cliquer"
Aide pour les commandes disponibles avec `help [command]` | Aide disponible avec `[function]?` ou effectuer une recherche dans le volet d'aide (Help)
Commenter le code avec * ou /// ou  /* TEXTE */ | Commenter le code avec #
Presque toutes les commandes sont intégrées à Stata. Les fonctions nouvelles/écrites par l'utilisateur peuvent être installées en tant que fichiers **ado** en utilisant **ssc install**[package] | R s'installe avec les fonctions de **base**, mais l'utilisation typique implique l'installation d'autres packages à partir de CRAN (voir la page sur [les bases de R](#rbasics)).
L'analyse est généralement écrite dans un fichier **do** | Analyse écrite dans un script R dans le panneau source de RStudio. Les scripts R markdown sont une alternative.


**Le fichier d'accès au travail**  

**STATA**                        | **R**  
-------------------------------- | ---------------------------------------------
Les répertoires d'un travail impliquent des chemins d'accès aux fichiers absolus (e.x. "C:/utilisateur/documents/projets/data/")| Les répertoires d'un travail peuvent être soit absolus, soit relatifs au dossier racine du projet en utilisant le package **here** (voir [Import et export](#import_export)) 
Voir le répertoire où se trouve le travail actuel avec **pwd** | Utilisez `getwd()` ou `here()` (si vous utilisez le package **here**), avec des parenthèses vides 
Définir le répertoire de travail avec **cd** “emplacement du dossier” | Utilisez `setwd(“emplacement du dossier”)`, ou `set_here("emplacement du dossier)` (si le package **here** est utilisé)

**Importation et visualisation des données**  

**STATA**                    | **R**  
-------------------------------- | ---------------------------------------------
Commandes spécifiques par type de fichier | Utilisez `import()` du package **rio** pour presque tous les types de fichiers. Des fonctions spécifiques existent comme alternatives (voir [Import et export](#import_export))
La lecture des fichiers csv se fait par **import delimited** “nomdufichier.csv” | Utilisez `import("nomdufichier.csv")`
La lecture des fichiers xslx se fait par **import excel** “nomdufichier.xlsx” | Utilisez `import("nomdufichier.xlsx")`
Parcourez vos données dans une nouvelle fenêtre en utilisant la commande **browse** | Visualisez un ensemble de données dans le volet source de RStudio en utilisant `View(dataset)`. *Vous devez spécifier le nom de votre ensemble de données à la fonction dans R car plusieurs ensembles de données peuvent être maintenus en même temps. Notez le "V" majuscule dans cette fonction*
Obtenez une vue d'ensemble de votre ensemble de données à l'aide de **summarize**, qui fournit les noms des variables et les informations de base | Obtenez une vue d'ensemble de votre ensemble de données à l'aide de `summary(dataset)`

**Manipulation de données de base**  

**STATA**                    | **R**  
-------------------------------- | ---------------------------------------------
Les colonnes des ensembles de données sont fréquemment appelées "variables" | Plus souvent appelés "colonnes" ou parfois "vecteurs" ou "variables"
Il n'est pas nécessaire de spécifier l'ensemble de données | Dans chacune des commandes ci-dessous, vous devez spécifier l'ensemble de données - voir la page [Nettoyage des données et des fonctions de base](#cleaning_data) pour des exemples
Les nouvelles variables sont créées à l'aide de la commande **generate** *varname* =  | Générez de nouvelles variables en utilisant la fonction `mutate(varname = )`. Voir la page [Nettoyage des données et des fonctions de base](#cleaning_data) pour des détails sur les fonctions **dplyr**.
Les variables sont renommées en utilisant **rename** *nouveau_nom ancien_nom* | Les colonnes peuvent être renommées à l'aide de la fonction `rename(nouveau_nom = ancien_nom)`
Les variables sont supprimées en utilisant **drop** *nom_variable* | Les colonnes peuvent être supprimées en utilisant la fonction `select()` avec le nom de la colonne dans les parenthèses suivant un signe moins
Les variables factorielles peuvent être étiquetées en utilisant une série de commandes telles que **label define** | L'étiquetage des valeurs peut se faire en convertissant la colonne en classe de facteurs et en spécifiant des niveaux. Voir la page sur [Facteurs](#factors). Les noms de colonnes ne sont pas typiquement étiquetés comme ils le sont dans Stata.

**Analyse descriptive**  

**STATA**                    | **R**  
-------------------------------- | ---------------------------------------------
Mettre en tableau les effectifs d'une variable en utilisant **tab** *nom_variable* | Fournissez l'ensemble de données et le nom de la colonne à `table()` tel que `table(ensemble_de_données$nomcolonne)`. Vous pouvez également utiliser `count(nom_variable)` du package **dplyr**, comme expliqué dans [Regroupement des données](#grouping_data).
Le tableau croisé de deux variables dans un tableau 2x2 se fait avec **tab** *nom_variable1 nom_variable2* | Utilisez `table(ensemble_de_données$nom_variable1, ensemble_de_données$nom_variable2` ou `count(nom_variable1, nom_variable2)`


Bien que cette liste donne un aperçu des bases de la conversion des commandes Stata en R, elle n'est pas complète. Il existe de nombreuses autres ressources intéressantes pour les utilisateurs de Stata qui passent à R:  

* https://dss.princeton.edu/training/RStata.pdf  
* https://clanfear.github.io/Stata_R_Equivalency/docs/r_stata_commands.html  
* http://r4stats.com/books/r4stata/  




## De SAS  
<!-- ======================================================= -->

**Passer de SAS à R**  

SAS est couramment utilisé dans les agences de santé publique et les domaines de recherche universitaires. Bien que la transition vers une nouvelle langue soit rarement un processus simple, la compréhension des principales différences entre SAS et R peut vous aider à commencer à naviguer dans cette nouvelle langue en utilisant votre langue maternelle. 
Vous trouverez ci-dessous les principales traductions en matière de gestion des données et d'analyse descriptive entre SAS et R. 


**Notes générales**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Communauté en ligne disponible via [SAS Support à la clientèle](https://support.sas.com/en/support-home.html)|Communauté en ligne disponible via RStudio, StackOverFlow et R-bloggers
Aide pour les commandes disponibles par `help [command]`| Aide disponible avec [function]? ou effectuer une recherche dans le volet d'aide
Commenter le code en utilisant `* TEXTE ;` ou `/* TEXTE */`|Commenter le code en utilisant #
Presque toutes les commandes sont intégrées.  Les utilisateurs peuvent écrire de nouvelles fonctions en utilisant les macros SAS, SAS/IML, SAS Component Language (SCL) et, plus récemment, les procédures `Proc Fcmp` et `Proc Proto`|R s'installe avec les fonctions **base**, mais l'utilisation typique implique l'installation d'autres packages de CRAN (voir page [Les bases de R](#rbasics))
L'analyse est généralement effectuée en écrivant un programme SAS dans la fenêtre de l'éditeur.|Analyse écrite dans un script R dans le volet source de RStudio. Les scripts R markdown constituent une alternative.

**Le fichier d'accès au travail**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Les répertoires de travail peuvent être soit absolus, soit relatifs au dossier racine d'un projet en définissant le dossier racine à l'aide de `%let rootdir=/chemin d'accès; %include “&cheminracine/nomsousdossier/nomfichier”`|Les répertoires de travail peuvent être soit absolus, soit relatifs au dossier racine du projet en utilisant le package **here** (voir [Import et export](#import_export))
Voir le répertoire de travail actuel avec `%put %sysfunc(getoption(work));`|Utilisez `getwd()` ou `here()` (si vous utilisez le package **here**), avec des parenthèses vides
Définir le répertoire de travail avec `libname “emplacement du dossier”`|Utilisez `setwd(“emplacement du dossier”)`, ou `set_here("emplacement du dossier)` si vous utilisez le package **here**


**Importation et visualisation des données**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Utilisez la procédure `Proc Import` ou l'instruction `Data Step Infile`.|Utilisez `import()` du package **rio** pour presque tous les types de fichiers. Des fonctions spécifiques existent comme alternatives (voir [Import et export](#import_export))
La lecture des fichiers csv se fait à l'aide de la fonction `Proc Import datafile=”nom_fichier.csv” out=travail.nom_fichier dbms=CSV; run;` OU en utilisant [L'instruction Data Step Infile](http://support.sas.com/techsup/technote/ts673.pdf)|Utilisez `import("nom_fichier.csv")`
La lecture des fichiers xlsx se fait à l'aide de la fonction `Proc Import datafile=”nom_fichier.xlsx” out=travail.nom_fichier dbms=xlsx; run;` OU en utilisant [L'instruction Data Step Infile](http://support.sas.com/techsup/technote/ts673.pdf)|Utilisez import("nom_fichier.xlsx")
Parcourez vos données dans une nouvelle fenêtre en ouvrant la fenêtre de l'Explorateur et sélectionnez la bibliothèque souhaitée et l'ensemble de données|Visualiser un ensemble de données dans le panneau source de RStudio en utilisant View(dataset). Vous devez spécifier le nom de votre ensemble de données à la fonction dans R car plusieurs ensembles de données peuvent être conservés en même temps. Notez le "V" majuscule dans cette fonction


**Manipulation de données de base**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Aucune procédure spéciale n'est nécessaire pour créer une variable. Les nouvelles variables sont créées simplement en tapant le nom de la nouvelle variable, suivi d'un signe égal, puis d'une expression pour la valeur. Générez de nouvelles variables en utilisant la fonction `mutate()`. Voir la page [Nettoyage des données et fonctions de base](#cleaning_data) pour plus de détails sur toutes les fonctions **dplyr** ci-dessous.
Les variables sont renommées en utilisant `rename *ancien_nom=nouveau_nom*`|Les colonnes peuvent être renommées à l'aide de la fonction `rename(nouveau_nom = ancien_nom)`
Les variables sont conservées en utilisant `**keep**=nom_variable`|Les colonnes peuvent être sélectionnées en utilisant la fonction `select()` avec le nom de la colonne entre parenthèses.
Les variables sont supprimées à l'aide de la fonction `**drop**=nom_variable`|Les colonnes peuvent être supprimées à l'aide de la fonction `select()` avec le nom de la colonne entre parenthèses après le signe moins.
Les variables factorielles peuvent être étiquetées dans l'étape de données (Data Step) en utilisant l'instruction `Label`. L'étiquetage des valeurs peut être fait en convertissant la colonne en classe factorielle et en spécifiant les niveaux. Voir la page sur les [Facteurs](#factors). Les noms de colonnes ne sont généralement pas étiquetés.
Les enregistrements sont sélectionnés à l'aide des instructions `Where` ou `If` dans l'étape de données (Data Step). Les enregistrements sont sélectionnés à l'aide de la fonction `filter()` avec plusieurs conditions de sélection séparées soit par un opérateur AND (&), soit par une virgule. 
Les ensembles de données sont combinés en utilisant l'instruction `Merge` dans l'étape Data Step. Les jeux de données à fusionner doivent d'abord être triés à l'aide de la procédure `Proc Sort`. Le package `**dplyr** offre quelques fonctions pour fusionner les jeux de données. Voir la page [Joindre des données](#joining_matching) pour plus de détails.

**Analyse descriptive**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Obtenez un aperçu de votre ensemble de données en utilisant la procédure `Proc Summary`, qui fournit les noms des variables et les statistiques descriptives|Obtenez un aperçu de votre ensemble de données en utilisant `summary(dataset)` ou `skim(dataset)` du package **skimr**
Mettre en tableau les effectifs d'une variable en utilisant `proc freq data=Dataset; Tables nom_variable; Run;`|Voir la page sur les [Tableaux descriptifs](#descriptive_tables)Les options incluent `table()` de R de **base**, et `tabyl()` du package **janitor**, entre autres. Notez que vous devrez spécifier le jeu de données et le nom de la colonne, car R contient plusieurs jeux de données.
Le tableau croisé de deux variables dans un tableau 2x2 est fait avec `proc freq data=Dataset ; Tables rowvar*colvar ; Run;`|Aussi, vous pouvez utiliser `table()`, `tabyl()` ou d'autres options comme décrit dans la page [Tableaux descriptifs](#descriptive_tables).  

**Quelques ressources utiles:**  

[R for SAS and SPSS Users (2011)](https://www.amazon.com/SAS-SPSS-Users-Statistics-Computing/dp/1461406846/ref=sr_1_1?dchild=1&gclid=EAIaIQobChMIoqLOvf6u7wIVAhLnCh1c9w_DEAMYASAAEgJLIfD_BwE&hvadid=241675955927&hvdev=c&hvlocphy=9032185&hvnetw=g&hvqmt=e&hvrand=16854847287059617468&hvtargid=kwd-44746119007&hydadcr=16374_10302157&keywords=r+for+sas+users&qid=1615698213&sr=8-1)

[SAS and R, Second Edition (2014)](https://www.amazon.com/SAS-Management-Statistical-Analysis-Graphics-dp-1466584491/dp/1466584491/ref=dp_ob_title_bk)



## Interopérabilité des données 
<!-- ======================================================= -->

Voir la page [Import et export](#import_export) pour des détails sur comment le package **rio** peut importer et experter des fichiers tels que des fichiers STATA .dta, des fichiers SAS .xpt et.sas7bdat, des fichiers SPSS .por et.sav, et plusieurs autres.  



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/transition_to_R.Rmd-->

# Paquets conseillés {#suggested_packages}

Vous trouverez ci-dessous une longue liste de paquets suggérés réaliser des tâches utiles lors d'analyses épidémiologies en R. Vous pouvez copier ce code, l'exécuter, et tous ces paquets seront installés à partir du CRAN et chargés pour être utilisés dans la session R actuelle. Si un paquet est déjà installé, il sera importé pour être utilisé mais pas réinstallé.  

Vous pouvez modifier le code avec les symboles `#` pour exclure les paquets que vous ne voulez pas.  


A noter :  

* Installez le paquet **pacman** avant d'exécuter le code ci-dessous. Vous pouvez le faire avec `install.packages("pacman")`. Dans ce guide, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* l'importe dans la session R actuelle. Vous pouvez également charger des paquets qui sont déjà installés avec `library()` depuis **base** R.  
* Dans le code ci-dessous, les paquets qui sont inclus lors de l'installation/de l'import d'un autre paquet sont indiqués par une indentation et un dièse. Par exemple, **ggplot2** est listé sous **tidyverse**.  
* Si plusieurs paquets ont des fonctions portant le même nom, un *masquage* peut se produire lorsque la fonction du paquet le plus récemment chargé prend le dessus. Pour en savoir plus, consultez la page [sur les bases de R](#rbasics). Vous pouvez utiliser le paquet **conflicted** pour gérer de tels conflits de manière explicite.  
* Voir la section [sur les bases de R](#rbasics) sur les paquets pour plus d'informations sur **pacman** et le masquage.  


Pour voir les versions de R, RStudio et les paquets R utilisés lors de la production de ce manuel, voir la page sur les [Notes techniques et choix éditoriaux](#editorial_notes). 


## Paquets disponibles sur le CRAN

```{r, eval=F}

##############################################
# Liste de paquets R utiles en épidémiologie #
##############################################

# Ce script utilise la fonction p_load() du paquet **pacman**, 
# qui installe le paquet si ce dernier n'est pas encore installé sur
# l'ordinateur, et l'importe dans la session active pour l'utiliser 
# s'il est déjà installé.


# S'assure de l'installation du paquet "pacman".
if (!require("pacman")) install.packages("pacman")


#  Paquets du CRAN
##############################
pacman::p_load(
     
     # Apprendre R
     ############
     learnr,   # tutos interactifs dans le volet tutos de RStudio
     swirl,    # tutoriels interactifs dans la console R
        
     # Gestion des projets et des dossiers
     #############################
     here,     # chemins de fichiers relatifs au dossier racine du projet R
     rio,      # import/export de nombreux types de données
     openxlsx, # import/export de classeurs Excel à feuilles multiples 
     
     # Installation et gestion des paquets
     ################################
     pacman,   # installation et importation des paquets
     renv,     # gérer les versions des paquets lors de collaborations
     remotes,  # installer des paquets provenant de Github
     
     # Paquets généralistes pour gérer les données
     #########################
     tidyverse,    # méta-paquet qui comprend de nombreux paquets pour le traitement et la présentation des données.
          # dplyr : gestion des données
          # tidyr : gestion des données
          # ggplot2 : visualisation de données
          # stringr : travailler avec des chaînes de caractères et des caractères
          # forcats : travailler avec des facteurs 
          # lubridate : travailler avec des dates
          # purrr : itération et travail avec des listes

     linelist,     # nettoyage de linelists
     naniar,       # évaluation des données manquantes
     
     # Statistiques
     ############
     janitor,      # nettoyage des données
     gtsummary,    # création de tableaux descriptifs et statistiques
     rstatix,      # exécution rapide de tests et de résumés statistiques
     broom,        # nettoyage des résultats des régressions
     lmtest,       # likelihood-ratio tests
     easystats,
          # parameters, # alternative pour ordonner les résultats des régressions
          # see,        # alternative pour visualiser les forest plots
     
     # modélisation épidémiologique
     ###################
     epicontacts,    # Analyse des réseaux de transmission
     EpiNow2,        # Estimation de Rt
     EpiEstim,       # Estimation Rt
     projections,    # Projections d'incidence
     incidence2,     # Création d'épicurves et traitement des données d'incidence
     i2extras,       # Fonctions supplémentaires pour le paquet incidence2
     epitrix,        # Fonctions epi utiles
     distcrete,      # Distributions discrètes de délais
     
     
     # graphiques - general
     #################
     #ggplot2,     # inclus dans tidyverse
     cowplot,      # combinaison de graphiques  
     patchwork,  # combinaison de graphiques (alternative à cowplot)     
     RColorBrewer, # échelles de couleurs
     ggnewscale,   # pour ajouter des couches supplémentaires de schémas de couleurs

     # graphiques - types spécifiques
     ########################
     DiagrammeR,  # diagrammes utilisant le langage DOT
     incidence2,  # courbes épidémiques
     gghighlight, # mettre en évidence un sous-ensemble
     ggrepel,     # étiquettes intelligentes
     plotly,      # graphiques interactifs
     gganimate,   # graphiques animés 
     

     # SIG
     ######
     sf,            # pour gérer les données spatiales en utilisant un format Simple Feature
     tmap,          # pour produire des cartes simples, fonctionne à la fois pour les cartes interactives et statiques
     OpenStreetMap, # pour ajouter la carte de base OSM dans la carte ggplot
     spdep,         # statistiques spatiales 
     
     # Rapports automatisés
     #################
     rmarkdown,     # produit des PDFs, des documents Word, des Powerpoints, et des fichiers HTML
     reportfactory, # auto-organisation des sorties R Markdown
     officer,       # création de documents word et powerpoints (alternative à Rmarkdown)
     
     # Tableaux de bord
     ############
     flexdashboard, # création de tableaux de bord (syntaxe Rmarkdown)
     shiny,         # applications web interactives
     
     # Créer des tableaux pour présenter des résultats
     #########################
     knitr,       # Génération de rapports R Markdown et tableaux html
     flextable,   # Tableaux HTML
     # DT,        # Tableaux HTML (alternative)
     # gt,        # Tableaux HTML (alternative)
     # huxtable,  # Tableaux HTML (alternative) 
     
     # Phylogenetique
     ###############
     ggtree,  # visualisation et annotation d'arbres phylogénétiques
     ape,     # analyse phylogénétique et évolutive
     treeio,  # visualision des fichiers phylogénétiques
 
)

```

## Paquets hébergés sur Github 


Vous trouverez ci-dessous les commandes pour installer des paquets directement depuis leur répertoire sur Github.  

* La version de développement de **epicontacts** contient la possibilité de faire des arbres de transmission avec un axe x temporel.  
* Le paquet **epirhandbook** contient toutes les données d'exemple pour ce manuel et peut être utilisé pour télécharger la version hors ligne du manuel.  


```{r, eval=F}
# Paquets à télécharger depuis Github (non disponibles sur CRAN)
##########################################################

# Version de développement d'epicontacts (pour les chaînes de transmission avec un axe x temporel)
pacman::p_install_gh("reconhub/epicontacts@timeline")

# Le paquet pour ce manuel, qui comprend toutes les données d'exemple 
pacman::p_install_gh("appliedepi/epirhandbook")

```

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/packages_suggested.Rmd-->


# Projets R {#r_projects}  


Un projet R permet de regrouper votre travail dans un dossier portable et autonome. A l'intérieur du projet, tous les scripts, fichiers de données, figures/sorties et historiques pertinents sont stockés dans des sous-dossiers et, surtout, le *répertoire de travail* est le dossier racine du projet.   


## Utilisation suggérée  

Une façon courante, efficace et sans probléme d'utiliser R consiste à combiner ces 3 éléments. Un projet de travail discret est hébergélé dans un projet R. Chaque élément est décrit dans les sections ci-dessous.  

1) Un **projet R**  
     - Un environnement de travail autonome avec des dossiers pour les données, les scripts, les résultats, etc.    
2) Le paquet **here** pour les chemins de fichiers relatifs  
     - Les chemins de fichiers sont écrits par rapport au dossier racine du projet R - voir [Importation et exportation](#import_export) pour plus d'informations.  
3) Le paquet **rio** pour les importations/exportations  
     - `import()` et `export()` traitent tout type de fichier par son extension (par exemple .csv, .xlsx, .png).  
     


<!-- ======================================================= -->
## Créer un projet R {}

Pour créer un projet R, sélectionnez "Nouveau projet" dans le menu Fichier.

* Si vous voulez créer un nouveau dossier pour le projet, sélectionnez "Nouveau répertoire" et indiquez où vous voulez qu'il soit créé.  
* Si vous voulez créer le projet dans un dossier existant, cliquez sur "Répertoire existant" et indiquez le dossier.  
* Si vous voulez cloner un dépôt Github, sélectionnez la troisiéme option "Version Control" et ensuite "Git". Voir la page [contrôle de version et collaboration avec Git et Github](#collaboration) pour plus de détails. 


```{r out.width = "75%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "create_project.png"))
```

Le projet R que vous créez se présente sous la forme d'un dossier contenant un fichier *.Rproj*. Ce fichier est un raccourci et probablement la principale façon d'ouvrir votre projet. Vous pouvez également ouvrir un projet en sélectionnant "Ouvrir un projet" dans le menu Fichier. Alternativement, sur le côté supérieur droit de RStudio, vous verrez une icône de projet R et un menu déroulant des projets R disponibles. 

Pour quitter un projet R, vous pouvez soit ouvrir un nouveau projet, soit fermer le projet (Fichier - Fermer le projet).  


### Changer de projet {.unnumbered}

Pour passer d'un projet à l'autre, cliquez sur l'icône et le menu déroulant du projet R tout en haut à droite de RStudio. Vous verrez les options Fermer le projet, Ouvrir le projet, et une liste de projets récents. 

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "Rproject_dropdown.png"))
```


### Paramétres {.unnumbered}  

Il est généralement conseillé de démarrer RStudio à chaque fois avec une "ardoise propre" - c'est-à-dire avec votre espace de travail **non** préservé de votre session précédente. Cela signifie que vos objets et résultats ne persisteront pas d'une session à l'autre (vous devrez les recréer en exécutant vos scripts). C'est une bonne chose, car cela vous obligera à écrire de meilleurs scripts et à éviter les erreurs à long terme.  

Pour configurer RStudio de maniére à ce qu'il fasse "table rase" à chaque démarrage :  

* Sélectionnez "Options du projet" dans le menu Outils.  
* Dans l'onglet "Général", configurez RStudio pour **ne pas** restaurer les .RData dans l'espace de travail au démarrage, et pour **ne pas** sauvegarder l'espace de travail en .RData à la sortie.  



### Organisation {.unnumbered}  

Il est courant d'avoir des sous-dossiers dans votre projet. Pensez à avoir des dossiers tels que "données", "scripts", "figures", "présentations". Vous pouvez ajouter des dossiers de la même maniére que vous ajouteriez un nouveau dossier sur votre ordinateur. Vous pouvez également consulter la page sur les [Interactions avec les répertoires](#directories) pour apprendre à créer de nouveaux dossiers à l'aide de commandes R.  


### Contrôle de version {.unnumbered}  

Pensez à un systéme de contrôle de version. Cela pourrait étre quelque chose d'aussi simple que d'avoir des dates sur les noms des scripts (par exemple "transmission_analysis_2020-10-03.R") et un dossier "archive". Vous pouvez également envisager d'avoir un texte d'en-téte commenté en haut de chaque script avec une description, des balises, des auteurs et un journal des modifications.  

Une méthode plus complexe consisterait à utiliser Github ou une plateforme similaire pour le contrôle de version. Voir la page [contrôle de version et collaboration avec Git et Github](#collaboration).  

Une astuce : vous pouvez effectuer une recherche dans l'ensemble d'un projet ou d'un dossier à l'aide de l'outil "Rechercher dans les fichiers" (menu édition). Il peut rechercher et même remplacer des chaînes de caractères dans plusieurs fichiers.  






## Exemples  

Voici quelques exemples d'importation/exportation/sauvegarde utilisant `here()` à partir d'un projet R. Pour en savoir plus sur l'utilisation du paquet **here**, consultez la page [Import and export](#import_export).  

*Importer `linelist_raw.xlsx` du dossier "data" de votre projet R*

```{r eval=F}
linelist <- import(here("data", "linelist_raw.xlsx"))
```

*Exportation de l'objet R `linelist` en tant que "my_linelist.rds" dans le dossier "clean" du dossier "data" de votre projet R.*   

```{r, eval=F}
export(linelist, here("data","clean", "my_linelist.rds"))
```

*Enregistrement du tracé le plus récemment imprimé sous le nom de "epicurve_2021-02-15.png" dans le dossier "epicurves" du dossier "outputs" de votre projet R.*  


```{r, eval=F}
ggsave(here("outputs", "epicurves", "epicurve_2021-02-15.png"))
```



<!-- ======================================================= -->



## Ressources {}

Page web de RStudio sur [l'utilisation de projets R](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects)

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/r_projects.Rmd-->

# Importer et exporter des données {#import_export}

```{r, out.width=c('100%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "Import_Export_1500x500.png"))
```

Sur cette page, nous décrivons les moyens de localiser, d'importer et d'exporter des fichiers :

-   Utilisation du package **rio** pour `import()` et `export()` de manière flexible de nombreux types de fichiers

-   Utilisation du package **here** pour localiser des fichiers relatifs à la racine d'un projet R - pour prévenir des complications liées aux chemins d'accès de fichiers qui sont spécifiques à un ordinateur

-   Des scénarios d'importation spécifiques, tels que :

    -   Feuilles d'Excel spécifiques\

    -   En-têtes désordonnés et lignes sautées

    -   Feulles de calcul Google\

    -   À partir des données publiées sur les sites web\

    -   Avec des APIs\

    -   Importer le *plus récent* fichier\

-   Saisie manuelle des données\

-   Type de fichier spécifique à R tels que RDS et RData\

-   Exportation/sauvegarde des fichiers et des graphiques

<!-- ======================================================= -->

## Aperçu

Lorsque vous importez un "jeu de données" dans R, vous créez généralement un nouvel objet appelé *data frame* dans votre environnement R et vous le définissez comme un fichier importé (par exemple Excel, CSV, TSV, RDS) qui se trouve dans votre répertoire de dossiers à un certain chemin d'accès au fichier.

Vous pouvez importer/exporter de nombreux types de fichiers, y compris ceux créés par d'autres programmes statistiques (SAS, STATA, SPSS). Vous pouvez également vous connecter à des bases de données relationnelles.

R a même ses propres formats de données :

-   Un fichier RDS (.rds) stocke un seul objet R tel qu'un dataframe. Ils sont utiles pour stocker des données nettoyées, car ils conservent les classes de colonnes R. Pour en savoir plus, consultez [cette section](#import_rds).
-   Un fichier RData (.Rdata) peut être utilisé pour stocker plusieurs objets, voire un espace de travail R complet. Pour en savoir plus, consultez [cette section](#import_rdata).

<!-- ======================================================= -->

## Le package **rio**

Le paquet R que nous recommandons est : **rio**. Le nom "rio" est une abréviation de "R I/O" (input/output).

Ses fonctions `import()` et `export()` peuvent traiter de nombreux types de fichiers différents (par exemple .xlsx, .csv, .rds, .tsv). Lorsque vous fournissez un chemin d'accès au fichier à l'une de ces fonctions (y compris l'extension de fichier comme ".csv"), **rio** lira l'extension et utilisera le bon outil pour importer ou exporter le fichier.

L'alternative à l'utilisation de **rio** est d'utiliser les fonctions de nombreux autres packages, chacun d'entre eux étant spécifique à un type de fichier. Par exemple, `read.csv()` (**base** R), `read.xlsx()` (**openxlsx** package), et `write_csv()` (**readr** pacakge), etc. Ces alternatives peuvent être difficiles à mémoriser, alors qu'utiliser `import()` et `export()` de **rio** est facile.

Les fonctions `import()` et `export()` de **rio** utilisent le package et la fonction appropriés pour un fichier donné, en se basant sur son extension. Consultez la fin de cette page pour un tableau complet des paquets/fonctions que **rio** utilise en arrière-plan. Il peut également être utilisé pour importer des fichiers STATA, SAS, et SPSS, parmi des dizaines d'autres types de fichiers.

L'importation/exportation de shapefile nécessite d'autres packages, tel que détaillé dans la page sur [Introduction aux SIG](#GIS).

## Le package **here** {#here}

Le package **here** et sa fonction `here()` permettent d'indiquer facilement à R où trouver et enregistrer vos fichiers - en fait, il construit les chemins d'accès aux fichiers.

Utilisé en conjonction avec un projet R, **here** vous permet de décrire l'emplacement des fichiers dans votre projet R par rapport au *répertoire racine* du projet R (le dossier de niveau supérieur). Cela est utile lorsque le projet R peut être partagé ou accessible par plusieurs personnes/ordinateurs. Cela évite les complications dues aux chemins d'accès aux fichiers uniques sur différents ordinateurs (par exemple `"C:/Users/Laura/Documents..."` en "commençant" le chemin d'accès à un endroit commun pour tous les utilisateurs (la racine du projet R).

Voici comment `here()` fonctionne dans un projet R :

-   Lorsque le package **here** est chargé pour la première fois dans le projet R, il place un petit fichier appelé ".here" dans le dossier racine de votre projet R comme "repère" ou "ancre".\
-   Dans vos scripts, pour référencer un fichier dans les sous-dossiers du projet R, vous utilisez la fonction `here()` pour construire le chemin d'accès au fichier *en relation avec cette ancre*
-   Pour construire le chemin d'accès au fichier, écrivez les noms des dossiers au-delà de la racine, entre guillemets, séparés par des virgules, et terminez par le nom du fichier et son extension, comme indiqué ci-dessous.\
-   Les chemins d'accès `here()` peuvent être utilisés à la fois pour l'importation et l'exportation.

Par exemple, ci-dessous, la fonction `import()` reçoit un chemin d'accès construit avec `here()`.

```{r, eval=F}
linelist <- import(here("data", "linelists", "ebola_linelist.xlsx"))
```

La commande `here("data", "linelists", "ebola_linelist.xlsx")` fournit en fait le chemin d'accès complet au fichier qui est *unique à l'ordinateur de l'utilisateur* :

    "C:/Users/Laura/Documents/my_R_project/data/linelists/ebola_linelist.xlsx"

L'avantage est que la commande R utilisant `here()` peut être exécutée avec succès sur n'importe quel ordinateur accédant au projet R.

[***TIP:*** Si vous n'êtes pas certaine de l'emplacement de la racine "here", exécutez la fonction `here()` avec des parenthèses vides. ]{style="color: darkgreen;"}

Plus d'informations sur le package **ici** [en cliquant sur ce lien](https://here.r-lib.org/).

<!-- ======================================================= -->

## Chemins d'accès aux fichiers

Lorsque vous importez ou exportez des données, vous devez fournir un chemin d'accès au fichier. Vous pouvez le faire de trois manières différentes:

1)  *Recommandé:* fournir un chemin d'accès "relatif" avec le package **here**
2)  Fournir le chemin d'accès "complet" / "absolu"
3)  Sélection manuelle des fichiers

### Chemins d'accès "relatifs" {.unnumbered}

Dans R, les chemins d'accès "relatifs" consistent en un chemin d'accès *relatif* à la racine d'un projet R. Ils permettent d'obtenir des chemins d'accès plus simples qui peuvent fonctionner sur différents ordinateurs (par exemple, si le projet R se trouve sur un disque partagé ou est envoyé par courrier électronique). Comme décrit [ci-dessus](#here), les chemins de fichiers relatifs sont facilités par l'utilisation du package **here**.

Voici un exemple de chemin d'accès relatif construit avec `here()`. Nous supposons que le travail se trouve dans un projet R qui contient un sous-dossier "data" et dans celui-ci un sous-dossier "linelists", dans lequel se trouve le fichier .xlsx qui nous intéresse.

```{r, eval=F}
linelist <- import(here("data", "linelists", "ebola_linelist.xlsx"))
```

### Chemins d'accès "absolus" {.unnumbered}

Les chemins d'accès absolus ou "complets" peuvent être fournis à des fonctions comme `import()` mais ils sont "fragiles" car ils sont uniques à l'ordinateur spécifique de l'utilisateur et donc *non recommandés*.

Vous trouverez ci-dessous un exemple de chemin d'accès absolu à un fichier. Dans l'ordinateur de Laura, il existe un dossier "analysis", un sous-dossier "data" et un sous-dossier "linelists", dans lequel se trouve le fichier .xlsx en question.

```{r, eval=F}
linelist <- import("C:/Users/Laura/Documents/analysis/data/linelists/ebola_linelist.xlsx")
```

Quelques points à noter concernant les chemins d'accès absolus aux fichiers:

-   **Évitez les chemins d'accès absolus** parce qu'ils vont interrompre le script si ils sont utilisés sur un autre ordinateur différent
-   Utilisez des barres obliques (/), comme dans l'exemple ci-dessus (remarque : ce n'est PAS l'option par défaut pour les chemins d'accès aux fichiers de Windows).
-   Les chemins de fichiers qui commencent par des doubles barres obliques (par exemple, "//...") ne seront probablement pas reconnus par R et produiront une erreur. Pensez à déplacer votre travail sur un lecteur "nommé" ou "lettré" qui commence par une lettre (par exemple "J :" ou "C :"). Voir la page sur les [Interactions avec les répertoires](#directories) pour plus de détails sur cette question.

Un scénario dans lequel les chemins de fichier absolus peuvent être appropriés est celui où vous voulez importer un fichier d'un lecteur partagé qui a le même chemin d'accès complet pour tous les utilisateurs.

[***TIP:*** Pour convertir rapidement tous les `\ à` `/`, surlignez le code d'intérêt, utilisez Ctrl+f (avec Windows), sélectionnez l'option "Dans la sélection", et ensuite utilisez la fonction remplacement pour les convertir.]{style="color: darkgreen;"}

<!-- ======================================================= -->

### Sélectionner les fichier manuellement {.unnumbered}

Vous pouvez importer des données de façon manuelle via une de ces méthodes:

1)  Dans le volet Environnement de RStudio, cliquez sur "Import Dataset", et sélectionnez le type de données
2)  Cliquez sur File / Import Dataset / (sélectionnez le type de données)\
3)  Pour une sélection manuelle complète, utilisez la commande `file.choose()` de la *base R* (en laissant les parenthèses vides) pour d'éclencher l'apparition d'une **fenêtre pop-up** qui permet à l'utilisateur de sélectionner manuellement le fichier sur son ordinateur. Par exemple :

```{r import_choose, eval=F}
# Sélection manuelle d'un fichier. Lorsque cette commande est exécutée, une fenêtre POP-UP apparaîtra.

# Le chemin du fichier sélectionné sera fourni à la commande import().

my_data <- import(file.choose())
```

[***TIP:*** Il est possible que la **fenêtre pop-up** apparaisse DERRIÈRE votre fenêtre de RStudio.]{style="color: darkgreen;"}

## Importer des données

Il est assez simple d'utiliser `import()` pour importer un ensemble des données. Il suffit de fournir le chemin d'accès au fichier (y compris le nom et l'extension du fichier) entre guillemets. Si vous utilisez `here()` pour construire le chemin d'accès au fichier, suivez les insTIPtions ci-dessus. Voici quelques exemples:

Importation d'un fichier csv situé dans votre "répertoire de travail" ou dans le dossier racine du projet R:

```{r, eval=F}
linelist <- import("linelist_cleaned.csv")
```

Importation de la première feuille d'un  Excel qui se trouve dans les sous-dossiers "data" et "linelists" du projet R (le chemin du fichier construit à l'aide de `here()`):

```{r, eval=F}
linelist <- import(here("data", "linelists", "linelist_cleaned.xlsx"))
```

Importing d'un data frame (un fichier .rds) à l'aide d'un chemin d'accès absolu:

```{r, eval=F}
linelist <- import("C:/Users/Laura/Documents/tuberculosis/data/linelists/linelist_cleaned.rds")
```

### Feuilles spécifiques d'Excel {.unnumbered}

Par défaut, si vous fournissez un fichier Excel (.xlsx) à import(), la première feuille du fichier sera importée. Si vous voulez importer une **feuille** spécifique, incluez le nom de la feuille dans l'argument which =. Par exemple:

```{r eval=F}
my_data <- import("my_excel_file.xlsx", which = "Sheetname")
```

Si vous utilisez la méthode `here()` pour fournir un chemin d'accès relatif à `import()`, vous pouvez toujours indiquer une feuille spécifique en ajoutant l'argument which = après les parenthèses de fermeture de la fonction `here()`.

```{r import_sheet_here, eval=F}
# Demonstration: importing a specific Excel sheet when using relative pathways with the 'here' package
linelist_raw <- import(here("data", "linelist.xlsx"), which = "Sheet1")`  
```

Pour *exporter* un data frame à partir de R vers une feuille Excel spécifique tout en gardant inchangé le reste du fichier Excel, vous devrez importer, modifier et exporter avec un package alternatif spécifique pour cet objectif tel que **openxlsx**. Pour davantage d'information consultez la page  [Interactions avec les répertoires](#directories) ou [cette page github](https://ycphs.github.io/openxlsx/).

Si votre fichier Excel est .xlsb (fichier Excel en format binaire) il est possible que vous ne puissiez pas l'importer en utilisant **rio**. Envisagez de le réenregistrer en format .xlsx, ou bien en utilisant un package tel que **readxlsb** qui fut conçu  [à cet effet](https://cran.r-project.org/web/packages/readxlsb/vignettes/read-xlsb-workbook.html).

<!-- ======================================================= -->

### Données manquantes {#import_missing .unnumbered}

Vous pouvez désigner la ou les valeurs de votre ensemble de données qui doivent être considérées comme manquantes. Comme expliqué dans la page [Données manquantes](#missing_data), la valeur dans R pour les données manquantes est `NA`, mais peut-être que l'ensemble de données que vous voulez importer utilise 99, "Manquant", ou juste un espace vide "" à la place.

Utilisez l'argument `na =` avec `import()` et fournissez la ou les valeurs entre guillemets (même si ce sont des nombres). Vous pouvez spécifier plusieurs valeurs en les incluant dans un vecteur, en utilisant `c()` comme indiqué ci-dessous.

Ici, la valeur "99" dans le jeu de données est considéré comme manquant et converti à `NA` dans R.

```{r, eval=F}
linelist <- import(here("data", "my_linelist.xlsx"), na = "99")
```

Ici, toutes les valeurs "Missing", "" (cellule vide), ou " " (espace unique) dans l'ensemble de données importées sont converties en `NA` dans R.

```{r, eval=F}
linelist <- import(here("data", "my_linelist.csv"), na = c("Missing", "", " "))
```

<!-- ======================================================= -->

### Sauter des lignes {.unnumbered}

Parfois, vous pouvez vouloir éviter d'importer une ligne de données. Vous pouvez le faire avec l'argument `skip =` si vous utilisez `import()` de **rio** sur un fichier .xlsx ou .csv. Indiquez le nombre de lignes que vous souhaitez ignorer.

```{r, eval=F}
linelist_raw <- import("linelist_raw.xlsx", skip = 1)  # n'importe pas la ligne d'en-tête
```

Malheureusement, `skip =` n'accepte qu'une seule valeur entière, *pas* une plage (par exemple, "2:10" ne fonctionne pas). Pour sauter l'importation de lignes spécifiques qui ne sont pas consécutives à partir du haut, pensez à importer plusieurs fois et à utiliser `bind_rows()` de **dplyr**. Voir l'exemple ci-dessous pour sauter seulement la ligne 2.

### Gérer une deuxième ligne d'en-tête {.unnumbered}

Parfois, vos données peuvent avoir une *deuxième* ligne, par exemple s'il s'agit d'une ligne de "dictionnaire de données" comme indiqué ci-dessous. Cette situation peut être problématique car elle peut entraîner l'importation de toutes les colonnes en tant que classe "caractère".

```{r, echo=F}
# HIDDEN FROM READER
####################
# Create second header row of "data dictionary" and insert into row 2. Save as new dataframe.
linelist_2headers <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds")) %>%         
        mutate(across(everything(), as.character)) %>% 
        add_row(.before = 1,
                #row_num = "000",
                case_id = "case identification number assigned by MOH",
                generation = "transmission chain generation number",
                date_infection = "estimated date of infection, mm/dd/yyyy",
                date_onset = "date of symptom onset, YYYY-MM-DD",
                date_hospitalisation = "date of initial hospitalization, mm/dd/yyyy",
                date_outcome = "date of outcome status determination",
                outcome = "either 'Death' or 'Recovered' or 'Unknown'",
                gender = "either 'm' or 'f' or 'unknown'",
                hospital = "Name of hospital of first admission",
                lon = "longitude of residence, approx",
                lat = "latitude of residence, approx",
                infector = "case_id of infector",
                source = "context of known transmission event",
                age = "age number",
                age_unit = "age unit, either 'years' or 'months' or 'days'",
                fever = "presence of fever on admission, either 'yes' or 'no'",
                chills = "presence of chills on admission, either 'yes' or 'no'",
                cough = "presence of cough on admission, either 'yes' or 'no'",
                aches = "presence of aches on admission, either 'yes' or 'no'",
                vomit = "presence of vomiting on admission, either 'yes' or 'no'",
                time_admission = "time of hospital admission HH:MM")
```

Voici un exemple de ce type d'ensemble de données (la première ligne étant le dictionnaire de données).

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist_2headers, 5), rownames = FALSE, filter="top", options = list(pageLength = 4, scrollX=T), class = 'white-space: nowrap' )
```

#### Supprimez la deuxième ligne d'en-tête {.unnumbered}

Pour supprimer la deuxième ligne d'en-tête, vous devrez probablement importer les données deux fois.

1)  Importez les données afin de stocker les bons noms des colonnes.
2)  Importez à nouveau les données, en sautant les *deux* premières lignes (en-tête et deuxième ligne).
3)  Reliez les noms corrects sur le cadre de données réduit.

L'argument exact utilisé pour relier les bons noms de colonnes dépend du type de fichier de données (.csv, .tsv, .xlsx, etc.). Ceci est dû au fait que **rio** utilise une fonction différente pour les différents types de fichiers (voir le tableau ci-dessus).

**Pour les documents Excel:** (`col_names =`)

```{r, eval=F}
# importer pour la première fois; stocker les noms des colonnes
linelist_raw_names <- import("linelist_raw.xlsx") %>% names()  # stocker les bons noms des colonnes

# importer pour la deuxième fois; sauter la deuxième ligne, et assigner les noms des colonnes à l'argument col_names =
linelist_raw <- import("linelist_raw.xlsx",
                       skip = 2,
                       col_names = linelist_raw_names
                       ) 
```

**Pour les fichiers CSV:** (`col.names =`)

```{r, eval=F}
# importer pour la première fois; stocker les noms des colonnes
linelist_raw_names <- import("linelist_raw.csv") %>% names() # save true column names

# notez que l'argument pour les fichiers CSV est 'col.names = '
linelist_raw <- import("linelist_raw.csv",
                       skip = 2,
                       col.names = linelist_raw_names
                       ) 
```

**Option de secours** - changer les noms des colonnes à l'aide d'une commande additionnelle

```{r, eval=F}
# attribuer/supprimer des en-têtes en utilisant la fonction de base 'colnames()'
colnames(linelist_raw) <- linelist_raw_names
```

#### Créer un dictionnaire de données {.unnumbered}

Bonus! Si vous avez une deuxième ligne qui est un dictionnaire de données, vous pouvez facilement créer un dictionnaire de données approprié à partir de celle-ci. Cette astuce est adaptée de cet  [article](https://alison.rbind.io/post/2018-02-23-read-multiple-header-rows/).

```{r}
dict <- linelist_2headers %>%             # début: liste des case avec le dictionnaire en première ligne
  head(1) %>%                             # garder seulement les noms des colonnes et la premièere ligne étant le dictionnaire                
  pivot_longer(cols = everything(),       # pivoter toutes les colonnes au format long
               names_to = "Column",       # définir de nouveaux noms pour les colonnes
               values_to = "Description")
```

```{r message=FALSE, echo=F}
DT::datatable(dict, rownames = FALSE, filter="top", options = list(pageLength = 4, scrollX=T), class = 'white-space: nowrap' )
```

#### Combiner les deux lignes d'en-tête {.unnumbered}

Dans certains cas, lorsque votre ensemble de données brutes comporte *deux* lignes d'en-tête (ou plus précisément, la deuxième ligne de données est un en-tête secondaire), vous pouvez souhaiter les "combiner" ou ajouter les valeurs de la deuxième ligne d'en-tête à la première ligne d'en-tête.

La commande ci-dessous définit les noms des colonnes du cadre de données comme la combinaison (collage) des premiers en-têtes (vrais) avec la valeur située immédiatement en dessous (dans la première ligne).

```{r, eval=F}
names(my_data) <- paste(names(my_data), my_data[1, ], sep = "_")
```

<!-- ======================================================= -->

### Feuille de calcul Google {.unnumbered}

Vous pouvez importer des données à partir d'une feuille de calcul Google en ligne avec le paquet **googlesheet4** et en authentifiant votre accès à la feuille de calcul.

```{r, eval=F}
pacman::p_load("googlesheets4")
```

Ci-dessous, une feuille de calcul Google de démonstration est importée et sauvegardée. Cette commande peut vous demander de confirmer l'authentification de votre compte Google. Suivez les insTIPtions et les fenêtres contextuelles de votre navigateur Internet pour accorder aux packages API Tidyverse les autorisations de modifier, créer et supprimer vos feuilles de calcul dans Google Drive.

La feuille ci-dessous est "consultable par toute personne ayant le lien" et vous pouvez essayer de l'importer.

```{r, eval=F}
Gsheets_demo <- read_sheet("https://docs.google.com/spreadsheets/d/1scgtzkVLLHAe5a6_eFQEwkZcc14yFUx1KgOMZ4AKUfY/edit#gid=0")
```

La feuille ci-dessous est "consultable par toute personne ayant le lien" et vous pouvez essayer de l'importer.

```{r, eval=F}
Gsheets_demo <- read_sheet("1scgtzkVLLHAe5a6_eFQEwkZcc14yFUx1KgOMZ4AKUfY")
```

Un autre package, **googledrive**, propose des fonctions utiles pour écrire, modifier et supprimer des feuilles Google. Par exemple, en utilisant les fonctions `gs4_create()` et `sheet_write()` trouvées dans ce package.

Voici d'autres tutoriels en ligne utiles:\
[Tutoriel de base sur l'importation de feuilles de calcul Google sheets](https://arbor-analytics.com/post/getting-your-data-into-r-from-google-sheets/)\
[tutoriel plus détaillé](https://googlesheets4.tidyverse.org/articles/googlesheets4.html)\
[intéraction entre googlesheets4 et  tidyverse](https://googlesheets4.tidyverse.org/articles/articles/drive-and-sheets.html)

## Fichiers multiples - importation, exportation, fractionnement, combinaison

Consultez la page [Itération, boucles et listes](#iteration) pour obtenir des exemples sur la manière d'importer et de combiner plusieurs fichiers, ou plusieurs fichiers de classeur Excel. Cette page contient également des exemples sur la façon de diviser un cadre de données en plusieurs parties et d'exporter chacune d'entre elles séparément, ou en tant que feuilles nommées dans un classeur Excel.

<!-- ======================================================= -->

## Importer de Github {#import_github}

L'importation de données directement de Github dans R peut être très facile ou nécessiter quelques étapes, selon le type de fichier. Voici quelques approches:

### Fichiers CSV {.unnumbered}

Il peut être facile d'importer un fichier .csv directement de Github dans R avec une commande R.

1)  Allez sur le repo Github, localisez le fichier qui vous intéresse et cliquez dessus.\
2)  Cliquez sur le bouton "Raw" (vous verrez alors les données csv "brutes", comme indiqué ci-dessous)\
3)  Copiez l'adresse URL\
4)  Placez l'URL entre guillemets dans la commande R `import()`

```{r, out.width=c('100%', '100%'), fig.align = "left", echo=F}
knitr::include_graphics(here::here("images", "download_csv_raw.png"))
```

### Fichiers XLSX {.unnumbered}

Il se peut que vous ne puissiez pas visualiser les données "brutes" pour certains fichiers (e.x. .xlsx, .rds, .nwk, .shp)

1)  Allez sur le repo Github, localisez le fichier qui vous intéresse et cliquez dessus.\
2)  Cliquez sur le bouton "Download ", comme indiqué ci-dessous\
3)  Sauvegardez le fichier sur votre ordinateur, et importez-le dans R

```{r , out.width=c('100%', '100%'), fig.align = "left", echo=F}
knitr::include_graphics(here::here("images", "download_xlsx.png"))
```

### Shapefiles {.unnumbered}

Les fichiers Shapefiles comportent de nombreux sous-fichiers, chacun avec une extension de fichier différente. Un fichier aura l'extension ".shp", mais d'autres peuvent avoir ".dbf", ".prj", etc. Pour télécharger un shapefile à partir de Github, vous devrez télécharger chacun des fichiers de sous-composants individuellement, et les enregistrer dans le *même* dossier sur votre ordinateur. Dans Github, cliquez sur chaque fichier individuellement et téléchargez-les en cliquant sur le bouton " Download ".

Une fois enregistré sur votre ordinateur, vous pouvez importer le shapefile comme indiqué sur la page [Bases de SIG](#gis) `st_read()` du paquet **sf**. Il vous suffit de fournir le chemin d'accès et le nom du fichier " .shp ", à condition que les fichiers associés se trouvent dans le même dossier sur votre ordinateur.

Ci-dessous, vous pouvez voir comment le shapefile "sle_adm3" se compose de plusieurs fichiers, chacun devant être téléchargé depuis Github.

```{r , out.width=c('100%', '100%'), fig.align = "left", echo=F}
knitr::include_graphics(here::here("images", "download_shp.png"))
```

<!-- ======================================================= -->

## Entrée de données de façon manuelle

### Entrée par lignes {.unnumbered}

Utilisez la fonction `tribble` du paquet **tibble** du tidyverse, [ici un référence tibble en ligne](https://tibble.tidyverse.org/reference/tribble.html)

Notez que les en-têtes de colonne commencent par une *tilde* (`~`). Notez également que chaque colonne ne doit contenir qu'une seule classe de données (caractère, numérique, etc.). Vous pouvez utiliser des tabulations, des espaces et de nouvelles lignes pour rendre la saisie des données plus intuitive et plus lisible. Les espaces ne comptent pas entre les valeurs, mais chaque ligne est représentée par une nouvelle ligne de code. Par exemple:

```{r import_manual_row}
# create the dataset manually by row
manual_entry_rows <- tibble::tribble(
  ~colA, ~colB,
  "a",   1,
  "b",   2,
  "c",   3
  )
```

Et maintenant nous affichons le nouveau jeu de données:

```{r, echo=F}
# display the new dataset
DT::datatable(manual_entry_rows)
```

### Entrée par colonnes {.unnumbered}

Étant donné qu'un cadre de données est constitué de vecteurs (colonnes verticales), l'approche **de base** de la création manuelle de cadres de données dans R prévoit que vous définissiez chaque colonne, puis que vous les reliiez entre elles. Cela peut être contre-intuitif en épidémiologie, car nous pensons généralement à nos données en lignes (comme ci-dessus).

```{r import_manual_col}
# define each vector (vertical column) separately, each with its own name
PatientID <- c(235, 452, 778, 111)
Treatment <- c("Yes", "No", "Yes", "Yes")
Death     <- c(1, 0, 1, 0)
```

[***ATTENTION:*** Tous les vecteurs doivent avoir la même longueur (même nombre de valeurs).]{style="color: orange;"}

Les vecteurs peuvent ensuite être liés entre eux à l'aide de la fonction `data.frame()`:

```{r}
# combiner les colonnes dans un cadre de données, en référençant les noms de vecteurs
manual_entry_cols <- data.frame(PatientID, Treatment, Death)
```

Et maintenant nous affichons le nouveau jeu de données:

```{r, echo=F}
# display the new dataset
DT::datatable(manual_entry_cols)
```

### Collage à partir du presse-papiers {.unnumbered}

Si vous copiez des données d'un autre endroit et que vous les avez dans votre presse-papiers, vous pouvez essayer l'une des deux méthodes ci-dessous :

A partir du package **clipr**, vous pouvez utiliser `read_clip_tbl()` pour importer comme un cadre de données, ou simplement `read_clip()` pour importer comme un vecteur de caractères. Dans les deux cas, laissez les parenthèses vides.

```{r, eval=F}
linelist <- clipr::read_clip_tbl()  # importe le presse-papiers actuel comme cadre de données
linelist <- clipr::read_clip()      # importations en tant que vecteur de caractères
```

Vous pouvez aussi facilement exporter vers le presse-papiers de votre système avec **clipr**. Voir la section ci-dessous sur l'Exportation.

Alternativement, vous pouvez utiliser la fonction `read.table()` de **base** R avec `file = "clipboard")` pour importer comme un cadre de données:

```{r, eval=F}
df_from_clipboard <- read.table(
  file = "clipboard",  # specify this as "clipboard"
  sep = "t",           # separator could be tab, or commas, etc.
  header=TRUE)         # if there is a header row
```

## Importer le fichier le plus récent

Il arrive souvent que vous receviez des mises à jour quotidiennes de vos ensembles de données. Dans ce cas, vous voudrez écrire un code qui importe le fichier le plus récent. Nous présentons ci-dessous deux façons d'aborder cette question:

-   Sélection du fichier en fonction de la date figurant dans le nom du fichier\
-   Sélection du fichier sur la base des métadonnées du fichier (dernière modification)

### Dates dans le nom du fichier {.unnumbered}

Cette approche repose sur trois prémisses:

1)  Vous faites confiance aux dates dans les noms de fichiers\
2)  Les dates sont numériques et apparaissent *généralement* dans le même format (par exemple, année, mois, jour)\
3)  Il n'y a pas d'autres chiffres dans le nom du fichier

Nous vous expliquerons chaque étape, puis nous vous les montrerons combinées à la fin.

Tout d'abord, utilisez `dir()` de **base** R pour extraire uniquement les noms de fichiers pour chaque fichier dans le dossier qui vous intéresse. Pour plus de détails sur `dir()`, consultez la page sur [Interactions avec les répertoires](#directories). Dans cet exemple, le dossier concerné est le dossier " linelists " situé dans le dossier " example " situé dans " data " au sein du projet R.

```{r}
linelist_filenames <- dir(here("data", "example", "linelists")) # get file names from folder
linelist_filenames                                              # print
```

Une fois que vous avez ce vecteur de noms, vous pouvez en extraire les dates en appliquant `str_extract()` de **stringr** en utilisant cette expression régulière. Elle extrait tous les nombres dans le nom de fichier (y compris tout autre caractère au milieu comme les tirets ou les barres obliques). Vous pouvez en savoir plus sur **stringr** à la page [Caractères et chaînes de caractères](#character_stings).

```{r}
linelist_dates_raw <- stringr::str_extract(linelist_filenames, "[0-9].*[0-9]") # extract numbers and any characters in between
linelist_dates_raw  # print
```

En supposant que les dates sont généralement écrites dans le même format (par exemple, Année puis Mois puis Jour) et que les années ont 4 chiffres, vous pouvez utiliser les fonctions de conversion flexibles de **lubridate** (`ymd()`, `dmy()`, ou `mdy()`) pour les convertir en dates. Pour ces fonctions, les tirets, les espaces ou les barres obliques n'ont pas d'importance, seul l'ordre des chiffres compte. Pour en savoir plus, consultez la page [Manipuler les dates](#dates).

```{r}
linelist_dates_clean <- lubridate::ymd(linelist_dates_raw)
linelist_dates_clean
```

La fonction R **base** `which.max()` peut alors être utilisée pour retourner la position de l'index (par exemple, 1ère, 2ème, 3ème, ...) de la valeur maximale de la date. Le dernier fichier est correctement identifié comme étant le 6ème fichier - "case_linelist_2020-10-08.xlsx".

```{r}
index_latest_file <- which.max(linelist_dates_clean)
index_latest_file
```

Si nous condensons toutes ces commandes, le code complet pourrait ressembler à ce qui suit. Notez que le `.` de la dernière ligne est un caractère de remplacement pour l'objet pipé à ce point de la séquence de pipelines. A ce stade, la valeur est simplement le nombre 6. Celui-ci est placé entre doubles crochets pour extraire le 6ème élément du vecteur de noms de fichiers produit par `dir()`.

```{r}
# Charger les packages
pacman::p_load(
  tidyverse,         # data management
  stringr,           # work with strings/characters
  lubridate,         # work with dates
  rio,               # import / export
  here,              # relative file paths
  fs)                # directory interactions

# extraire le nom de fichier du dernier fichier
latest_file <- dir(here("data", "example", "linelists")) %>%  # noms du fichier du sous-dossier "linelists"          
  str_extract("[0-9].*[0-9]") %>%                  # extraire les dates (nombres)
  ymd() %>%                                        # convertir les nombres en dates (assumant le format année-mois-jour)
  which.max() %>%                                  # obtenir l'index de la date maximale (dernier fichier)
  dir(here("data", "example", "linelists"))[[.]]              # retourne le nom de fichier de la dernière liste de diffusion

latest_file  # imprimer le nom du dernier fichier
```

Vous pouvez maintenant utiliser ce nom pour terminer le chemin de fichier relatif, avec `here()`:

```{r, eval=F}
here("data", "example", "linelists", latest_file) 
```

Et vous pouvez maintenant importer le dernier fichier:

```{r, eval=F}
# import
import(here("data", "example", "linelists", latest_file)) # import 
```

### Utiliser l'information du fichier {.unnumbered}

Si vos fichiers n'ont pas de date dans leur nom (ou si vous ne faites pas confiance à ces dates), vous pouvez essayer d'extraire la date de dernière modification à partir des métadonnées du fichier. Utilisez les fonctions du paquet **fs** pour examiner les informations des métadonnées de chaque fichier, qui comprennent l'heure de dernière modification et le chemin d'accès au fichier.

Ci-dessous, nous fournissons le dossier d'intérêt à la fonction `dir_info()` de **fs**. Dans ce cas, le dossier d'intérêt se trouve dans le projet R dans le dossier "data", le sous-dossier "example", et son sous-dossier "linelists". Le résultat est un cadre de données avec une ligne par fichier et des colonnes pour `modification_time`, `path`, etc. Vous pouvez voir un exemple visuel de ceci dans la page sur [Interactions avec les répertoires](#directories).

Nous pouvons trier ce cadre de données de fichiers par la colonne `modification_time`, et ensuite ne garder que la ligne (fichier) la plus haute/la plus récente avec la fonction **base** de R `head()`. Ensuite, nous pouvons extraire le chemin d'accès de ce dernier fichier uniquement avec la fonction **dplyr** `pull()` sur la colonne `path`. Enfin, nous pouvons passer ce chemin de fichier à `import()`. Le fichier importé est enregistré sous le nom de `latest_file`.

```{r, eval=F}
latest_file <- dir_info(here("data", "example", "linelists")) %>%  # collecte des informations sur tous les fichiers du répertoire
  arrange(desc(modification_time)) %>%      # trier par temps de modification
  head(1) %>%                               # ne conserver que le fichier le plus récent
  pull(path) %>%                            # extraire uniquement le chemin du fichier
  import()                                  # importer le fichier

```

<!-- ======================================================= -->

## APIs {#import_api}

Une "interface de programmation automatisée" (API) peut être utilisée pour demander directement des données à un site web. Les API sont un ensemble de règles qui permettent à une application logicielle d'interagir avec une autre. Le client (vous) envoie une "requête" et reçoit une "réponse" contenant du contenu. Les packages R **httr** et **jsonlite** peuvent faciliter ce processus.

Chaque site Web compatible avec l'API possède sa propre documentation et ses propres spécificités avec lesquelles il faut se familiariser. Certains sites sont accessibles au public et peuvent être consultés par n'importe qui. D'autres, comme les plates-formes avec des identifiants et des références d'utilisateur, nécessitent une authentification pour accéder à leurs données.

Il va sans dire qu'il est nécessaire de disposer d'une connexion Internet pour importer des données via l'API. Nous donnerons brièvement des exemples d'utilisation des API pour importer des données, et nous vous renverrons à d'autres ressources.

*Note : rappelons que les données peuvent être* affichées\* sur un site web sans API, ce qui peut être plus facile à récupérer. Par exemple, un fichier CSV affiché peut être accessible simplement en fournissant l'URL du site à `import()` comme décrit dans la section sur [importation de Github](#import_github).\*

### Requête HTTP {.unnumbered}

L'échange d'API se fait le plus souvent par le biais d'une requête HTTP. HTTP, qui signifie Hypertext Transfer Protocol, est le format sous-jacent d'une demande/réponse entre un client et un serveur. Les données d'entrée et de sortie exactes peuvent varier en fonction du type d'API, mais le processus est le même : une "demande" (souvent une requête HTTP) de la part de l'utilisateur, contenant souvent une requête, suivie d'une "réponse", contenant des informations d'état sur la demande et éventuellement le contenu demandé.

Voici quelques éléments d'une *requête HTTP*:

-   L'URL du point de terminaison de l'API\
-   La "Méthode" (ou "Verbe")\
-   En-tête\
-   Corps

La "méthode" de la requête HTTP est l'action que vous voulez effectuer. Les deux méthodes HTTP les plus courantes sont `GET` et `POST` mais d'autres peuvent inclure `PUT`, `DELETE`, `PATCH`, etc. Lorsque vous importez des données dans R, il est très probable que vous utilisiez la méthode `GET`.

Après votre requête, votre ordinateur recevra une "réponse" dans un format similaire à celui que vous avez envoyé, comprenant l'URL, l'état HTTP (l'état 200 est ce que vous voulez !), le type de fichier, la taille et le contenu souhaité. Vous devrez ensuite analyser cette réponse et la transformer en un cadre de données exploitable dans votre environnement R.

### Packages {.unnumbered}

Le package **httr** fonctionne bien pour traiter les requêtes HTTP dans R. Il nécessite peu de connaissances préalables sur les API Web et peut être utilisé par des personnes moins familières avec la terminologie du développement logiciel. En outre, si la réponse HTTP est un fichier .json, vous pouvez utiliser **jsonlite** pour analyser la réponse.

```{r, eval=F}
# load packages
pacman::p_load(httr, jsonlite, tidyverse)
```

### Données disponibles au public {.unnumbered}

Voici un exemple de requête HTTP, emprunté à un tutoriel du site [the Trafford Data Lab](https://www.trafforddatalab.io/open_data_companion/#A_quick_introduction_to_APIs). Ce site propose plusieurs autres ressources pour apprendre et des exercices API.

Scénario : Nous souhaitons importer une liste des établissements de restauration rapide de la ville de Trafford, au Royaume-Uni. Les données sont accessibles à partir de l'API de la Food Standards Agency, qui fournit des données sur l'évaluation de l'hygiène alimentaire au Royaume-Uni.

Voici les paramètres de notre requête :

-   Verbe HTTP: GET\
-   URL du point d'accès à l'API: <http://api.ratings.food.gov.uk/Establishments>\
-   Paramètres sélectionnés: name, address, longitude, latitude, businessTypeId, ratingKey, localAuthorityId\
-   En-tête: "x-api-version", 2\
-   Format(s) des données: JSON, XML\
-   Documentation: <http://api.ratings.food.gov.uk/help>

Le code R serait le suivant:

```{r, eval=F, warning=F, message=F}
# préparer la requête
path <- "http://api.ratings.food.gov.uk/Establishments"
request <- GET(url = path,
             query = list(
               localAuthorityId = 188,
               BusinessTypeId = 7844,
               pageNumber = 1,
               pageSize = 5000),
             add_headers("x-api-version" = "2"))

# Vérifier s'il y a une erreur de serveur ("200" est bon !)
request$status_code

# soumettre la requête, analyser la réponse et la convertir en un cadre de données
response <- content(request, as = "text", encoding = "UTF-8") %>%
  fromJSON(flatten = TRUE) %>%
  pluck("establishments") %>%
  as_tibble()
```

Vous pouvez maintenant nettoyer et utiliser le cadre de données `response`, qui contient une ligne par établissement de restauration rapide.

### Authentification requise {.unnumbered}

Certaines API nécessitent une authentification, c'est-à-dire que vous devez prouver votre identité pour pouvoir accéder à des données restreintes. Pour importer ces données, vous devrez peut-être d'abord utiliser une méthode POST pour fournir un nom d'utilisateur, un mot de passe ou un code. Vous obtiendrez alors un jeton d'accès, qui pourra être utilisé pour les demandes ultérieures par la méthode GET afin de récupérer les données souhaitées.

Vous trouverez ci-dessous un exemple d'interrogation de données à partir de *Go.Data*, qui est un outil d'investigation des épidémies. *Go.Data* utilise une API pour toutes les interactions entre le frontal web et les applications pour smartphones utilisées pour la collecte des données. *Go.Data* est utilisé dans le monde entier. Comme les données sur les épidémies sont sensibles et que vous ne devez pouvoir accéder qu'aux données concernant *votre* épidémie, une authentification est requise.

Vous trouverez ci-dessous un exemple de code R utilisant **httr** et **jsonlite** pour se connecter à l'API *Go.Data* afin d'importer des données sur le suivi des contacts de votre épidémie.

```{r, eval=F}
# définir les informations d'identification pour l'autorisation
url <- "https://godatasampleURL.int/"           # url d'instance valide de Go.Data
username <- "username"                          # nom d'utilisateur valide Go.Data 
password <- "password"                          # mot de passe Go,Data valide 
outbreak_id <- "xxxxxx-xxxx-xxxx-xxxx-xxxxxxx"  # identifiant d'épidémie Go.Data valide

# obtenir le jeton d'accès
url_request <- paste0(url,"api/oauth/token?access_token=123") # define base URL request

# préparer la requête
response <- POST(
  url = url_request,  
  body = list(
    username = username,    # utiliser le nom d'utilisateur et le mot de passe enregistrés ci-dessus pour l'autorisation                              
    password = password),                                       
    encode = "json")

# exécuter la demande et analyser la réponse
content <-
  content(response, as = "text") %>%
  fromJSON(flatten = TRUE) %>%          # aplatir les JSON imbriqués
  glimpse()

# Sauvegarder le jeton d'accès de la réponse
access_token <- content$access_token    # sauvegarder le jeton d'accès pour permettre les appels API suivants

# importer les contacts de l'épidémie
# Utiliser le jeton d'accès 
response_contacts <- GET(
  paste0(url,"api/outbreaks/",outbreak_id,"/contacts"),          # obtenir (GET) la requête
  add_headers(
    Authorization = paste("Bearer", access_token, sep = " ")))

json_contacts <- content(response_contacts, as = "text")         # convertir en texte JSON

contacts <- as_tibble(fromJSON(json_contacts, flatten = TRUE))   # aplatir JSON en tibble
```

[***ATTENTION:*** Si vous importez de grandes quantités de données à partir d'une API nécessitant une authentification, il se peut que le délai d'attente soit dépassé. Pour éviter cela, récupérez à nouveau l'access_token avant chaque requête GET de l'API et essayez d'utiliser des filtres ou des limites dans la requête. ]{style="color: orange;"}

[***TIP:*** La fonction `fromJSON()` du package **jsonlite** ne désimbrique pas complètement la première fois qu'elle est exécutée, donc vous aurez probablement encore des éléments de liste dans votre tibble résultant. Vous aurez besoin de désimbriquer davantage certaines variables, en fonction de l'imbrication de votre fichier .json. Pour plus d'informations à ce sujet, consultez la documentation du package **jsonlite**, notamment la fonction  `flatten()`](https://rdrr.io/cran/jsonlite/man/flatten.html). {style="color: darkgreen;"}

Pour plus de détails, consultez la documentation sur [LoopBack Explorer](https://loopback.io/doc/en/lb4/index.html), la page [Suivi des contacts](#contact_tracing) ou les astuces API sur [Go.Data Github repository](https://worldhealthorganization.github.io/godata/api-docs)

Vous pouvez en savoir plus sur le package *httr*  [here](https://httr.r-lib.org/articles/quickstart.html)

Cette section a aussi été informée par [ce tutoriel](https://www.dataquest.io/blog/r-api-tutorial/) et [ce tutoriel](https://medium.com/@traffordDataLab/querying-apis-in-r-39029b73d5f1).

<!-- ======================================================= -->

## Exporter

### Avec le package **rio**  {.unnumbered}

Avec **rio**, vous pouvez utiliser la fonction `export()` de manière très similaire à `import()`. Donnez d'abord le nom de l'objet R que vous voulez sauvegarder (par exemple `linelist`) et ensuite entre guillemets mettez le chemin du fichier où vous voulez sauvegarder le fichier, en incluant le nom de fichier désiré et l'extension de fichier. Par exemple :

Cette opération permet de sauvegarder le cadre de données `linelist` comme un classeur Excel dans le répertoire de travail/ dossier racine du projet :

```{r, eval=F}
export(linelist, "my_linelist.xlsx") # will save to working directory
```

Vous pouvez enregistrer le même cadre de données comme un fichier csv en changeant l'extension. Par exemple, nous l'enregistrons également dans un chemin de fichier construit avec `here()` :

```{r, eval=F}
export(linelist, here("data", "clean", "my_linelist.csv"))
```

### Vers le presse-papiers {.unnumbered}

Pour exporter un cadre de données vers le "presse-papiers" de votre ordinateur (pour ensuite le coller dans un autre logiciel comme Excel, Google Spreadsheets, etc.) vous pouvez utiliser `write_clip()` du paquet **clipr**.

```{r, eval=F}
# exporter le cadre de données de la liste de cas vers le presse-papiers de votre système
clipr::write_clip(linelist)
```

## Documents RDS {#import_rds}

Outre les fichiers .csv, .xlsx, etc., vous pouvez également exporter/enregistrer des cadres de données R sous forme de fichiers .rds. Il s'agit d'un format de fichier spécifique à R, très utile si vous savez que vous allez retravailler les données exportées dans R.

Les classes de colonnes sont stockées, de sorte que vous n'avez pas à refaire le nettoyage lors de l'importation (avec un fichier Excel ou même un fichier CSV, cela peut être un casse-tête !) C'est aussi un fichier plus petit, ce qui est utile pour l'exportation et l'importation si votre ensemble de données est grand.

Par exemple, si vous travaillez dans une équipe d'épidémiologie et que vous devez envoyer des fichiers à une équipe SIG pour la cartographie, et qu'ils utilisent également R, envoyez-leur simplement le fichier .rds ! Toutes les classes de colonnes sont alors conservées et ils ont moins de travail à faire.

```{r, eval=F}
export(linelist, here("data", "clean", "my_linelist.rds"))
```

<!-- ======================================================= -->

## Fichiers Rdata et listes {#import_rdata}

Les fichiers `.Rdata` peuvent stocker plusieurs objets R - par exemple plusieurs cadres de données, des résultats de modèles, des listes, etc. Cela peut être très utile pour consolider ou partager un grand nombre de vos données pour un projet donné.

Dans l'exemple ci-dessous, plusieurs objets R sont stockés dans le fichier exporté "my_objects.Rdata":

```{r, eval=F}
rio::export(my_list, my_dataframe, my_vector, "my_objects.Rdata")
```

Note : si vous essayez d'*importer* une liste, utilisez `import_list()` de **rio** pour l'importer avec la sTIPture et le contenu originaux complets.

```{r, eval=F}
rio::import_list("my_list.Rdata")
```

<!-- ======================================================= -->

## Sauvegarde des graphiques

Les insTIPtions sur la façon de sauvegarder les tracés, tels que ceux créés par `ggplot()`, sont discutées en profondeur dans la page [bases de ggplot](#ggplot_basics).

En bref, lancez `ggsave("my_plot_filepath_and_name.png")` après avoir imprimé votre tracé. Vous pouvez soit fournir un objet de tracé sauvegardé à l'argument `plot =`, ou seulement spécifier le chemin du fichier de destination (avec l'extension du fichier) pour sauvegarder le tracé le plus récemment affiché. Vous pouvez aussi contrôler le `width =`, `height =`, `units =`, et `dpi =`.

L'enregistrement d'un graphe de réseau, tel qu'un arbre de transmission, est abordé dans la page [Chaînes de transmission](#transmission_chains).

<!-- ======================================================= -->

## Ressources

Le [Manuel pour Importer/Exporter](https://cran.r-project.org/doc/manuals/r-release/R-data.html)\
[Chapitre sur l'importaiton de données de R 4 Data Science](https://r4ds.had.co.nz/data-import.html#data-import)\
[Documation pour ggsave()](https://ggplot2.tidyverse.org/reference/ggsave.html)

Voici un tableau tiré de la [vignette](https://cran.r-project.org/web/packages/rio/vignettes/rio.html) en ligne de **rio**. Pour chaque type de données, il indique : l'extension de fichier attendue, le package que **rio** utilise pour importer ou exporter les données, et si cette fonctionnalité est incluse dans la version installée par défaut de **rio**.

| Format                              | Extension Typique       | Package d'Importation| Package d'Exportation| Installé par Defaut |
|-------------------------------------|-------------------------|----------------------|----------------|----------------------|
| Comma-separated data                | .csv                    | data.table `fread()` | data.table     | Oui                  |
| Pipe-separated data                 | .psv                    | data.table `fread()` | data.table     | Oui                  |
| Tab-separated data                  | .tsv                    | data.table `fread()` | data.table     | Oui                  |
| SAS                                 | .sas7bdat               | haven                | haven          | Oui                  |
| SPSS                                | .sav                    | haven                | haven          | Oui                  |
| Stata                               | .dta                    | haven                | haven          | Oui                  |
| SAS                                 | XPORT                   | .xpt                 | haven          | haven                |
| SPSS Portable                       | .por                    | haven                |                | Oui                  |
| Excel                               | .xls                    | readxl               |                | Oui                  |
| Excel                               | .xlsx                   | readxl               | openxlsx       | Oui                  |
| R syntax                            | .R                      | base                 | base           | Oui                  |
| Saved R objects                     | .RData, .rda            | base                 | base           | Oui                  |
| Serialized R objects                | .rds                    | base                 | base           | Oui                  |
| Epiinfo                             | .rec                    | foreign              |                | Oui                  |
| Minitab                             | .mtp                    | foreign              |                | Oui                  |
| Systat                              | .syd                    | foreign              |                | Oui                  |
| "XBASE"                             | database files          | .dbf                 | foreign        | foreign              |
| Weka Attribute-Relation File Format | .arff                   | foreign              | foreign        | Oui                  |
| Data Interchange Format             | .dif                    | utils                |                | Oui                  |
| Fortran data                        | pas d'extension reconnue| utils                |                | Oui                  |
| Fixed-width format data             | .fwf                    | utils                | utils          | Oui                  |
| gzip comma-separated data           | .csv.gz                 | utils                | utils          | Oui                  |
| CSVY (CSV + YAML metadata header)   | .csvy                   | csvy                 | csvy           | Non                  |
| EViews                              | .wf1                    | hexView              |                | Non                  |
| Feather R/Python interchange format | .feather                | feather              | feather        | Non                  |
| Fast Storage                        | .fst                    | fst                  | fst            | Non                  |
| JSON                                | .json                   | jsonlite             | jsonlite       | Non                  |
| Matlab                              | .mat                    | rmatio               | rmatio         | Non                  |
| OpenDocument Spreadsheet            | .ods                    | readODS              | readODS        | Non                  |
| HTML Tables                         | .html                   | xml2                 | xml2           | Non                  |
| Shallow XML documents               | .xml                    | xml2                 | xml2           | Non                  |
| YAML                                | .yml                    | yaml                 | yaml           | Non                  |
| Clipboard default is tsv            |                         | clipr                | clipr          | Non                  |
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/importing.Rmd-->

# (PART) Data Management {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_data_management.Rmd-->

# Nettoyage de données et fonctions essentielles {#cleaning_data}


```{r, out.height = "10%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "cleaning.png"))
```


Cette page présente les étapes courantes du processus de "nettoyage" d'un jeu de données, et apporte également des explications sur l'utilisation de nombreuses fonctions essentielles dans le traitement des données R.  

Pour expliquer le nettoyage des données, cette page commence par l'importation d'un jeu de données brutes de cas dénommé linelist, et présente le processus de nettoyage étape par étape. Dans le code R, cela se manifeste par une chaîne "pipe", qui fait référence à l'opérateur "pipe" ` %>%` qui fait passer un jeu de données d'une opération à l'autre.  


### Fonctions principales {.unnumbered}  

Ce manuel met l'accent sur l'utilisation des fonctions  provenant  du package [**tidyverse**](https://www.tidyverse.org/), appartenant à la famille des packages de R. L'essentiel des fonctions de R présentées dans cette page sont listées ci-dessous.  

Beaucoup de ces fonctions appartiennent au package R [**dplyr**](https://dplyr.tidyverse.org/), qui fournit des fonctions "verbes" pour résoudre les problèmes de manipulation des données (le nom fait référence à un ["dataframe"- plier](https://en.wikipedia.org/wiki/Pliers). **dplyr** fait partie de la famille de packages R **tidyverse** (qui qui consiste de **ggplot2**, **tidyr**, **stringr**, **tibble**, **purrr**, **magrittr**, et **forcats** entre autres).  




Fonction       | Utilité                               | Package
---------------|---------------------------------------|------------------------------
` %>% `|"pipe" (passe) les données d'une fonction à l'autre|**magrittr** 
`mutate()`|créer, transformer et redéfinir des colonnes|**dplyr**  
`select()`|conserver, supprimer, sélectionner ou renommer des colonnes|**dplyr**
`rename()`|renommer des colonnes|**dplyr** 
`clean_names()`|standardiser la syntaxe des noms de colonnes|**janitor**
`as.character()`, `as.numeric()`, `as.Date()`, etc. |convertir la classe d'une colonne|**base** R
`across()`|transformer plusieurs colonnes en même temps|**dplyr** 
Fonctions **tidyselect**|utiliser la logique pour sélectionner des colonnes|**tidyselect**.   
`filter()`|Ne conserver que certaines lignes|**dplyr** 
`distinct()`|dédupliquer des lignes|**dplyr***. 
`rowwise()`|effectuer ds opérations par/dans chaque ligne|**dplyr**  
`add_row()`|ajouter  manuellement des lignes|**tibble** 
`arrange()`|ordonner des lignes|**dplyr**
`recode()`|re-coder les valeurs dans une colonne|**dplyr** 
`case_when()`|re-coder les valeurs d'une colonne en utilisant des critères logiques plus complexes|**dplyr** 
`replace_na()`, `na_if()`, `coalesce()`|des fonctions spéciales pour le recodage|**tidyr**  
`age_categories()` et `cut()`|créer des groupes catégoriels à partir d'une colonne numérique|**epikit** et **base** R
`match_df()`|re-coder/nettoyer des valeurs en utilisant un dictionnaire de données|**matchmaker**
`which()`|application de critères logiques ; retourne des indices|**base** R

Si vous souhaitez comparer ces fonctions aux commandes de Stata ou de SAS, consultez la page [Transition vers R](#transition_to_r).  

Vous pouvez rencontrer une méthode alternative de gestion des données à partir du package  R **data.table** avec des opérateurs comme `:=` et l'utilisation fréquente de crochets `[ ]`. Cette approche et cette syntaxe sont brièvement expliquées dans la page [Data Table](#data_table).  


### Nomenclature {.unnumbered}  

Dans ce manuel, nous faisons généralement référence aux "colonnes" et aux "lignes" au lieu des "variables" et des "observations". Comme l'explique cet introduction sur ["tidy data"](https://tidyr.tidyverse.org/articles/tidy-data.html), la plupart des jeux de données statistiques épidémiologiques sont sous le format de lignes, de colonnes et de valeurs.  

*Les variables* contiennent les valeurs qui mesurent le même attribut sous-jacent (comme le groupe d'âge, le résultat ou la date d'apparition). *Les observations* contiennent toutes les valeurs mesurées sur la même unité (par exemple, une personne, un site ou un échantillon de laboratoire). Ces aspects peuvent donc être plus difficiles à définir de manière précise.  

Dans les jeux de données "tidy", chaque colonne est une variable, chaque ligne est une observation et chaque cellule est une valeur unique. Cependant, certains jeu de données que vous rencontrerez ne correspondront pas à ce format - un jeu de données  "étendu" (ou "large") peut avoir une variable répartie sur plusieurs colonnes (voir un exemple dans la page [Pivoter les données](#pivoting_data)). De même, les observations peuvent être réparties sur plusieurs lignes.  

La majeure partie de ce manuel porte sur la gestion et la transformation des données, et il est donc plus pertinent de se référer aux structures de données concrètes que sont les lignes et les colonnes qu'aux observations et aux variables plus abstraites. Les exceptions se produisent principalement dans les pages sur l'analyse des données, où vous verrez davantage de références aux variables et aux observations.  





<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Méthodologie de nettoyage 

**Cette page passe en revue les étapes importantes du nettoyage, en les ajoutant séquentiellement à une "chaîne de nettoyage".**

Dans l'analyse épidémiologique et le traitement des données, les étapes de nettoyage sont souvent effectuées de manière séquentielle, reliées entre elles. En R, cela se manifeste souvent sous la forme d'un "pipeline" de nettoyage, où *le jeu de données brutes est passé ou "pipé" d'une étape de nettoyage à une autre*.  

De telles chaînes utilisent les fonctions "verbes" de **dplyr** et l'opérateur pipe `%>%` de **magrittr**. Ce pipe commence avec les données "brutes" ("linelist_raw.xlsx") et se termine avec un dataframe "propre" (`linelist`) qui peut être utilisé, enregistré, exporté, etc.  

Dans un pipeline de nettoyage, l'ordre des étapes est important. Les étapes de nettoyage peuvent inclure :  

* Importation des données  
* Nettoyage ou modification des noms de colonnes  
* Déduplication  
* Création et transformation de colonnes (par exemple, recodage ou normalisation des valeurs).    
* Sélection ou ajout de lignes  



<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Charger les packages  

Ces lignes de code chargent les paquets nécessaires aux analyses. Dans ce guide, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *puis* l'importe pour l'utiliser. Vous pouvez également charger les paquets installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les paquets R.  
  

```{r, message = F}
pacman::p_load(
  rio,        # importation données  
  here,       # chemins d'accès relatifs aux fichiers  
  janitor,    # nettoyage des données et tables
  lubridate,  # manipuler les dates
  epikit,     # age_categories() fonction
  matchmaker, # nettoyage basé sur un dictionnaire
  tidyverse   # manipulation et visualisation  des donnees 
)
```




<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Import data  

### Import {.unnumbered}  

Ici, nous importons le fichier Excel "brut" de cas linelist en utilisant la fonction `import()` du package **rio**. Cet dernier gère de manière flexible de nombreux types de fichiers (par exemple, .xlsx, .csv, .tsv, .rds). [Importation et exportation des données](import_export) pour plus d'informations et des conseils sur les situations inhabituelles (par exemple, sauter des lignes, définir des valeurs manquantes, importer des feuilles Google, etc.)  

Pour reproduire les étapes, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_raw.xlsx' class='download-button'>cliquer pour télécharger le jeu de données  "brutes" linelist</a> (comme fichier .xlsx ).  

Si votre jeu de données est important et prend beaucoup de temps à s'importer, il peut valloir le coup de séparer la commande d'importation des étapes de nettoyage, pour que que la donnée "brute" soit enregistré dans un dataframe distinct. Cela permet également de comparer facilement les versions originales et nettoyées.  

Ci-dessous, nous importons le fichier Excel brut et le sauvegardons dans le dataframe `linelist_raw`. Nous supposons que le fichier est situé dans votre répertoire de travail ou à la racine de votre projet R, et donc aucun sous-dossier n'est spécifié dans le chemin du fichier.  

```{r, echo=F, message=F}
# CACHER POUR LES LECTEURS
# chargeons le jeu  de donnée en utilisant  here()
linelist_raw <- rio::import(here::here("data", "case_linelists", "linelist_raw.xlsx"))
```

```{r, eval=F}
linelist_raw <- import("linelist.xlsx")
```

Vous pouvez visualiser les 50 premières lignes du dataframe ci-dessous. Remarque : la fonction de **base** de R `head(n)` vous permet de visualiser uniquement les `n` premières lignes dans la console R.  

```{r message=FALSE, echo=F}
# Afficher la donnée linelist comme un tableau
DT::datatable(head(linelist_raw,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Vue d'ensemble {.unnumbered}  

Vous pouvez utiliser la fonction `skim()` du package **skimr** pour obtenir une vue d'ensemble du dataframe (voir la page sur les [Tableaux descriptifs](#descriptive_tables) pour plus d'informations). Les colonnes sont résumées par classe/type, telles que "chaîne de caractère", "numérique"... Note : "POSIXct" est un type de classe de date brute (voir [Working with dates](#working_dates)).  


```{r, eval=F}
skimr::skim(linelist_raw)
```

```{r, echo=F}
skimr::skim_without_charts(linelist_raw)
```




 





<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Noms de colonnes {} 

En R, les *noms* de colonnes sont la valeur "d'en-tête" ou la "première cellule" d'une colonne. Ils sont utilisés pour faire référence aux colonnes dans le code et servent de labels par défaut dans les figures.  

D'autres logiciels statistiques tels que SAS et STATA utilisent des *"labels"* qui représentent un format plus long et détaillé des noms de colonnes plus courts existants. Bien que R offre la possibilité d'ajouter des étiquettes de colonne aux données, cela n'est pas mis en avant en pratique. Pour rendre les noms de colonne explicites et descriptifs pour les figures, on ajuste généralement leur affichage dans les commandes de qui créent les graphiques (par exemple, les titres des axes ou des légendes d'une graphique, ou les en-têtes de colonne dans un tableau imprimé - voir la section [section échelles de la page Astuces de ggplot](#ggplot_tips_scales) et les pages [Tableaux pour la présentation](#tables_presentation)). Si vous souhaitez attribuer des étiquettes de colonne dans les données, lisez la suite en ligne [ici](https://cran.r-project.org/web/packages/expss/vignettes/labels-support.html) et [ici](https://cran.r-project.org/web/packages/labelled/vignettes/intro_labelled.html).  

Comme les noms de colonnes sur R sont utilisés très souvent, ils doivent avoir une syntaxe "propre". Nous suggérons ce qui suit :  

* Noms courts  
* Pas d'espaces (remplacez-les par des traits de soulignement _ )  
* Pas de caractères inhabituels (&, #, <, >, ...)   
* Nomenclature homogène (par exemple, toutes les colonnes de date nommées comme **date_**apparition, **date_**rapport, **date_**mort...)  

Les noms des colonnes de `linelist_raw` sont affichés ci-dessous en utilisant la fonction `names()` de **base** R. On peut voir qu'initialement :  

* Certains noms contiennent des espaces (par exemple `date d'infection`)  
* Des motifs de noms différents sont utilisés pour les dates (`date onset` vs. `infection date`).  
* Il doit y avoir un *même nom* attribué pour les deux dernières colonnes du fichier .xlsx. Nous le savons parce que le nom de deux colonnes fusionnées ("merged_header") a été attribué par R à la première colonne, et que la deuxième colonne s'est vue attribuer un nom fictif "...28" (puisqu'elle était alors vide et qu'il s'agit de la 28ième colonne).  

```{r}
names(linelist_raw)
```

<span style="color : black ;">**_REMARQUE:_** Pour faire référence à un nom de colonne qui comprend des espaces, entourez le nom de contre-tirets, par exemple : linelist$`` `'\x60infection date\x60'` ``. Notez que sur votre clavier, le contre-tiret (`) est différent du guillemet simple (').</span>


### Labels {.unnumbered}  

D'autres logiciels statistiques tels que SAS ont des *labels* de variables.


### Nettoyage automatique {.unnumbered}  

La fonction `clean_names()` du package **janitor** normalise les noms de colonnes et les rend uniques en effectuant les opérations suivantes :  

* Convertit tous les noms pour qu'ils ne soient composés que de underscores (sous-tiret/tiret "bas"/tiret 8), de chiffres et de lettres.  
* Les caractères accentués sont translittérés en ASCII (par exemple, le o allemand avec tréma devient "o", le "enye" espagnol devient "n").   
* La préférence de capitalisation pour les nouveaux noms de colonnes peut être spécifiée en utilisant l'argument `case = ` ("snake" est le défaut, les alternatives incluent "phrase", "title", "small_camel"...)   
* Vous pouvez spécifier des remplacements de noms spécifiques en fournissant un vecteur à l'argument `replace = ` (par exemple, `replace = c(onset = "date_of_onset")`)  

Pour en savoir plus, voici la [vignette](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#cleaning) en ligne. .

Ci-dessous, le pipeline de nettoyage commence par utiliser `clean_names()` sur le dataframe contenant les données brutes.  

```{r clean_names}
# pipe le jeu de données brutes à travers clean_names(), puis assigne le resultat à un nouveau dataframe, "linelist"  
linelist <- linelist_raw %>% 
  janitor::clean_names()

# voir les nouveaux noms de colonnes
names(linelist)
```

<span style="color: black;">**_NOTE:_** le dernier nom de colonne "...28" est changé pour devenir "x28".</span>


### Nettoyage manuel des noms {.unnumbered}  

Il est souvent nécessaire de renommer les colonnes manuellement, même après l'étape de normalisation ci-dessus. Ci-dessous, le changement de nom est effectué en utilisant la fonction `rename()` du package **dplyr**, dans une  chaine de commandes  pipées. `rename()` utilise le style `NEW = OLD` - le nouveau nom de colonne est donné avant l'ancien nom de colonne.  

Ci-dessous, une commande de re-nommage est ajoutée au pipeline de nettoyage. Des espaces ont été ajoutés stratégiquement pour aligner le code afin de faciliter la lecture.  

```{r}
# NETTOYAGE: enchainement de commandes  'PIPE'  (debute avec la donnée brute  et suivi de  pipes avec les commandes pour le nettoyage)
##################################################################################
linelist <- linelist_raw %>%
    
    # standardiser la syntaxe des noms de colonnes 
    janitor::clean_names() %>% 
    
    # renommer manuellement les noms de colonnes
           # Nouveau noms             # Ancien noms
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome)
```


Vous pouvez maintenant voir que les noms des colonnes ont été modifiés :  

```{r message=FALSE, echo=F}
names(linelist)
```


#### Renommer par la position des colonnes{.unnumbered} 

Vous pouvez également renommer par position de colonne, au lieu du nom de colonne, par exemple :  

```{r, eval=F}
#rename(newNameForFirstColumn  = 1,
       #newNameForSecondColumn = 2)
```



#### Renommer via `select()` et `summarise()` {.unnumbered}  

Comme raccourci, vous pouvez aussi renommer les colonnes dans les fonctions **dplyr** `select()` et `summarise()`. `select()` est utilisé pour ne garder que certaines colonnes (et est couvert plus loin dans cette page). La fonction `summarise()` est traitée dans les pages [Regroupement des données](#grouping_data) et [Tableaux descriptifs](#descriptive_tables). Ces fonctions utilisent également le format `nouveau_nom = ancien_nom`. Voici un exemple :  

```{r, eval=F}
linelist_raw %>% 
  select(# NOUVEAU nom             # ANCIEN nom
         date_infection       = `infection date`,    # renommer and CONSEVER  que  ces colonnes
         date_hospitalisation = `hosp date`)
```





### Autres challenges {.unnumbered}  


#### fichiers excels sans noms de colonnes {.unnumbered} 

R ne peut pas travailler sur des jeux de données qui n'ont pas de noms de colonne (en-têtes). Ainsi, si vous importez un jeu de données Excel contenant des données mais pas d'en-tête de colonne, R remplira les en-têtes avec des noms tels que "...1" ou "...2". Le chiffre représente le numéro de la colonne (par exemple, si la quatrième colonne de l'ensemble de données n'a pas d'en-tête, R la nommera "...4").  

Vous pouvez nettoyer ces noms manuellement en faisant référence à leur numéro de position (voir l'exemple ci-dessus), ou au nom qui leur est attribué (`linelist_raw$...1`).  



#### Fusion des noms de colonnes et de cellules sur Excel {.unnumbered}  

La fusion de cellules dans un fichier Excel est un phénomène courant lors de la réception de données. Comme expliqué dans [Transition vers R](#transition_to_R), les cellules fusionnées peuvent être agréables pour la lecture humaine des données, mais ne sont pas des "tidy data" et posent de nombreux problèmes pour la lecture automatique des données. R ne peut pas prendre en compte les cellules fusionnées.  

Rappelez aux personnes chargées de la saisie des données que **les données lisibles par l'homme ne sont pas les mêmes que celles lisibles par la machine**. Efforcez-vous de former les utilisateurs aux principes des [**Tidy Data**] (https://r4ds.had.co.nz/tidy-data.html). Dans la mesure du possible, essayez de modifier les procédures pour que les données arrivent dans un format tidy, sans cellules fusionnées.  

* Chaque variable doit avoir sa propre colonne.  
* Chaque observation doit avoir sa propre ligne.  
* Chaque valeur doit avoir sa propre cellule.  

Lorsque vous utilisez la fonction `import()` de **rio**, la valeur d'une cellule fusionnée sera assignée à la première cellule et les cellules suivantes seront vides.  

Une solution pour gérer les cellules fusionnées est d'importer les données avec la fonction `readWorkbook()` du package **openxlsx**. Définissez l'argument `fillMergedCells = TRUE`. Cela donne la valeur d'une cellule fusionnée à toutes les cellules de la plage de fusion.

```{r, eval=F}
linelist_raw <- openxlsx::readWorkbook("linelist_raw.xlsx", fillMergedCells = TRUE)
```

<span style="color: red;">**_DANGERS:_** Si les noms de colonnes sont fusionnés avec `readWorkbook()`, vous vous retrouverez avec des noms de colonnes en double, que vous devrez corriger manuellement - R ne fonctionne pas bien avec des noms de colonnes en double ! Vous pouvez les renommer en faisant référence à leur position (par exemple colonne 5), comme expliqué dans la section sur le nettoyage manuel des noms de colonnes.</span>






<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Sélectionner ou réorganiser les colonnes {} 

Utilisez `select()` de **dplyr** pour sélectionner les colonnes que vous voulez conserver, et pour spécifier leur ordre dans le cadre de données. 

<span style="color: orange;">**_Avertissement:_** Dans les exemples ci-dessous, le dataframe `linelist` est modifié avec `select()` et affiché, mais pas enregistré. Ceci est pour les besoins de la démonstration. Les noms de colonnes modifiés sont renvoyés en passant (pipe)le dataframe dans  `names()`.</span>

**Voici TOUS les noms de colonnes dans la liste de lignes à ce stade de la chaîne de nettoyage :**.

```{r}
names(linelist)
```

### Conserver des  colonnes {.unnumbered}  

**Selectionner uniquement les colonnes que vous voulez conserver**  

Mettez leurs noms dans la commande `select()`, sans guillemets. Elles apparaîtront dans le dataframe dans l'ordre que vous avez indiqué. Notez que si vous incluez une colonne qui n'existe pas, R retournera une erreur (voir l'utilisation de `any_of()` ci-dessous si vous ne voulez pas d'erreur dans cette situation).  

```{r}
# jeu de donnée linelist est pipé  à travers la commande select() et names() affiche les noms de colones
linelist %>% 
  select(case_id, date_onset, date_hospitalisation, fever) %>% 
  names()  # affiche le nom des colonnes
```




### fonction d'aide "tidyselect"  {#clean_tidyselect .unnumbered}  

Ces fonctions d'aide existent pour faciliter la spécification des colonnes à conserver, à éliminer ou à transformer. Elles sont issues du package **tidyselect**, qui est inclus dans **tidyverse** et qui sous-tend la façon dont les colonnes sont sélectionnées dans les fonctions **dplyr**.  

Par exemple, si vous voulez réordonner les colonnes, `everything()` est une fonction utile qui correspond à "toutes les autres colonnes non encore mentionnées". La commande ci-dessous déplace les colonnes `date_onset` et `date_hospitalisation` au début (à gauche) du jeu de donnée, mais conserve toutes les autres colonnes par la suite. Notez que `everything()` est écrit avec des parenthèses vides :  

```{r}

# deplacer les colonnes data_onset et date_hospilisation au debut du jeu de donnée
linelist %>% 
  select(date_onset, date_hospitalisation, everything()) %>% 
  names()
```

Voici d'autres fonctions d'aide "tidyselect" qui fonctionnent également *dans* les fonctions **dplyr** comme `select()`, `across()`, et `summarise()` :  

* `everything()` - toutes les autres colonnes non mentionnées  
* `last_col()` - la dernière colonne  
* `where()` - applique une fonction à toutes les colonnes et sélectionne celles qui sont VRAIES.  
* `contains()` - colonnes contenant une chaîne de caractères.  
  * Exemple : `select(contains("time"))`.  
* `starts_with()` - correspond à un préfixe spécifié.  
  * Exemple : `select(starts_with("date_"))`.  
* `ends_with()` - correspond à un suffixe spécifié.  
  * exemple : `select(ends_with("_post"))`  
* `matches()` - pour appliquer une expression régulière (regex)  
  * exemple : `select(matches("[pt]al"))`  
* `num_range()` - une plage numérique comme x01, x02, x03  
* `any_of()` - correspond si la colonne existe mais ne renvoie pas d'erreur si elle n'est pas trouvée.  
  * Exemple : `select(any_of(date_onset, date_death, cardiac_arrest))`.  

De plus, utilisez des opérateurs normaux tels que `c()` pour lister plusieurs colonnes, `:` pour des colonnes consécutives, `!` pour l'opposé, `&` pour AND, et `|` pour OR.  


Utilisez `where()` pour spécifier des critères logiques pour les colonnes. Si vous fournissez une fonction dans `where()`, n'incluez pas les parenthèses vides de la fonction. La commande ci-dessous sélectionne les colonnes de la classe Numeric.

```{r}
# selectionner les colonnes de classe Numeric
linelist %>% 
  select(where(is.numeric)) %>% 
  names()
```

Utilisez `contains()` pour ne sélectionner que les colonnes dont le nom contient une chaîne de caractères donnée. `ends_with()` et `starts_with()` apportent plus de nuances.  

```{r}
# Selectionner des colonnes qui contiennent une caractere defini
linelist %>% 
  select(contains("date")) %>% 
  names()
```

La fonction `matches()` fonctionne de la même manière que `contains()` mais on peut lui fournir une expression régulière (voir la page sur les [Caractères et chaînes de caractères](#character_strings)), comme plusieurs chaînes de caractères séparées par des barres OR à l'intérieur des parenthèses :  

```{r}
# searched for multiple character matches
#  rechercher plusieurs caracteres 
linelist %>% 
  select(matches("onset|hosp|fev")) %>%   # noter le symbole de OR  "|"
  names()
```

<span style="color: orange;">**_AVERTISSEMENT:_** Si un nom de colonne que vous fournissez spécifiquement n'existe pas dans les données, il peut retourner une erreur et arrêter votre code. Pensez à utiliser `any_of()` pour citer des colonnes qui peuvent ou non exister, particulièrement utile dans les sélections négatives (enlever).</span>

Une seule de ces colonnes existe, mais aucune erreur n'est produite et le code continue sans arrêter votre chaîne de nettoyage.  

```{r}
linelist %>% 
  select(any_of(c("date_onset", "village_origin", "village_detection", "village_residence", "village_travel"))) %>% 
  names()
```



### Supprimer colonnes {.unnumbered} 

**Indiquez les colonnes à supprimer** en plaçant un symbole moins "-" devant le nom de la colonne (par exemple, `select(-outcome)`), ou un vecteur de noms de colonnes (comme ci-dessous). Toutes les autres colonnes seront conservées. 

```{r}
linelist %>% 
  select(-c(date_onset, fever:vomit)) %>% #supprimer la colonne date_onset et tout les colonnes allant de fever à vomit
  names()
```

Vous pouvez également supprimer une colonne en utilisant la syntaxe R **base**, en la définissant comme `NULL`. Par exemple :  

```{r, eval=F}
linelist$date_onset <- NULL   
# supprimer colonne avec la syntaxe native de R
```



### Autres {.unnumbered}

`select()` peut aussi être utilisé comme une commande indépendante (pas dans une chaîne de tuyaux). Dans ce cas, le premier argument est le dataframe original sur lequel on veut travailler.  

```{r}
# creer un nouvelle donnée linelist avec des colonnes  id et age-related
linelist_age <- select(linelist, case_id, contains("age"))

# afficher les noms de colonnes
names(linelist_age)
```



#### ajouter à la chaine de commande pipé {.unnumbered}  

Dans la `linelist_raw`, il y a quelques colonnes dont nous n'avons pas besoin : `row_num`, `merged_header`, et `x28`. Nous les supprimons avec une commande `select()` dans la chaîne de nettoyage :  

```{r}
# NETTOYAGE: enchainement de commandes  'PIPE'  (debute avec la donnée brute  et le  pipes avec les commandes pour le nettoyage)
##################################################################################


# Debuter le processus de Nettoyage pipé 
###########################
linelist <- linelist_raw %>%
    
     # standardiser le syntaxe des noms de colonnes
    janitor::clean_names() %>% 
    
    
     # renommer manuellement les colonnes
           # NOUVEAU nom            # ANCIEN nom
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    
     # CI DESSUS LES ETAPES DU NETTOYAGE DEJA ABORDEES EN AMONT 
    #####################################################

    # supprimer des colonnes
    select(-c(row_num, merged_header, x28))
```




<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Deduplication


Voir la page du manuel sur la [déduplication](#deduplication) pour de nombreuses options sur la façon de dédupliquer les données. Seul un exemple très simple de déduplication de lignes est présenté ici.  

Le package **dplyr** offre la fonction `distinct()`. Cette fonction examine chaque ligne et réduit le dataframe à seulement les lignes uniques. C'est-à-dire qu'elle supprime les lignes qui sont à 100% des doublons.  

Lors de l'évaluation des lignes dupliquées, elle prend en compte un eventail de colonnes defini - par défaut, elle considère toutes les colonnes. Comme le montre la page  dediée à la déduplication, vous pouvez ajuster cet eventail de colonnes afin que l'unicité des lignes ne soit évaluée que par rapport à certaines colonnes.  

Dans cet exemple simple, nous ajoutons simplement la commande vide `distinct()` à la chaîne de commande pipé. Cela permet de s'assurer qu'il n'y a pas de lignes qui sont des doublons à 100% d'autres lignes (évaluées sur toutes les colonnes).  

Nous commençons avec `nrow(linelist)` lignes dans `linelist`. 

```{r}
linelist <- linelist %>% 
  distinct()
```

Après la déduplication, il y a ` nrow(linelist)` lignes. Toutes les lignes supprimées auraient été des doublons à 100% d'autres lignes.  

Ci-dessous, la commande `distinct()` est ajoutée à la chaîne de nettoyage :

```{r}
# NETTOYAGE: enchainement de commandes  'PIPE'  (debute avec la donnée brute  et le  pipe avec les commandes pour le nettoyage)
##################################################################################

# Debuter le processus de Nettoyage pipé 

###########################
linelist <- linelist_raw %>%
    
    # standardiser le syntaxe des noms de colonnes
    janitor::clean_names() %>% 
    
    # Renommer manuellement les noms de colonnes
           # Nouveau nom             # Ancien nom
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # supprimer les colonnes
    select(-c(row_num, merged_header, x28)) %>% 
  
    # CI DESSUS LES ETAPES DU NETTOYAGE DEJA ABORDEES EN AMONT 
    #####################################################
    
    # Supprimer les doublons
    distinct()
```





<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Creation et transformation de colonne { }


**Nous recommandons d'utiliser la fonction `mutate()` du package dplyr  pour ajouter une nouvelle colonne, ou pour modifier une colonne existante.**  

Vous trouverez ci-dessous un exemple de création d'une nouvelle colonne avec `mutate()`. La syntaxe est la suivante : `mutate(nouveau_nom_de_colonne = valeur ou transformation)`.  

Dans Stata, ceci est similaire à la commande `generate`, mais la fonction `mutate()` de R peut également être utilisée pour modifier une colonne existante.  


### Nouvelles colonnes {.unnumbered}

La commande `mutate()` la plus basique pour créer une nouvelle colonne peut ressembler à ceci. Elle crée une nouvelle colonne `new_col` dont la valeur dans chaque ligne est 10.  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(new_col = 10)
```

Vous pouvez également référencer des valeurs dans d'autres colonnes, pour effectuer des calculs. Ci-dessous, une nouvelle colonne `bmi` est créée pour contenir l'indice de masse corporelle (IMC) pour chaque cas - tel que calculé en utilisant la formule IMC = kg/m^2, en utilisant la colonne `ht_cm` et la colonne `wt_kg`.  

```{r}
linelist <- linelist %>% 
  mutate(bmi = wt_kg / (ht_cm/100)^2)
```

Si vous créez plusieurs nouvelles colonnes, séparez-les par une virgule et une nouvelle ligne. Vous trouverez ci-dessous des exemples de nouvelles colonnes, y compris celles qui sont constituées de valeurs provenant d'autres colonnes combinées à l'aide de `str_glue()` du package **stringr** (voir la page sur [Caractères et chaînes de caractères](#character_strings).  

```{r}
new_col_demo <- linelist %>%                       
  mutate(
    new_var_dup    = case_id,             
    # nouveau colonne= dupliquer ou copier une autre colonne existante
    new_var_static = 7,                   # nouveau colonne = meme valeur sur toute les lignes
    new_var_static = new_var_static + 5,  
    # On peut ecraser une colonne et le recreer par un calcul  utilisant d'autres variables
    new_var_paste  = stringr::str_glue("{hospital} on ({date_hospitalisation})") # nouveau colonne = pasting together values from other columns
    # regrouper les valeurs de differentes  colonnes
    ) %>% 
  select(case_id, hospital, date_hospitalisation, contains("new"))        
# montrer seulement les nouveaux colonnes  pour besoin de demonstration
```


Examinez les nouvelles colonnes. À des fins de démonstration, seules les nouvelles colonnes et les colonnes utilisées pour les créer sont affichées :  


```{r message=FALSE, echo=F}
# afficher la donnée linelist comme un tableau
DT::datatable(head(new_col_demo,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

<span style="color: darkgreen;">**_CONSEILS:_** Une variante de `mutate()` est la fonction `transmute()`. Cette fonction ajoute une nouvelle colonne comme `mutate()`, mais supprime également toutes les autres colonnes que vous ne mentionnez pas entre ses parenthèses..</span>


```{r, eval=F}

# CACHER POUR LECTEUR
# Supprimer les nouveaux colonnes demo créees en haut
# linelist <- linelist %>% 
#   select(-contains("new_var"))
```



### Convertir la classe des colonnes  {.unnumbered}
  
Les colonnes contenant des valeurs qui sont des dates, des nombres ou des valeurs logiques (VRAI/FAUX) ne se comporteront comme prévu que si elles sont dans la classe appropriée. Il y a une différence entre "2" de classe caractère et 2 de classe numérique !  

Il existe des moyens de définir la classe des colonnes avec les commandes d'importation, mais cela est souvent fastidieux. Consultez la section [Bases de R](#rbasics) sur les classes d'objets pour en savoir plus sur la conversion de la classe des objets et des colonnes.  

Tout d'abord, effectuons quelques vérifications sur les colonnes importantes pour voir si elles sont de la bonne classe. Nous avons également vu cela au début lorsque nous avons lancé `skim()`.  

Actuellement, la classe de la colonne `age` est un caractère. Pour effectuer des analyses quantitatives, nous avons besoin que ces nombres soient reconnus comme numériques ! 

```{r}
class(linelist$age)
```

La classe de la colonne `date_onset` est aussi un caractère ! Pour effectuer des analyses, ces dates doivent être reconnues comme des dates! 
 
```{r}
class(linelist$date_onset)
```


Pour résoudre ce problème, utilisez la capacité de `mutate()` pour redéfinir une colonne avec une transformation. Nous définissons la colonne comme elle-même, mais convertie en une classe différente. Voici un exemple de base, convertissons ou assurerons nous que la colonne `age` est de classe Numeric :  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(age = as.numeric(age))
```

De la même manière, vous pouvez utiliser `as.character()` et `as.logical()`. Pour convertir en classe Factor, vous pouvez utiliser `factor()` de **base** R ou `as_factor()` de **forcats**. Pour en savoir plus, consultez la page [Facteurs](#factors).  

Vous devez faire attention lorsque vous convertissez en classe Date. Plusieurs méthodes sont expliquées sur la page [Manipuler les dates](#working_dates). En général, les valeurs brutes de la date doivent toutes être dans le même format pour que la conversion fonctionne correctement (par exemple "MM/JJ/AAAA", ou "JJ MM AAAA"). Après la conversion en classe Date, vérifiez vos données pour confirmer que chaque valeur a été convertie correctement.  




### Données groupées {.unnumbered}  

Si votre dataframe est déjà *groupée* (voir la page sur [Travailler sur des données groupées](#grouping_data)), `mutate()` peut se comporter différemment que si la base de données n'est pas groupée. Toutes les fonctions de résumé, comme `mean()`, `median()`, `max()`, etc. seront calculées par groupe, et non par toutes les lignes.     

```{r, eval=F}
# Normalisation de l'age en fonction la moyenne
linelist %>% 
  mutate(age_norm = age / mean(age, na.rm=T))


# Normalisation de l'age en fonction de la moyenne du jeu donne groupe  à partir de la colonne hospital
linelist %>% 
  group_by(hospital) %>% 
  mutate(age_norm = age / mean(age, na.rm=T))
```

Pour en savoir plus sur l'utilisation de `mutate ()` sur des blocs de données groupés, consultez la [documentation tidyverse mutate](https://dplyr.tidyverse.org/reference/mutate.html).  



### Transformer plusieurs colonnes {#clean_across .unnumbered}


Souvent, pour écrire un code concis, vous voulez appliquer la même transformation à plusieurs colonnes à la fois. Une transformation peut être appliquée à plusieurs colonnes à la fois en utilisant la fonction `across()` du package **dplyr** (également contenu dans le package **tidyverse**). `across()` peut être utilisé avec n'importe quelle fonction **dplyr**, mais est couramment utilisé dans `select()`, `mutate()`, `filter()`, ou `summarise()`. Voir comment il est appliqué à `summarise()` dans la page sur les [Tableaux descriptifs](#decriptive_tables).  

Spécifiez les colonnes à l'argument `.cols = ` et la ou les fonctions à appliquer à `.fns = `. Tout argument supplémentaire à fournir à la fonction `.fns` peut être inclus après une virgule, toujours dans `across()`.   

#### `across()` selection de colonne {.unnumbered}  

Spécifiez les colonnes à l'argument `.cols = `. Vous pouvez les nommer individuellement, ou utiliser les fonctions d'aide "tidyselect". Spécifiez la fonction en argument `.fns = `. Notez qu'en utilisant le mode fonction démontré ci-dessous, la fonction est écrite *sans* ses parenthèses ( ).  

Ici, la transformation `as.character()` est appliquée à des colonnes spécifiques nommées dans `across()`. 

```{r, eval=F}
linelist <- linelist %>% 
  mutate(across(.cols = c(temp, ht_cm, wt_kg), .fns = as.character))
```

Les fonctions d'aide "tidyselect" sont disponibles pour vous aider à spécifier les colonnes. Elles sont détaillées ci-dessus dans la section sur la sélection et le réordonnancement des colonnes, et elles incluent : `everything()`, `last_col()`, `where()`, `starts_with()`, `ends_with()`, `contains()`, `matches()`, `num_range()` et `any_of()`.  

Voici un exemple de la façon dont on peut changer **toutes les colonnes** en classe de caractères :  

```{r, eval=F}
# Changer toutes les colonnes en classe caractère
linelist <- linelist %>% 
  mutate(across(.cols = everything(), .fns = as.character))
```

Convertissez en caractères toutes les colonnes dont le nom contient la chaîne "date" (notez le placement des virgules et des parenthèses) :  

```{r, eval=F}
# Changer toutes les colonnes en classe caractère
linelist <- linelist %>% 
  mutate(across(.cols = contains("date"), .fns = as.character))
```

Ci-dessous, un exemple de mutation des colonnes qui sont actuellement de classe POSIXct (une classe de date brute qui montre les timestamps) - en d'autres termes, où la fonction `is.POSIXct()` évalue à `TRUE`. Ensuite, nous voulons appliquer la fonction `as.Date()` à ces colonnes pour les convertir en une classe normale de Date.  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(across(.cols = where(is.POSIXct), .fns = as.Date))
```

* Notez que dans `across()`, nous utilisons également la fonction `where()` car `is.POSIXct` est évalué à TRUE ou FALSE.  
* Notez que `is.POSIXct()` fait partie du paquet **lubridate**. D'autres fonctions "is" similaires comme `is.character()`, `is.numeric()`, et `is.logical()` sont issues de **base R**.  

#### fonction `across()`  {.unnumbered}

Vous pouvez lire la documentation avec `?across` pour des détails sur la façon de fournir des fonctions à `across()`. Quelques points récapitulatifs : il y a plusieurs façons de spécifier la ou les fonctions à exécuter sur une colonne et vous pouvez même définir vos propres fonctions :  

* Vous pouvez fournir le nom de la fonction seul (par exemple `mean` ou `as.character`)  
* Vous pouvez fournir la fonction dans le style **purrr** (par exemple `~ mean(.x, na.rm = TRUE)`) (voir [cette page][#iteration])  
* Vous pouvez spécifier plusieurs fonctions en fournissant une liste (par exemple, `list(mean = mean, n_miss = ~ sum(is.na(.x))`).  
  * Si vous fournissez plusieurs fonctions, plusieurs colonnes transformées seront retournées par colonne d'entrée, avec des noms uniques dans le format `col_fn`. Vous pouvez ajuster la façon dont les nouvelles colonnes sont nommées avec l'argument `.names =` en utilisant la syntaxe **glue** (voir la page sur [Caractères et chaînes de caractères](#character_strings)) où `{.col}` et `{.fn}` sont des raccourcis pour la colonne d'entrée et la fonction.  
  
  
Voici quelques ressources en ligne sur l'utilisation de `across()` : [creator Hadley Wickham's thoughts/rationale](https://www.tidyverse.org/blog/2020/04/dplyr-1-0-0-colwise/)




### `coalesce()` {.unnumbered}  

Cette fonction **dplyr** trouve la première valeur non manquante à chaque position. Elle "remplit" les valeurs manquantes avec la première valeur disponible dans l'ordre que vous spécifiez.

Voici un exemple *En dehors du contexte de dataframe* : Disons que vous avez deux vecteurs, l'un contenant le village de détection du patient et l'autre contenant le village de résidence du patient. Vous pouvez utiliser coalesce pour choisir la première valeur non manquante pour chaque indice :  

```{r}
village_detection <- c("a", "b", NA,  NA)
village_residence <- c("a", "c", "a", "d")

village <- coalesce(village_detection, village_residence)
village    # print
```

Cela fonctionne de la même manière si vous fournissez des colonnes de cadre de données : pour chaque ligne, la fonction attribuera la nouvelle valeur de la colonne avec la première valeur non manquante dans les colonnes que vous avez fournies (dans l'ordre fourni).

```{r, eval=F}
linelist <- linelist %>% 
  mutate(village = coalesce(village_detection, village_residence))
```

Il s'agit d'un exemple d'opération "ligne par ligne". Pour des calculs par rangée plus complexes, voir la section ci-dessous sur les calculs par rangée.  



### mathématique cumulative {.unnumbered}

Si vous voulez qu'une colonne reflète somme/moy/min/max  cumulée etc. pour les differntes observations du dataframe, utilisez les fonctions suivantes :  

`cumsum()` renvoie la somme cumulée, comme indiqué ci-dessous :  

```{r}
sum(c(2,4,15,10))     # retourne un nombre unique
cumsum(c(2,4,15,10))  # renvoie la somme cumulé a chaque élémenet parcouru du vecteur 
```

Ceci peut être utilisé dans un dataframe lors de la création d'une nouvelle colonne. Par exemple, pour calculer le nombre cumulé de cas par jour dans une épidémie, envisagez un code comme celui-ci :  

```{r, warning=F, message=F}
cumulative_case_counts <- linelist %>%  # Commencons avec la donnne linelist 
  count(date_onset) %>%                 # Creons une colonne 'n' qui totalise le nombre de ligne par jour   
  mutate(cumulative_cases = cumsum(n))  #  nouveau colonne representant la somme cumulée par ligne

```

Voici les 10 premières rangées :  

```{r}
head(cumulative_case_counts, 10)
```

Voir la page sur les [Courbes épidémiques] pour savoir comment tracer l'incidence cumulée avec l'épicurve.  

Voir aussi :  
`cumsum()`, `cummean()`, `cummin()`, `cummax()`, `cumany()`, `cumall()`.  





### Utiliser les fonctions  **base** de R {.unnumbered}  

Pour définir une nouvelle colonne (ou redéfinir une colonne) en utilisant **base** R, écrivez le nom du cadre de données, relié par `$`, à la *nouvelle* colonne (ou à la colonne à modifier). Utilisez l'opérateur d'affectation `<-` pour définir la ou les nouvelles valeurs. N'oubliez pas que lorsque vous utilisez **base** R, vous devez à chaque fois spécifier le nom du dataframe avant le nom de la colonne (par exemple, `dataframe$column`). Voici un exemple de création de la colonne `bmi` en utilisant **base** R :  

```{r, eval=F}
linelist$bmi = linelist$wt_kg / 
     (linelist$ht_cm /100) ^ 2
```




### Ajouter à la chaine de commande pipé {.unnumbered}  

**Au-dessous, une nouvelle colonne est ajoutée à la chaîne de tuyaux et certaines classes sont converties.**  

```{r }
# NETTOYAGE: enchainement de commandes  'PIPE'  (debute avec la donnée brute  et le  pipes avec les commandes pour le nettoyage)
##################################################################################

# Debuter le processus de Nettoyage pipé
###########################
linelist <- linelist_raw %>%
    
    # standardiser le syntaxe des noms de colonnes
    janitor::clean_names() %>% 
    
    # renommer manuellement les noms de colonnes
           # Nouveau nom             # ANCIEN nom
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # supprimer colonne
    select(-c(row_num, merged_header, x28)) %>% 
  
    # supprimer les doublons
    distinct() %>% 
  
    
    # CI DESSUS LES ETAPES DU NETTOYAGE DEJA ABORDEES EN AMONT 
    ###################################################
    # ajouter un nouveau colonne
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>% 
  
    # convertir les classes des colonnes
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) 
```





## Re-coder les valeurs

Voici quelques situations dans lesquelles vous devez recoder (modifier) des valeurs :  

* pour modifier une valeur spécifique (par exemple, une date dont l'année ou le format est incorrect)  
* pour uniformiser des valeurs dont l'orthographe n'est pas la même
* pour créer une nouvelle colonne de valeurs catégorielles  
* pour créer une nouvelle colonne de catégories numériques (par exemple, des catégories d'âge).  



### Valeurs spécifiques {.unnumbered}  

Pour modifier les valeurs manuellement, vous pouvez utiliser la fonction `recode()` au sein de la fonction `mutate()`. 

Imaginez qu'il y ait une date érronée dans les données (par exemple "2014-14-15") : vous pouvez corriger la date manuellement dans  les données brutes, ou vous pouvez operer le changement dans le pipeline de nettoyage via `mutate()` et `recode()`. Cette dernière solution est plus transparente et reproductible pour toute autre personne cherchant à comprendre ou à répéter votre analyse.  

```{r, eval=F}
# Corriger  les valeurs erronnées                   # ancienne valeur        # nouvelle valeur
linelist <- linelist %>% 
  mutate(date_onset = recode(date_onset, "2014-14-15" = "2014-04-15"))
```

La ligne `mutate()` ci-dessus peut être lue comme : "muter la colonne `date_onset` pour qu'elle soit égale à la colonne `date_onset` recodée de façon à ce que l'ancienne Valeur soit changée en Nouvelle Valeur". Notez que ce modèle (Ancienne = Nouvelle) pour `recode()` est l'opposé de la plupart des modèles R (new = old). La communauté de développement de R travaille à la révision de ce modèle.  

**Voici un autre exemple de recodage de plusieurs valeurs dans une même colonne.** 

Dans `linelist`, les valeurs de la colonne "hospital" doivent être nettoyées. Il y a plusieurs orthographes différentes et de nombreuses valeurs manquantes.

```{r}
table(linelist$hospital, useNA = "always")  
# Afficher un tableau avec toutes les valeurs uniques y compris les les valeurs manquantes
```

La commande `recode()` ci-dessous redéfinit la colonne "hospital" comme la colonne actuelle "hospital", mais avec les changements de recode spécifiés. N'oubliez pas les virgules après chacun d'eux !  

```{r}
linelist <- linelist %>% 
  mutate(hospital = recode(hospital,
                # Pour reference: ANCIEN =NOUVEAU
                      "Mitylira Hopital"  ="Military Hospital",
                      "Mitylira Hospital" ="Military Hospital",
                      "Military Hopital"  ="Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      ))
```


Nous voyons maintenant que les orthographes de la colonne `hospital` ont été corrigées et consolidées :  

```{r}
table(linelist$hospital, useNA = "always")
```

<span style="color: darkgreen;">**_CONSEIL:_** Le nombre d'espaces avant et après un signe égal n'a pas d'importance. Rendez votre code plus facile à lire en alignant le signe = pour toutes ou la plupart des lignes. En outre, envisagez d'ajouter une ligne de commentaires  afin de clarifier pour les futurs lecteurs quel côté est l'ANCIEN et quel côté est le NOUVEAU. </span>  

<span style="color: darkgreen;">**_CONSEIL:_** Parfois, une valeur de caractère *vide* existe dans un jeu de donnée (non reconnue comme la valeur de R pour les manquants - `NA`). Vous pouvez référencer cette valeur avec deux guillemets sans espace entre eux ("").</span>  




### par logique {.unnumbered}

Nous démontrons ci-dessous comment recoder les valeurs d'une colonne en utilisant la logique et les conditions :  

* Utiliser `replace()`, `ifelse()` et `if_else()` pour une logique simple.
* Utilisation de `case_when()` pour une logique plus complexe.  



### Logique simple {.unnumbered}  


#### `replace()` {.unnumbered}  

Pour recoder avec des critères logiques simples, vous pouvez utiliser `replace()` dans `mutate()`. `replace()` est une fonction de **base** R. Utilisez une condition logique pour spécifier les lignes à changer . La syntaxe générale est la suivante :  

`mutate(col_a_change = replace(col_a_changer, condition sur les lignes, nouvelle valeur))`.  

Une situation courante pour utiliser `replace()` est **la modification d'une seule valeur dans une ligne, en utilisant un identifiant propre à une ligne **. Ci-dessous, le sexe est changé en "Female" dans la ligne où la colonne `case_id` est "2195".  

```{r, eval=F}

# Exemple : Changer en "Female" le genre pour une observation definie
linelist <- linelist %>% 
  mutate(gender = replace(gender, case_id == "2195", "Female"))
```

La commande équivalente utilisant la syntaxe **base** R et les crochets d'indexation `[ ]` est presenté ci-dessous. Elle se lit comme suit : "Changez la valeur de la colonne `gender` du dataframe `linelist` (pour les lignes où la colonne `case_id` de `linelist` a la valeur '2195') en 'Female'".   

```{r, eval=F}
linelist$gender[linelist$case_id == "2195"] <- "Female"
```




#### `ifelse()` et `if_else()` {.unnumbered}  

Une autre fonction pour la logique simple est `ifelse()` et son partenaire `if_else()`. Cependant, dans la plupart des cas de recodage, il est plus clair d'utiliser `case_when()` (détaillé ci-dessous). Ces commandes "if else" sont des versions simplifiées d'une instruction de programmation `if` et `else`. La syntaxe générale est la suivante :  
`ifelse(condition, valeur à renvoyer si la condition vaut VRAI, valeur à renvoyer si la condition vaut FAUX)`. 

Ci-dessous, la colonne `source_known` est définie. Sa valeur dans une ligne donnée est définie comme "connue" si la valeur de la colonne `source` de cette ligne n'est pas *manquante*. Si la valeur de la colonne `source` *est* manquante, alors la valeur de la colonne `source_known` est définie comme "inconnue".  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(source_known = ifelse(!is.na(source), "known", "unknown"))
```

`if_else()` est une version spéciale de **dplyr** qui gère les dates. Notez que si la valeur "true" est une date, la valeur "false" doit aussi être une date, d'où l'utilisation de la valeur spéciale "NA_real_" au lieu de "NA".

```{r, eval=F}

# Creer une colonne nommé date of death qui a comme valeur NA si le patient n'est pas mort
linelist <- linelist %>% 
  mutate(date_death = if_else(outcome == "Death", date_outcome, NA_real_))
```

**Évitez d'enchaîner les commandes ifelse... utilisez plutôt `case_when()`!** `case_when()` est beaucoup plus facile à lire et vous ferez moins d'erreurs.  

```{r, fig.align = "center", out.width = "100%", echo=F}
knitr::include_graphics(here::here("images", "ifelse bad.png"))
```

En dehors du contexte d'un dataframe, si vous voulez qu'un objet utilisé dans votre code change de valeur, pensez à utiliser `switch()` une fonction  **base** de R.  




### Logique complexe {#clean_case_when .unnumbered}  

Utilisez la fonction `case_when()` de **dplyr** si vous effectuez un recodage dans de nombreux nouveaux groupes, ou si vous devez utiliser des instructions logiques complexes pour recoder des valeurs. Cette fonction évalue chaque ligne du cadre de données, détermine si les lignes répondent aux critères spécifiés et attribue la nouvelle valeur correcte.  

Les commandes `case_when()` sont des instructions qui ont un côté droit (CD) et un côté gauche (CG) séparés par un "tilde" `~`. Les critères logiques se trouvent dans la partie gauche et les valeurs d'application dans la partie droite de chaque instruction. Les déclarations sont séparées par des virgules.  

Par exemple, ici nous utilisons les colonnes `age` et `age_unit` pour créer une colonne `age_years` :  


```{r}
linelist <- linelist %>% 
  mutate(age_years = case_when(
            age_unit == "years"  ~ age,       # Si l'age est donné en années
            age_unit == "months" ~ age/12,    # si l'age est donnée en mois, divise par 12
            is.na(age_unit)      ~ age))      # Si l'unite d'age est une valeur maquante, garde comme années
                                              # toute valeur non prise en compte par ces conditions seront consideres commme valeur NA (manquante)
```


Lorsque chaque ligne des données est évaluée, les critères sont appliqués/évalués dans l'ordre où les instructions `case_when()` sont écrites - de haut en bas. Si le premier critère est évalué à `TRUE` pour une ligne donnée, la valeur CD est attribuée, et les autres critères ne sont même pas testés pour cette ligne. Il est donc préférable d'écrire les critères les plus restrictifs en premier, et les plus généraux en dernier. Une ligne de données qui ne répond à aucun des critères de droite se verra attribuer la valeur "NA" (manquante).   

Parfois, vous pouvez écrire une instruction finale qui attribue une valeur pour tous les autres scénarios non décrits par l'une des lignes précédentes. Pour faire cela, placez `TRUE` sur le côté gauche, ce qui permettra de capturer toute ligne qui ne répond à aucun des critères précédents. Le côté droit de cette déclaration pourrait se voir attribuer une valeur comme "vérifiez-moi !" ou manquante.  

Voici un autre exemple de `case_when()` utilisé pour créer une nouvelle colonne avec la classification du patient, selon une définition de cas pour les cas confirmés et suspectés :  


```{r, eval=F}
linelist <- linelist %>% 
     mutate(case_status = case_when(
          
          # si le patient a fait un test de laboratoire et le test est positif,
          # il est marqué comme un cas confirmé 
          ct_blood < 20                   ~ "Confirmed",
          
          # étant donné qu'un patient n'a pas de résultat de laboratoire positif,
          # si le patient a une "source" (lien épidémiologique) ET a de la fièvre, 
          # alors il est considéré comme un cas suspect
          !is.na(source) & fever == "yes" ~ "Suspect",
          
          # tout autre patient non traité ci-dessus 
          # est marqué pour un suivi
          TRUE                            ~ "To investigate"))
```


<span style="color: red;">**_DANGER:_** **Les valeurs du côté droit doivent toutes être de la même classe** - soit numérique, caractère, date, logique, etc. Pour attribuer des valeurs manquantes (`NA`), Il est dnas certaine situation important d'utiliser  des variantes spéciales de `NA` telles que `NA_caracter_`, `NA_real_` (pour les numériques ou POSIX), et `as.Date(NA)`. Pour en savoir plus, lisez [Manipuler les dates](#working_dates).</span>  




### Valeurs manquantes {.unnumbered} 

Vous trouverez ci-dessous des fonctions spéciales pour le traitement des valeurs manquantes dans le cadre du nettoyage des données.  

Voir la page sur les [Données manquantes] pour des conseils plus détaillés sur l'identification et la gestion des valeurs manquantes. Par exemple, la fonction `is.na()` qui teste logiquement l'absence de données.  


**`replace_na()`**  

Pour changer les valeurs manquantes (`NA`) en une valeur spécifique, telle que "Missing", utilisez la fonction **dplyr** `replace_na()` dans `mutate()`. Notez que cette fonction est utilisée de la même manière que `recode` ci-dessus - le nom de la variable doit être mentionnée dans `replace_na()`.  

```{r}
linelist <- linelist %>% 
  mutate(hospital = replace_na(hospital, "Missing"))
```


**fct_explicit_na()**  

C'est une fonction du package **forcats** qui permet de manipuler les colonnes de la classe Factor. Les facteurs constitue  la facon dont  R  gère les valeurs *ordonnées* telles que `c("Premier", "Deuxieme", "Troisieme")` ou pour définir l'ordre dans lequel les valeurs (par exemple les hôpitaux) apparaissent dans les tableaux et les graphiques. Voir la page sur les [Facteurs](#factors).  

Si vos données sont de la classe Factor et que vous essayez de convertir `NA` en "Missing" en utilisant `replace_na()`, vous obtiendrez cette erreur : `invalid factor level, NA generated`. Vous avez essayé d'ajouter "Missing" comme valeur, alors qu'il n'était pas défini comme un niveau possible du facteur, et il a été rejeté.  

La façon la plus simple de résoudre ce problème est d'utiliser la fonction  `fct_explicit_na()` du package **forcats** qui convertit une colonne en classe facteur , et convertit les valeurs `NA` en caractère "(Missing)".  

```{r, eval=F}
linelist %>% 
  mutate(hospital = fct_explicit_na(hospital))
```

Une alternative plus lente serait d'ajouter le niveau du facteur en utilisant `fct_expand()` et ensuite de convertir les valeurs manquantes.  

**`na_if()`**  

Pour convertir une valeur *spécifique en* `NA`, utilisez la fonction `na_if()` de **dplyr**. La commande ci-dessous effectue l'opération inverse de `replace_na()`. Dans l'exemple ci-dessous, toutes les valeurs de "Missing" dans la colonne `hospital` sont converties en `NA`.  

```{r}
linelist <- linelist %>% 
  mutate(hospital = na_if(hospital, "Missing"))
```

Remarque : `na_if()` **ne peut pas être utilisé pour des critères logiques** (par exemple "toutes les valeurs > 99") - utilisez `replace()` ou `case_when()` pour cela :  

```{r, eval=F}
# remplacer les temperature superieure à 40 par NA
linelist <- linelist %>% 
  mutate(temp = replace(temp, temp > 40, NA))

# Convert onset dates earlier than 1 Jan 2000 to missing
# Convertir en valeur manquante toutes dates d'appartion  avant le 1 Jan 2000 
linelist <- linelist %>% 
  mutate(date_onset = replace(date_onset, date_onset > as.Date("2000-01-01"), NA))
```




### Dictionnaire contenant les parametres de nettoyage {.unnumbered}

Utilisez le package **matchmaker** et sa fonction `match_df()` pour nettoyer un dataframe avec un *dictionnaire de nettoyage*. 

1) Créez un dictionnaire de nettoyage avec 3 colonnes :  
    * Une colonne "from" (la valeur incorrecte)  
    * Une colonne "to" (la valeur correcte)  
    * Une colonne spécifiant la colonne pour laquelle les changements doivent être appliqués (ou ".global" pour appliquer à toutes les colonnes).  

Remarque : les entrées du dictionnaire .global seront remplacées par les entrées du dictionnaire spécifiques à la colonne.  

```{r, fig.align = "center", out.width = "75%", echo=F}
knitr::include_graphics(here::here("images", "cleaning_dict.png"))
```


2) Importez le fichier du dictionnaire dans R. Cet exemple peut être téléchargé via les instructions de la page [Télécharger le manuel et les données](#download_book_data).  

```{r, echo=F}
cleaning_dict <- rio::import(here("data", "case_linelists", "cleaning_dict.csv"))
```

```{r, eval=F}
cleaning_dict <- rio::import("cleaning_dict.csv")
```

3) Pipez le jeu de donnée brute linelist à `match_df()`, en spécifiant à `dictionary = ` le dataframe du dictionnaire de nettoyage.L'argument `from = ` doit être le nom de la colonne du dictionnaire qui contient les "anciennes" valeurs, l'argument `by = ` doit être la colonne du dictionnaire qui contient les "nouvelles" valeurs correspondantes, et la troisième colonne indique la colonne dans laquelle effectuer le changement. Utilisez `.global` dans la colonne `by = ` pour appliquer un changement à toutes les colonnes. Une quatrième colonne de dictionnaire `order` peut être utilisée pour spécifier l'ordre des facteurs des nouvelles valeurs.  
Vous trouverez plus de détails dans la [documentation du package](https://cran.r-project.org/web/packages/matchmaker/vignettes/intro.html) en exécutant `?match_df`. Notez que l'exécution de cette fonction peut prendre beaucoup de temps pour un grand jeu de données.  

```{r}

linelist <- linelist %>% # fournissez ou pipez votre jeu de données
     matchmaker::match_df(
          dictionary = cleaning_dict, # nom de votre dictionnaire
          from = "from", # colonne avec les valeurs à remplacer (par défaut, col 1)
          to = "to", # colonne avec les valeurs finales (par défaut col 2)
          by = "col", # colonne avec les noms de colonnes (par défaut col 3)
  )

```

Maintenant, faites défiler vers la droite pour voir comment les valeurs ont changé - en particulier le `genre`(de minuscule à majuscule), et toutes les colonnes de symptômes ont été transformées de oui/non à 1/0.  

```{r message=FALSE, echo=F}
# Afficher la donnée linelist comme un tableau
DT::datatable(head(linelist,50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


Notez que les noms de vos colonnes dans le dictionnaire de nettoyage doivent correspondre aux noms *à ce stade* dans votre script de nettoyage. Voir cette [référence en ligne pour le package linelist](https://www.repidemicsconsortium.org/linelist/reference/clean_data.html) pour plus de détails.





#### Ajouter à la chaine de commande pipé{.unnumbered}  

**Ci dessous quelques nouveaux colonnes et des transformations operes sur les colonnes existantes sont implementés dans la chaine pipé.**  

```{r}
# NETTOYAGE: enchainement de commandes  'PIPE'  (debute avec la donnée brute  et le  pipe avec les commandes pour le nettoyage)
##################################################################################

# Debuter le processus de Nettoyage pipé
###########################
linelist <- linelist_raw %>%
    
    # standardiser la syntaxe des noms de colonnes
    janitor::clean_names() %>% 
    
    # renommer manuellement les noms de colonnes
           # Nouveau nom             # Ancien nom
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # supprimer colonne
    select(-c(row_num, merged_header, x28)) %>% 
  
    # supprimer les doublon
    distinct() %>% 
  
    # ajouter colonne
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>%     

    # convertir les classes des colonnes
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) %>% 
    
    # ajout colonnes: Delai de l'apparition de la maladie et  l'hospitalisation
    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %>% 
    
   # CI DESSUS LES ETAPES DU NETTOYAGE DEJA ABORDEES EN AMONT
   ###################################################

    # Nettoyer les  valeurs de la colonne hospital 
    mutate(hospital = recode(hospital,
                      # ANCIEN = NOUVEAU
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      )) %>% 
    
    mutate(hospital = replace_na(hospital, "Missing")) %>% 

    # creer une colonne  age_years  (à partir des solonnes  age et age_unit)
    mutate(age_years = case_when(
          age_unit == "years" ~ age,
          age_unit == "months" ~ age/12,
          is.na(age_unit) ~ age,
          TRUE ~ NA_real_))
```






<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Classes numeriques {#num_cats}


Nous décrivons ici quelques approches spéciales pour créer des catégories à partir de colonnes numériques. Les exemples les plus courants sont les catégories d'âge, les groupes de valeurs de laboratoire, etc. Nous discuterons ici :  

* `age_categories()`, du paquet **epikit**.  
* `cut()`, du package **base** R  
* `case_when()`  
* Les ruptures quantiles avec `quantile()` et `ntile()`. 


### Revoir la distribution {.unnumbered}

Pour cet exemple, nous allons créer une colonne `age_cat` en utilisant la colonne `age_years`.  

```{r}

# Verifions la class de la colonne age_years
class(linelist$age_years)
```

Tout d'abord, examinez la distribution de vos données, afin de définir limites  appropriés. Voir la page sur [les bases de ggplot](#ggplot_basics).  

```{r, out.height='50%'}
# examine the distribution
hist(linelist$age_years)
```

```{r}
summary(linelist$age_years, na.rm=T)
```

<span style="color: orange;">**_ATTENTION:_** Parfois, les variables numériques sont importées en tant que classe "character". Cela se produit s'il y a des caractères non numériques dans certaines des valeurs, par exemple une entrée de "2 mois" pour l'âge, ou (en fonction des paramètres par defaut de R) si une virgule est utilisée à la place des décimales (par exemple "4,5" pour signifier quatre ans et demi)..</span>


<!-- ======================================================= -->
### `age_categories()` {.unnumbered}

Avec le packagge **epikit**, vous pouvez utiliser la fonction `age_categories()` pour catégoriser et étiqueter facilement les colonnes numériques (noté que cette fonction peut aussi être appliquée à des variables non numériques d'âge). En guise de bonus, la colonne de sortie est automatiquement une variable categorielle ordonnée.  

Voici les entrées requises :  

* Un tableau numérique (colonne)  
* L'argument `breakers = ` - fournit un vecteur numérique de points de rupture pour les nouveaux groupes.  

Tout d'abord, l'exemple le plus simple :  

```{r}
# Exemple basique
################
pacman::p_load(epikit)                    # chargerle  package

linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(             # creer un nouveau colonne
      age_years,                            # colonne numerique pour  concevoir des groupes
      breakers = c(0, 5, 10, 15, 20,        # les  bornes
                   30, 40, 50, 60, 70)))

# afficher le tableau
table(linelist$age_cat, useNA = "always")
```

Les valeurs de bornes que vous spécifiez sont par défaut les limites inférieures - c'est-à-dire qu'elles sont incluses dans le groupe "supérieur" / les groupes sont "ouverts" du côté inférieur/gauche. Comme indiqué ci-dessous, vous pouvez ajouter 1 à chaque valeur de rupture pour obtenir des groupes ouverts en haut/à droite.
 
```{r}
# Inclure les bornes superieures pour les memes classes
############################################
linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(
      age_years, 
      breakers = c(0, 6, 11, 16, 21, 31, 41, 51, 61, 71)))

# afficher le tableau
table(linelist$age_cat, useNA = "always")
```


Vous pouvez ajuster la façon dont les étiquettes sont affichées avec `separator = `. La valeur par défaut est "-".  

Vous pouvez ajuster la façon dont les numéros supérieurs sont traités, avec l'argument `ceiling = `. Pour définir une coupure supérieure, mettez `ceiling = TRUE`. Dans cette utilisation, la valeur de rupture la plus élevée fournie est un "plafond" et une catégorie "XX+" n'est pas créée. Toutes les valeurs supérieures à la valeur de rupture la plus élevée (ou à `upper = `, s'il est défini) sont classées dans la catégorie `NA`. Voici un exemple avec `ceiling = TRUE`, de sorte qu'il n'y a pas de catégorie XX+ et que les valeurs supérieures à 70 (la valeur de rupture la plus élevée) sont classées comme NA.  

```{r}
# Avec l'argument ceiling definit comme  TRUE
##########################
linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(
      age_years, 
      breakers = c(0, 5, 10, 15, 20, 30, 40, 50, 60, 70),
      ceiling = TRUE)) # 70 is le plafond, toute valeur au dela  devient  NA

# afficher le tableau
table(linelist$age_cat, useNA = "always")
```

Alternativement, au lieu de `breakers = `, vous pouvez fournir tous les `lower = `, `upper = `, et `by = ` :  

* `lower = ` Le nombre le plus bas que vous voulez prendre en compte - la valeur par défaut est 0  
* `upper = ` Le nombre le plus élevé que vous voulez considérer  
* `by = ` Le nombre d'années entre les groupes  

```{r}
linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(
      age_years, 
      lower = 0,
      upper = 100,
      by = 10))

# afficher tableau
table(linelist$age_cat, useNA = "always")
```


Consultez la page d'aide de la fonction pour plus de détails (entrez `?age_categories` dans la console R). 


<!-- ======================================================= -->
### `cut()` {.unnumbered}

`cut()` est une alternative à `age_categories()` présente dans les packages de **base** de R ,  vous verrez pourquoi `age_categories()` a été développé pour simplifier ce processus. Quelques différences notoires  avec `age_categories()` sont :  

* Vous n'avez pas besoin d'installer/charger un autre package  
* Vous pouvez spécifier si les groupes sont ouverts/fermés à droite/à gauche.  
* Vous devez fournir vous-même des étiquettes précises  
* Si vous voulez que 0 soit inclus dans le groupe le plus bas, vous devez le spécifier.  

La syntaxe de base de `cut()` est de fournir d'abord la colonne numérique à découper (`age_years`), puis l'argument *breaks*, qui est un vecteur numérique `c()` de points de rupture. En utilisant `cut()`, la colonne résultante est un facteur ordonné.  

Par défaut, la catégorisation se produit de sorte que le côté droit/supérieur est "ouvert" et inclusif (et le côté gauche/inférieur est "fermé" ou exclusif). C'est le comportement opposé de la fonction `age_categories()`. Les étiquettes par défaut utilisent la notation "(A, B]", ce qui signifie que A n'est pas inclus mais que B l'est. **Inversez ce comportement en fournissant l'argument `right = TRUE`.   

Ainsi, par défaut, les valeurs "0" sont exclues du groupe le plus bas, et catégorisées comme `NA` ! Les valeurs "0" pourraient être des nourrissons codés comme ayant l'âge 0, alors faites attention ! Pour changer cela, ajoutez l'argument `include.lowest = TRUE` pour que toutes les valeurs "0" soient incluses dans le groupe le plus bas. L'étiquette générée automatiquement pour la catégorie la plus basse sera alors "[A],B]". Notez que si vous incluez l'argument `include.lowest = TRUE` **et** `right = TRUE`, l'inclusion extrême s'appliquera maintenant à la valeur et à la catégorie du point de rupture *haut*, et non à la plus basse.  

Vous pouvez fournir un vecteur d'étiquettes personnalisées en utilisant l'argument `labels = `. Comme ils sont écrits manuellement, faites très attention à ce qu'ils soient exacts ! Vérifiez votre travail en utilisant des tableaux croisés, comme décrit ci-dessous. 

Voici un exemple de `cut()` appliqué à `age_years` pour créer la nouvelle variable `age_cat` :  

```{r}

# Creons une nouvelle variable en decoupant par interval la variable numerique age

#La valeur de la borne inferieure est exclu mais la borne superieu est inclue dans chaque groupe
linelist <- linelist %>% 
  mutate(
    age_cat = cut(
      age_years,
      breaks = c(0, 5, 10, 15, 20,
                 30, 50, 70, 100),
      include.lowest = TRUE         # inclu le 0 dans le premier interval crée
      ))

# Representer dans un tableau les nombres d'observations en fonction des categories créee
table(linelist$age_cat, useNA = "always")
```


**Vérifiez que chaque valeur d'âge a été affectée à la bonne catégorie en croisant les colonnes numériques et de catégorie. Examinez l'attribution des valeurs limites (par exemple 15, si les catégories voisines sont 10-15 et 16-20).  

```{r}
# Cross tabulation of the numeric and category columns. 
# tableau croisé entre les colonnes numeriques et categorielles
table("Numeric Values" = linelist$age_years,   # names specified in table for clarity.
      "Categories"     = linelist$age_cat,
      useNA = "always")                        
# N'oublier pas d'examiner les valeurs NA
```





**Réétiquetage des valeurs `NA` **.

Vous pouvez vouloir attribuer aux valeurs `NA` une étiquette telle que "Missing". Comme la nouvelle colonne est de la classe Factor (valeurs restreintes), vous ne pouvez pas simplement la muter avec `replace_na()`, car cette valeur sera rejetée. A la place, utilisez `fct_explicit_na()` de **forcats** comme expliqué dans la page [Facteurs](#factors).   

```{r}
linelist <- linelist %>% 
  
  # cut() creates age_cat, automatically of class Factor      
     # cut() crée  automatique une colonne dénommée age_cat avec des valeurs categorielles 
  mutate(age_cat = cut(
    age_years,
    breaks = c(0, 5, 10, 15, 20, 30, 50, 70, 100),          
    right = FALSE,
    include.lowest = TRUE,        
    labels = c("0-4", "5-9", "10-14", "15-19", "20-29", "30-49", "50-69", "70-100")),
         
   
    # nommer explicitemment les valeurs manquantes
    age_cat = fct_explicit_na(
      age_cat,
      na_level = "Missing age")  # you can specify the label
    # on peut specifier les etiquettes
  )    

# tableau pour voir les effectifs
table(linelist$age_cat, useNA = "always")
```

**Créer des seuils de rupture et des étiquettes**  

Pour une manière rapide de faire des pauses et de labelliser des vecteurs, utilisez quelque chose comme ci-dessous. Voir la page [Bases de R](#rbasics) pour les références sur `seq()` et `rep()`.  

```{r, eval=F}
# Make break points from 0 to 90 by 5
# Crée un vecteur allant de 0 à 90 avec des pas de 5
age_seq = seq(from = 0, to = 90, by = 5)
age_seq

# Make labels for the above categories, assuming default cut() settings
# Créé des etiquettes pour les categories concues ci dessus avec les parametres de defaut de cut() 
age_labels = paste0(age_seq + 1, "-", age_seq + 5)
age_labels

# check that both vectors are the same length
# Montrons que que les deux vecteurs sont de meme longueurs
length(age_seq) == length(age_labels)
```


Lisez plus sur `cut()` dans sa page d'aide en entrant `?cut` dans la console R.  




### Seuil de rupture par le Quantile  {.unnumbered}  

Dans le langage courant, les "quantiles" ou "percentiles" font généralement référence à une valeur en dessous de laquelle se situe une proportion de valeurs. Par exemple, le 95ème percentile des âges dans `linelist` serait l'âge en dessous duquel 95% de l'âge tombe.  

Cependant, dans le langage courant, les "quartiles" et les "déciles" peuvent également faire référence aux *groupes de données* divisés de manière égale en 4 ou 10 groupes (notez qu'il y aura un point de rupture de plus que le groupe).    

Pour obtenir les points de rupture des quantiles, vous pouvez utiliser `quantile()` du paquet **stats** de **base** R. Vous fournissez un vecteur numérique (par exemple une colonne dans un ensemble de données) et un vecteur de valeurs numériques de probabilité allant de 0 à 1,0. Les points de rupture sont renvoyés sous la forme d'un vecteur numérique. Explorez les détails des méthodologies statistiques en entrant `?quantile`.  

* Si votre tableau numérique d'entrée a des valeurs manquantes, il est préférable de définir `na.rm = TRUE`.  
* Définissez `names = FALSE` pour obtenir un tableau numérique sans nom.  

```{r}
quantile(linelist$age_years,              #specifier le vecteur numerique sur lequel on travaille
         
  probs = c(0, .25, .50, .75, .90, .95),  #specifier les centiles qui vous interesse
  na.rm = TRUE)                            # ignorer les valeurs manquantes 
```

Vous pouvez utiliser les résultats de `quantile()` comme points de rupture dans `age_categories()` ou `cut()`. Ci-dessous, nous créons une nouvelle colonne `déciles` en utilisant `cut()` où les ruptures sont définies en utilisant `quantiles()` sur `age_years`. Ci-dessous, nous affichons les résultats en utilisant `tabyl()` de **janitor** pour que vous puissiez voir les pourcentages (voir la page [Tableaux descriptifs](#descriptive_tables)). Notez comment ils ne sont pas exactement 10% dans chaque groupe.  

```{r}
linelist %>%                              #commencer avec la donnéé linelist
  mutate(deciles = cut(age_years,           # creer un nouveau colonne decile qui represente des classes issues de l'application de cut() sur age_years 
    breaks = quantile(                      #definir les seuils  de la fonction cut en utilisant quantile()
      age_years,                               # utiliser  la colonne age_years
      probs = seq(0, 1, by = 0.1),             # 0.0 à 1.0 pas de  0.1
      na.rm = TRUE),                           # ignorer les valeurs manquantes
    include.lowest = TRUE)) %>%             #Pour cut() inclure age 0
  janitor::tabyl(deciles)                   # piper pour obtenir un tableau à afficher
```

### Groupes de taille égale {.unnumbered}  

Un autre outil pour créer des groupes numériques est la fonction **dplyr** `ntile()`, qui tente de diviser vos données en n *groupes de taille égale* - *mais sachez que contrairement à `quantile()`, la même valeur peut apparaître dans plus d'un groupe.* Fournissez le tableau numérique et ensuite le nombre de groupes. Les valeurs dans la nouvelle colonne créée sont juste des "numéros" de groupe (par exemple 1 à 10), et non la plage de valeurs elle-même comme lors de l'utilisation de `cut()`.  

```{r}
# créer des classes avec ntile()
ntile_data <- linelist %>% 
  mutate(even_groups = ntile(age_years, 10))

# créer un tableau avec les effectifs et les frequences des classes
ntile_table <- ntile_data %>% 
  janitor::tabyl(even_groups)
  

# ajouter les valeurs min/max pour voir l'etendu des classe
ntile_ranges <- ntile_data %>% 
  group_by(even_groups) %>% 
  summarise(
    min = min(age_years, na.rm=T),
    max = max(age_years, na.rm=T)
  )

# combine and print - note that values are present in multiple groups
#
left_join(ntile_table, ntile_ranges, by = "even_groups")
```


<!-- ======================================================= -->
### `case_when()` { .unnumbered}

Il est possible d'utiliser la fonction **dplyr** `case_when()` pour créer des classes à partir d'une colonne numérique, mais il est plus facile d'utiliser `age_categories()` de **epikit** ou `cut()` car ceux-ci créeront un facteur ordonné automatiquement. 

Si vous utilisez `case_when()`, veuillez revoir l'utilisation correcte comme décrit précédemment dans la section Re-coder les valeurs de cette page. Sachez également que toutes les valeurs du côté droit doivent être de la même classe. Ainsi, si vous voulez que `NA` figure à droite, vous devez soit écrire "Missing", soit utiliser la valeur spéciale `NA`, `NA_character_`.  


### Ajouter à la chaine de commande pipé {.unnumbered}  

Ci-dessous, le code pour créer deux colonnes d'âge catégorique est ajouté à la chaîne de nettoyage :  

```{r}
# NETTOYAGE: enchainement de commandes  'PIPE'  (debute avec la donnée brute  et suivi de  pipe avec les commandes pour le nettoyage)
##################################################################################

# Debuter le nettoyage de la chaine de commande pipé
###########################
linelist <- linelist_raw %>%
    
    # standardiser la syntaxe des noms de colonnes
    janitor::clean_names() %>% 
    
    # renommons manuellement les noms de colonnes
           # Nouveau nom             # Ancien nom
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # supprimer colonne
    select(-c(row_num, merged_header, x28)) %>% 
  
    # supplimer les doublons
    distinct() %>% 

    # ajouter colonnes
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>%     

    # convertir la classe des colonnes
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) %>% 
    
    # creer colonne: retard d'hospitalisation
    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %>% 
    
    # rendre propre les valeurs contenu dans la colonne hospitalisation
    mutate(hospital = recode(hospital,
                      # OLD = NEW
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      )) %>% 
    
    mutate(hospital = replace_na(hospital, "Missing")) %>% 

    # creons la colonne age_years column (à partir des colonnes age et age_unit)
    mutate(age_years = case_when(
          age_unit == "years" ~ age,
          age_unit == "months" ~ age/12,
          is.na(age_unit) ~ age)) %>% 
  
    # CI DESSUS LES ETAPES DU NETTOYAGE DEJA ABORDEES EN AMONT
    ###################################################   
    mutate(
          # age classe: difinition
          age_cat = epikit::age_categories(age_years, breakers = c(0, 5, 10, 15, 20, 30, 50, 70)),
        
          # age classe: 0 à 85 par pas de 5s
          age_cat5 = epikit::age_categories(age_years, breakers = seq(0, 85, 5)))
```








<!-- ======================================================= -->
## Ajouter des lignes

### une à une {.unnumbered}  

Ajouter des lignes une par une manuellement est fastidieux mais peut être fait avec `add_row()` de **dplyr**. Rappelez-vous que chaque colonne doit contenir des valeurs d'une seule classe (soit caractère, numérique, logique, etc.). Ainsi, l'ajout d'une ligne nécessite de la nuance pour maintenir cela. 

```{r, eval=F}
linelist <- linelist %>% 
  add_row(row_num = 666,
          case_id = "abc",
          generation = 4,
          `infection date` = as.Date("2020-10-10"),
          .before = 2)
```

Utilisez `.before` et `.after.` pour spécifier le placement de la ligne que vous voulez ajouter. `.before = 3` placera la nouvelle ligne avant la 3ème ligne actuelle. Le comportement par défaut est d'ajouter la ligne à la fin. Les colonnes non spécifiées seront laissées vides (`NA`).  

Le nouveau *numéro de ligne* peut sembler étrange ("...23") mais les numéros de ligne dans les lignes préexistantes *ont changé*. Donc, si vous utilisez la commande deux fois, examinez/testez soigneusement l'insertion.

Si une classe que vous avez fournie est incorrecte, vous verrez une erreur comme celle-ci :  

```
Error: Can't combine ..1$infection date <date> and ..2$infection date <character>.
```

(lorsque vous insérez une ligne avec une valeur de date, n'oubliez pas d'envelopper la date dans la fonction `as.Date()` comme `as.Date("2020-10-10")`.


### coller des lignes {.unnumbered}  

Pour combiner des ensembles de données ensemble en liant les lignes d'un cadre de données au bas d'un autre cadre de données, vous pouvez utiliser `bind_rows()` de **dplyr**. Ceci est expliqué plus en détail dans la page [Joindre des données](#joining_matching).  




<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Filtrer les lignes {  }


Une étape typique de nettoyage après avoir nettoyé les colonnes et recodé les valeurs est de *filtrer* le cadre de données pour des lignes spécifiques en utilisant le verbe **dplyr** `filter()`.  

Dans `filter()`, spécifiez la logique qui doit être `TRUE` pour qu'une ligne de l'ensemble de données soit conservée. Nous montrons ci-dessous comment filtrer des lignes sur la base de conditions logiques simples et complexes.  



<!-- ======================================================= -->
### Filtre simple {.unnumbered} 

Cet exemple simple redéfinit le dataframe `linelist` comme lui-même, après avoir filtré les lignes pour répondre à une condition logique. **Seules les lignes où l'énoncé logique entre parenthèses est évalué à "VRAI" sont conservées.  

Dans cet exemple, l'instruction logique est `gender == "f"`, qui demande si la valeur de la colonne `gender` est égale à "f" (sensible à la casse).   

Avant que le filtre ne soit appliqué, le nombre de lignes dans `linelist` est `nrow(linelist)`.

```{r, eval=F}
linelist <- linelist %>% 
  filter(gender == "f")   # garder unique les lignes ou le est egale à "f"
```

Une fois le filtre appliqué, le nombre de lignes dans `linelist` est ` linelist %>% filter(gender == "f") %>% nrow()`.


### Filtrer les valeurs manquantes {.unnumbered}  

Il est assez courant de vouloir filtrer les lignes qui ont des valeurs manquantes. Résistez à l'envie d'écrire `filter(!is.na(column) & !is.na(column))` et utilisez plutôt la fonction **tidyr** qui est spécialement conçue à cet effet : `drop_na()`. Si elle est exécutée avec des parenthèses vides, elle supprime les lignes avec *toutes* les valeurs manquantes. Alternativement, vous pouvez fournir des noms de colonnes spécifiques à évaluer pour les valeurs manquantes, ou utiliser les fonctions d'aide "tidyselect" décrites [ci-dessus](#clean_tidyselect).  

```{r, eval=F}
linelist %>% 
  drop_na(case_id, age_years)  # enlever les lignes avec des valeurs manquantes pour les colonnes case_id ou age_years
```

Voir la page sur les [Données manquantes] pour de nombreuses techniques d'analyse et de gestion des données manquantes dans vos données. 




### Filtrer par numéro de ligne {.unnumbered}  

Dans un cadre de données ou un tibble, chaque ligne aura généralement un "numéro de ligne" qui (vu dans R Viewer) apparaît à gauche de la première colonne. Ce n'est pas en soi une vraie colonne dans les données, mais il peut être utilisé dans une instruction `filter()`.  

Pour filtrer sur la base du "numéro de ligne", vous pouvez utiliser la fonction **dplyr** `row_number()` avec des parenthèses ouvertes dans le cadre d'une instruction de filtrage logique. Souvent, vous utiliserez l'opérateur `%in%` et une plage de nombres dans le cadre de cette instruction logique, comme indiqué ci-dessous. Pour voir les *premières* N lignes, vous pouvez également utiliser la fonction spéciale **dplyr** `head()`.   

```{r, eval=F}

# montrer les 100 premiere lignes
linelist %>% head(100)     # ou utiliser tail() pour voir les n derniers lignes 

# afficher uniquement la cinquieme ligne
linelist %>% filter(row_number() == 5)

# voir la 2ème à la 20ème ligne et 3 colonnes specifiques
linelist %>% filter(row_number() %in% 2:20) %>% select(date_onset, outcome, age)
```

Vous pouvez également convertir les numéros de ligne en une vraie colonne en passant votre cadre de données à la fonction **tibble** `rownames_to_column()` (ne mettez rien entre les parenthèses).  


<!-- ======================================================= -->
### Filtre complexe {.unnumbered} 

Des instructions logiques plus complexes peuvent être construites en utilisant les opérateurs parenthèses `( )`, OR `|`, négation `!`, `%in%`, et AND `&`. Un exemple est donné ci-dessous :  


Remarque : Vous pouvez utiliser l'opérateur `!` devant un critère logique pour le nier. Par exemple, `!is.na(column)` est évalué à true si la valeur de la colonne n'est *pas* manquante. De même, `!column %in% c("a", "b", "c")` est évalué comme vrai si la valeur de la colonne n'est *pas* dans le vecteur.  


#### Examiner la donnée  {.unnumbered}  

Vous trouverez ci-dessous une commande simple en une ligne pour créer un histogramme des dates d'apparition. Notez qu'une deuxième épidémie plus petite, datant de 2012-2013, est également incluse dans cet ensemble de données brutes. **Pour nos analyses, nous voulons supprimer les entrées de cette épidémie antérieure.  

```{r, out.width = "50%"}
hist(linelist$date_onset, breaks = 50)
```


#### Comment les filtres traitent les valeurs numériques et les dates manquantes {.unnumbered}  

Peut-on simplement filtrer par `date_onset` les lignes après Juin 2013 ? **Attention ! L'application du code `filter(date_onset > as.Date("2013-06-01")))` supprimerait toutes les lignes de la dernière épidémie avec une date d'apparition manquante!**.  

<span style="color: red;">**_DANGERS:_** Le fait de filtrer sur une date ou un nombre supérieur (>) ou inférieur (<) peut supprimer toutes les lignes contenant des valeurs manquantes (`NA`) ! En effet, `NA` est considéré comme infiniment grand et petit.</span>

*(Voir la page [Manipuler les dates](#working_dates) pour plus d'informations sur le travail avec des dates et le paquet **lubridate**)*.

#### Concevoir le filtre {.unnumbered}  

Examinez un tableau croisé pour vous assurer que nous excluons uniquement les bonnes lignes :  


```{r}
table(Hospital  = linelist$hospital,                     # nom d'hopital
      YearOnset = lubridate::year(linelist$date_onset),  # annee des dates d'apparition 
      useNA     = "always")                              # montrer les valeurs manquantes
```

Quels autres critères pouvons-nous filtrer pour éliminer la première épidémie (en 2012 et 2013) de l'ensemble de données ? Nous constatons que :  

* La première épidémie en 2012 & 2013 a eu lieu à l'hôpital A, à l'hôpital B, et qu'il y avait aussi 10 cas à l'hôpital du Port.  
* Les hôpitaux A et B n'ont *pas* eu de cas lors de la deuxième épidémie, mais l'hôpital du Port en a eu.  

Nous voulons exclure :  

* Les ` nrow(linelist %>% filter(hospital %in% c("Hospital A", "Hospital B") | date_onset < as.Date("2013-06-01")))` lignes avec une apparition en 2012 et 2013 à l'hôpital A, B ou Port :  
  * Exclure ` nrow(linelist %>% filter(date_onset < as.Date("2013-06-01")))` les lignes dont l'apparition s'est produite en 2012 et 2013.
  * Exclure ` nrow(linelist %>% filter(hospital %in% c('Hospital A', 'Hospital B') & is.na(date_onset)))` les lignes des hôpitaux A et B avec des dates de début manquantes.  
  * Ne **pas** exclure ` nrow(linelist %>% filter(!hospital %in% c('Hospital A', 'Hospital B') & is.na(date_onset)))` d'autres lignes avec des dates de début manquantes.  

Nous commençons avec une linelist de ``nrow(linelist)`. Voici notre déclaration de filtre :  

```{r}
linelist <- linelist %>% 
  
 # conserver les lignes où le début de la maladie est postérieur 1 June 2013 OU où le début de la maladie n'a pas de valeur renseignée et où il s'agissait d'un hôpital AUTRE que l'hôpital A ou B.
  filter(date_onset > as.Date("2013-06-01") | (is.na(date_onset) & !hospital %in% c("Hospital A", "Hospital B")))

nrow(linelist)
```

Lorsque nous refaisons le tableau croisé, nous constatons que les hôpitaux A et B sont complètement retirés, ainsi que les 10 cas de l'hôpital du Port de 2012 et 2013, et que toutes les autres valeurs sont les mêmes - exactement comme nous le voulions.  
 
```{r}
table(Hospital  = linelist$hospital,                     # nom de l'hopital
      YearOnset = lubridate::year(linelist$date_onset),   # année d'apparition de la maladie
      useNA     = "always")                              # montrer les valeurs manquantes
```

Plusieurs déclarations peuvent être incluses dans une commande de filtre (séparées par des virgules), ou vous pouvez toujours utiliser une commande filter() séparée pour plus de clarté.  


*Note : certains lecteurs peuvent remarquer qu'il serait plus facile de filtrer simplement par `date_hospitalisation` parce qu'il est 100% complet, sans valeurs manquantes. Ceci est vrai. Mais `date_onset` est utilisé dans le but de démontrer un filtre complexe.* 




### Autres complements {.unnumbered}  

Le filtrage peut également être effectué comme une commande autonome (ne faisant pas partie d'une chaîne de tuyaux). Comme les autres verbes **dplyr**, dans ce cas, le premier argument doit être le jeu de données lui-même.  

```{r, eval=F}
# dataframe <- filter(dataframe, condition(s)pour les lignes à garder)

linelist <- filter(linelist, !is.na(case_id))
```

Vous pouvez également utiliser **base** R pour effectuer un sous-ensemble en utilisant des crochets qui reflètent les [lignes, colonnes] que vous souhaitez conserver.  

```{r, eval=F}
# dataframe <- dataframe[lignes conditions, colonnes conditions] (vide signifie garder tous les colonnes ou lignes)

linelist <- linelist[!is.na(case_id), ]
```





### Examiner rapidement la donnée {.unnumbered} 

Souvent, vous voulez examiner rapidement quelques enregistrements, pour seulement quelques colonnes. La fonction R **base** `View()` imprimera un cadre de données pour le visualiser dans votre RStudio. 

Visualisez la liste des lignes dans RStudio :  

```{r, eval=F}
View(linelist)
```

Voici deux exemples d'affichage de cellules spécifiques (lignes spécifiques et colonnes spécifiques) :  


**Avec les fonctions dplyr `filter()` et `select()`:**  

Dans `View()`, passez le jeu de données dans `filter()` pour conserver certaines lignes, puis dans `select()` pour conserver certaines colonnes. Par exemple, pour examiner les dates de début et d'hospitalisation de 3 cas spécifiques :   

```{r, eval=F}
View(linelist %>%
       filter(case_id %in% c("11f8ea", "76b97a", "47a5f5")) %>%
       select(date_onset, date_hospitalisation))
```


Vous pouvez faire la même chose avec la syntaxe **base** de R, en utilisant les crochets `[ ]`` pour le sous-ensemble que vous voulez voir. 

```{r, eval=F}
View(linelist[linelist$case_id %in% c("11f8ea", "76b97a", "47a5f5"), c("date_onset", "date_hospitalisation")])
```





#### Ajouter à la chaine de commande pipé to pipé {.unnumbered}  


```{r}
# NETTOYAGE: enchainement de commandes  'PIPE'  (debute avec la donnée brute  et le  pipes avec les commandes pour le nettoyage)
##################################################################################

# Debuter la chaine de commande pipé pour le nettoyage 
###########################
linelist <- linelist_raw %>%
    
    # standardiser la syntaxe des noms de colonnes
    janitor::clean_names() %>% 
    
    # manuallement renommer le noms des colonnes
           # Nouveau nom             # Ancienne nom
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # supprimer les colonnes
    select(-c(row_num, merged_header, x28)) %>% 
  
    # supprimer les doublons
    distinct() %>% 

    # ajouter colonne
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>%     

    # convertir la classe des colonnes
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) %>% 
    
    # ajout de colonne: retard d'hospitalisation
    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %>% 
    
    # Nettoyer les valeurs de la colonne hospitalisation
    mutate(hospital = recode(hospital,
                      # OLD = NEW
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      )) %>% 
    
    mutate(hospital = replace_na(hospital, "Missing")) %>% 

    # creer la colonne age_years (à partir de age et age_unit)
    mutate(age_years = case_when(
          age_unit == "years" ~ age,
          age_unit == "months" ~ age/12,
          is.na(age_unit) ~ age)) %>% 
  
    mutate(
          # age classe: definition
          age_cat = epikit::age_categories(age_years, breakers = c(0, 5, 10, 15, 20, 30, 50, 70)),
        
          # age classes: 0 à 85 par pas de 5s
          age_cat5 = epikit::age_categories(age_years, breakers = seq(0, 85, 5))) %>% 
    
     # CI DESSUS LES ETAPES DU NETTOYAGE DEJA ABORDEES EN AMONT
    ###################################################
    filter(
          # conserver uniquement les lignes ou les valeurs  case_id ne sont pas  manquantes
          !is.na(case_id),  
          
          # egalement filtrons pour garder uniquement la deuxieme pandemie
          date_onset > as.Date("2013-06-01") | (is.na(date_onset) & !hospital %in% c("Hospital A", "Hospital B")))
```







<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Calculs par rangée  

Si vous voulez effectuer un calcul dans une ligne, vous pouvez utiliser `rowwise()` de **dplyr**. Voir cette vignette en ligne sur [calculs par ligne](https://cran.r-project.org/web/packages/dplyr/vignettes/rowwise.html).  
Par exemple, ce code applique `rowwise()` et crée une nouvelle colonne qui additionne le nombre de colonnes de symptômes spécifiées qui ont la valeur "yes", pour chaque ligne de la linelist. Les colonnes sont spécifiées dans `sum()` par leur nom dans un vecteur `c()`. `rowwise()` est essentiellement un type spécial de `group_by()`, il est donc préférable d'utiliser `ungroup()` lorsque vous avez terminé (page sur [Travailler sur des données groupées](#grouping_data)).  

```{r,}
linelist %>%
  rowwise() %>%
  mutate(num_symptoms = sum(c(fever, chills, cough, aches, vomit) == "yes")) %>% 
  ungroup() %>% 
  select(fever, chills, cough, aches, vomit, num_symptoms) # pour affichage
```

  
Lorsque vous spécifiez la colonne à évaluer, vous pouvez utiliser les fonctions d'aide "tidyselect" décrites dans la section `select()` de cette page. Vous devez juste faire un ajustement (parce que vous ne les utilisez pas dans une fonction **dplyr** comme `select()` ou `summarise()`).  

Placez les critères de spécification des colonnes dans la fonction **dplyr** `c_across()`. Ceci marche parce que `c_across` ( [documentation](https://dplyr.tidyverse.org/reference/c_across.html)) est conçue pour fonctionner avec `rowwise()` spécifiquement. Par exemple, le code suivant :  

* Applique `rowwise()` pour que l'opération suivante (`sum()`) soit appliquée à chaque ligne (sans additionner des colonnes entières)  
* Crée la nouvelle colonne `num_NA_dates`, définie pour chaque ligne comme le nombre de colonnes (dont le nom contient "date") pour lesquelles `is.na()` a donné la valeur TRUE (ce sont des données manquantes).  
* `ungroup()` pour supprimer les effets de `rowwise()` pour les étapes suivantes.  

```{r,}
linelist %>%
  rowwise() %>%
  mutate(num_NA_dates = sum(is.na(c_across(contains("date"))))) %>% 
  ungroup() %>% 
  select(num_NA_dates, contains("date")) # pour affichage
```

Vous pouvez également fournir d'autres fonctions, telles que `max()` pour obtenir la dernière ou la plus récente date pour chaque ligne :  

```{r}
linelist %>%
  rowwise() %>%
  mutate(latest_date = max(c_across(contains("date")), na.rm=T)) %>% 
  ungroup() %>% 
  select(latest_date, contains("date"))  # pour affichage 
```


## Arranger et trier  

Utilisez la fonction **dplyr** `arrange()` pour trier ou ordonner les lignes par les valeurs des colonnes.  

Listez simplement les colonnes dans l'ordre où elles doivent être triées. Spécifiez `.by_group = TRUE` si vous voulez que le tri se fasse d'abord par tout *groupement* appliqué aux données (voir la page sur [Travailler sur des données groupées](#grouping_data)).  

Par défaut, les colonnes seront triées dans l'ordre "ascendant" (ce qui s'applique aux colonnes numériques et aussi aux colonnes de caractères). Vous pouvez trier une variable dans l'ordre "descendant" en l'entourant de `desc()`.  

Le tri des données avec `arrange()` est particulièrement utile lorsque vous créez des [Tableaux de présentation](#tables_presentation), lorsque vous utilisez `slice()` pour prendre les lignes "supérieures" par groupe, ou lorsque vous définissez l'ordre des niveaux de facteurs par ordre d'apparition.  

Par exemple, pour trier les lignes de notre linelist par `hospital`, puis par `date_onset` en ordre décroissant, nous utiliserons :  

```{r, eval=F}
linelist %>% 
   arrange(hospital, desc(date_onset))
```


```{r, echo=F}
# Caché

# convertir une ancienne ligne d'épidémie restante en manquante pour plus de facilité.
linelist <- linelist %>% 
  mutate(
    date_hospitalisation = case_when(
      date_hospitalisation < as.Date("2013-01-01") ~ as.Date(NA),
      TRUE                                         ~ date_hospitalisation),
    date_outcome = case_when(
      date_outcome < as.Date("2013-01-01") ~ as.Date(NA),
      TRUE                                 ~ date_outcome)
    )

#min(linelist$date_hospitalisation, na.rm=T)
#min(linelist$date_outcome, na.rm=T)
```



```{r echo=F}
# Ordonner les COLONNESS POUR L'EXPORTATION 
linelist <- linelist %>% 
  select(case_id:gender, age, age_unit, age_years, age_cat, age_cat5, everything())
```

```{r echo=F}
# EXPORTER LE FICHIER DE LA LISTE DES LIGNES NETTOYÉ DANS LE DOSSIER "DATA".
rio::export(linelist, here::here("data", "case_linelists", "linelist_cleaned.xlsx"))
rio::export(linelist, here::here("data", "case_linelists", "linelist_cleaned.rds"))
```
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cleaning.Rmd-->


# Manipuler les dates {#working_dates}


```{r, out.width=c('50%'), fig.align='center', echo=F, message=F}
knitr::include_graphics(here::here("images", "Dates_500x500.png"))
```


Travailler avec des dates dans R demande plus d'attention que de travailler avec d'autres classes d'objets. Nous vous proposons ci-dessous quelques outils et exemples pour rendre ce processus moins pénible. Heureusement, les dates peuvent étre facilement manipulées avec de la pratique et avec un ensemble de paquets utiles tels que **lubridate**.  

Lors de l'importation de données brutes, R interpréte souvent les dates comme des objets de type caractère, ce qui signifie qu'elles ne peuvent pas être utilisêes pour des opérations générales sur les dates, comme la création de séries chronologiques et le calcul des intervalles de temps. Pour compliquer encore les choses, il existe de nombreuses façons de formater une date et vous devez aider R à savoir quelle partie d'une date représente quoi (mois, jour, heure, etc.). 

Les dates dans R sont leur propre classe d'objet - la classe `Date`. Il est à noter qu'il existe également une classe qui stocke les objets de date *et* d'heure. Les objets date-heure sont formellement appelés classes `POSIXt`, `POSIXct`, et/ou `POSIXlt` (la différence n'est pas importante). Ces objets sont appelés de maniére informelle des classes *datetime*.

* Il est important de s'assurer que R reconnaît lorsqu'une colonne contient des dates.  
* Les dates sont une classe d'objet et peut être difficile de travailler avec.  
* Nous présentons ici plusieurs façons de convertir les colonnes de dates en classe Date. 


<!-- ======================================================= -->
## étapes préliminaires

### Importation des paquets {.unnumbered}  

Ces lignes de code chargent les paquets nécessaires aux analyses. Dans ce guide, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *puis* l'importe pour l'utiliser. Vous pouvez également charger les paquets installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les paquets R.  


```{r dates_packages, warning=F, message=F}
# Vérifie si le paquet est installé, l'installe si nécessaire, et charge le paquet pour la session en cours.

pacman::p_load(
  lubridate,  # paquet général pour la manipulation et la conversion des dates 
  parsedate,   # a une fonction pour "deviner" les dates désordonnées
  aweek,      # une autre option pour convertir les dates en semaines, et les semaines en dates
  zoo,        # fonctions supplémentaires de date et d'heure
  tidyverse,  # gestion et visualisation des données  
  rio)        # import des fichiers
```

###  Importation des données {.unnumbered}  

Nous importons le jeu de données des cas d'une épidémie d'Ebola simulée. Si vous souhaitez télécharger les données pour suivre le processus étape par étape, consultez les instructions de la page [télécharger le manuel et les données](#download_book_data). Pour ce script, nous supposons que le fichier se trouve dans le répertoire de travail de la session R. Aucun sous-dossier n'est donc spécifié dans ce chemin de fichier.  


```{r,  echo=F}
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

```

```{r, eval=F}
linelist <- import("linelist_cleaned.xlsx")

```



<!-- ======================================================= -->
## Date actuelle 

Vous pouvez obtenir la date ou l'heure "système" actuelle de votre ordinateur en effectuant les opérations suivantes avec **base** R.  

```{r}
# obtenir la date du système - il s'agit d'une classe DATE
Sys.Date()

# obtenir l'heure du système - il s'agit d'une classe DATETIME
Sys.time()
```


Avec le paquet **lubridate**, la date et l'heure du système peuvent aussi être retournées avec `today()` et `now()`, respectivement. `date()` renvoie la date et l'heure actuelles avec les noms des jours de la semaine et du mois.    
  

<!-- ======================================================= -->
## Convertir en Date  

Après avoir importé un ensemble de données dans R, les valeurs des colonnes de date peuvent ressembler à "1989/12/30", "05/06/2014" ou "13 Jan 2020". Dans ces cas, R traite probablement encore ces valeurs comme des valeurs de caractères. Il faut *dire* à R que ces valeurs sont des dates... et quel est le format de la date (quelle partie est le jour, le mois, l'année, etc.).  

Une fois informé, R convertit ces valeurs en classe Date. En arriére-plan de l'interface, R stockera les dates sous forme de nombres (le nombre de jours depuis sa date "d'origine", le 1er janvier 1970). Vous n'utiliserez pas souvent le nombre de dates, mais cela permet à R de traiter les dates comme des variables continues. Cela permet également à R d'autoriser des opérations spéciales pour les objets Date, comme le calcul de la distance entre les dates.  


Par défaut, les valeurs de la classe Date dans R sont affichées sous la forme YYYY-MM-DD. Plus tard dans cette section, nous verrons comment modifier l'affichage des valeurs de date.  

Nous présentons ci-dessous deux approches pour convertir une colonne de valeurs de caractères en classe Date.  


<span style="color: darkgreen;">**__CONSEIL:_** Vous pouvez vérifier la classe actuelle d'une colonne avec la fonction R **base** `class()`, comme `class(linelist$date_onset)`.</span>  

  

### **base** R {.unnumbered}  

`as.Date()` est la fonction standard de **base** R pour convertir un objet ou une colonne en classe Date (notez la majuscule de "D").  

L'utilisation de `as.Date()` nécessite que :

* Vous *spécifiez le format **existant** de la date brute en caractères* ou la date d'origine si vous fournissez des dates en forme de nombres (voir la section sur les dates Excel).  
* Si vous l'utilisez sur une colonne de caractères, toutes les valeurs de date doivent avoir le même format exact (si ce n'est pas le cas, essayez `parse_date()` du paquet **parsedate**)  

Processus d'utilisation de as.Date() :

**Premiérement**, vérifiez la classe de votre colonne avec `class()` de **base** R. Si vous n'étiez pas sûr de la classe ou sommes confus au sujet de la classe de vos données (par exemple, vous voyez "POSIXct", etc.), il peut être plus facile de convertir d'abord la colonne en classe Character avec `as.character()`, et ensuite de la convertir en classe Date.  

**Deuxiémement**, dans la fonction `as.Date()`, utilisez l'argument `format =` pour indiquer à R le format *actuel* de la chaîne de caractères de la date - quels caractères font référence au mois, au jour et à l'année, et comment ils sont séparés.  Si vos valeurs sont déjà dans un des formats de date standard de R ("YYYY-MM-DD" ou "YYYY/MM/DD") l'argument `format =` n'est pas nécessaire.  

Pour `format =`, fournissez une chaîne de caractères (entre guillemets) qui représente le format de date *actuel* en utilisant les abréviations spéciales "strptime" ci-dessous. Par exemple, si les dates de vos caractères sont actuellement au format "DD/MM/YYYY", comme "24/04/1968", vous utiliserez `format = "%d/%m/%Y"` pour convertir les valeurs en dates. **Il est nécessaire de mettre le format entre guillemets. Et n'oubliez pas les barres obliques ou les tirets!**  

```{r eval=F}
# Convertir en classe de date
linelist <- linelist %>% 
  mutate(date_onset = as.Date(date_of_onset, format = "%d/%m/%Y"))
```

La plupart des abréviations de strptime sont listées ci-dessous. Vous pouvez voir la liste compléte en exécutant `?strptime`.  

%d = numéro du jour du mois (5, 17, 28, etc.)  
%j = numéro du jour de l'année (Jour 001-366)  
%a = Jour de la semaine abrégé (Lun, Mar, Mer, etc.)  
%A = Jour de la semaine (Lundi, Mardi, etc.)
%w = numéro du jour de la semaine (0-6, le dimanche est 0)  
%u = numéro du jour de la semaine (1-7, le lundi est 1)  
%W = numéro de semaine(00-53, le lundi est le début de la semaine)  
%U = numéro de semaine (01-53, le dimanche est le début de la semaine)  
%m = numéro du mois (e.g. 01, 02, 03, 04)  
%b = Mois abrégé (jan, févr, etc.)  
%B = Mois complet (janvier, février, etc.)  
%y = année à 2 chiffres (par example 89)  
%Y = année à 4 chiffres (par example 1989)  
%h = heures (horloge de 24 heures)  
%m = minutes  
%s = secondes
%z = décalage par rapport à GMT  
%Z = Fuseau horaire (caractère)  

<span style="color: darkgreen;">**_CONSEIL:_** L'argument `format =` de la fonction `as.Date()` n'indique pas à R le format que vous voulez donner aux dates, mais plutôt comment identifier les parties de la date telles qu'elles sont *avant* que vous lanciez la commande.</span>  

<span style="color: darkgreen;">**_CONSEIL:_** Assurez-vous que dans l'argument `format =` vous utilisez le *séparateur de partie de date* (par exemple /, -, ou espace) qui est présent dans vos dates.</span>  

Une fois que les valeurs sont dans la classe Date, R les affichera par défaut dans le format standard, qui est AAAA-MM-JJ.



### **lubridate** {.unnumbered}  

La conversion d'objets caractères en dates peut être plus facile en utilisant le paquet **lubridate**. **lubridate** est un paquet **tidyverse** créé pour rendre le travail avec les dates et les heures plus simple et plus cohérent que dans **base** R. Pour ces raisons, **lubridate** est souvent considéré comme le paquet de référence pour les dates et les heures, et il est recommandé de travailler avec des variables de date ou d'heure.

Le paquet **lubridate** fournit plusieurs fonctions d'aide différentes créées pour convertir les objets caractères en dates de maniére intuitive, et plus facile que de spécifier le format dans `as.Date()`. Ces fonctions sont spécifiques au format de date brut, mais permettent une variété de séparateurs et de synonymes pour les dates (par exemple 01 vs Jan vs Janvier) - les synonymes sont nommés d'après les abréviations des formats de date.


```{r, }
# installez/chargez lubridate
pacman::p_load(lubridate)
```

La fonction `ymd()` convertit de maniére flexible les valeurs de date fournies sous la forme **année, mois, jour**. Cette fonction fonctionne avec n'importe quel séparateur utilisé dans les variables.  

```{r}
# lire la date au format année-mois-jour
ymd("2020-10-11")
ymd("20201011")
```

La fonction `mdy()` convertit de maniére flexible les valeurs de date fournies sous la forme **mois, jour, année**.  

```{r}
# lire la date au format mois-jour-année
mdy("10/11/2020")
mdy("Oct 11 20")
```

La fonction `dmy()` convertit de maniére flexible les valeurs de date fournies sous la forme **jour, mois, année**.  

```{r}
# lire la date au format jour-mois-année
dmy("11 10 2020")
dmy("11 October 2020")
```

<!-- Les fonctions `as.character()` et `as.Date()` peuvent aussi être combinées comme :   -->

<!-- ```{r eval=F} -->
<!-- linelist_cleaned$date_of_onset <- as.Date(as.character(linelist_cleaned$date_of_onset), format = "%d/%m/%Y") -->
<!-- ``` -->

Si vous utilisez le piping, la conversion d'une colonne de caractères en dates avec **lubridate** pourrait ressembler à ceci : 

```{r, eval=F}
linelist <- linelist %>%
  mutate(date_onset = lubridate::dmy(date_onset))
```

Une fois terminé, vous pouvez exécuter `class()` pour vérifier la classe de la colonne 

```{r, eval=F}
#Vérifiez la classe de la colonne
class(linelist$date_onset)  
```


Une fois que les valeurs sont dans la classe Date, R les affichera par défaut dans le format standard, qui est AAAA-MM-JJ.  

Notez que les fonctions ci-dessus fonctionnent mieux avec des années à 4 chiffres. Les années à 2 chiffres peuvent produire des résultats inattendus, car lubridate va deviner le siècle.  

Pour convertir une année à 2 chiffres en une année à 4 chiffres (toutes dans le même siècle), vous pouvez convertir en caractères de classe, puis combiner les chiffres existants avec un préfixe en utilisant `str_glue()` du paquet **stringr** (voir la page [Caractères et chaînes de caractères](#character_strings)). Vous pouvez ensuite convertir la colonne en date.  

```{r}
two_digit_years <- c("15", "15", "16", "17")
str_glue("20{two_digit_years}")
```



### Combine columns {.unnumbered}  

Vous pouvez utiliser les fonctions **lubridate** `make_date()` et `make_datetime()` pour combiner plusieurs colonnes numériques en une seule colonne de date. Par exemple, si vous avez les colonnes numériques `onset_day`, `onset_month`, et `onset_year` dans le cadre de données `linelist` :  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(onset_date = make_date(year = onset_year, month = onset_month, day = onset_day))
```




<!-- ======================================================= -->
## Dates en Excel

En arriére-plan, la plupart des logiciels stockent les dates sous forme de nombres. R stocke les dates à partir du 1er janvier 1970. Ainsi, si vous exécutez `as.numeric(as.Date("1970-01-01))` vous obtiendrez `0`. 

Microsoft Excel enregistre les dates à partir du 30 décembre 1899 (Windows) ou du 1er janvier 1904 (Mac), selon votre système d'exploitation. Consultez ce [guide Microsoft](https://docs.microsoft.com/en-us/office/troubleshoot/excel/1900-and-1904-date-system) pour plus d'informations.  

Les dates d'Excel sont souvent importées dans R sous la forme de ces valeurs numériques plutôt que sous la forme de caractères. Si le jeu de données que vous avez importé d'Excel montre des dates sous forme de nombres ou de caractères comme "41369"... utilisez la fonction `as.Date()` (ou la fonction `as_date()` de **lubridate**) pour convertir, mais **au lieu de fournir un "format" comme ci-dessus, fournissez la date d'origine Excel** à l'argument `origin = ` dans la fonction.   

Cela ne fonctionnera pas si la date Excel est stockée dans R comme un type de caractère, donc assurez-vous que le nombre est de classe Numérique!

<span style="color: black;">**_NOTE:_** Vous devez fournir la date d'origine dans le format de date par défaut de R ("YYYY-MM-DD").</span>

```{r, eval = FALSE}
# Un exemple de fourniture de la "date d'origine" d'Excel lors de la conversion de dates numériques d'Excel.
data_cleaned <- data %>% 
  mutate(date_onset = as.numeric(date_onset)) %>%   # ensure class is numeric
  mutate(date_onset = as.Date(date_onset, origin = "1899-12-30")) # convert to date using Excel origin
```



<!-- ======================================================= -->
## Dates désordonnées 

Vous pouvez utiliser la fonction `parse_date()` du paquet **parsedate** pour lire une colonne de date "désordonnée" qui contient des dates dans de nombreux formats différents et convertir les dates dans un format standard. Vous pouvez [en savoir plus en ligne sur `parse_date()`](https://readr.tidyverse.org/reference/parse_datetime.html). 

Par exemple, `parse_date` lira un vecteur des dates en caractères suivantes : "03 Jan 2018", "07/03/1982", et "08/20/85" et les convertira en classe Date sous la forme de : `2018-01-03`, `1982-03-07`, et `1985-08-20`.  

```{r, }
parsedate::parse_date(c("03 January 2018",
                        "07/03/1982",
                        "08/20/85"))
```



```{r eval = FALSE}
# Un exemple d'utilisation de parse_date() sur la colonne date_onset
linelist <- linelist %>%                 # le dataframe s'appelle linelist, à ne pas confondre avec le paquet "linelist"
  mutate(
    date_onset = parse_date(date_onset))
    
```




<!-- ======================================================= -->
## Travailler avec la classe date-heure  

Comme nous l'avons déjà mentionné, R supporte également une classe `datetime` - une colonne qui contient des informations de date **et** d'heure. Comme pour la classe `Date`, il faut souvent convertir les objets `caracter` en objets `datetime`. 

### Convertir des dates avec des heures {.unnumbered}  

Un objet `datetime` standard est formaté avec la date en premier, qui est suivie par une entrée de temps - par exemple _01 Jan 2020, 16:30_. Comme pour les dates, il existe de nombreuses façons de formater cet objet. Il existe de nombreux niveaux de précision (heures, minutes, secondes) qui peuvent être fournis à un objet "datetime".   

Il existe des fonctions du paquet **lubridate** pour aider à convertir ces chaînes en objets `datetime`. Ces fonctions sont des extensions des fonctions d'aide de date, avec `_h` (seulement les heures fournies), `_hm` (heures et minutes fournies), ou `_hms` (heures, minutes et secondes fournies) ajoutées à la fin (par exemple, `dmy_hms()`). Ils peuvent être utilisês comme indiqué :

Convertir une date avec seulement des heures en objet datetime  

```{r}
ymd_h("2020-01-01 16hrs")
ymd_h("2020-01-01 4PM")
```

Converter une date avec des heures et des minutes en objet datetime. 

```{r}
dmy_hm("01 Janvier 2020 16:20")
```

Converrtir une date avec des heures, des minutes et des secondes en un objet de type datetime.  

```{r}
mdy_hms("01 Janvier 2020, 16:20:40")
```

Vous pouvez fournir le fuseau horaire mais il est ignoré par la fonction. Allez à la section plus bas dans le chapitre pour en savoir plus sur les fuseaux horaires dans R. 

```{r}
mdy_hms("01 Janvier 2020, 16:20:40 PST")

```

Avec un dataframe chargé dans R, les colonnes d'heure et de date peuvent être combinées pour créer une colonne de date en utilisant la fonction `str_glue()` du paquet **stringr** et la fonction appropriée du paquet **lubridate**, selon le format de date et d'heure dans le dataframe. Voir la page [caractères et chaînes de caractères](#character_strings) pour plus de détails sur **stringr**.  

Dans cet exemple, le dataframe `linelist` a une colonne au format "heures:minutes". Pour la convertir en date, nous suivons quelques étapes :

1) Vous créez une colonne de temps d'admission "propre" avec les valeurs manquantes remplies avec la médiane de la colonne. Nous faisons cela parce que **lubridate** ne fonctionne pas sur les valeurs manquantes. Combinez la nouvelle colonne avec la colonne `date_hospitalisation`, puis utilisez la fonction `ymd_hm()` pour convertir en objet datetime. 

```{r, eval = FALSE}
# packages
pacman::p_load(tidyverse, lubridate, stringr)

# time_admission est une colonne en heures:minutes
linelist <- linelist %>%
  
#si l'heure d'admission n'est pas donnée, attribuez l'heure d'admission médiane.

  mutate(
    time_admission_clean = ifelse(
      is.na(time_admission),         # si l'heure est manquante
      median(time_admission),          # assigner la médiane
      time_admission                   # si elle n'est pas manquante, la garder telle quelle
  ) %>%
  
    # Utilisez str_glue() pour combiner les colonnes de date et d'heure afin de créer une colonne de caractères.
    # puis utiliser ymd_hm() pour la convertir en classe datetime
  mutate(
    date_time_of_admission = str_glue("{date_hospitalisation} {time_admission_clean}") %>% 
      ymd_hm()
  )

```

### Convertir uniquement les temps {.unnumbered}  

Si vos données ne contiennent qu'un caractère temps (heures et minutes), vous pouvez les convertir et les manipuler comme des temps en utilisant la fonction `strptime()` de **base** R. Par exemple, pour obtenir la différence entre deux temps :   

```{r}
# temps comme objets de caractère
time1 <- "13:45" 
time2 <- "15:20"

# temps converties en une classe de datetime
time1_clean <- strptime(time1, format = "%H:%M")
time2_clean <- strptime(time2, format = "%H:%M")

# La différence est de classe "difftime" par défaut, ici convertie en heures numériques. 
as.numeric(time2_clean - time1_clean)   # différence en heures

```

Notez cependant que si aucune valeur de date n'est fournie, la fonction suppose que la date est aujourd'hui. Pour combiner une chaîne de date et une chaîne d'heure, voyez comment utiliser **stringr** dans la section juste au-dessus. Pour en savoir plus sur `strptime()` [ici](https://rdrr.io/r/base/strptime.html).    

Pour convertir des nombres à un chiffre en nombres à deux chiffres (par exemple, pour ajouter des zéros aux heures ou aux minutes afin d'obtenir deux chiffres), consultez la section ["Longueur des caractères" de la page caractères et chaînes de caractères](#str_pad). 


### Extraction d'éléments du temps {.unnumbered}  

Vous pouvez extraire des éléments du temps avec `hour()`, `minute()`, ou `second()` du paquet **lubridate**.  

Voici un exemple d'extraction de l'heure, puis de classement par partie de la journée. Nous commençons par la colonne `time_admission`, qui est dans la classe Character au format "HH:MM". D'abord, la fonction `strptime()` est utilisêe comme décrit ci-dessus pour convertir les caractères en classe datetime. Ensuite, l'heure est extraite avec `hour()`, retournant un nombre de 0-24. Enfin, une colonne `time_period` est crée en utilisant la logique de `case_when()` pour classer les lignes en Matin/Après-midi/Soir/Nuit en fonction de leur heure d'admission.   

```{r}
linelist <- linelist %>%
  mutate(hour_admit = hour(strptime(time_admission, format = "%H:%M"))) %>%
  mutate(time_period = case_when(
    hour_admit > 06 & hour_admit < 12 ~ "Matin",
    hour_admit >= 12 & hour_admit < 17 ~ "Après-midi",
    hour_admit >= 17 & hour_admit < 21 ~ "Soir",
    hour_admit >=21 | hour_admit <= 6 ~ "Nuit"))
```

Pour en savoir plus sur la fonction `case_when()`, consultez la page [Nettoyage des données et des fonctions de base](#cleaning_data). 

<!-- ======================================================= -->
## Travailler avec des dates     

Le paquet `lubridate` peut également être utilisê pour une variété d'autres fonctions, telles que **l'extraction d'aspects d'une date/heure**, **l'exécution d'arithmétique pour les dates**, ou **le calcul d'intervalles de dates**.

Nous définissons ici une date à utiliser dans les exemples :  

```{r, }
# créer un objet de la classe Date
example_date <- ymd("2020-03-01")
```

### Extraire les composants de la date {.unnumbered}  

Vous pouvez extraire des aspects communs commes le mois, le jour, le jour de la semaine :  

```{r}
month(example_date) # numéro du mois
day(example_date) # jour (numéro) du mois
wday(example_date) # numéro du jour de la semaine (1-7)
```

Vous pouvez également extraire les composants temporels d'un objet ou d'une colonne en classe `datetime`. Cela peut être utile si vous voulez visualiser la distribution des temps d'admission.  

```{r, eval=F}
example_datetime <- ymd_hm("2020-03-01 14:45")

hour(example_datetime) # extraire l'heure
minute(example_datetime) # extrait la minute
second(example_datetime) # extrait les secondes
```

Il existe plusieurs options pour récupérer les semaines. Lisez la section sur les semaines épidémiologiques ci-dessous pour en savoir plus. 

Notez que si vous cherchez à *afficher* une date d'une certaine maniére (par exemple "Jan 2020" ou "Jeudi 20 mars" ou "Semaine 20, 1977"), vous pouvez le faire assez facilement en utilisant les méthodes décrites dans la section sur l'affichage des dates.    


### Mathématiques de dates {.unnumbered}  

Vous pouvez ajouter certains nombres de jours ou de semaines en utilisant leur fonction respective du paquet **lubridate**.  

```{r}
# ajouter 3 jours à cette date
example_date + days(3)
  
# ajoute 7 semaines et soustrait deux jours à cette date
example_date + weeks(7) - days(2)
```

### Intervalles entre les dates  {.unnumbered}  

La différence entre les dates peut être calculée par :  

1. Assurez-vous que les deux dates sont de la classe Date  
2. Utilisez la soustraction pour obtenir la différence "difftime" entre les deux dates.  
3. Si nécessaire, convertissez le résultat en classe numérique pour effectuer les calculs mathématiques suivants.  

Ci-dessous, l'intervalle entre deux dates est calculé et affiché. Vous pouvez trouver des intervalles en utilisant le symbole "moins" de la soustraction sur des valeurs de la classe Date. Notez que la classe de la valeur retournée est "difftime" comme affiché ci-dessous, et doit être convertie en classe numérique. 


```{r}
# trouver l'intervalle entre example_date et le 20 février 2020 
output <- example_date - ymd("2020-02-20")
output    # imprimer
class(output)
```

Pour effectuer d'autres opérations sur un objet de classe "difftime", convertissez-le en numérique avec la fonction `as.numeric()`. 

Tout ceci peut être rassemblé pour travailler avec des données - par exemple :

```{r, eval = F}
pacman::p_load(lubridate, tidyverse) # charger les paquets

linelist <- linelist %>% # conversion de la date d'apparition en caractères
  
  # convertir la date d'apparition de objets caractères en objets date en spécifiant le format dmy
  mutate(date_onset = dmy(date_onset),
         date_hospitalisation = dmy(date_hospitalisation)) %>%
  
  # filtre tous les cas dont la date d'apparition n'est pas en mars
  filter(month(date_onset) == 3) %>% # filtrer tous les cas sans apparition en mars
    
  # trouver la différence en jours entre l'apparition et l'hospitalisation pour tous les cas
  mutate(days_onset_to_hosp = date_hospitalisation - date_of_onset)
```




Dans le dataframe,  si l'une des dates ci-dessus est manquante, l'opération échouera pour cette ligne. Il en résultera un `NA` au lieu d'une valeur numérique. Lorsque vous utilisez cette colonne pour des calculs, assurez-vous de mettre l'argument `na.rm = ` à `TRUE` (vrai). Par exemple :

```{r, eval = FALSE}
# Calculez le nombre médian de jours d'hospitalisation pour tous les cas pour lesquels des données sont disponibles.
median(linelist_delay$days_onset_to_hosp, na.rm = T)
```


<!-- ======================================================= -->
## Affichage des dates  

Une fois que les dates ont la bonne classe, vous voudrez peut-être qu'elles s'affichent différemment, par exemple qu'elles s'affichent comme "Lundi 05 Janvier" au lieu de "2018-01-05". Vous pouvez aussi vouloir ajuster l'affichage afin de regrouper ensuite les lignes en fonction des éléments de date affichés - par exemple pour regrouper par mois-année.  

### `format()` {.unnumbered}  

Ajustez l'affichage de la date avec la fonction `format()` de **base** R. Cette fonction accepte une chaîne de caractères (entre guillemets) spécifiant le format final *souhaité* dans les abréviations "%" strptime (la même syntaxe que celle utilisêe dans la fonction `as.Date()`). Vous trouverez ci-dessous la plupart des abréviations courantes.  

<span style="color: black;">**_NOTE:_** l''utilisation de `format()` convertira les valeurs dans la classe Character, donc cette fonction est généralement utilisêe vers la fin d'une analyse ou uniquement pour l'affichage ! Vous pouvez voir la liste complète en exécutant `?strptime`.  

%d = numéro du jour du mois (5, 17, 28, etc.)  
%j = numéro du jour de l'année (Jday 001-366)  
%a = Jour de la semaine abrégé (lun, mar, mer, etc.)  
%A = Jour de la semaine complet (lundi, mardi, etc.)  
%w = numéro du jour de la semaine (0-6, le dimanche est 0)  
%u = numéro du jour de la semaine (1-7, le lundi est 1)  
%W = numéro de la semaine (00-53, le lundi est le début de la semaine)  
%U = numéro de la semaine (01-53, le dimanche est le début de la semaine)  
%m = numéro du mois (par exemple 01, 02, 03, 04)  
%b = Mois abrégé (Jan, Feb, etc.)  
%B = Mois complet (janvier, février, etc.)  
%y = année à 2 chiffres (ex. 89)  
%Y = année à 4 chiffres (ex. 1989)  
%h = heures (horloge de 24 heures)  
%m = minutes  
%s = secondes  
%z = décalage par rapport à GMT  
%Z = Fuseau horaire (caractère)

Un exemple de formatage de la date du jour:  

```{r}
# date du jour, avec formatage
format(Sys.Date(), format = "%d %B %Y")

# moyen simple d'obtenir la date et l'heure complètes (formatage par défaut)
date()

# formatage de la date, de l'heure et du fuseau horaire combinés ensemble avec la fonction str_glue()
str_glue("{format(Sys.Date(), format = '%A, %B %d %Y, %z %Z, ')}{format(Sys.time(), format = '%H:%M:%S')}")

#Utilisation du formatage pour afficher les semaines
format(Sys.Date(), "%Y Week %W")
```

Notez que si vous utilisez la fonction `str_glue()`, sachez qu'à l'intérieur des guillemets doubles " vous ne devez utiliser que des guillemets simples (comme ci-dessus).  


### Mois-année {.unnumbered}  

Pour convertir une colonne de date au format mois-année, nous vous suggérons d'utiliser la fonction `as.yearmon()` du paquet **zoo**. Cette fonction convertit la date en classe "yearmon" et conserve l'ordre correct. En revanche, l'utilisation de la fonction `format(colonne, "%Y %B")` convertira la date en classe caractère et classera les valeurs par ordre alphabétique (de maniére incorrecte). 

Ci-dessous, une nouvelle colonne `yearmonth` est créée à partir de la colonne `date_onset`, en utilisant la fonction `as.yearmon()`. L'ordre par défaut (correct) des valeurs résultantes est indiqué dans le tableau.   

```{r}
# créer une nouvelle colonne 
test_zoo <- linelist %>% 
     mutate(yearmonth = zoo::as.yearmon(date_onset))

# imprimer le tableau
table(test_zoo$yearmon)
```

En revanche, vous pouvez voir comment la seule utilisation de `format()` permet d'obtenir le format d'affichage souhaité, mais pas l'ordre correct.  

```{r}
# créer une nouvelle colonne
test_format <- linelist %>% 
     mutate(yearmonth = format(date_onset, "%b %Y"))

# imprimer le tableau
table(test_format$yearmon)
```

Note : si vous travaillez dans un `ggplot()` et vous voulez ajuster la façon dont les dates sont *affichées* uniquement, il peut être suffisant de fournir un format strptime à l'argument `date_labels = ` dans `scale_x_date()` - vous pouvez utiliser `"%b %Y"` ou `"%Y %b"`. Consultez la page [Astuces de ggplot](#ggplot_tips) pour une explication plus approfondie. 


Alternativement, le paquet **zoo** posséde la fonction `as.yearqtr()`, et vous pouvez utiliser `scale_x_yearmon()` lorsque vous utilisez `ggplot()` pour ajuster la façon dont les dates sont *affichées* sur le graphique.   



<!-- ======================================================= -->
## Semaines épidémiologiques {#dates_epi_wks}

### **lubridate** {.unnumbered}  

Consultez la page [Regroupement de données](#grouping_data) pour des exemples plus complets de regroupement de données par date. Ci-dessous, nous décrivons briévement le regroupement des données par semaine.   

Nous recommandons généralement d'utiliser la fonction `floor_date()` du package **lubridate**, avec l'argument `unit = "week"`. Cela arrondit la date au "début" de la semaine, comme défini par l'argument `week_start = `. Le début de semaine par défaut est 1 (pour les lundis) mais vous pouvez spécifier n'importe quel jour de la semaine comme début (par exemple 7 pour les dimanches). `floor_date()` est polyvalent et peut être utilisê pour arrondir à d'autres unités de temps en définissant `unit = ` à "seconde", "minute", "heure", "jour", "mois" ou "année".  

La valeur retournée est la date de début de la semaine, dans la classe Date. La classe Date est utile pour tracer les données, car elle est ordonnée correctement et elle sera facilement reconnue par `ggplot()`.

Si vous êtes seulement intéressé par l'ajustement des dates pour *afficher* par semaine dans un graphique, voyez la section de ce chapitre sur l'affichage des dates. Par exemple, lorsque vous tracez une épicurve, vous pouvez formater l'affichage de la date en fournissant la nomenclature strptime "%" désirée. Par exemple, utilisez "%Y-%W" ou "%Y-%U" pour spécifier l'année et le numéro de semaine (respectivement le lundi ou le dimanche en début de semaine).   

### Comptages hebdomadaires  {.unnumbered}  

Allez à la page [Regroupement des données](#grouping_data) pour une explication détaillée du regroupement des données avec `count()`, `group_by()`, et `summarise()`. Voici un bref exemple.  

1) créez une nouvelle colonne 'semaine' avec `mutate()`, en utilisant `floor_date()` avec `unit = "week"`.  
2) Obtenez le nombre de lignes (cas) par semaine avec `count()` ; filtrez les cas dont la date est manquante.  
3) Terminez avec `complete()` du paquet **tidyr** pour vous assurer que *toutes* les semaines apparaissent dans les données - même celles qui n'ont pas de lignes/cas. Par défaut, les valeurs de comptage pour toutes les "nouvelles" lignes sont NA, mais vous pouvez les rendre 0 avec l'argument `fill = `, qui attend une liste nommée (ci-dessous, `n` est le nom de la colonne de comptage).  

```{r}
# créez un jeu de données agrégé des comptes hebdomadaires de cas.
weekly_counts <- linelist %>% 
  drop_na(date_onset) %>%             # Suppression des cas pour lesquels il manque la date de début de la maladie
  mutate(weekly_cases = floor_date(   # créer une nouvelle colonne, semaine d'apparition du cas
    date_onset,
    unit = "week")) %>%            
  count(weekly_cases) %>%           # grouper les données par semaine et compter les lignes par groupe (crée la colonne 'n')
  tidyr::complete(                  # Assurez-vous que toutes les semaines sont présentes, même celles où aucun cas n'a été observé.
    weekly_cases = seq.Date(          # definir la colonne "weekly_cases" comme une séquence complète
      from = min(weekly_cases),       # à partir de la date minimum
      to = max(weekly_cases),         # jusqu'à la date maximale
      by = "week"),                   # agrégé par semaines
    fill = list(n = 0))             # Remplir les NA dans la colonne des comptes n avec 0
```

Voici les premiéres lignes du jeu de données résultant :  

```{r message=FALSE, echo=F}
DT::datatable(head(weekly_counts, 20), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### différentes méthodes pour les epiweeks  {.unnumbered}  

Notez que le paquet **lubridate** posséde des fonctions alternatives `week()`, `epiweek()`, et `isoweek()`, dont chacune a des dates de début légérement différentes et d'autres nuances. De maniére générale, `floor_date()` devrait suffire à vos besoins. Pour en savoir plus sur ces fonctions, entrez `?week` dans la console ou lisez la documentation [ici](https://www.rdocumentation.org/packages/lubridate/versions/1.7.4/topics/week). 


Vous pouvez également utiliser le paquet **aweek** pour définir les semaines épidémiologiques. Vous pouvez en savoir plus sur le paquet **aweek** [sur le site de RECON](https://www.repidemicsconsortium.org/aweek/). **aweek** contient les fonctions `date2week()` et `week2date()` dans lesquelles vous pouvez définir le jour de début de semaine avec `week_start = "Monday"`. **aweek** est le plus simple si vous voulez spécifier vos dates sous forme de numéro de semaine (par exemple "2020-W12"). Un autre avantage du package **aweek** est que lorsque `date2week()` est appliqué à une colonne de date, la colonne retournée (format de semaine) est automatiquement de la classe Factor et inclut les niveaux de toutes les semaines de l'intervalle de temps (cela évite l'étape supplémentaire de `complete()` décrite ci-dessus). Cependant, **aweek** n'a pas la fonctionnalité d'arrondir les dates à d'autres unités de temps comme les mois, les années, etc.    


Une autre alternative pour les séries temporelles qui fonctionne également bien pour afficher un format "semaine" ("2020 W12") est `yearweek()` du paquet **tsibble**, comme démontré dans la page sur [Série temporelle et détection des épidémies](#time_series).  

<!-- ======================================================= -->
##  Conversion des dates/fuseaux horaires

Lorsque des données sont présentes dans différents fuseaux horaires, il peut souvent être important de standardiser ces données dans un fuseau horaire unifié. Cela peut présenter un défi supplémentaire, car la l'élément fuseau horaire des données doit être codée manuellement dans la plupart des cas.

Dans R, chaque objet *datetime* posséde un élément fuseau horaire. Par défaut, tous les objets datetime portent le fuseau horaire local de l'ordinateur utilisê - ce fuseau est généralement spécifique à une *localisation* plutôt qu'à un fuseau horaire nommé, car les fuseaux horaires changent souvent en fonction de l'heure d'été. Il n'est pas possible de compenser avec précision les fuseaux horaires sans la composante temporelle d'une date, car l'événement que représente une colonne de date ne peut être attribué à une heure spécifique, et les décalages horaires mesurés en heures ne peuvent donc pas être raisonnablement pris en compte.

Pour définir des fuseaux horaires différents, il existe un certain nombre de fonctions dans le paquet **lubridate** qui peuvent être utilisêes pour changer le fuseau horaire d'un objet datetime du fuseau horaire local à un fuseau horaire différent. Les fuseaux horaires sont définis en attribuant un fuseau horaire valide de la base de données tz à l'objet datetime. Une liste de ces fuseaux est disponible ici - si le lieu dont vous utilisez les données ne figure pas sur cette liste, les grandes villes voisines dans le fuseau horaire sont disponibles et servent le même objectif.  

https://en.wikipedia.org/wiki/List_of_tz_database_time_zones


```{r}
# Assignez l'heure actuelle à une colonne 
time_now <- Sys.time()
time_now

# utilisez with_tz() pour affecter un nouveau fuseau horaire à la colonne, tout en CHANGEANT l'heure de l'horloge
time_london_real <- with_tz(time_now, "Europe/London")

# Utilisez force_tz() pour assigner un nouveau fuseau horaire à la colonne, tout en conservant l'heure de l'horloge.
time_london_local <- force_tz(time_now, "Europe/London")


# notez que tant que l'ordinateur qui a été utilisê pour exécuter ce code n'est PAS réglé sur l'heure de Londres,
# il y aura une différence dans les heures 
# (le nombre d'heures de différence entre le fuseau horaire de l'ordinateur et l'heure de Londres)
time_london_real - time_london_local

```

Cela peut sembler largement abstrait, et n'est souvent pas nécessaire si l'utilisateur ne travaille pas sur plusieurs fuseaux horaires.  





<!-- ======================================================= -->
##  Calculs de décalage et d'avance  

`lead()` et `lag()` sont des fonctions du paquet **dplyr** qui aident à trouver les valeurs précédentes (décalées) ou suivantes (en avance) dans un vecteur - généralement pour un vecteur numérique ou de date. Ces fonctions sont utiles pour calculer les changements/différences entre les unités de temps.  


```{r, echo=F}
counts <- import(here("data", "example", "district_weekly_count_data.xlsx")) %>% 
  filter(District == "Nibari") %>% 
  mutate(Date = as.Date(Date),
         week_start = lubridate::floor_date(Date, "week")) %>%
  group_by(week_start) %>% 
  summarize(cases_wk = sum(Cases, na.rm=T)) %>% 
  complete(week_start = seq.Date(min(week_start), max(week_start), by = "week"), fill = list(cases_wk = 0))
```

Supposons que vous vouliez calculer la différence de cas entre la semaine en cours et la semaine précédente. Les données sont initialement fournies en nombre de cas hebdomadaires, comme indiqué ci-dessous.  

```{r message=FALSE, echo=F}
DT::datatable(counts, rownames = FALSE,  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

**Lorsque vous utilisez `lag()` ou `lead()`, l'ordre des lignes dans le dataframe est très important ! - Faites attention à ce que vos dates/chiffres soient ascendants ou descendants**.  

La premiére étape consiste à créer une nouvelle colonne contenant la valeur de la semaine précédente (décalée).  

* Contrôlez le nombre d'unités en arriére/en avant avec `n = ` (doit être un integer non-négatif)  
* Utilisez `default = ` pour définir la valeur placée dans les lignes inexistantes (par exemple, la premiére ligne pour laquelle il n'y a pas de valeur décalée). Par défaut, c'est `NA`.  
* Utilisez `order_by = TRUE` si vos lignes ne sont pas ordonnées par votre colonne de référence.  


```{r}
counts <- counts %>% 
  mutate(cases_prev_wk = lag(cases_wk, n = 1))
```

```{r message=FALSE, echo=F}
DT::datatable(counts, rownames = FALSE,  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

L'étape suivante consiste à créer une nouvelle colonne qui est la différence entre les deux colonnes de cas :  

```{r}
counts <- counts %>% 
  mutate(cases_prev_wk = lag(cases_wk, n = 1),
         case_diff = cases_wk - cases_prev_wk)
```

```{r message=FALSE, echo=F}
DT::datatable(counts, rownames = FALSE,  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


Vous pouvez en savoir plus sur `lead()` et `lag()` dans la documentation [ici](https://dplyr.tidyverse.org/reference/lead-lag.html) ou en entrant `?lag` dans votre console.  


<!-- ======================================================= -->
## Ressources  

**lubridate** [tidyverse page](https://lubridate.tidyverse.org/)  
**lubridate** RStudio [cheatsheet](https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf)  
R for Data Science page on [dates and times](https://r4ds.had.co.nz/dates-and-times.html)  
[Online tutorial](https://www.statmethods.net/input/dates.html)
[Date formats](https://www.r-bloggers.com/2013/08/date-formats-in-r/)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/dates.Rmd-->


# Caractères et chaînes de caractères {#character_strings}  

```{r, out.width=c('100%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "Characters_Strings_1500x500.png"))
```



Cette page démontre l'utilisation du paquet **stringr** pour évaluer et manipuler des valeurs de caractères (ou chaînes de caractères, ce que nous appelons "strings" en anglais).  

1. Combiner, ordonner, séparer, arranger - `str_c()`, `str_glue()`, `str_order()`, `str_split()`  
2. Nettoyer et standardiser  
    * Ajuster la longueur - `str_pad()`, `str_trunc()`, `str_wrap()`.  
    * Changez la casse - `str_to_upper()`, `str_to_title()`, `str_to_lower()`, `str_to_sentence()`  
3. Évaluer et extraire par position - `str_length()`, `str_sub()`, `word()`.  
4. Modèles  
    * détecter et localiser - `str_detect()`, `str_subset()`, `str_match()`, `str_extract()`  
    * Modifier et remplacer - `str_sub()`, `str_replace_all()`  
7. Expressions régulières ("regex")


Afin d'expliquer facilement l'utilisation des chaînes de caractères dans le présent chapitre, la plupart des exemples utilisent un vecteur de caractères court et défini, mais ils peuvent facilement être adaptés à une colonne dans un dataframe.  

Une grande partie de l'inspiration pour ce chapitre est basée sur cette [example de le paquet stringr](https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html).  



<!-- ======================================================= -->
## Preparation {}

### Charger les paquets {.unnumbered}   

Installez ou chargez le paquet **stringr** et d'autres paquets de **tidyverse**. 

```{r}
# installer/charger les paquets
pacman::p_load(
  stringr,    # de nombreuses fonctions pour la manipulation des chaînes de caractères
  tidyverse,  # pour diverses options de manipulation des données 
  tools)      # alternative pour la conversion en majuscules

```


### Importation de données  {.unnumbered}  


Dans cette page, il y aura parfois des références à la `linelist` (liste de cas ou liste linéaire) nettoyée des cas d'une épidémie d'Ebola simulée. Si vous voulez suivre les exemples dans votre propre script, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez ici pour télécharger la "linelist" nettoyée</a> (as .rds file). Importez les données avec la fonction `import()` du paquet **rio** (la fonction `import()` peut être utilisée pour importer plusieurs types de fichiers comme .xlsx, .csv, .rds - voir la page [Importation et exportation](#import_export) pour plus de détails). 

```{r, echo=F}
# importez la linelist dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# importez la linelist de cas
linelist <- import("linelist_cleaned.rds")
```

Les 50 premiéres lignes de la linelist (liste linéaire ou liste de cas) sont affichées ci-dessous.

```{r, message=FALSE, echo=F}
# affiche les données de la linelist dans un tableau
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




<!-- ======================================================= -->
## Unir, séparer et arranger { }

Cette section couvre :  

* Utilisation de `str_c()`, `str_glue()` et `unite()` pour combiner des chaînes de caractères.  
* Utiliser `str_order()` pour organiser des chaînes de caractères.  
* Utiliser `str_split()` et `separate()` pour séparer des chaînes de caractères.  



<!-- ======================================================= -->
### Combiner des chaînes de caractères {.unnumbered}

Pour combiner ou concaténer plusieurs chaînes de caractères en une seule chaîne, nous suggérons d'utiliser la fonction `str_c()` du paquet **stringr**. Si vous avez des valeurs de caractères distinctes à combiner, fournissez-les simplement comme arguments uniques, séparés par des virgules.     

```{r}
str_c("String1", "String2", "String3")
```

L'argument `sep = ` insére une valeur de caractère entre chacun des arguments que vous avez fournis (par exemple, fournir une virgule, un espace ou une nouvelle ligne insérerait ce caractère entre chaque argument `"\n"`)  

```{r}
str_c("String1", "String2", "String3", sep = ", ")
```

L'argument `collapse = ` est pertinent si vous mettez plusieurs *vecteurs* comme arguments à `str_c()`. L'argument `collapse = ` est utilisé pour séparer les éléments inclus dans le vecteur produit par `str_c()`, où le vecteur produit est un long vecteur de type caractère. 

L'exemple ci-dessous montre la combinaison de deux vecteurs en un seul vecteur (prénoms et noms de famille). Un autre exemple similaire pourrait être montré avec des régions et le nombre de cas dans chaque région. Dans cet exemple :  

* La valeur `sep = ` apparaît entre chaque prénom et chaque nom.  
* La valeur `collapse = ` apparaît entre chaque personne.   


```{r}
first_names <- c("abdul", "fahruk", "janice") 
last_names  <- c("hussein", "akinleye", "okeke")

# sep s'affiche entre les chaînes de caractères d'entrée respectives, tandis que collapse s'affiche entre les éléments produits
str_c(first_names, last_names, sep = " ", collapse = ";  ")
```

Remarque: Selon le contexte d'affichage souhaité, lorsque vous imprimez une telle chaîne combinée avec des nouvelles lignes, vous devrez peut-être envelopper la phrase entière dans `cat()` pour que les nouvelles lignes s'impriment correctement:  

```{r}
# Pour que les nouvelles lignes s'impriment correctement, il peut être nécessaire d'envelopper la phrase dans cat()
cat(str_c(first_names, last_names, sep = " ", collapse = ";\n"))
```



<!-- ======================================================= -->
### Chaînes de caractères dynamiques {.unnumbered}

Utilisez la fonction `str_glue()` pour insérer du code R dynamique dans une chaîne de caractères. C'est une fonction trés utile pour créer des légendes de graphiques dynamiques, comme démontré ci-dessous.  

* Tout le contenu est placé entre guillemets doubles comme ceci `str_glue("")`.  
* Tout code dynamique ou référence à des valeurs prédéfinies est placé entre des accolades `{}` à l'intérieur des guillemets doubles. Il peut y avoir plusieurs accolades dans la même commande de `str_glue()`.  
* Pour afficher les guillemets de caractères '', utilisez des guillemets simples entre les guillemets doubles (par exemple, pour le format de date - voir l'exemple ci-dessous).  
* Conseil : Vous pouvez utiliser `\n` pour forcer une nouvelle ligne.  
* Conseil : Vous pouvez utiliser `format()` pour ajuster l'affichage de la date, et utiliser `Sys.Date()` pour afficher la date actuelle.  


Un exemple simple, d'une légende d'un graphique dynamique :  

```{r}
str_glue("Les données incluent {nrow(linelist)} cas et sont actuelles à {format(Sys.Date(), '%d %b %Y')}.")
```

Un format alternatif consiste à utiliser des caractères de remplacement à l'intérieur des parenthèses et à définir le code dans des arguments séparés à la fin de la fonction `str_glue()`, comme ci-dessous. Cela peut améliorer la lisibilité du code si le texte est long.

```{r}
str_glue("Linelist à la {current_date}.\nDernier cas hospitalisé à l'hôpital {last_hospital}.\n{n_missing_onset} cas n'ont pas de date d'apparition et ne sont pas représentés.",
         current_date = format(Sys.Date(), '%d %b %Y'),
         last_hospital = format(as.Date(max(linelist$date_hospitalisation, na.rm=T)), '%d %b %Y'),
         n_missing_onset = nrow(linelist %>% filter(is.na(date_onset)))
         )

```


**Extraction d'un dataframe**

Il est parfois utile d'extraire des données d'un dataframe et de les coller ensemble en séquence.Vous trouverez ci-dessous un exemple de dataframe comprenant la juridiction (zone), les nouvelles affaires et les affaires totales. Nous allons l'utiliser pour faire une description résumée du nombre de nouveaux cas et du nombre total de cas par juridiction. 

```{r}
# créer un dataframe de cas
case_table <- data.frame(
  zone        = c("Zone 1", "Zone 2", "Zone 3", "Zone 4", "Zone 5"),
  new_cases   = c(3, 0, 7, 0, 15),
  total_cases = c(40, 4, 25, 10, 103)
  )
```

```{r, echo=F}
DT::datatable(case_table, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Utilisez la fonction `str_glue_data()`, qui est spécifiquement utilisée pour prendre des données à partir des lignes du dataframe: 

```{r}
case_table %>% 
  str_glue_data("{zone}: {new_cases} ({total_cases} total cases)")
```


**Combinaison des chaînes de caractères entre les lignes**  

Si vous essayez "d'enrouler" des valeurs dans une colonne d'un dataframe, c'est-à-dire de combiner des valeurs de plusieurs lignes en une seule ligne en les collant ensemble avec un séparateur. Consultez la section de la page [Deduplication](#deduplication) sur les valeurs ["enroulées"](#str_rollup).    

**Dataframe combiné en une seule ligne**  

Vous pouvez faire apparaêtre la description résumée sur une seule ligne en utilisant `str_c()` (en spécifiant le dataframe et les noms des colonnes), et en fournissant les arguments `sep = ` et `collapse = `.  


```{r}
str_c(case_table$zone, case_table$new_cases, sep = " = ", collapse = ";  ")
```

Vous pouvez ajouter le pré-fixe "New Cases :" (nouveaux cas en français) au début de la description en l'entourant d'une `str_c()` distincte (si "New Cases :" se trouvait dans la `str_c()``, il apparaîtrait plusieurs fois).   

```{r}
str_c("New Cases: ", str_c(case_table$zone, case_table$new_cases, sep = " = ", collapse = ";  "))
```




###  Unir les colonnes {#str_unite .unnumbered}

Dans un cadre de données, le regroupement des valeurs de caractères de plusieurs colonnes peut être réalisé avec la fonction `unite()` du paquet **tidyr**. C'est l'inverse de la fonction `separate()`.   

Vous devez fournir le nom de la nouvelle colonne unie et ensuite fournir les noms des colonnes que vous souhaitez unir.  

* Par défaut, le séparateur utilisé dans la colonne unie est le caractère de soulignement `_`, mais cela peut être changé avec l'argument `sep = `.  
* L'argument `remove = ` supprime les colonnes qui seront unies du dataframe (VRAI par défaut).  
* L'argument `na.rm = ` supprime les valeurs manquantes lors de l'unification (FAUX par défaut)  

Ci-dessous, nous définissons un mini dataframe pour la démonstration:  

```{r, message = F, warning=F}
df <- data.frame(
  case_ID = c(1:6),
  symptoms  = c("jaundice, fever, chills",     # patient 1
                "chills, aches, pains",        # patient 2 
                "fever",                       # patient 3
                "vomiting, diarrhoea",         # patient 4
                "bleeding from gums, fever",   # patient 5
                "rapid pulse, headache"),      # patient 6
  outcome = c("Recover", "Death", "Death", "Recover", "Recover", "Recover"))
```


```{r}
df_split <- separate(df, symptoms, into = c("sym_1", "sym_2", "sym_3"), extra = "merge")
```

Voici l'exemple avec le dataframe dessus: 

```{r, echo=F}
DT::datatable(df_split, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Ci-dessous, nous réunissons les trois colonnes de symptômes:  

```{r}
df_split %>% 
  unite(
    col = "all_symptoms", # nom de la nouvelle colonne unie
    c("sym_1", "sym_2", "sym_3"), # colonnes à unir
    sep = ", ", # séparateur à utiliser dans la colonne unie
    remove = TRUE, # si TRUE (VRAI), supprime les colonnes d'entrée du dataframe
    na.rm = TRUE # Si TRUE (VRAI), les valeurs manquantes sont supprimées avant l'unification.
  )
```







<!-- ======================================================= -->
### séparer {.unnumbered}  

Pour diviser une chaîne de caractères en fonction d'un motif, utilisez la fonction `str_split()`. Cette fonction évalue la (ou les) chaîne(s) de caractères et renvoie une `liste` de vecteurs de caractères constituée des valeurs nouvellement séparées.

L'exemple simple ci-dessous évalue une chaîne de caractères et la divise en trois. Par défaut, il retourne un objet de la classe `list` avec un élément (un vecteur de caractères) pour chaque chaîne initialement fournie. Si `simplify = TRUE`, (en français VRAI), il retourne une matrice de caractères.   

Dans cet exemple, une chaîne de caractères est fournie, et la fonction renvoie une liste avec un élément - un vecteur de caractères avec trois valeurs.   

```{r}
str_split(string = "jaundice, fever, chills",
          pattern = ",")
```

Si les valeurs séparées sont enregistrées dans un objet, vous pouvez alors accéder à la n-iéme valeur séparée avec la syntaxe des crochets. Pour accéder à une valeur spécifique, vous pouvez utiliser une syntaxe comme celle-ci : `nouveau_objet[[1]][2]`, qui accéderait à la deuxiéme valeur de la premiére chaîne évaluée ("fever", ou fiévre en français, dans le dataframe de l'exemple). Consultez la page [Bases de R](#rbasics) pour plus de détails sur l'accés aux éléments.  

```{r}
pt1_symptoms <- str_split("jaundice, fever, chills", ",")

pt1_symptoms[[1]][2]  # extrait la 2éme valeur du 1er (et dans ce cas unique) élément de la liste
```

Si plusieurs chaînes de caractères sont fournies par `str_split()`, il y aura plus d'un élément dans la liste retournée. 

```{r}
symptoms <- c("jaundice, fever, chills",     # patient 1
              "chills, aches, pains",        # patient 2 
              "fever",                       # patient 3
              "vomiting, diarrhoea",         # patient 4
              "bleeding from gums, fever",   # patient 5
              "rapid pulse, headache")       # patient 6

str_split(symptoms, ",")                     # split each patient's symptoms
```


Pour retourner une "matrice de caractères" à la place, ce qui peut être utile pour créer des colonnes dans votre dataframe, définissez l'argument `simplify = TRUE` comme indiqué ci-dessous :  

```{r}
str_split(symptoms, ",", simplify = TRUE) #simplify = VRAI
```

Vous pouvez également ajuster le nombre de séparations à créer avec l'argument `n = `. Par exemple, l'exemple ci-dessous limite le nombre de séparations à deux. Toutes les autres virgules restent dans les deuxiémes valeurs. 

```{r}
str_split(symptoms, ",", simplify = TRUE, n = 2)
```

*Note - les mêmes résultats peuvent être obtenus avec `str_split_fixed()`, dans lequel vous ne donnez pas l'argument `simplify`, mais devez à la place désigner le nombre de colonnes (`n`).* 

```{r, eval=F}
str_split_fixed(symptoms, ",", n = 2)
```




### Séparer les colonnes {.unnumbered}  

Si vous essayez de séparer une colonne dans un dataframe, il est préférable d'utiliser la fonction `separate()` de **dplyr**. Cette fonction est utilisée pour séparer une colonne de caractères en d'autres colonnes. 

Disons que nous avons un simple dataframe `df` (défini et uni dans la [section unite](#str_unite)) contenant une colonne `case_ID`, une colonne de caractères avec plusieurs symptômes, et une colonne de résultats. Notre objectif est de séparer la colonne `symptoms` en plusieurs colonnes, chacune contenant un symptôme.   


```{r, echo=F}
DT::datatable(df, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

En supposant que les données sont passées ('pipe', en utilisant %>%) dans `separate()`, vous devez d'abord fournir la colonne à séparer dans la fonction `seperate()`. Ensuite, fournissez `into = ` comme un vecteur `c( )` contenant les noms des *nouvelles* colonnes, comme indiqué ci-dessous.  

* `sep = ` le séparateur, peut être un caractère, ou un nombre (interprété comme la position du caractère à séparer) 
* `remove = ` FAUX par défaut, supprime la colonne sélectionnée du dataframe une fois séparée.  
* `convert = ` FALSE par défaut, si TRUE, les "NA" de la chaîne deviendront des `NA`.  
* `extra = ` ce contréle ce qui se passe s'il y a plus de valeurs créées par la séparation de la colonne que de nouveaux noms de colonnes fournis.
     * `extra = "warn"` signifie que vous verrez un avertissement mais que les valeurs excédentaires seront supprimées (**par défaut**).  
     * `extra = "drop"`, signifie que les valeurs excédentaires seront abandonnées sans avertissement.  
     **`extra = "merge"` ne fractionnera que le nombre de nouvelles colonnes listées dans `into` - *_cette configuration préservera toutes vos données_**.  


Voici un exemple avec `extra = "merge"` - Deux nouvelles colonnes sont définies mais tous les troisiémes symptômes ou plus sont combinés dans la deuxiéme colonne:  

```{r}
# troisiémes symptômes combinés dans la deuxiéme nouvelle colonne
df %>% 
  separate(symptoms, into = c("sym_1", "sym_2"), sep=",", extra = "merge")
```

Lorsque l'option par défaut est utilisée ci-dessous, R retourne un avertissement mais les troisiémes symptômes sont perdus:  

```{r}
# les troisiémes symptômes sont perdus
df %>% 
  separate(symptoms, into = c("sym_1", "sym_2"), sep=",")
```


<span style="color: orange;">**_CAUTION:_** Si vous ne fournissez pas suffisamment de valeurs `into` pour les nouvelles colonnes, vos données peuvent être tronquées.</span>  






<!-- ======================================================= -->
### Classer par ordre alphabétique {.unnumbered} 

Vous pouvez trier plusieurs chaînes de caractères par ordre alphabétique. `str_order()` renvoie l'ordre (numérique), tandis que `str_sort()` renvoie les chaînes de caractères dans l'ordre alphabétique.

```{r}
# chaînes de caractères
health_zones <- c("Alba", "Takota", "Delta")

# renvoie l'ordre alphabétique
str_order(health_zones)

# retourne les chaînes de caractères par ordre alphabétique
str_sort(health_zones)
```

Pour utiliser un alphabet différent, ajoutez l'argument `locale = `. Voir la liste complète des locales en entrant `stringi::stri_locale_list()` dans la console R.  





<!-- ======================================================= -->
### Fonctions de la base R {.unnumbered}

Il est courant de voir les fonctions **base** R `paste()` et `paste0()`, qui concaténent des vecteurs aprés avoir converti toutes les parties en caractères. Ces fonctions agissent de manière similaire à `str_c()` mais la syntaxe est sans doute plus compliquée - dans les parenthèses, chaque partie est séparée par une virgule. Les parties sont soit du texte en caractères (entre guillemets), soit des objets de code prédéfinis (sans guillemets). Par exemple:

```{r}
n_beds <- 10
n_masks <- 20

paste0("Regional hospital needs ", n_beds, " beds and ", n_masks, " masks.")
```

Les arguments `sep = ` et `collapse = ` peuvent être spécifiés. `paste()` est simplement `paste0()` avec un `sep = " "` (un espace) par défaut. 




## Nettoyer et standardiser    


<!-- ======================================================= -->
### Changer la casse {.unnumbered}

Souvent, on doit modifier la casse/capitalisation d'une valeur de chaîne de caractères, par exemple les noms des jursidictions. Utilisez `str_to_upper()`, `str_to_lower()`, et `str_to_title()`, de **stringr**, comme indiqué ci-dessous :  

```{r}
str_to_upper("California")

str_to_lower("California")
```

En utilisant *base** R, l'exemple ci-dessus peut également être réalisé avec `toupper()` ou `tolower()`.  


**Mots en majuscules**  

La transformation de la chaîne pour que chaque mot soit en majuscule peut être réalisée avec `str_to_title()`:  

```{r}
str_to_title("go to the US state of california ")
```

Utilisez la fonction `toTitleCase()` du paquet **tools** pour ne mettre en majuscules que certains mots (les mots comme "to", "the" et "of" ne sont pas mis en majuscules).  

```{r}
tools::toTitleCase("This is the US state of california")
```

Vous pouvez également utiliser la fonction `str_to_sentence()`, qui ne met en majuscule que la premiére lettre de la chaîne.

```{r}
str_to_sentence("the patient must be transported")
```



### Longueur du bloc {#str_pad .unnumbered}

Utilisez la fonction `str_pad()` pour ajouter des caractères à une chaîne, jusqu'à  une longueur minimale. Par défaut, des espaces sont ajoutés, mais vous pouvez également ajouter d'autres caractères en utilisant l'argument `pad = `.  


```{r}
# Codes ICD de longueur différente
ICD_codes <- c("R10.13",
               "R10.819",
               "R17")

# Les codes ICD sont complétés à 7 caractères sur la droite
str_pad(ICD_codes, 7, "right")

# Remplir avec des points au lieu d'espaces
str_pad(ICD_codes, 7, "right", pad = ".")
```

Par exemple, pour complèter des nombres avec des zéros en tête (comme pour les heures ou les minutes), vous pouvez complèter le nombre à une longueur minimale de 2 avec `pad = "0"`.

```{r}
# Ajoutez des zéros à deux chiffres (par exemple, pour les minutes/heures)
str_pad("4", 2, pad = "0") 

# Exemple utilisant une colonne numérique nommée "heures" (hours en Anglais)
# hours <- str_pad(hours, 2, pad = "0")
```


### Tronquer {.unnumbered} 

La fonction `str_trunc()` définit une longueur maximale pour chaque chaîne. Si une chaîne dépasse cette longueur, elle est tronquée (raccourcie) et une ellipse (...) est incluse pour indiquer que la chaîne était auparavant plus longue. Notez que l'ellipse *est* comptée dans la longueur. Les caractères d'ellipses peuvent être changés avec l'argument `ellipsis = `.  L'argument optionnel `side = ` spécifie où l'ellipse apparaîtra dans la chaîne tronquée ("gauche", "droite", ou "centre").  

```{r}
original <- "Symptom onset on 4/3/2020 with vomiting"
str_trunc(original, 10, "center")
```


### Normaliser la longueur {.unnumbered}

Utilisez la fonction `str_trunc()` pour définir une longueur maximale, puis utilisez `str_pad()` pour étendre les chaînes trés courtes à cette longueur tronquée. Dans l'exemple ci-dessous, 6 est défini comme longueur maximale (une valeur est tronquée), puis une valeur trés courte est ajoutée pour atteindre la longueur de 6.       

```{r}
# Codes CIM de longueur différente
ICD_codes   <- c("R10.13",
                 "R10.819",
                 "R17")

# tronquer à la longueur maximale de 6
ICD_codes_2 <- str_trunc(ICD_codes, 6)
ICD_codes_2

# étendre à une longueur minimale de 6
ICD_codes_3 <- str_pad(ICD_codes_2, 6, "right")
ICD_codes_3
```


### Supprimer les espaces avant/aprés les chaînes de caractères {.unnumbered}  

Utilisez la fonction `str_trim()` pour supprimer les espaces, les nouvelles lignes (`\n`) ou les tabulations (`\t`) sur les côtés d'une entrée de chaîne de caractères. Ajoutez `"right"` (en français droite), `"left"` (en français gauche) ou `"both"` (en français les deux) à la commande pour spécifier le côté à découper (par exemple, `str_trim(x, "right")`.

```{r}
# Numéros d'identification avec espaces excédentaires à droite
IDs <- c("provA_1852  ", # deux espaces excédentaires
         "provA_2345",   # zero espace excédentaire
         "provA_9460 ")  # un espace excédentaire

#  les identifiants sont coupés pour supprimer les espaces excédentaires du côté droit uniquement
str_trim(IDs)
```


### Supprimer les espaces répétés dans les chaînes de caractères {.unnumbered}  

Utilisez la fonction `str_squish()` pour supprimer les espaces répétés qui apparaissent *à l'intérieur* d'une chaîne. Par exemple, pour convertir les espaces doubles en espaces simples. Elle supprime également les espaces, les retours à la ligne ou les tabulations à l'extérieur de la chaîne, comme `str_trim()`.  


```{r}
# L'original contient des espaces supplémentaires dans la chaîne
str_squish("  Pt requires   IV saline\n") 
```

Entrez `?str_trim`, `?str_pad` dans votre console R pour voir plus de détails.   


### Transformer en paragraphes {.unnumbered}  

Utilisez `str_wrap()` pour transformer un long texte non structuré en un paragraphe structuré avec une longueur de ligne fixe. Fournissez la longueur idéale de caractères pour chaque ligne, et la fonction applique un algorithme pour insérer des nouvelles lignes (`\n`) dans le paragraphe, comme dans l'exemple ci-dessous. 

```{r}
pt_course <- "Début des symptômes 1/4/2020 : vomissements, frissons, fièvre. Le patient a vu un guérisseur traditionnel dans son village natal le 2/4/2020. Le 5/4/2020, les symptômes du patient se sont aggravés et il a été admis à la clinique Lumta. Un échantillon a été prélevé et le patient a été transporté à l'hôpital régional le 6/4/2020. Le patient est décédé à l'hôpital régional le 7/4/2020."

str_wrap(pt_course, 40)
```

La fonction de **base** R cat()` peut être enroulée autour de la commande ci-dessus afin d'imprimer le résultat dans la console R. 

```{r}
cat(str_wrap(pt_course, 40))
```






<!-- ======================================================= -->
##  Gérer par position { }


### Extraire par position de caractère  {.unnumbered}  

Utilisez la fonction `str_sub()` pour retourner seulement une partie d'une chaîne de caractères. La fonction prend trois arguments principaux :  

1) le(s) vecteur(s) de caractères  
2) la position de départ dans le vecteur
3) la position finale dans le vecteur

Quelques remarques sur les numéros de position :  

* Si le numéro de position est positif, la position est comptée à partir de l'extrémité gauche de la chaîne.  
* Si le numéro de position est négatif, il est compté à partir de l'extrémité droite de la chaîne.  
* Les numéros de position sont inclusifs.  
* Les positions qui dépassent la chaîne de caractères seront tronquées (supprimées).  

Voici quelques exemples appliqués à la chaîne "pneumonie" :  

```{r}
# position de départ et position finale troisiéme en partant de la gauche (3éme lettre en partant de la gauche)
str_sub("pneumonia", 3, 3)

# 0 n'est pas présent, donc cela renvoie ""
str_sub("pneumonia", 0, 0)

# # 6éme en partant de la gauche, jusqu'à  la 1ére en partant de la droite
str_sub("pneumonia", 6, -1)

# 5éme de la droite, vers le 2éme de la droite
str_sub("pneumonia", -5, -2)

# 4éme en partant de la gauche, jusqu'à  une position en dehors de la chaîne de caractères
str_sub("pneumonia", 4, 15)
```



### Extraire par position de mot {.unnumbered} 

Pour extraire le niéme "mot", utilisez la fonction `word()`, du paquet **stringr**. Fournissez la ou les chaînes de caractères, puis la premiére position du mot à extraire, et la dernière position du mot à extraire.  

Par défaut, le séparateur entre les mots est supposé être un espace, sauf indication contraire avec `sep = ` (par exemple, `sep = "_"` où les mots sont séparés par des caractères de soulignement).   


```{r}
# chaînes de caractères à évaluer
chief_complaints <- c("I just got out of the hospital 2 days ago, but still can barely breathe.",
                      "My stomach hurts",
                      "Severe ear pain")

# extract 1st to 3rd words of each string
word(chief_complaints, start = 1, end = 3, sep = " ")
```


###  Remplacer par la position du caractère {.unnumbered} 

`str_sub()` apparié avec l'opérateur d'affectation (`<-`) peut être utilisé pour modifier une partie d'une chaîne : 

```{r}
word <- "pneumonia"

# convertissez les troisiéme et quatriéme caractères en X 
str_sub(word, 3, 4) <- "XX"

# imprimer
word
```

Voici ci-dessus Un exemple appliqué à plusieurs chaînes de caractères (par exemple, un vecteur de mots ou une colonne). Notez l'expansion en longueur de "HIV".   

```{r}
words <- c("pneumonia", "tubercolosis", "HIV")

# convertissez les troisiéme et quatriéme caractères en X 
str_sub(words, 3, 4) <- "XX"

words
```



### Evaluer la longueur {.unnumbered}


```{r}
str_length("abc")
```

Alternativement, utilisez la fonction `nchar()` de **base** R.





<!-- ======================================================= -->
## Motifs { }

De nombreuses fonctions du paquet **stringr** fonctionnent pour détecter, localiser, extraire, correspondre, remplacer et séparer en fonction d'un *motif* spécifié.  


<!-- ======================================================= -->
### détecter un motif {.unnumbered}

Utilisez la fonction `str_detect()` comme ci-dessous pour détecter la présence/absence d'un motif dans une chaîne de caractères. Fournissez d'abord la chaîne ou le vecteur à rechercher (`string = `), puis le motif à rechercher (`pattern = `). Notez que par défaut, la recherche *est sensible à la casse*!

```{r}
str_detect(string = "primary school teacher", pattern = "teach")
```

L'argument `negate = ` peut être inclus et mis à `TRUE` (en français VRAI) si vous voulez savoir si le motif n'est PAS présent.  

```{r}
str_detect(string = "primary school teacher", pattern = "teach", negate = TRUE)
```

Pour ignorer les majuscules et les minuscules, intégrez le motif dans `regex()`, et *dans* `regex()` ajoutez l'argument `ignore_case = TRUE` (ou `T` en raccourci).  

```{r}
str_detect(string = "Teacher", pattern = regex("teach", ignore_case = T))
```

Lorsque `str_detect()` est appliqué à un vecteur de caractères ou à une colonne de dataframe, il renvoie TRUE (en français VRAI) ou FALSE (en français FAUX) pour chacune des valeurs. 

```{r}
# un vecteur/colonne de professions 
occupations <- c("field laborer",
                 "university professor",
                 "primary school teacher & tutor",
                 "tutor",
                 "nurse at regional hospital",
                 "lineworker at Amberdeen Fish Factory",
                 "physican",
                 "cardiologist",
                 "office worker",
                 "food service")

# détecter la présence du motif "teach" dans chaque chaîne - la valeur retournée est un vecteur de VRAI/FAUX
str_detect(occupations, "teach")

```

Si vous avez besoin de compter les `TRUE`s, utilisez la fonction `sum()` sur les valeurs ou le vecteur retourné(es). Ceci compte le nombre de `TRUE` (VRAI) dans les valeurs ou le vecteur retourné(es). 

```{r}
sum(str_detect(occupations, "teach"))
```

Pour effectuer une recherche incluant plusieurs termes, incluez-les séparés par des barres OR (`|`) dans l'argument `pattern = `, comme indiqué ci-dessous: 

```{r}
sum(str_detect(string = occupations, pattern = "teach|professor|tutor"))
```

Si vous avez besoin de construire une longue liste de termes de recherche, vous pouvez les combiner en utilisant `str_c()` et `sep = |`, et assigner ceci à un objet. Vous pouvez ensuite référencer le vecteur par le nom de l'objet. L'exemple ci-dessous combine les termes de recherche d'une occupation possible en un seul objet.  

```{r}
# termes de recherche
occupation_med_frontline <- str_c("medical", "medicine", "hcw", "healthcare", "home care", "home health",
                                "surgeon", "doctor", "doc", "physician", "surgery", "peds", "pediatrician",
                               "intensivist", "cardiologist", "coroner", "nurse", "nursing", "rn", "lpn",
                               "cna", "pa", "physician assistant", "mental health",
                               "emergency department technician", "resp therapist", "respiratory",
                                "phlebotomist", "pharmacy", "pharmacist", "hospital", "snf", "rehabilitation",
                               "rehab", "activity", "elderly", "subacute", "sub acute",
                                "clinic", "post acute", "therapist", "extended care",
                                "dental", "dential", "dentist", sep = "|")

occupation_med_frontline
```

Cette commande renvoie le nombre de professions qui contiennent l'un des termes de recherche pour les praticiens médicaux (`occupation_med_frontline`):  

```{r}
sum(str_detect(string = occupations, pattern = occupation_med_frontline))
```



**Fonctions de recherche de chaînes de caractères de base R**  

La fonction de R **base** `grepl()` fonctionne de manière similaire à `str_detect()`, dans le sens qu'elle recherche les correspondances avec un motif et retourne un vecteur logique. La syntaxe de base est `grepl(pattern, strings_to_search, ignore.case = FALSE, ...)`. Un avantage est que l'argument `ignore.case` est plus facile à écrire (il n'y a pas besoin d'impliquer la fonction `regex()`).  

Les fonctions `sub()` et `gsub()` de base R agissent de manière similaire à `str_replace()`. Leur syntaxe générale suit ce format : `gsub(motif, remplacement, chaînes_a_rechercher, ignore.case = FALSE)`. `sub()` remplacera seulement la premiére instance du motif, alors que `gsub()` remplacera toutes les instances du motif.   


#### Convertir les virgules en points {.unnumbered}  

Voici un exemple d'utilisation de `gsub()` pour convertir des virgules en points dans un vecteur de nombres. Cela peut être utile si vos données proviennent de différents endroits dans le monde avec une syntaxe de langue différente.  

L'exemple ci-dessous utilise deux applications de `gsub`. L'application interne `gsub()`, qui agit en premier sur l'objet `lengths`, convertit tous les points en "" sans espace. Le caractère point "." doit être "spécifié" avec deux slashs pour signifier réellement un point, parce que "." en regex sans deux slashs signifie " n'importe quel caractère". Ensuite, le résultat (avec seulement des virgules) est passé à la fonction externe `gsub()` dans laquelle les virgules sont remplacées par des points.     

```{r, eval=F}
lengths <- c("2.454,56", "1,2", "6.096,5")

as.numeric(gsub(pattern = ",",                # trouver les virgules    
                replacement = ".",            # remplacer par des points
                x = gsub("\\.", "", lengths)  # vecteur avec d'autres points supprimés 
                )
           )                                  # convertir le résultat en numérique
```





### Remplacer tous les éléments {.unnumbered}  

Utilisez la fonction `str_replace_all()` comme un outil de "recherche et remplacement". Vous fournissez d'abord les chaînes à évaluer à `string = `, puis le motif à remplacer à `pattern = `, et enfin la valeur de remplacement à `replacement = `. L'exemple ci-dessous remplace toutes les occurrences de "dead" (en français mort) par "deceased" (en français décédés). Notez que cette opération est sensible à la casse.   

```{r}
outcome <- c("Karl: dead",
            "Samantha: dead",
            "Marco: not dead")

str_replace_all(string = outcome, pattern = "dead", replacement = "deceased")
```

Remarques :  

* Pour remplacer un motif par `NA`, utilisez `str_replace_na()`.  
* La fonction `str_replace()` remplace uniquement la premiére instance du motif dans chaque chaîne évaluée.  


<!-- ======================================================= -->
### détecter en utilisant des instructions logiques {.unnumbered}


**A l'intérieur de `case_when()`**  

`str_detect()` est souvent utilisé dans `case_when()` (du paquet **dplyr**). Disons que `occupations` est une colonne de la linelist (liste linéaire ou liste de cas). La fonction `mutate()` ci-dessous crée une nouvelle colonne appelée `is_educator` en utilisant la logique conditionnelle via `case_when()`. Consultez la page sur le nettoyage des données pour en savoir plus sur la méthode `case_when()`.  


```{r, eval=F}
df <- df %>% 
  mutate(is_educator = case_when(
    #  recherche de termes dans la profession, non sensible à la casse
    str_detect(occupations,
               regex("teach|prof|tutor|university",
                     ignore_case = TRUE))              ~ "Educator",
    # all others
    TRUE                                               ~ "Not an educator"))
```

Par ailleurs, il peut être important d'ajouter des critéres d'exclusion à la logique conditionnelle (`negate = F`) :  

```{r, eval=F}
df <- df %>% 
  # la valeur de la nouvelle colonne is_educator est basée sur la logique conditionnelle
  mutate(is_educator = case_when(
    
    # la colonne occupation doit répondre à 2 critéres pour se voir attribuer "Educateur" :
    # elle doit avoir un terme de recherche ET PAS de terme d'exclusion
    
    #  Doit avoir un terme de recherche
    str_detect(occupations,
               regex("teach|prof|tutor|university", ignore_case = T)) &              
    
    # AND ne doit PAS avoir de terme d'exclusion
    str_detect(occupations,
               regex("admin", ignore_case = T),
               negate = TRUE                        ~ "Educator"
    
    # Toutes les lignes ne répondant pas aux critéres ci-dessus
    TRUE                                            ~ "Not an educator"))
```





<!-- ======================================================= -->
### Localiser la position du motif {.unnumbered}  

Pour localiser la *premiére* position d'un motif, utilisez la fonction `str_locate()`. Elle produit une position de début et de fin.   

```{r}
str_locate("I wish", "sh")
```

Comme les autres fonctions de la famille `str`, il existe une version "_all" (`str_locate_all()`) qui renvoie les positions de *toutes* les instances du motif dans chaque chaîne. Cette fonction renvoie les valeurs sous forme de `liste`.  

```{r}
phrases <- c("I wish", "I hope", "he hopes", "He hopes")

str_locate(phrases, "h" ) # position de la *premiére* instance du motif
str_locate_all(phrases, "h" ) # position de *chaque* instance du motif
```





<!-- ======================================================= -->
### Extraire une correspondance {.unnumbered}  

`str_extract_all()` retourne les motifs de correspondance eux-mêmes, ce qui est trés utile lorsque vous avez proposé plusieurs motifs via des conditions "OR" (en français OU). Par exemple, si vous cherchez dans le vecteur de chaînes de professions (voir l'exemple précédent) *soit* "teach", "prof", ou "tutor".

`str_extract_all()` renvoie une `liste` qui contient *toutes les correspondances* pour chaque chaîne évaluée. Voyez ci-dessous comment l'occupation 3 contient deux correspondances de motifs.  

```{r}
str_extract_all(occupations, "teach|prof|tutor")
```


`str_extract()` extrait *seulement la premiére correspondance* dans chaque chaîne évaluée, produisant un vecteur de caractères avec un élément pour chaque chaîne évaluée. Elle retourne `NA` lorsqu'il n'y a pas de correspondance. Les `NA`s peuvent être supprimés en enveloppant le vecteur retourné avec `na.exclude()`. Notez que la deuxiéme correspondance de l'occupation 3 n'est pas affichée.  

```{r}
str_extract(occupations, "teach|prof|tutor")
```

<!-- ======================================================= -->
### Sous-ensemble et comptage {.unnumbered}  

Les fonctions alignées comprennent `str_subset()` et `str_count()`.  

`str_subset()` renvoie les valeurs réelles qui contiennent le motif : 

```{r}
str_subset(occupations, "teach|prof|tutor")
```

`str_count()` renvoie un vecteur de nombres : le **nombre de fois** qu'un terme de recherche apparaît dans chaque valeur évaluée.  

```{r}
str_count(occupations, regex("teach|prof|tutor", ignore_case = TRUE))
```


<!-- ======================================================= -->
### Groupes de regex {.unnumbered}

EN CONSTRUCTION



<!-- ======================================================= -->
## caractères spéciaux  

**Slash arriére `\` comme échappement**  

La barre oblique inverse `\` est utilisée pour "échapper" à la signification du caractère suivant. Ainsi, une barre oblique inversée peut être utilisée pour afficher un guillemet *dans* d'autres guillemets (`\"`) - le guillemet central ne "cassera" pas les guillemets environnants.  

Note - Si vous voulez *afficher* une barre oblique inverse, vous devez échapper à sa signification avec *une autre* barre oblique inverse. Ainsi, vous devez écrire deux barres obliques inversées `\\` pour en afficher une.  

**caractères spéciaux " 

caractère spécial | Représentation  
----------------- | --------------------------------------------------------------    
`"\\"` | barre oblique inversée  
`"\n"` | une nouvelle ligne (newline)   
`"\""` | guillemets doubles *entre* guillemets doubles  
`"\""` | guillemets simples *entre guillemets simples  
`"\`"` | accent grave  
`"\r"` | retour chariot  
`"\t"` | tabulation  
`"\v"` | tabulation verticale 
`"\b"` | retour arriére  

Exécutez ` ?"'"` dans la console R pour afficher la liste complète de ces caractères spéciaux (elle apparaîtra dans le volet d'aide de RStudio). 



<!-- ======================================================= -->
## Expressions régulières (regex) 


<!-- ======================================================= -->
## Regex et caractères spéciaux { } 

Les expressions régulières, ou "regex", sont un langage concis pour décrire des motifs dans des chaînes de caractères. Si vous n'étes pas familier avec ce langage, une expression régulière peut ressembler à une langue étrangére. Nous allons essayer de démystifier un peu ce langage.  

*Une grande partie de cette section est adaptée de [ce tutoriel](https://towardsdatascience.com/a-gentle-introduction-to-regular-expressions-with-r-df5e897ca432) et de [cette cheatsheet](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)*. Nous adaptons ici de manière sélective, sachant que ce manuel pourrait être consulté par des personnes n'ayant pas d'accés à l'Internet pour consulter les autres tutoriels.    


Une expression régulière est souvent utilisée pour extraire des motifs spécifiques d'un texte "non structuré", par exemple des notes médicales, des plaintes principales, des antécédents du patient ou d'autres colonnes de texte libre dans un dataframe.

Il existe quatre outils de base que l'on peut utiliser pour créer une expression régulière de base :  

1) Jeux de caractères  
2) Méta-caractères  
3) Quantificateurs  
4) Groupes  


**Jeux de caractères**  

Les jeux de caractères sont une façon d'exprimer les options de liste pour une correspondance de caractères, entre parenthèses. Ainsi, une correspondance sera déclenchée si l'un des caractères entre parenthèses est trouvé dans la chaîne. Par exemple, pour rechercher des voyelles, on peut utiliser ce jeu de caractères: "[aeiou]". Voici d'autres jeux de caractères courants:  

Jeu de caractères | Correspond à  
----------------- | --------------------------------------------------------------    
`"[A-Z]"` | toute lettre majuscule unique  
`"[a-z]"` | toute lettre minuscule unique
`"[0-9]"` | n'importe quel chiffre  
`[:alnum:]` | tout caractère alphanumérique  
`[:digit:]` | tout chiffre numérique  
`[:alpha:]` | toute lettre (majuscule ou minuscule)  
`[:upper:]` | toute lettre majuscule  
`[:lower:]` | toute lettre minuscule  



Les jeux de caractères peuvent être combinés à l'intérieur d'une même parenthèse (sans espace !), par exemple `"[A-Za-z]"` (toute lettre majuscule OU minuscule), ou un autre exemple `"[t-z0-5]"` (minuscules de t à z OU nombre de 0 à 5).  



**Meta characters**  

Les métacaractères sont des raccourcis pour les jeux de caractères. Certains des plus importants sont énumérés ci-dessous :  

caractère méta | Représentation  
----------------- | --------------------------------------------------------------    
`"\\s"` | un seul espace  
`"\\w"` | un seul caractère alphanumérique (A-Z, a-z, ou 0-9)  
`"\\d"` | un seul chiffre (0-9) 


**Quantificateurs**  

En général, vous ne souhaitez pas rechercher une correspondance sur un seul caractère. Les quantificateurs vous permettent de désigner la longueur des lettres/chiffres pour permettre une correspondance plus spécifique.  

Les quantificateurs sont des nombres écrits entre accolades `{ }` *aprés* le caractère qu'ils quantifient, par exemple:

* `"A{2}"` renverra les instances de **deux** lettres majuscules A.  
* `"A{2,4}"` renverra les instances de **deux à quatre** lettres majuscules A *(ne mettez pas d'espace !)*.  
* `"A{2,}"` renverra les instances de **deux lettres A majuscules ou plus**.  
* `"A+"` renverra les instances de **une ou plusieurs** lettres majuscules A (groupe étendu jusqu'à  ce qu'un caractère différent soit rencontré).  
* Faites précéder d'un astérisque `*` pour obtenir **zéro ou plus** de correspondances (utile si vous n'étes pas sûr que le motif soit présent).  


En utilisant le symbole plus `+` comme quantificateur, la correspondance se fera jusqu'à  ce qu'un caractère différent soit rencontré. Par exemple, cette expression retournera tous les *mots* (caractères alpha : `"[A-Za-z]+"`).


```{r}
# chaîne de caractères pour tester les quantificateurs
test <- "A-AA-AAA-AAAA"
```

Lorsqu'un quantificateur de {2} est utilisé, seules les paires de A consécutifs sont renvoyées. Deux paires sont identifiées dans `AAAA`.  

```{r}
str_extract_all(test, "A{2}")
```

Lorsqu'un quantificateur de {2,4} est utilisé, les groupes de A consécutifs d'une longueur de deux à quatre sont renvoyés.  

```{r}
str_extract_all(test, "A{2,4}")
```

Avec le quantificateur `+`, les groupes de **un ou plus** sont renvoyés :   

```{r}
str_extract_all(test, "A+")
```

**Relative position**  

Il existe des exigences spécifiques pour ce qui précéde ou suit un motif. Par exemple, pour extraire des phrases, "deux chiffres qui sont suivis d'un point" (`""`).  (?<=\\.)\\s(?=[A-Z]) 

```{r}
str_extract_all(test, "")
```


déclaration de position | Correspond à  
----------------- | --------------------------------------------------------------    
`"(?<=b)a"` | Un "a" qui **est précédé** d'un "b".  
`"(?<!b)a"` | "a" qui **n'est PAS précédé** par un "b"  
`"a(?=b)"` | "a" qui **est suivi** par un "b"  
`"a(?!b)"` | "a" qui **n'est PAS suivi** d'un "b".  




**Groupes**  

La capture de groupes dans votre expression régulière est un moyen d'obtenir des valeurs plus organisées lors de l'extraction.  




**Exemples de regex**  

Vous trouverez ci-dessous un texte libre pour les exemples. Nous allons essayer d'en extraire des informations utiles en utilisant un terme de recherche par expression régulière.    

```{r}
pt_note <- "Le patient est arrivé aux urgences de l'hôpital Broward à 18h00 le 6/12/2005. Le patient s'est présenté avec une douleur abdominale irradiant du quadrant LR. La peau du patient était pâle, fraîche et moite. Sa température était de 99,8 degrés Farinheit. Le pouls du patient était de 100 bpm et filant. La fréquence respiratoire était de 29 par minute."
```

Cette expression correspond à tous les mots (tout caractère jusqu'à  la frappe d'un non-caractère tel qu'un espace):  

```{r}
str_extract_all(pt_note, "[A-Za-z]+")
```

L'expression `"[0-9]{1,2}"` correspond à des nombres consécutifs de 1 ou 2 chiffres. On pourrait aussi l'écrire `"\d{1,2}"`, ou `"[:digit:]{1,2}"`.   

```{r}
str_extract_all(pt_note, "[0-9]{1,2}")
```

"
<!-- Cette expression va extraire toutes les phrases (en supposant que la premiére lettre est en majuscule, et que la phrase se termine par un point). Le motif se lit en anglais comme suit : "Une lettre majuscule suivie de quelques lettres minuscules, d'un espace, de quelques lettres, d'un espace, -->".

<!-- ```{r} -->
<!-- str_extract_all(pt_note, "[A-Z][a-z]+\\s\\w+\\s\\d{1,2}\\s\\w+\\s*\\w*") -->
<!-- ``` -->


Vous pouvez consulter une liste utile d'expressions regex et de conseils à la page 2 de [cette cheatsheet](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf).  

Voir également ce [tutoriel](https://towardsdatascience.com/a-gentle-introduction-to-regular-expressions-with-r-df5e897ca432).  




<!-- ======================================================= -->
## Resources { }

Une fiche de référence pour les fonctions du paquet **stringr** peut être trouvée [ici](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)


Une vignette sur **stringr** peut être trouvée [ici](https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html).



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/characters_strings.Rmd-->

# Facteurs {#factors}


```{r, out.width=c('100%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "Factors_1500x500.png"))
```

En R, les *facteurs* sont une classe de données qui permettent de créer des catégories ordonnées avec un ensemble fixe de valeurs acceptables.  

En règle générale, vous convertissez une colonne de la classe caractères ou numérique en un facteur si vous souhaitez définir un ordre intrinsèque aux valeurs ("*niveaux*") afin qu'elles puissent être affichées de manière non alphabétique dans les graphiques et les tableaux. Une autre utilisation courante des facteurs consiste à normaliser les légendes des graphiques afin qu'elles ne fluctuent pas si certaines valeurs sont temporairement absentes des données.   

Cette page présente l'utilisation des fonctions du package **forcats** (un nom court pour "**For** **cat**egorical variables") et de certaines fonctions R **base**. Nous abordons également l'utilisation de **lubridate** et **aweek** pour les cas de facteurs spéciaux liés aux semaines épidémiologiques.  

Une liste complète des fonctions du pacakge **forcats** est disponible en ligne [ici](https://forcats.tidyverse.org/reference/index.html). Nous présentons ci-dessous quelques-unes des fonctions les plus courantes.  


<!-- ======================================================= -->
## Preparation  

### Chargement des paquets {.unnumbered}  

Ce morceau de code montre le chargement des packages nécessaires aux analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le package si nécessaire *et* le charge pour l'utiliser. Vous pouvez également charger les packages installés avec `library()` de **base** R. Voir la page sur [R basics](#rbasics) pour plus d'informations sur les packages R.  

```{r}
pacman::p_load(
  rio,           # importer/exporter
  here,          # chemin des fichiers
  lubridate,     # travailler avec les dates
  forcats,       # facteurs
  aweek,         # creer epiweeks avec les niveaux des facteurs
  janitor,       # tableau
  tidyverse      # données management et visualisation
  )
```



### Importer données{.unnumbered}  

Nous importons le jeu de données des cas d'une épidémie d'Ebola simulée. Si vous voulez suivre, , <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquer pour telecharger  le jeu de données "nettoyé"  linelist</a> (as .rds file). Importez vos données avec la fonction `import()` du package **rio** (elle accepte de nombreux types de fichiers comme .xlsx, .rds, .csv - voir la page [Importation et exportation](#import_export) pour plus de détails).  

```{r, echo=F}
# importer linelist dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```


```{r, eval=F}
# importer ton jeu de donnée
linelist <- import("linelist_cleaned.rds")
```


### Nouveau variable qualitative {#fct_newcat .unnumbered}  

Pour la démonstration dans cette page, nous utiliserons un scénario commun - la création d'une nouvelle variable catégorielle.

Notez que si vous convertissez une colonne numérique en facteur de classe, vous ne serez pas en mesure de calculer des statistiques numériques sur celle-ci.  

#### Creation de  colonnes {.unnumbered}  

Nous utilisons la colonne existante `days_onset_hosp` (jours entre l'apparition des symptômes et l'admission à l'hôpital) et créons une nouvelle colonne `delay_cat` en classant chaque ligne dans l'une de plusieurs catégories. Nous faisons cela avec la fonction **dplyr** `case_when()`, qui applique séquentiellement des critères logiques (côté droit) à chaque ligne et renvoie la valeur correspondante côté gauche pour la nouvelle colonne `delay_cat`. Vous trouverez plus d'informations sur la fonction `case_when()` dans [Nettoyage des données et des fonctions de base](#cleaning_data).


```{r}
linelist <- linelist %>% 
  mutate(delay_cat = case_when(
    # critere                                 # nouveau valeur si vrai
    days_onset_hosp < 2                        ~ "<2 days",
    days_onset_hosp >= 2 & days_onset_hosp < 5 ~ "2-5 days",
    days_onset_hosp >= 5                       ~ ">5 days",
    is.na(days_onset_hosp)                     ~ NA_character_,
    TRUE                                       ~ "Check me"))  
```


#### Ordre des valeurs par defaut {.unnumbered}  

Telle que créée avec `case_when()`, la nouvelle colonne `delay_cat` est une colonne catégorielle de la classe Character - *pas* encore un facteur. Ainsi, dans un tableau de fréquence, nous voyons que les valeurs uniques apparaissent dans un ordre alphanumérique par défaut - un ordre qui n'a pas beaucoup de sens intuitif :  

```{r}
table(linelist$delay_cat, useNA = "always")
```

De même, si nous réalisons un diagramme à barres, les valeurs apparaissent également dans cet ordre sur l'axe des x (voir la page sur [les bases de ggplot](#ggplot_basics) pour en savoir plus sur **ggplot2** - le package de visualisation le plus courant dans R).  

```{r, warning=F, message=F}
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = delay_cat))
```



## Convertir en facteur

Pour convertir une colonne de caractères ou de chiffres en facteur de classe, vous pouvez utiliser n'importe quelle fonction du package **forcats** (plusieurs sont détaillées [ci-dessous](#fct_adjust)). Elles convertiront en facteur de classe et effectueront ou permettront un certain ordre des niveaux - par exemple, l'utilisation de `fct_relevel()` vous permet de spécifier manuellement l'ordre des niveaux. La fonction `as_factor()` convertit simplement la classe sans autres capacités.  

La fonction R **base** `factor()` convertit une colonne en facteur et vous permet de spécifier manuellement l'ordre des niveaux, comme un vecteur de caractères à son argument `levels = `.  

Ci-dessous, nous utilisons `mutate()` et `fct_relevel()` pour convertir la colonne `delay_cat` de la classe caractère à la classe facteur. La colonne `delay_cat` est créée dans la section [Préparation](#fct_newcat) ci-dessus. 



```{r}
linelist <- linelist %>%
  mutate(delay_cat = fct_relevel(delay_cat))
```

*Les "valeurs" uniques de cette colonne sont maintenant considérées comme des "niveaux" du facteur.* 
Les niveaux ont un *ordre*, qui peut être affiché avec la fonction **base** R `levels()`, ou alternativement visualisé dans un tableau de comptage via `table()` de **base** R ou `tabyl()` de **janitor**. Par défaut, l'ordre des niveaux sera alpha-numérique, comme auparavant. Notez que `NA` n'est pas un niveau de facteur.  

```{r}
levels(linelist$delay_cat)
```

La fonction `fct_relevel()` a l'utilité supplémentaire de vous permettre de spécifier manuellement l'ordre des niveaux. Il suffit d'écrire les valeurs des niveaux dans l'ordre, entre guillemets, séparés par des virgules, comme indiqué ci-dessous. Notez que l'orthographe doit correspondre exactement aux valeurs. Si vous voulez créer des niveaux qui n'existent pas dans les données, utilisez plutôt [`fct_expand()`](#fct_add)).  

```{r}
linelist <- linelist %>%
  mutate(delay_cat = fct_relevel(delay_cat, "<2 days", "2-5 days", ">5 days"))
```


Nous pouvons maintenant voir que les niveaux sont ordonnés, comme spécifié dans la commande précédente, dans un ordre raisonnable.  

```{r}
levels(linelist$delay_cat)
```


Maintenant, l'ordre de l'intrigue a aussi un sens plus intuitif.  

```{r, warning=F, message=F}
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = delay_cat))
```


## Ajouter ou enlever des niveaux 

### Ajouter {#fct_add .unnumbered}

Si vous devez ajouter des niveaux à un facteur, vous pouvez le faire avec `fct_expand()`. Il suffit d'écrire le nom de la colonne suivi des nouveaux niveaux (séparés par des virgules). En tabulant les valeurs, nous pouvons voir les nouveaux niveaux et le nombre de zéros. Vous pouvez utiliser `table()` de **base** R, ou `tabyl()` de **janitor** :  

```{r}
linelist %>% 
  mutate(delay_cat = fct_expand(delay_cat, "Not admitted to hospital", "Transfer to other jurisdiction")) %>% 
  tabyl(delay_cat)   # afficher le tableau
```



Note : il existe une fonction spéciale **forcats** pour ajouter facilement les valeurs manquantes (`NA`) comme niveau. Voir la section sur les [Valeurs manquantes](#fct_missing) ci-dessous.  


### Enlever {.unnumbered}

Si vous utilisez `fct_drop()`, les niveaux "inutilisés" avec des comptes nuls seront supprimés de l'ensemble des niveaux. Les niveaux que nous avons ajoutés ci-dessus ("Non admis à l'hôpital") existent en tant que niveau mais aucune ligne n'a réellement ces valeurs. Ils seront donc supprimés en appliquant `fct_drop()` à notre colonne de facteurs :  

```{r}
linelist %>% 
  mutate(delay_cat = fct_drop(delay_cat)) %>% 
  tabyl(delay_cat)
```




## Ajuster l'ordre des niveaux {#fct_adjust} 

Le package **forcats** offre des fonctions utiles pour ajuster facilement l'ordre des niveaux d'un facteur (après qu'une colonne ait été définie comme facteur de classe) : 

Ces fonctions peuvent être appliquées à une colonne de facteurs dans deux contextes :  

1) A la colonne dans le dataframe, comme d'habitude, afin que la transformation soit disponible pour toute utilisation ultérieure des données.  
2) *À l'intérieur d'un graphique*, de sorte que la modification soit appliquée uniquement à l'intérieur du graphique.  



### Manuellement {.unnumbered} 

Cette fonction est utilisée pour ordonner manuellement les niveaux des facteurs. Si elle est utilisée sur une colonne sans facteur, la colonne sera d'abord convertie en facteur de classe.  

Entre les parenthèses, fournissez d'abord le nom de la colonne de facteur, puis fournissez soit :  

* Tous les niveaux dans l'ordre désiré (comme un vecteur de caractères `c()`), ou bien  
* Un niveau et son placement corrigé en utilisant l'argument `after = `.  

Voici un exemple de redéfinition de la colonne `delay_cat` (qui est déjà de la classe Factor) et de spécification de tous les niveaux dans l'ordre souhaité.  

```{r}
# re-define level order
linelist <- linelist %>% 
  mutate(delay_cat = fct_relevel(delay_cat, c("<2 days", "2-5 days", ">5 days")))
```

Si vous voulez seulement déplacer un niveau, vous pouvez le spécifier à `fct_relevel()` seul et donner un nombre à l'argument `after = ` pour indiquer où dans l'ordre il doit être. Par exemple, la commande ci-dessous déplace "<2 jours" en deuxième position : 

```{r, eval=F}
# re-define level order
linelist %>% 
  mutate(delay_cat = fct_relevel(delay_cat, "<2 days", after = 1)) %>% 
  tabyl(delay_cat)
```




### Dans un graphe {.unnumbered}  

Les commandes **forcats** peuvent être utilisées pour définir l'ordre des niveaux dans le dataframe, ou seulement dans un graphique. En utilisant la commande pour "envelopper" le nom de la colonne *dans* la commande de traçage `ggplot()`, vous pouvez inverser/niveler/etc. la transformation ne s'appliquera que dans ce tracé.  

Ci-dessous, deux tracés sont créés avec `ggplot()` (voir la page [les bases de ggplot](#ggplot_basics)). Dans le premier, la colonne `delay_cat` est mise en correspondance avec l'axe des x du graphique, avec son ordre de niveau par défaut comme dans les données `linelist`. Dans le second exemple, elle est enveloppée dans `fct_relevel()` et l'ordre est modifié dans le graphe.  

```{r, echo =F}
linelist <- linelist %>% 
  mutate(delay_cat = fct_relevel(delay_cat, c("2-5 days", "<2 days", ">5 days")))

```



```{r, warning=F, message=F, out.width = c('50%', '50%'), fig.show='hold'}
# Ordre alphanumerique par defaut - pas d'ajustement dans ggplot
ggplot(data = linelist)+
    geom_bar(mapping = aes(x = delay_cat))

# Ordonner des niveaux de facteurs dans ggplot
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = fct_relevel(delay_cat, c("<2 days", "2-5 days", ">5 days"))))
```

Notez que le titre par défaut de l'axe des x est maintenant assez compliqué - vous pouvez changer ce titre avec l'argument **ggplot2** `labs()`.  




### Inverser {.unnumbered}  

Il est assez fréquent que vous vouliez inverser l'ordre des niveaux. Enveloppez simplement le facteur avec `fct_rev()`.  

Notez que si vous voulez inverser *seulement* la légende du graphique mais pas les niveaux réels du facteur, vous pouvez le faire avec `guides()` (voir la page [Astuces avec ggplot](#ggplot_tips))).  




### Par fréquence {.unnumbered}  

Pour ordonner par fréquence que la valeur apparaît dans les données, utilisez `fct_infreq()`. Toute valeur manquante (`NA`) sera automatiquement incluse à la fin, à moins qu'elle ne soit convertie en un niveau explicite (voir [cette section](#fct_missing)). Vous pouvez inverser l'ordre en enveloppant davantage avec `fct_rev()`.  

Cette fonction peut être utilisée dans un `ggplot()`, comme indiqué ci-dessous.  

```{r, out.width = c('50%', '50%', '50%'), fig.show='hold', warning=F, message=F}
# ordered by frequency
ggplot(data = linelist, aes(x = fct_infreq(delay_cat)))+
  geom_bar()+
  labs(x = "Delay onset to admission (days)",
       title = "Ordered by frequency")

# fréquence inversée
ggplot(data = linelist, aes(x = fct_rev(fct_infreq(delay_cat))))+
  geom_bar()+
  labs(x = "Delay onset to admission (days)",
       title = "Reverse of order by frequency")
```


### Par apparence {.unnumbered}  

Utilisez `fct_inorder()` pour définir l'ordre des niveaux afin de correspondre à l'ordre d'apparition dans les données, en commençant par la première ligne. Cela peut être utile si vous avez d'abord soigneusement `arrange()` les données dans le cadre de données, et ensuite l'utiliser pour définir l'ordre des facteurs.  




### Par la statistique sommaire d'une autre colonne {.unnumbered}  

Vous pouvez utiliser `fct_reorder()` pour ordonner les niveaux d'une colonne *par une statistique sommaire d'une autre colonne*. Visuellement, cela peut donner des graphiques agréables où les barres/points montent ou descendent régulièrement sur le graphique.  

Dans les exemples ci-dessous, l'axe des x est `delay_cat`, et l'axe des y est la colonne numérique `ct_blood` (valeur du seuil de cycle). Les box plots montrent la distribution des valeurs CT par groupe `delay_cat`. Nous voulons ordonner les box plots dans l'ordre croissant de la valeur médiane du groupe. 

Dans le premier exemple ci-dessous, l'ordre par défaut alpha-numérique est utilisé. Vous pouvez voir que les hauteurs des box plots sont mélangées et ne sont pas dans un ordre particulier. Dans le deuxième exemple, la colonne `delay_cat` (mappée sur l'axe des x) a été enveloppée dans `fct_reorder()`, la colonne `ct_blood` est donnée comme deuxième argument, et "median" est donné comme troisième argument (vous pourriez aussi utiliser "max", "mean", "min", etc). Ainsi, l'ordre des niveaux de `delay_cat` reflètera maintenant les valeurs médianes croissantes de la valeur médiane de CT de chaque groupe `delay_cat`. Ceci est reflété dans le deuxième graphique - les box plots ont été réarrangés pour être ascendants. Notez comment `NA` (manquant) apparaîtra à la fin, à moins d'être converti en un niveau explicite.  
```{r, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# boxplots ordonnés par les niveaux des facteurs initiaux
ggplot(data = linelist)+
  geom_boxplot(
    aes(x = delay_cat,
        y = ct_blood, 
        fill = delay_cat))+
  labs(x = "Délai d'apparition à l'admission (jours)",
       title = "Classé par niveaux alphanumériques originaux")+
  theme_classic()+
  theme(legend.position = "none")


# boxplots ordonner par la mediane des valeurs de CT 
ggplot(data = linelist)+
  geom_boxplot(
    aes(x = fct_reorder(delay_cat, ct_blood, "median"),
        y = ct_blood,
        fill = delay_cat))+
  labs(x = "Délai d'apparition à l'admission (jours)",
       title = "Classé par valeur médiane de CT dans par groupe" )+
  theme_classic()+
  theme(legend.position = "none")
```

Notez que dans l'exemple ci-dessus, aucune étape n'est nécessaire avant l'appel à `ggplot()` - le regroupement et les calculs sont tous effectués en interne par la commande ggplot.  


### Par la valeur "end" {.unnumbered}  

Utilisez `fct_reorder2()` pour des tracés de lignes groupées. Il ordonne les niveaux (et donc le *légende*) pour s'aligner avec l'ordre vertical des lignes à la "fin" du tracé. Techniquement parlant, il "ordonne par les valeurs y associées aux plus grandes valeurs x".  

Par exemple, si vous avez des lignes montrant le nombre de cas par hôpital au fil du temps, vous pouvez appliquer `fct_reorder2()` à l'argument `color = ` dans `aes()`, de sorte que l'ordre vertical des hôpitaux apparaissant dans la légende s'aligne sur l'ordre des lignes à l'extrémité du tracé. Pour en savoir plus, consultez la [documentation en ligne](https://forcats.tidyverse.org/reference/fct_reorder.html).  

```{r, warning=F, message=F}
epidemic_data <- linelist %>%         # commencer avec linelist  
    filter(date_onset < as.Date("2014-09-21")) %>%    # point de coupure  la date, pour une meilleur visualisation
    count(                                            # obtenir le nombre de cas par semaine et par hôpital
      epiweek = lubridate::floor_date(date_onset, "week"),  
      hospital                                            
    ) 
  
ggplot(data = epidemic_data)+                       # debut pour representaton graphique
  geom_line(                                        # faire des lignes
    aes(
      x = epiweek,                                  # l'axe x est epiweek
      y = n,                                        # l'axe y est le nombre de cas par semaine
      color = fct_reorder2(hospital, epiweek, n)))+ # données regroupées et colorées par hôpital, avec un ordre des facteurs par hauteur à la fin du graphique
  labs(title = "Niveaux des facteurs (et affichage de la légende) par hauteur de ligne à la fin du grahpique",
       color = "Hôpital")                          # changer le titre de la legende
```




## Valeurs manquantes {#fct_missing}  

Si vous avez des valeurs `NA` dans votre colonne de facteurs, vous pouvez facilement les convertir en un niveau nommé tel que "Missing" avec `fct_explicit_na()`. Les valeurs `NA` sont converties en "(Missing)" à la fin de l'ordre des niveaux par défaut. Vous pouvez ajuster le nom du niveau avec l'argument `na_level = `.  

Ci-dessous, cette opération est effectuée sur la colonne `delay_cat` et un tableau est imprimé avec `tabyl()` avec `NA` converti en "Missing delay".  

```{r}
linelist %>% 
  mutate(delay_cat = fct_explicit_na(delay_cat, na_level = "Missing delay")) %>% 
  tabyl(delay_cat)
```





### Combiner les niveaux  


### Manuellement {.unnumbered}  

Vous pouvez ajuster l'affichage des niveaux manuellement avec `fct_recode()`. C'est comme la fonction **dplyr** `recode()` (voir [Nettoyage des données et fonctions de base](#cleaning_data)), mais elle permet la création de nouveaux niveaux de facteurs. Si vous utilisez le simple `recode()` sur un facteur, les nouvelles valeurs recodées seront rejetées à moins qu'elles n'aient déjà été définies comme des niveaux admissibles. 

Cet outil peut aussi être utilisé pour "combiner" des niveaux, en assignant à plusieurs niveaux la même valeur re-codée. Veillez simplement à ne pas perdre d'informations ! Pensez à effectuer ces étapes de combinaison dans une nouvelle colonne (sans écraser la colonne existante).  

`fct_recode()` a une syntaxe différente de celle de `recode()`. `recode()` utilise `OLD = NEW`, alors que `fct_recode()` utilise `NEW = OLD`.     

Les niveaux actuels de `delay_cat` sont :  
```{r, echo=F}
linelist <- linelist %>% 
  mutate(delay_cat = fct_relevel(delay_cat, "<2 days", after = 0))
```


```{r}
levels(linelist$delay_cat)
```

Les nouveaux niveaux sont créés à l'aide de la syntaxe `fct_recode(colonne, "nouveau" = "ancien", "nouveau" = "ancien", "nouveau" = "ancien")` et imprimés :  

```{r}
linelist %>% 
  mutate(delay_cat = fct_recode(
    delay_cat,
    "Less than 2 days" = "<2 days",
    "2 to 5 days"      = "2-5 days",
    "More than 5 days" = ">5 days")) %>% 
  tabyl(delay_cat)
```

Ici, ils sont combinés manuellement avec `fct_recode()`. Notez qu'aucune erreur n'est soulevée lors de la création d'un nouveau niveau "Moins de 5 jours".  


```{r, warning=F, message=F}
linelist %>% 
  mutate(delay_cat = fct_recode(
    delay_cat,
    "Less than 5 days" = "<2 days",
    "Less than 5 days" = "2-5 days",
    "More than 5 days" = ">5 days")) %>% 
  tabyl(delay_cat)
```





### Réduire en "Autre" {.unnumbered}  

Vous pouvez utiliser `fct_other()` pour assigner manuellement des niveaux de facteurs à un niveau "Autre". Ci-dessous, tous les niveaux de la colonne `hospital`, à part "Port Hospital" et "Central Hospital", sont combinés dans "Other". Vous pouvez fournir un vecteur pour soit `maintenir = `, soit `drop = `. Vous pouvez modifier l'affichage du niveau "Autre" avec `other_level = `.  

```{r}
linelist %>%    
  mutate(hospital = fct_other(                      # ajuster niveaux
    hospital,
    keep = c("Port Hospital", "Central Hospital"),  # garder  ceux ci separer
    other_level = "Other Hospital")) %>%            # Considerer tout autre niveau comme  "Other Hospital"
  tabyl(hospital)                                   # afficher tableau

```




### Réduire par fréquence {.unnumbered}

Vous pouvez combiner automatiquement les niveaux de facteurs les moins fréquents en utilisant `fct_lump()`.  

Pour "regrouper" plusieurs niveaux à basse fréquence dans un groupe "Autre", faites l'une des choses suivantes :  

* Définissez `n = ` comme le nombre de groupes que vous voulez garder. Les n niveaux les plus fréquents seront conservés, et tous les autres seront regroupés dans "Autres".  
* Définissez `prop = ` comme étant la proportion de fréquence seuil pour les niveaux au-dessus desquels vous voulez garder. Toutes les autres valeurs seront regroupées dans "Autres".  

Vous pouvez modifier l'affichage du niveau "Autre" avec `other_level = `. Ci-dessous, tous les hôpitaux sauf les deux les plus fréquents sont combinés dans "Autre hôpital".  

```{r, warning=F, message=F}
linelist %>%    
  mutate(hospital = fct_lump(                      # ajuster niveaux
    hospital,
    n = 2,                                          # garder les  2 premiers  niveaux
    other_level = "Other Hospital")) %>%            #  Considerer tout autre niveau comme  "Other Hospital"
  tabyl(hospital)                                   # afficher tableau

```




, avertir
## Afficher tous les niveaux  

L'un des avantages de l'utilisation des facteurs est de standardiser l'apparence des légendes et des tableaux des graphiques, quelles que soient les valeurs réellement présentes dans un ensemble de données. 

Si vous préparez de nombreuses figures (par exemple, pour plusieurs juridictions), vous voudrez que les légendes et les tableaux apparaissent de manière identique, même si les niveaux de complétion ou de composition des données varient.  

### Dans les graphiques {.unnumbered}  

Dans une figure `ggplot()`, ajoutez simplement l'argument `drop = FALSE` dans la fonction `scale_xxxx()` concernée. Tous les niveaux de facteurs seront affichés, qu'ils soient présents ou non dans les données. Si les niveaux de vos colonnes de facteurs sont affichés en utilisant `fill = `, alors dans scale_fill_discrete() vous incluez `drop = FALSE`, comme indiqué ci-dessous. Si vos niveaux sont affichés avec `x = ` (sur l'axe des x) `color = ` ou `size = `, vous devez le fournir à `scale_color_discrete()` ou `scale_size_discrete()` en conséquence.  

Cet exemple est un diagramme à barres empilées de la catégorie d'âge, par hôpital. L'ajout de `scale_fill_discrete(drop = FALSE)` garantit que tous les groupes d'âge apparaissent dans la légende, même s'ils ne sont pas présents dans les données. 

```{r}
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = hospital, fill = age_cat)) +
  scale_fill_discrete(drop = FALSE)+                        # montrer tous les groupes d'âge dans la légende, même ceux qui ne sont pas présents
  labs(
    title = "Tous les groupes d'âge apparaissent dans la légende, même s'ils ne sont pas présents dans les données")
```

### Dans les tableaux {.unnumbered}  

Tant le **base** R `table()` que `tabyl()` de **janitor** montreront tous les niveaux de facteurs (même les niveaux non utilisés).  

Si vous utilisez `count()` ou `summarise()` de **dplyr** pour faire une table, ajoutez l'argument `.drop = FALSE` pour inclure les comptes pour tous les niveaux de facteurs, même ceux qui ne sont pas utilisés.  

Pour en savoir plus, consultez la page [Descriptive tables](#descriptive_tables), ou la [documentation scale_discrete](https://ggplot2.tidyverse.org/reference/scale_discrete.html), ou la [documentation count()](https://dplyr.tidyverse.org/reference/count.html). Vous pouvez voir un autre exemple à la page [Suivi des contacts](#contact_tracing).  


## Epiweeks  

Veuillez consulter la discussion approfondie sur la création de semaines épidémiologiques à la page [Regroupement des données](#grouping_data).  
Veuillez également consulter la page [Travailler avec des dates](#working_dates) pour obtenir des conseils sur la façon de créer et de formater des semaines épidémiologiques.  


### Semaines épidémiologiques dans un graphique {.unnumbered}  

Si votre objectif est de créer des semaines épidémiologiques à afficher dans un graphique, vous pouvez le faire simplement avec la fonction `floor_date()` de **lubridate**, comme expliqué dans la page [Regroupement de données](#grouping_data). Les valeurs retournées seront de la classe Date avec le format AAAA-MM-JJ. Si vous utilisez cette colonne dans un graphique, les dates seront naturellement ordonnées correctement, et vous n'aurez pas à vous soucier des niveaux ou de la conversion en classe Facteur. Voir l'histogramme `ggplot()` des dates d'apparition ci-dessous.  

Dans cette approche, vous pouvez ajuster l'affichage des dates sur un axe avec `scale_x_date()`. Voir la page sur les [Courbes épidémiques](#epicurves) pour plus d'informations. Vous pouvez spécifier un format d'affichage "strptime" à l'argument `date_labels = ` de `scale_x_date()`. Ces formats utilisent des caractères de remplacement "%" et sont traités dans la page [Manipuler les dates](#working_dates). Utilisez "%Y" pour représenter une année à 4 chiffres, et "%W" ou "%U" pour représenter le numéro de la semaine (semaines du lundi ou du dimanche respectivement).  

```{r, warning=F, message=F}
linelist %>% 
  mutate(epiweek_date = floor_date(date_onset, "week")) %>%  # créer une colonne semaine
  ggplot()+                                                  # commencer ggplot
  geom_histogram(mapping = aes(x = epiweek_date))+           # histogramme de la date d'apparition
  scale_x_date(date_labels = "%Y-W%W")                       # ajuster la répartition des dates pour qu'elle soit YYYY-WWw
```


### Epiweeks dans les données {.unnumbered}  

Cependant, si le but de la factorisation n'est *pas* de tracer, vous pouvez l'aborder de deux façons :  

1) 1) *Pour un contrôle précis de l'affichage*, convertissez la colonne **lubridée** des épihebdomadaires (AAAA-MM-JJ) au format d'affichage souhaité (AAAA-WWW) *dans le cadre de données lui-même*, puis convertissez-la en classe Factor.  

Tout d'abord, utilisez `format()` de **base** R pour convertir l'affichage de la date de YYYY-MM-DD en YYYY-Www (voir la page [Manipuler les dates](#working_dates)). Dans ce processus, la classe sera convertie en caractère. Ensuite, convertissez le caractère en classe Factor avec `factor()`.  


```{r}
linelist <- linelist %>% 
  mutate(epiweek_date = floor_date(date_onset, "week"),       # creer epiweeks (YYYY-MM-DD)
         epiweek_formatted = format(epiweek_date, "%Y-W%W"),  # Convertir pour afficher (YYYY-WWw)
         epiweek_formatted = factor(epiweek_formatted))       # Convertir un facteur

# Afficher les niveaux
levels(linelist$epiweek_formatted)
```

<span style="color: red;">**_DANGERS:_** Si vous placez les semaines avant les années ("Www-YYYY") ("%W-%Y"), l'ordre par défaut des niveaux alphanumériques sera incorrect (par exemple, 01-2015 sera avant 35-2014). Vous pourriez avoir besoin d'ajuster manuellement l'ordre, ce qui serait un processus long et difficile..</span>  

2) *Pour un affichage rapide par défaut*, utilisez le package **aweek** et sa fonction `date2week()`. Vous pouvez définir le jour `week_start = `, et si vous définissez `factor = TRUE` alors la colonne de sortie est un facteur ordonné. En prime, le facteur inclut des niveaux pour *toutes* les semaines possibles dans l'intervalle - même s'il n'y a pas de cas cette semaine-là.  

```{r, eval=F}
df <- linelist %>% 
  mutate(epiweek = date2week(date_onset, week_start = "Monday", factor = TRUE))

levels(df$epiweek)
```

Voir la page [Manipuler les dates](#working_dates) pour plus d'informations sur **aweek**. Il propose également la fonction inverse `week2date()`.  



<!-- ======================================================= -->
## Ressources {} 

R for Data Science page on [factors](https://r4ds.had.co.nz/factors.html)  
[aweek package vignette](https://cran.r-project.org/web/packages/aweek/vignettes/introduction.html)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/factors.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
# Restructurer les données {#pivoting_data}

```{r, warning=F, message=F, out.height = c('50%'), fig.align="center", fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "pivoting", "Pivoting_500x500.png"))

#knitr::include_graphics(here::here("images", "pivoting", "pivot_longer_new.png"))
#knitr::include_graphics(here::here("images", "pivoting", "pivot_bar.png"))
#knitr::include_graphics(here::here("images", "pivoting", "pivot_wider_new.png"))
```

Dans le contexte de la gestion des données, le *pivot des données* fait référence à l'un des deux processus suivants :  

1. La création de *tableaux croisés*, qui sont des tableaux de statistiques résumant les données d'un tableau plus étendu.  

2. La restructuration d'un tableau du format **long** au format **large**, ou vice versa. 

Dans ce chapitre, nous allons nous focaliser sur le second processus. Résumer ses données dans des tableau est une étape cruciale de l'analyse des données et est traitée dans les chapitres sur [le regroupement des données](#grouping_data) et les [tableaux descriptifs](#descriptive_tables). 

Ce chapitre traite des formats de données. Il est utile de connaître l'idée de "données bien rangées / ordonnées", dans laquelle chaque variable a sa propre colonne, chaque observation a sa propre ligne et chaque valeur a sa propre cellule. Vous trouverez plus d'informations sur ce sujet [dans ce chapitre en ligne de R for Data Science](https://r4ds.had.co.nz/tidy-data.html)(en Anglais). 


## Étapes préliminaires {#pivot_prep_data}

### Importation des paquets {.unnumbered}  

Ces lignes de code chargent les paquets nécessaires aux analyses. Dans ce guide, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *puis* l'importe pour l'utiliser. Vous pouvez également charger les paquets installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les paquets R.  


```{r}
pacman::p_load(
  rio,          # import des fichiers
  here,         # gestion des chemins d'accès
  kableExtra,
  tidyverse)    # gestion des données + graphiques (ggplot2)
```



### Importation des données {.unnumbered}


### Cas de Malaria {-}  

Dans ce chapitre, nous utiliserons un jeu de données fictif de cas quotidiens de paludisme, par établissement et par groupe d'âge. Pour reproduire les étapes, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/malaria_facility_count_data.rds' class='download-button'>cliquez ici pour télécharger les données (en tant que fichier .rds)<span></a>. Ou importez des données avec la fonction `import()` du paquet **rio** (elle gère de nombreux types de fichiers comme .xlsx, .csv, .rds - voir la page [Importation et exportation des données](import_export) pour plus de détails).  


```{r, echo=F}
count_data <- rio::import(
  here::here("data", "malaria_facility_count_data.rds")) %>% 
  as_tibble()
```

```{r, eval=F}
# Importation des données
count_data <- import("malaria_facility_count_data.rds")
```

Les premières cinquantes lignes sont affichées ci-dessous.  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(count_data, 50), 
              rownames = FALSE, 
              options = list(pageLength = 5, scrollX = T), 
              class = 'white-space: nowrap' )
```


### Linelist des cas {-}  

A la fin de ce chapitre, nous utiliserons également une liste des cas d'une épidémie d'Ebola simulée. Pour reproduire les étapes, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la linelist "propre"</a> (en tant que fichier .rds). Importez vos données avec la fonction `import()` du paquet **rio** (elle accepte de nombreux types de fichiers comme .xlsx, .rds, .csv - voir la page [Importation et exportation des données](import_export) pour plus de détails).  


```{r, echo=F}
# importer la linelist dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```


```{r, eval=F}
# Importer la linelist
linelist <- import("linelist_cleaned.xlsx")
```



<!-- ======================================================= -->
## Transformation du format large vers long {}

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "pivoting", "pivot_longer_new.png"))
```


<!-- ======================================================= -->
### Le format "large" {.unnumbered}

Les données sont souvent saisies et stockées dans un format "large" (ou "étendu"), où les caractéristiques ou les réponses d'un sujet/d'un item sont entrées dans une même ligne. Cette structure de données est utile pour la saisie et la présentation des données, mais elle n'est pas appropriée pour de nombreuses analyses.  

Par exemple, dans le jeu de données `count_data` importé auparavant, chaque ligne représente un établissement à une date donnée. Les nombres de cas 
sont contenus dans les colonnes les plus à droites, avec une colonne par classe d'age, et une colonne pour le nombre total de cas ce jour là dans cet établissement. L'information "nombre de cas" est donc contenues sur plusieurs colonnes, au lieu d'une seule, d'où la structure dite "large".


```{r, echo=F}
DT::datatable(count_data, 
              rownames = FALSE, 
              options = list(pageLength = 10, 
                             scrollX = T) )
```

Plus précisément, chaque ligne dans ce tableau contient le nombre de cas de paludisme dans l'un des 65 établissements à une date donnée, dans la période allant de ` count_data$data_date %>% min()` à ` count_data$data_date %>% max()`. Ces établissements sont situés dans une `province` (Nord) et quatre `districts` (Spring, Bolo, Dingo, et Barnard). Le dataframe contient les totaux des cas de paludisme, globaux et pour chaque classe d'age (<4 ans, 5-14 ans, et 15 ans et plus).

Les données sous format "large" comme celle-ci ne respectent pas les normes de "données rangées", car les en-têtes de colonne ne représentent pas réellement des "variables": ils contiennent les *valeurs* d'une hypothétique variable "groupe d'âge".

Ce format est utile pour présenter les informations dans un tableau, ou pour saisir des données (dans Excel par exemple) à partir de formulaires de notification des cas. Cependant, au stade de l'analyse, ces données doivent généralement être restructurées et rangées en un format plus long. Le paquet de visualisations **ggplot2**, fonctionne également mieux lorsque les données sont dans un format "long".  


La visualisation du nombre *total* de cas de paludisme dans le temps ne pose aucun problème avec les données dans leur format actuel :  

```{r, warning=F, message=F}
ggplot(count_data) +
  geom_col(aes(x = data_date, y = malaria_tot), width = 1)
```

Cependant, les choses se compliquent si l'on veut visualiser les contributions relatives de chaque groupe d'âge au total des cas ? Nous devons alors nous assurer que la variable d'intérêt (le groupe d'âge) ait sa propre colonne dans le dataframe, colonne qui peut être passée à l'argument "mapping aesthetics" `aes()` de `{ggplot2}`.


<!-- ======================================================= -->
### `pivot_longer()` {.unnumbered}

La fonction `pivot_longer()` de **tidyr** transforme un jeu de données au format "large" en un jeu de données "plus long". **tidyr** fait partie du méta-paquet **tidyverse**.   

Elle accepte une ou plusieurs colonnes à transformer (argument `cols = `), ce qui donne un contrôle fin sur les colonnes à restructurer. Par exemple, pour les données sur le paludisme, nous ne voulons faire pivoter que les colonnes contenant des nombre de cas.  

Suite à ce processus, nous obtenons deux "nouvelles" colonnes: l'une contenant les catégories (anciennement sotckées dans les noms de colonnes), et l'autre avec les valeurs correspondantes (ici, le nombre de cas). Nous pouvons accepter les noms par défaut pour ces nouvelles colonnes, ou spécifier de nouveaux noms dans `names_to = ` et `values_to = ` respectivement.  

Voyons comment utiliser `pivot_longer()`... 



### Transformation simple {.unnumbered}  

Nous utilisons la fonction `pivot_longer()` de **tidyr** pour convertir les données d'un format "large" à un format "long". Plus précisément, il s'agit de convertir les quatre colonnes numériques contenant des nombre de cas de paludisme en deux nouvelles colonnes : une qui contient les *groupes d'âge* et une qui contient les *valeurs* correspondantes. 

```{r, eval=F}
df_long <- count_data %>% 
  pivot_longer(
    cols = c(`malaria_rdt_0-4`, `malaria_rdt_5-14`, 
             `malaria_rdt_15`, `malaria_tot`)
  )

df_long
```

Notons que le dataframe nouvellement crée (`df_long`) a plus de lignes (12 152 contre 3 038) : il est devenu *plus long*. Pour être précis, il est quatre fois plus long, car chaque ligne du tableau d'origine a donné quatre lignes dans `df_long``, une pour chacun des colonnes restructurées (<4 ans, 5-14 ans, 15 ans et plus, et total).  

Le nouveau tableau a également moins de colonnes (8 contre 10), car les données précédemment stockées dans quatre colonnes (celles qui commencent par le préfixe `malaria_`) sont maintenant stockées dans deux colonnes.  


*Note :* puisque les noms de quatre colonnes transformées commencent tous par le préfixe `malaria_`, nous aurions pu sélectionner les colonnes à transformer en utilisant la fonction `starts_with()` pour obtenir le même résultat (voir la page sur [le nettoyage des données et les fonctions de base](#cleaning_data) pour plus de ces fonctions d'aide de type "tidyselect").   


```{r}
# choisir les colonnes avec l'aide d'une fonction "tidyselect"
count_data %>% 
  pivot_longer(
    cols = starts_with("malaria_")
  )
```

ou par position :   

```{r, eval=F}
# Choisir les colonnes à partir de leur position dans le tableau
count_data %>% 
  pivot_longer(
    cols = 6:9
  )
```

ou dans le cas de colonnes consécutives, avec la première et la dernière colonne :  

```{r, eval=F}
# Choisir les colonnes avec un "intervalle"
count_data %>% 
  pivot_longer(
    cols = `malaria_rdt_0-4`:malaria_tot
  )
```

Les deux nouvelles colonnes crées lors de la restructuration reçoivent les noms par défaut de `name` et `value`, mais nous pouvons remplacer ces valeurs par défaut par des noms qui décrivent mieux le contenu des colonnes en utilisant les arguments `names_to` et `values_to`. Par exemple, si nous voulons renommer les colonnes `age_group` et `counts` :  

```{r}
df_long <- count_data %>% 
  pivot_longer(
    cols      = starts_with("malaria_"),
    names_to  = "age_group",
    values_to = "counts"
  )

df_long
```


Nous pouvons maintenant passer ce nouveau jeu de données à `{ggplot2}`, et placer la nouvelle colonne `count` dans l'axe des y et colorer les barres en fonction des valeurs de la colonne `age_group` grâce à l'argument `fill = `. Nous obtenons alors un diagramme en bâtons des cas de paludisme par groupe d'âge :  

```{r, warning=F, message=F}
ggplot(data = df_long) +
  geom_col(
    mapping = aes(x = data_date,
                  y = counts, 
                  fill = age_group),
    width = 1
  )
```

Examinez ce nouveau tracé et comparez-le avec le tracé que nous avons créé précédemment : *qu'est-ce qui cloche ?*  

Nous fait une erreur classique du traitement des données de surveillance et inclus le nombre de cas totaux de la colonne `malaria_tot`. La conséquence est que chaque barre du graphique est deux fois plus élevée qu'elle ne devrait l'être. 

Nous pouvons résoudre ce problème de plusieurs façons. Tout d'abord nous pouvons simplement filtrer les données avant de les passer à `ggplot()` :  


```{r, warning=F, message=F}
df_long %>% 
  filter(age_group != "malaria_tot") %>% 
  ggplot() +
  geom_col(
    aes(x = data_date, 
        y = counts, 
        fill = age_group),
    width = 1
  )
```

Autrement, nous aurions pu exclure cette variable lors du `pivot_longer()`, la conservant comme une variable séparée dans le tableau :   

```{r, warning=F, message=F}
count_data %>% 
  pivot_longer(
    cols = `malaria_rdt_0-4`:malaria_rdt_15,   # does not include the totals column
    names_to = "age_group",
    values_to = "counts"
  )
```

Les valeurs sont alors répétées dans les lignes des groupes d'age.



### Transformer les données de plusieurs classes {.unnumbered}

L'exemple ci-dessus fonctionne bien dans les situations où toutes les colonnes que l'on veut faire *pivoter* sont de la même classe (chaîne de caractère, numérique, logique...). 

Cependant, en tant qu'épidémiologiste de terrain, vous serez amené à travailler avec des données qui ont été préparées par des non-spécialistes, appliquant leur propre logique. Cela aboutit parfois à des jeux de données *non-standard*, voire totalement désorganisés. Comme Hadley Wickham l'a noté (en faisant référence à Tolstoï) dans son [article séminal](https://vita.had.co.nz/papers/tidy-data.pdf) sur les principes des **données rangées, organisées (tidy data)** : 

>Comme les familles, les tableaux de données rangés et organisés se ressemblent tous, mais chaque fichier de données désorganisé / mal rangé l'est à sa manière.    


Un problème particulièrement courant est la nécessité de restructurer des colonnes qui contiennent différentes types de données. Cette transformation aurait pour conséquence de stocker différents types de données dans une seule colonne, ce qui est déconseillé. Il y a plusieurs manière de gérer les problèmes associés à ce type de données mais la restructuration avec `pivot_longer()` est une étape importante. 

Imaginons cette situation : une série d'observations a été effectuée à différents pas de temps pour chacun des trois éléments A, B et C. Il peut s'agir d'individus (par exemple, les contacts d'un cas d'Ebola sont suivis chaque jour pendant 21 jours) ou de postes de santé de villages éloignés qui sont contrôlés une fois par an pour s'assurer qu'ils sont toujours fonctionnels. Reprenons l'exemple de la recherche des contacts. Imaginons que les données soient stockées comme suit :


```{r, message=FALSE, echo=F}

df <- 
  tibble::tribble(
     ~id,   ~obs1_date, ~obs1_status,   ~obs2_date, ~obs2_status,   ~obs3_date, ~obs3_status,
     "A", "2021-04-23",    "Healthy", "2021-04-24",    "Healthy", "2021-04-25",     "Unwell",
     "B", "2021-04-23",    "Healthy", "2021-04-24",    "Healthy", "2021-04-25",    "Healthy",
     "C", "2021-04-23",    "Missing", "2021-04-24",    "Healthy", "2021-04-25",    "Healthy"
     ) 

DT::datatable(df, rownames = FALSE)

```

Le format de ces données est un peu plus compliqué que dans l'exemple précédent :   chaque ligne stocke des informations sur un élément, et des paires de colonnes contiennent des séries d'observations à différentes dates. Le fichier s'allonge avec de nouvelles colonnes à droite au fur et à mesure des observations. Les classes de colonnes alternent entre dates et chaînes de caractères.  

Pour la petite histoire, un des pires exemples de ce type de données qu'il m'ait été donné de rencontrer concernait des données de surveillance du choléra, dans lesquelles 8 nouvelles colonnes d'observations étaient ajoutées *chaque jour*, pendant __4 ans__. L'ouverture du fichier Excel sur mon ordinateur portable a pris plus de dix minutes...  

Pour travailler avec ces données, nous devons les transformer en format "long" tout en gardant la séparation entre une colonne `date` et une colonne `caractère` (statut), pour chaque observation pour chaque élément. Ceci afin d'éviter de nous retrouver avec un mélange de types de variables dans une seule colonne, une situation que vous devrez chercher à éviter à tout prix dans votre gestion de données, en particulier avec des données ordonnées.  

C'est malheureusement ce qui se produit si l'on effectue une transformation simple :  
```{r}
df %>% 
  pivot_longer(
    cols     = -id,
    names_to = c("observation")
  )

```

Ci-dessus, notre restructuration a fusionné *dates* et *caractères* en une seule colonne `valeur`. Face à deux colonnes de classes différentes, la fonction convertit par défaut la colonne entière en chaîne de caractères.  

Pour éviter cette situation, nous pouvons tirer parti de la structure des noms de colonnes dans le tableau original, qui respectent le même format : le numéro de l'observation, un "_" puis soit "statut" soit "date".  


Pour cela, il nous faut :  

* fournir un vecteur de chaîne de caractères un peu spécial à l'argument `names_to = `. Dans ce vecteur, le second élément est `".value"`, ce terme spécial indiquant que les colonnes restructurées seront divisées sur la base d'un mot dans le nom des colonnes.  

* fournir le caractère utilisé comme séparateur à l'argument `names_sep = ` . Dans le cas présent, il s'agit du tiret-bas "_".  

Ainsi, le nommage et la division des nouvelles colonnes sont basés sur le "_" dans les noms des variables existantes.  


```{r}

df_long <- 
  df %>% 
  pivot_longer(
    cols      = -id,
    names_to  = c("observation", ".value"),
    names_sep = "_"
  )

df_long
```

__Derniers détails__ :  

La colonne `date` est actuellement sous la forme d'une *chaîne de caractères* mais nous pouvons facilement la convertir en classe date en utilisant les fonctions `mutate()` et `as_date()` décrites dans la page [travailler avec des dates](#working_dates).  

Nous pouvons aussi améliorer la colonne `observation` en supprimant le préfixe "obs"  et en la convertissant en format `numérique`. Nous pouvons le faire avec `str_remove_all()` du paquet **stringr** (voir la page sur les [chaînes de caractères](#character_strings)). 


```{r}

df_long <- 
  df_long %>% 
  mutate(
    date = date %>% lubridate::as_date(),
    observation = 
      observation %>% 
      str_remove_all("obs") %>% 
      as.numeric()
  )

df_long

```

Nous pouvons maintenant travailler avec les données dans ce format *allongé*. par exemple, en créant une carte de chaleur :  

```{r}
ggplot(data = df_long, 
       mapping = aes(x = date, 
                     y = id, 
                     fill = status)) +
  geom_tile(colour = "black") +
  scale_fill_manual(
    values = 
      c("Healthy" = "lightgreen", 
        "Unwell"  = "red", 
        "Missing" = "orange")
  )

```





<!-- ======================================================= -->
## Transformation du format long en large {}

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "pivoting", "pivot_wider_new.png"))
```


Il peut être utile de transformer un jeu de données d'un format "long" en un format plus large à l'aide de la fonction `pivot_wider()`.

Un cas d'utilisation typique est lorsque qu'il faut transformer les résultats d'une analyse dans un format plus digeste pour le lecteur, tel qu'un [tableau résumé](#presentation_tables). En général, il s'agit de transformer un dataframe dans lequel les informations relatives à un sujet sont réparties sur plusieurs lignes en un format dans lequel ces informations sont stockées sur une seule ligne.


### Données utilisées {.unnumbered}

Pour cette section nous utiliserons la liste des cas (voir la section [Etapes préliminaires](#pivot_prep_data)), qui contient une ligne par cas.  

Voici les 50 premières lignes :  


```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Imaginons que nous voulions voir les nombres d'individus dans les différentes classes d'age, par genre :  

```{r}
df_wide <- 
  linelist %>% 
  count(age_cat, gender)

df_wide
```

Cette commande renvoi un dataframe en format long, qui est très adapté à la création de graphiques avec **ggplot2**, mais moins idéal pour un tableau résumé dans un rapport :  

```{r}
ggplot(df_wide) +
  geom_col(aes(x = age_cat, y = n, fill = gender))
```


Nous pouvons utiliser la fonction `pivot_wider()` pour restructurer les données dans un format plus lisible par un lecteur humain.  


### `pivot_wider()` {.unnumbered}  

L'argument `names_from` spécifie la colonne *depuis laquelle* générer les nouveaux *noms* de colonne, tandis que l'argument `values_from` spécifie la colonne *depuis laquelle* prendre les *valeurs* pour remplir les cellules. L'argument `id_cols = ` est facultatif, mais peut servir à fournir un vecteur de noms de colonnes qui ne doivent pas être pivotées, et qui serviront d'identifiant pour chaque ligne.  


```{r}
table_wide <- 
  df_wide %>% 
  pivot_wider(
    id_cols     = age_cat,
    names_from  = gender,
    values_from = n
  )

table_wide
```

Ce tableau est beaucoup plus facile à lire, et est une base pour créer des tableaux résumés dans des rapports et articles. Nous pouvons améliorer son apparence à l'aide de paquets tels que **flextable** et **knitr** (voir la page [Tableaux pour la présentation](presentation_tables).  


```{r}
table_wide %>% 
  janitor::adorn_totals(c("row", "col")) %>% # adds row and column totals
  knitr::kable() %>% 
  kableExtra::row_spec(row = 10, bold = TRUE) %>% 
  kableExtra::column_spec(column = 5, bold = TRUE) 
```

---


<!-- ======================================================= -->
## Remplissage des colonnes 

Parfois, après un `pivot`, ou plus fréquemment après un `bind`, nous nous retrouvons avec des vides dans certaines cellules que nous aimerions remplir.  

<!-- ======================================================= -->
### Données {.unnumbered}

Par exemple, prenons deux jeux de données qui contiennent tout deux des observations pour le numéro de mesure, le nom de l'établissement et le nombre de cas à ce moment-là. En plus de cela, le deuxième jeu de données a également une variable `Year`. 


```{r}
df1 <- 
  tibble::tribble(
       ~Measurement, ~Facility, ~Cases,
                  1,  "Hosp 1",     66,
                  2,  "Hosp 1",     26,
                  3,  "Hosp 1",      8,
                  1,  "Hosp 2",     71,
                  2,  "Hosp 2",     62,
                  3,  "Hosp 2",     70,
                  1,  "Hosp 3",     47,
                  2,  "Hosp 3",     70,
                  3,  "Hosp 3",     38,
       )

df1 

df2 <- 
  tibble::tribble(
    ~Year, ~Measurement, ~Facility, ~Cases,
     2000,            1,  "Hosp 4",     82,
     2001,            2,  "Hosp 4",     87,
     2002,            3,  "Hosp 4",     46
  )

df2
```


Lorsque nous effectuons une liaison avec `bind_rows()` pour joindre les deux ensembles de données ensemble, la variable `Year` est remplie avec de `NA` pour les lignes où il n'y avait pas d'information préalable (c'est-à-dire le premier ensemble de données) :  


```{r}
df_combined <- 
  bind_rows(df1, df2) %>% 
  arrange(Measurement, Facility)

df_combined

```

<!-- ======================================================= -->
### `fill()` {.unnumbered}

Dans ce cas, `Year` est une variable utile à inclure si nous souhaitons explorer les tendances temporelle. Nous pouvons utiliser `fill()` pour *remplir* les cellules vides, en spécifiant la colonne à remplir et la direction (dans ce cas **up**) :  


```{r}
df_combined %>% 
  fill(Year, .direction = "up")
```

Alternativement, nous pouvons réordonner les données pour remplir vers le bas :  

```{r}
df_combined <- 
  df_combined %>% 
  arrange(Measurement, desc(Facility))

df_combined

df_combined <- 
  df_combined %>% 
  fill(Year, .direction = "down")

df_combined
```

Nous avons à présent un jeu de données facilement visualisable à l'aide de **ggplot2** :  

```{r}
ggplot(df_combined) +
  aes(Year, Cases, fill = Facility) +
  geom_col()
```

Mais ce jeu de données est peu lisible si présenté tel quel dans un rapport. Nous pouvons appliquer `pivot_larger()` pour le transformer en un format plus large :  

```{r}
df_combined %>% 
  pivot_wider(
    id_cols     = c(Measurement, Facility),
    names_from  = "Year",
    values_from = "Cases"
  ) %>% 
  arrange(Facility) %>% 
  janitor::adorn_totals(c("row", "col")) %>% 
  knitr::kable() %>% 
  kableExtra::row_spec(row = 5, bold = TRUE) %>% 
  kableExtra::column_spec(column = 5, bold = TRUE) 
```


Note : dans ce cas, nous avons dû spécifier de n'inclure que les trois variables `Facility`, `Year`, et `Cases` car la variable supplémentaire `Measurement` interférait avec la création de la table :  

```{r}
df_combined %>% 
  pivot_wider(
    names_from = "Year",
    values_from = "Cases"
  ) %>% 
  knitr::kable()
```

## Resources  

Voici un [tutoriel utilel](https://datacarpentry.org/r-socialsci/03-dplyr-tidyr/index.html)

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/pivoting.Rmd-->


# Travailler sur des données groupées {#grouping_data}  


```{r, out.width=c('100%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "Grouping_1500x500.png"))
```

Ce chapitre explique comment grouper et agréger des données lors une analyse descriptive. Il utilise les fonctions du méta-paquet  **tidyverse** pour des fonctions communes et faciles à utiliser. 


Grouper les données est une étape essentielle de la gestion et de l'analyse de données. Par exemple, il est souvent nécessaire de créer des résumés statistiques ou des figures "par groupe". Les fonctions du paquet **dplyr** (qui fait partie de **tidyverse**) permettent de grouper les données et d'effectuer de nombreuses 
actions "par groupes très facilement.  


Ce chapitre aborde les sujets suivants :  

* Grouper les données avec la fonction `group_by()`.  
* Dé-grouper des données  
* Résumer les données groupées avec des statistiques (`summarise()`)  
* La différence entre `count()` et `tally()`.  
* Trier les données groupées avec la fonction `arrange()`  
* Filtrer les données groupées avec la fonction `filter()`  
* Créer de nouvelles  colonnes avec la fonction `mutate()`  
* Sélectionner les colonnes avec la fonction `select()`  
* La commande **base** R `aggregate()`, qui est une alternative aux fonctions de **dplyr**.  





<!-- ======================================================= -->
## Étapes préliminaires {  }

### Importation des paquets {.unnumbered}  

Ces lignes de code chargent les paquets nécessaires aux analyses. Dans ce guide, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *puis* l'importe pour l'utiliser. Vous pouvez également charger les paquets installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les paquets R.  

```{r}
pacman::p_load(
  rio,       # import des fichiers
  here,      # gestion des chemins d'accès
  tidyverse, # gestion des données + graphiques (inclus dplyr)
  janitor)   # ajout de totaux aux lignes et colonnes
```




### Import des données {.unnumbered}

Dans ce chapitre, nous utiliserons un jeu de données fictif pour une épidémie d'Ebola. Pour reproduire les étapes, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la liste "nettoyée"</a> (sous forme de fichier .rds). Le jeu de données est importé à l'aide de la fonction `import()` du paquet **rio**. voir la page [Importation et exportation des données](import_export) pour plus de détails).  


```{r, echo=F}
linelist <- rio::import(here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
linelist <- import("linelist_cleaned.rds")
```


Les premières cinquante lignes de la `linelist` :  

```{r message=FALSE, echo=F}
DT::datatable(head(linelist,50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
## Grouper des données {  }
     
La fonction `group_by()` de **dplyr** permet de définir des groupes de lignes à partir des valeurs d’une ou de plusieurs colonnes. Chaque valeur unique (ou combinaison de valeurs unique, dans le cas où plusieurs colonnes sont spécifiées) constitue un groupe. Une fois les données groupées, de nombreuses fonctions utilisées pour le nettoyage ou des analyses descriptives seront appliquées _à chaque groupe_.

Par exemple, le code ci-dessous groupe la `linelist` en fonction des valeurs uniques de la colonne `outcome`. La ou les colonnes selon lesquelles grouper les données sont placées entre parenthèses dans la fonction `group_by()`. La fonction génère un nouveau tableau de données, que nous nommons `ll_by_outcome`.  


```{r}
ll_by_outcome <- linelist %>% 
  group_by(outcome)
```

**Notez que les données elles mêmes n'ont pas été modifiées** après avoir exécuté `group_by()`. Le fait que le dataframe soit "groupé" se verra lorsqu'une autre fonction du paquet **dplyr** tel que `mutate()`, `summarise()`, ou `arrange()` sera appliquée sur le dataframe "groupé".  

Vous pouvez cependant savoir qu'un dataframe est groupé en l'imprimant dans la console. Vous verrez alors qu'il a été transformé en un [objet de classe `tibble`](https://tibble.tidyverse.org/) qui, lorsqu'il est affiché, indique les groupements présents et le nombre de groupes qu'il y a juste au-dessus de la ligne d'en-tête.  


```{r}
# Faire afficher pour voir le schéma de groupement
ll_by_outcome
```


### Groupes distincts {.unnumbered}  

**Les groupes sont basés sur les combinaisons uniques de valeurs dans les colonnes de groupement**. 

Pour afficher les groupes *et le nombre de lignes de chaque groupe*, passez les données groupées à la fonction `tally()`. Pour afficher les groupes présents mais pas le nombre de lignes, passez les données à la fonction `group_keys()`.  

Dans l'exemple ci-dessous, il y a **trois** valeurs uniques dans la colonne de groupement `outcome` : "Death", "Recover", et "NA". Vous voyez qu'il y avait ` nrow(linelist %>% filter(outcome == "Death"))` morts, ` nrow(linelist %>% filter(outcome == "Recover"))` guéris, et ` nrow(linelist %>% filter(is.na(outcome)))` individus sans information renseignée.  


```{r}
linelist %>% 
  group_by(outcome) %>% 
  tally()
```

Vous pouvez regrouper par plus d'une colonne. Ci-dessous, nous groupons le dataframe par `outcome` et `gender`, puis comptons le nombre de lignes dans chaque groupe. Chaque combinaison unique de `outcome` et `gender` crée un groupe différent, y compris les valeurs manquantes pour chaque colonne.  


```{r}
linelist %>% 
  group_by(outcome, gender) %>% 
  tally()
```

### Nouvelle colonne {.unnumbered} 

Vous pouvez également grouper selon une colonne crée *directement dans* la fonction `group_by()`. Cela revient à appeler `mutate()` avant le `group_by()`. Cela peut être intéressant pour créer de petites tables descriptives rapidement, mais dans d'autres cas, il sera plus lisible de créer la nouvelle colonne avec la fonction `mutate()` avant de passer le tableau à `group_by()`.  


```{r}
# grouper les données sur la base d'une colonne crée dans la commande group_by()
linelist %>% 
  group_by(
    age_class = ifelse(age >= 18, "adult", "child")) %>% 
  tally(sort = TRUE)
```


### Grouper selon plus ou moins de colonnes {.unnumbered}  

Par défaut, si vous exécutez `group_by()` sur des données déjà groupées, les anciens groupes seront supprimés et le ou les nouveaux groupes s'appliqueront. Si vous voulez ajouter de nouveaux groupes à ceux qui existent déjà, incluez l'argument `.add = TRUE`.  

```{r, eval=F}
# Grouper par  outcome
by_outcome <- linelist %>% 
  group_by(outcome)

# Ajouter gender aux définition de groupe (grouper par une combinaison
# de gender et outcome)
by_outcome_gender <- by_outcome %>% 
  group_by(gender, .add = TRUE)
```


### Conserver tous les groupes {.unnumbered} 


Si vous groupez les données sur la base d'une colonne de type "facteur", il se peut que certains niveaux du facteur ne soient pas présents dans le jeu de données dans son état actuel. Dans ce cas, ces niveaux non représentés seront abandonnés par défaut et n’apparaîtront pas dans les groupes. Pour éviter ce comportement et prendre en compte tous les niveaux du facteur, _y compris lorsqu'ils ne contiennent pas de données_, utilisez l'argument `.drop = FALSE` dans votre commande `group_by()`.  

## Dégrouper les données

Les dataframes qui ont été groupés le resteront jusqu'à ce qu'ils soient spécifiquement dégroupés grâce à la fonction `ungroup()`.  

Attention à ne pas oublier de dégrouper les données avant de passer aux étapes qui nécessitent le jeu de données complet et non groupé.  

```{r, eval=F}
linelist %>% 
  group_by(outcome, gender) %>% 
  tally() %>% 
  ungroup()
```

On peut également dégrouper seulement certaines colonnes, en passant le nom de la colonne à `ungroup()`.   

```{r, eval=F}
linelist %>% 
  group_by(outcome, gender) %>% 
  tally() %>% 
  ungroup(gender) # dégroupe gender, mais garde le groupement par outcome
```


<span style="color: black;">**_NOTE:_** Le verbe `count()` dégroupe automatiquement les données après avoir compté les lignes.</span>



## Résumer les données par groupe {#group_summarise} 

Voir la section **dplyr** du chapitre sur les [Tableaux descriptifs](#descriptive_tables) pour une explication détaillée sur comment produire des tableaux récapitulatifs à l'aide de la fonction `summarise()`. Ici, nous décrivons le comportement de `summarise()` lorsque la fonction est appliquée à des données groupées.  

La fonction de **dplyr** `summarise()` (ou `summarize()`) prend un dataframe en entrée et le convertit en un *nouveau dataframe* contenant des statistiques de synthèse définies par l'utilisateur. Sur un tableau non groupé, le calcul de synthèse est effectuée sur toutes les lignes. Sur un tableau groupé, le calcul est effectué *pour chaque groupe*.  


Plus précisement, la syntaxe de la fonction `summarise()` est du type :
"NOM_NOUVELLE_COLONNE = fonction résumé d'une ou plusieurs colonnes des données source". Dans la fonction statistique, indiquez la colonne à traiter et tout argument pertinent (par exemple, `na.rm = TRUE`). Les fonctions régulièrement utilisées incluent par exemple `mean()`, `min()`, `max()`, `median()`, ou `sd()`, mais on peut également utiliser `sum()` pour compter le nombre de lignes qui répondent à un critère logique (avec l'opérateur `==`).    

Vous trouverez ci-dessous un premier où `summarise()` est appliquée *sur des données non groupées* : les statistiques retournées sont produites à partir de l'ensemble des données.     


```{r}
# statistiques résumées appliquées sur le jeu de données complet
linelist %>% 
  summarise(
    n_cases  = n(),
    mean_age = mean(age_years, na.rm = T),
    max_age  = max(age_years,  na.rm = T),
    min_age  = min(age_years,  na.rm = T),
    n_males  = sum(gender == "m", na.rm = T))
```

Maintenant, la même commande est appliquée sur la linelist groupée, ce qui génère les résumés statistique pour chaque groupe. Notez que les colonnes utilisées pour définir les groupes sont gardées dans le tableau agrégé généré par `summarise()`.   

```{r}
# statistiques résumées appliquées sur le jeu de données complet 
# mais groupé par outcome
linelist %>% 
  group_by(outcome) %>% 
  summarise(
    n_cases  = n(),
    mean_age = mean(age_years, na.rm = T),
    max_age  = max(age_years,  na.rm = T),
    min_age  = min(age_years,  na.rm = T),
    n_males  = sum(gender == "m", na.rm = T))
```

<span style="color: darkgreen;">**_Note:_** il est possible d'appeler la fonction en utilisant l'orthographe britannique et américaine : `summarise()` et `summarize()` sont équivalentes.</span>



## Comptes et additions  

Les fonctions `count()` et `tally()` fournissent des fonctionnalités similaires mais légèrement différentes. Pour plus de détails sur la distinction entre les deux, voir [ici](https://dplyr.tidyverse.org/reference/tally.html).    

### `tally()` {.unnumbered}  

`tally()` est un raccourci pour `summarise(n = n())`, et *ne groupe pas* les données d'elle même. Ainsi, pour obtenir des totaux groupés, il faut d'abord exécuter la commande `group_by()` avant la commande `tally()`. On peut ajouter `sort = TRUE` pour voir les plus grands groupes en premier.   

Exemple sans grouper les données :  

```{r}
linelist %>% 
  tally()
```


En groupant les données avant d'applique la fonction `tally()` :  

```{r}
linelist %>% 
  group_by(outcome) %>% 
  tally(sort = TRUE)
```


### `count()`  {.unnumbered}  

En revanche, la fonction `count()` effectue les actions suivantes :  

1) applique `group_by()` sur la ou les colonnes spécifiées  
2) applique `summarise()` et retourne la colonne `n` avec le nombre de lignes par groupe  
3) puis applique la fonction `ungroup()`.  
  

```{r}
linelist %>% 
  count(outcome)
```

Tout comme avec `group_by()` il est possible de créer une nouvelle colonne directement dans la commande `count()` :  

```{r}
linelist %>% 
  count(age_class = ifelse(age >= 18, "adult", "child"), 
        sort = T)
```

`count()` peut être utilisée plusieurs fois à la suite pour résumer des données de manière plus en plus compacte. Par exemple, pour résumer le nombre d'hôpitaux présents pour chaque sexe, exécutez ce qui suit. Notez que le nom de la dernière colonne est changé de la valeur par défaut "n" pour plus de clarté (avec `name = `).  

```{r}
linelist %>% 
  # compte le nombre de lignes pour chaque combinaison gender x hospital
  count(gender, hospital) %>% 
  # en utilisant le jeu de données agrégées, compte le nombre d’hôpitaux pour chaque genre.
  count(gender, name = "hospitals per gender" ) 
```


### Ajouter des colonnes contenant les décomptes {.unnumbered}  

Construites sur des principes similaires à `count()` et `tally()`, vous pouvez utiliser les fonctions `add_count()` et `add_tally()` pour *ajouter une nouvelle colonne* `n` avec le nombre de lignes par groupe *tout en conservant toutes les autres colonnes* du dataframe. Cela signifie que le nombre de lignes total d'un groupe est ajouté pour chaque ligne du groupe dans une nouvelle colonne `n`.

Dans l'exemple suivant, nous ajoutons cette colonne et ré-arrangeons ensuite les colonnes pour une lecture plus aisée du tableau. Pour un autre exemple, voir la section plus bas sur comment [filtrer sur la taille du groupe](#group_filter_grp_size).   

```{r}
linelist %>% 
  as_tibble() %>%  # conversion en tibble pour un meilleur affichage
  add_count(hospital) %>%  # ajoute la colonne n avec les totaux par hôpitaux
  select(hospital, n, everything()) # trie les colonnes
```



### Ajouter les totaux {.unnumbered} 

Pour facilement ajouter les totaux par lignes ou colonnes d'un tableau après avoir utilisé `tally()` ou `count()`, consultez la section **janitor** de la page [sur les tables descriptives](#tbl_janitor). Ce paquet offre des fonctions telles que `adorn_totals()` et `adorn_percentages()` pour ajouter des totaux et pourcentages. Par exemple :   

```{r}
linelist %>%                                  
  tabyl(age_cat, gender) %>%                  # décomptes croisés de deux colonnes
  adorn_totals(where = "row") %>%             # ajoute ligne de totaux
  adorn_percentages(denominator = "col") %>%  # ajoute proportions (dénominateur colonne)
  adorn_pct_formatting() %>%                  # formate en %
  adorn_ns(position = "front") %>%            # formate en : "N (%)"
  adorn_title(                                # ajuste les titres
    row_name = "Catégorie d'âge",
    col_name = "Sexe")
```


Pour ajouter des lignes de totaux plus complexes qui impliquent des statistiques récapitulatives autres que des *sommes*, voir [cette section de la page Tables descriptives](#tbl_dplyr_totals).  


## Grouper par date 

Pour grouper des données par date, il faut avoir, ou créer une colonne contenant l'unité de temps qui vous intéresse (par exemple : "jour", "semaine épidémiologique", "mois", etc). Vous pouvez créer cette colonne en utilisant `floor_date()` du paquet **lubridate**, tel qu'expliqué dans la section sur les [Semaines épidémiologiques](#dates_epi_wks) du chapitre sur les [dates](#working_dates). Cette colonne peut être simplement passée à `group_by()` ou `count()` de **dplyr** pour grouper les lignes par les valeurs uniques de date ou obtenir le nombre de lignes par date.  

Un besoin spécifique à la gestion et l'analyse de données par date consiste à compléter les dates de la séquence qui ne sont pas présentes dans les données. Pour cela, on peut utiliser `complete()` du paquet **tidyr** pour que la série de dates agrégées comprenne *toutes les unités de dates possibles* dans la plage. Sans cette étape, une semaine où aucun cas n'a été signalé n’apparaîtrait pas dans les données...  

 
La fonction `complete()`, *redéfinit* la colonne contenant les dates comme une *séquence* de dates (en passant `seq.Date()` du minimum au maximum comme argument). Par défaut, les valeurs du nombre de cas (et autres colonnes) dans les nouvelles lignes "développées" contient des "NA", mais l'on peut modifier ce comportement. Par exemple, on peut mettre le nombre de cas à 0 en utilisant l'argument `fill = ` de `complete()`, qui prend en entrée une liste nommée (si votre colonne de nombre de cas est nommée `n`, fournissez `fill = list(n = 0)`. Voir `?complete` pour plus de détails et la page [Manipuler les dates](#working_dates) pour un exemple.  



### Grouper par jours (linelist) {.unnumbered}  

Voici un exemple où l'on va grouper le nombre de cas de la linelist par jour, *sans utiliser* la fonction `complete()`. Note : la première ligne permet d'ignorer les cas où il n'y a pas eu de date de rentrée.  

```{r}
daily_counts <- linelist %>% 
  drop_na(date_onset) %>%        # Exclut les cas où date_onset est vide
  count(date_onset)              # Compte le nombre de lignes par date
```

```{r message=FALSE, echo=F}
DT::datatable(daily_counts, 
              rownames = FALSE, 
              options = list(pageLength = 5, scrollX = T), 
              class = 'white-space: nowrap' )
```

Maintenant, le même exemple en utilisant la commande `complete()` pour s'assurer que tous les jours dans la fourchette temporelle seront représentés dans les données.  

```{r, eval=F}
daily_counts <- linelist %>% 
  drop_na(date_onset) %>%     # Exclut les cas où date_onset est vide
  count(date_onset) %>%       # Compte le nombre de lignes par jour
  complete(                   # Ajoute les jours manquants (sans cas)
    date_onset = seq.Date(    # redéfinit la colonne comme une séquence de dates
      from = min(date_onset, na.rm=T), 
      to   = max(date_onset, na.rm=T),
      by   = "day"),
    fill   = list(n = 0))     # remplit les nouvelles dates ajoutées de 0 (aurait été des NA par défaut) 
```

```{r message=FALSE, echo=F}
DT::datatable(daily_counts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Grouper par semaines (linelist) {.unnumbered}  

Le même principe peut être appliqué au groupement par semaine. Dans cet exemple, on va d'abord créer une nouvelle colonne contenant la semaine à l'aide de la fonction `floor_date()` du package **lubridate** (avec `unit = "week"`). Cela arrondit chaque date au premier jour de la semaine correspondante. Ensuite, on utilise la fonction `count()` pour obtenir le nombre de cas hebdomadaires. On termine enfin avec  un `complete()` pour compléter toutes les semaines dans le jeu de données agrégées, même il n'y a pas eu de cas cette semaine là.


```{r}
weekly_counts <- linelist %>% 
  drop_na(date_onset) %>%        # Exclut les cas où date_onset est vide
  mutate(week = lubridate::floor_date(date_onset, 
                                      unit = "week")) %>%  # Crée colonne avec la date de début des symptomes
  count(week) %>%                # Compte le nombre de lignes par semaine
  complete(                      # Ajoute les semaines non représentées (sans cas)
    week = seq.Date(             # redéfinit la colonne comme une séquence de dates
      from = min(week, na.rm=T), 
      to = max(week, na.rm=T),
      by = "week"),
    fill = list(n = 0))          # remplit les nouvelles dates ajoutées de 0 (aurait été des NA par 
```

Voici les 50 premières lignes du jeu de données créé :   

```{r message=FALSE, echo=F}
DT::datatable(weekly_counts, rownames = FALSE, options = list(pageLength = 5, scrollX = T), class = 'white-space: nowrap' )
```

### Grouper par mois (linelist){.unnumbered}

Pour agréger les cas par mois, nous utiliserons à nouveau `floor_date()`, avec l'argument `unit = "months"`. Cette commande arrondit chaque date au 1er de son mois. La sortie sera donc de la classe Date. Notez que dans l'étape `complete()`, nous utilisons également `by = "months"`.  


```{r}
monthly_counts <- linelist %>% 
  drop_na(date_onset) %>% 
  mutate(month = lubridate::floor_date(date_onset, 
                                       unit = "months")) %>%  # nouvelle colonne, 1st du mois de début des symptomes
  count(month) %>%             # Compte le nombre de cas par mois
  complete(
    month = seq.Date(
      min(month, na.rm=T),     # Ajoute les mois non représentées (sans cas)
      max(month, na.rm=T),
      by="month"),
    fill = list(n = 0))
```

```{r message=FALSE, echo=F}
DT::datatable(monthly_counts, rownames = FALSE, options = list(pageLength = 5, scrollX = T), class = 'white-space: nowrap' )
```


### Comptes journaliers en semaines (données agrégées) {.unnumbered}

Pour agréger les nombres de cas quotidiens (données déjà agrégées par jour, donc) en nombre de cas hebdomadaires, utilisez `floor_date()` de la même manière que dans les exemples précédents. Cependant, il faut ensuite utiliser les fonctions `group_by()` et `summarize()` au lieu de `count()` car il faut _faire la somme des nombres de cas quotidiens_ au lieu de simplement compter le nombre de lignes par semaine.  


#### Comptes journaliers en mois (données agrégées) {.unnumbered}

Pour agréger les nombres de cas journaliers par mois, utilisez `floor_date()` de la même manière que dans les exemples précédents (avec `unit = "month"`). Cependant, il faut ensuite utiliser les fonctions `group_by()` et `summarize()` au lieu de `count()` car il faut additionner le nombre de cas quotidiens au lieu de simplement compter le nombre de lignes par mois.  


## Trier les données groupées

La fonction `arrange()` de  **dplyr** qui permet d'ordonner les lignes d'un dataframe se comporte de la même manière lorsque les données sont groupées, *sauf* si vous définissez l'argument `.by_group = TRUE`. Dans ce cas, les lignes sont d'abord ordonnées par les colonnes de regroupement, puis par toutes les autres colonnes que vous spécifiez à `arrange()`.   


## Filtrer les données groupées

### `filter()` {.unnumbered}

Lorsque l'on utilise la fonction `filter` en conjonction avec des fonctions qui évaluent le dataframe (`max()`, `min()` ou `mean()` par exemple), la commande est désormais appliquée à chaque groupe indépendamment. Par exemple, pour filtrer et conserver les lignes où les patients ont un âge supérieur à l'âge médian, le filtre s'appliquera désormais à l'intérieur de chaque groupe, pour pour conserver les lignes où l'age des patients est supérieur à l'âge médian *du groupe*.  


### `slice()` {.unnumbered} 

La fonction **dplyr** `slice()`, qui [filtre les lignes en fonction de leur position dans les données](https://dplyr.tidyverse.org/reference/slice.html), peut également être appliquée par groupe. N'oubliez pas de trier les données au sein de chaque groupe pour obtenir la "tranche" souhaitée.  

Par exemple, pour extraire uniquement les 5 dernières admissions de chaque hôpital :  

1) Groupez les données de la linelist par la colonne `hospital`.   
2) Triez les enregistrements du plus récent au plus ancien grâce à la colonne `date_hospitalisation` *dans chaque groupe d'hôpitaux*.   
3) Tranchez pour récupérer les 5 premières lignes de chaque hôpital   

```{r,}
linelist %>%
  group_by(hospital) %>%
  arrange(hospital, date_hospitalisation) %>%
  slice_head(n = 5) %>% 
  arrange(hospital) %>%                            # (pour l'affichage)
  select(case_id, hospital, date_hospitalisation)  # (pour l'affichage)
```

`slice_head()` : sélectionne les n premières lignes ("par le haut")  
`slice_tail()` : sélectionne les n dernières lignes ("par le bas")  
`slice_sample()` : sélectionne n lignes aléatoirement. Utiliser `replace = TRUE` pour un échantillonnage avec remplacement    
`slice_min()` : sélectionne les n lignes avec les plus petites valeurs dans une colonne donnée (argument `order_by = `). Utiliser `with_ties = TRUE` pour garder les ex-æquo   
`slice_max()` : sélectionne les n lignes avec les plus grandes valeurs dans une colonne donnée (argument `order_by = `)

Voir le chapitre sur la [dé-duplication](#deduplication) pour plus d'exemples et de détails sur la fonction `slice()`.  



### Filtrer sur la taille des groupes {#group_filter_grp_size .unnumbered} 

La fonction `add_count()` ajoute une colonne `n` aux données originales, ajoutant ainsi, pour chaque ligne, le nombre de lignes du  groupe auquel cette ligne appartient.  

Dans l'exemple ci-dessous, `add_count()` est appliqué à la colonne `hospital`, de sorte que les valeurs de la nouvelle colonne `n` reflètent le nombre de lignes dans le groupe hospitalier de cette ligne. Bien sûr, cela veut dire que la valeur de la colonne "n" est répétée pour chaque ligne du groupe.  

Dans l'exemple ci-dessous, le nom de la colonne `n` pourrait être modifié en utilisant `name = ` dans `add_count()`.

```{r}
linelist %>% 
  as_tibble() %>% 
  add_count(hospital) %>%          # ajoute le nombre de patients admis dans cette hôpital, pour chaque groupe
  select(hospital, n, everything()) # Pour un meilleur affichage
```

Il est alors facile de filtrer les lignes de cas qui ont été hospitalisés dans un "petit" hôpital. Par exemple un hôpital qui a admis moins de 500 patients :    
```{r, eval=F}
linelist %>% 
  add_count(hospital) %>% 
  filter(n < 500)
```



## `mutate()` 

Pour conserver toutes les colonnes et lignes (sans les résumer) et *ajouter une nouvelle colonne contenant des statistiques de groupe*, utilisez `mutate()` après `group_by()` au lieu de `summarise()`. 

Ceci est utile si vous voulez les statistiques de groupe dans le jeu de données original *avec toutes les autres colonnes présentes* - par exemple pour les calculs qui comparent une ligne à son groupe.  

Par exemple, le code ci-dessous calcule la différence entre le délai d'admission d'une ligne et le délai d'admission médian pour son hôpital. Les étapes sont les suivantes :  

1) Groupez les données par hôpital  
2) Utilisez la colonne `days_onset_hosp` (délai à l'hospitalisation) pour créer une nouvelle colonne contenant le délai moyen de l'hôpital pour chaque patient de cet hôpital  
3) Calculez la différence entre les deux colonnes  
  

```{r}
linelist %>% 
  # grouper les données par hôpital
  group_by(hospital) %>% 
  
  # Ajoute de nouvelles colonnes (conserve toutes lies lignes)
  mutate(
    # Délai moyen d'admission pour chaque hôpital (arrondi à la 1re décimale)
    group_delay_admit = round(mean(days_onset_hosp, na.rm = T), 1),
    
    # Différence entre le délai de chaque patient et le délai moyen de son hôpital
    diff_to_group = round(days_onset_hosp - group_delay_admit, 1)) %>%
  
  # Sélectionne colonnes (pour l'affichage)
  select(case_id, hospital, days_onset_hosp, group_delay_admit, diff_to_group)
```



## `select()` sur les données groupées

La fonction `select()` fonctionne sur les données groupées, à ce détail près que les colonnes utilisées pour les groupes sont toujours inclues, même si elles n'ont pas été mentionnées dans les colonnes à conserver. Pour se débarrasser de ces colonnes, il faut utiliser `ungroup()` avant de dégrouper.  




<!-- ======================================================= -->
## Resources {  }

Pour plus d'information, voici quelques ressources utiles :  

Vous pouvez utiliser n'importe quelle fonction agrégeant sur des données groupées ; Voir l'antisèche sur [la transformation des données avec Rstudio](https://github.com/rstudio/cheatsheets/blob/master/data-transformation.pdf)  

La page de The Data Carpentry sur [**dplyr**](https://datacarpentry.org/R-genomics/04-dplyr.html)  

La page de référence de l'aide du **tidyverse** sur [group_by()](https://dplyr.tidyverse.org/reference/group_by.html) et [grouping](https://dplyr.tidyverse.org/articles/grouping.html)  

Cette page sur la [manipulation des données](https://itsalocke.com/files/DataManipulationinR.pdf)  

[Résummer les données avec des conditions avec dplyr](https://stackoverflow.com/questions/23528862/summarize-with-conditions-in-dplyr)  






```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/grouping.Rmd-->


# Joindre des données {#joining_matching}  

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "left-join.gif"))
```

*Ci-dessus : animation d'une jointure par la gauche ([source de l'image ](https://github.com/gadenbuie/tidyexplain/tree/master/images))*  

Ce chapitre décrit les méthodes permettant de joindre / fusionner / faire correspondre / lier / "merger" / unir / combiner des tableaux.  

Lors de l'analyse de données épidémiologiques, il est rare que votre processus de nettoyage des données n'implique pas des sources de données multiples, et donc leur mise en relation. 
Par exemple, vous aurez peut-être à joindre des données de laboratoire aux résultats cliniques des patients, ou des données de mobilité Google aux tendances des maladies infectieuses, ou même un jeu de données à un stade donné de l'analyse et une version transformée de lui-même.  

Dans ce chapitre, nous allons :  

* Réaliser des *jointures* de deux dataframes en faisant correspondre les lignes sur la base d'une clef primaire (dans une ou plusieurs colonnes).  
* Joindre des jeux de données sur la base de correspondances *probabilistes* (probables) entre les observations  
* Étendre un jeu de données en *concaténant* des lignes ou des colonnes d'un autre jeu de données.   


<!-- ======================================================= -->
## Étapes préliminaires { }

### Importation des paquets {.unnumbered}  

Ces lignes de code chargent les paquets nécessaires aux analyses. Dans ce guide, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *puis* l'importe pour l'utiliser. Vous pouvez également charger les paquets installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les paquets R.  

```{r}
pacman::p_load(
  rio,            # import des fichiers
  here,           # chemins d'accès
  tidyverse,      # gestion des données + graphiques (ggplot2)
  RecordLinkage,  # correspondances probabilistes
  fastLink        # correspondances probabilistes
)
```


### Importation des données {.unnumbered}

Nous importons un jeu de données de cas d'une épidémie d'ébola fictive. Pour reproduire les étapes, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la linelist "propre"</a> (as .rds file). Importez vos données avec la fonction `import()` du paquet **rio** (elle accepte de nombreux types de fichiers comme .xlsx, .rds, .csv - voir la page [Importation et exportation des données](import_export) pour plus de détails).  


```{r, echo=F}
# importer la linelist dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# importer la linelist dans R
linelist <- import("linelist_cleaned.rds")
```

Les cinquantes premières lignes sont affichées ci-dessous :  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter = "top",
              options = list(pageLength = 5, scrollX = T), 
              class = 'white-space: nowrap' )
```


<!-- ======================================================= -->
### Jeux de données simplifiés {.unnumbered}

Dans les exemples ci-dessous, nous utiliserons des jeux de données simplifiés pour mieux voir ce qui se passe :  


1) Une version "miniature" de la linelist (liste des cas), contenant seulement les colonnes `case_id`, `date_onset`, et `hospital`, et seulement les 10 premières lignes.   
2) Une table nommée `hosp_info`, qui contient des détails sur chaque hôpital.  

Dans la section sur l'appariement probabiliste, nous utiliserons deux petits ensembles de données différents. Le code pour créer ces jeux de données sera donné dans cette section.  


#### Linelist miniature {#joins_llmini .unnumbered}  

Nous générons ici la linelist miniature des cas, qui contient seulement 10 lignes et seulement les colonnes `case_id`, `date_onset`, et `hospital`.  


```{r}
linelist_mini <- linelist %>%                 
  select(case_id, date_onset, hospital) %>%   # sélectionne les colonnes
  head(10)                                    # garde les 10 premières lignes
```

```{r message=FALSE, echo=F}
DT::datatable(linelist_mini, rownames = FALSE, options = list(pageLength = nrow(10)))
```




#### Jeu de données des hôpitaux {#joins_hosp_info .unnumbered}  

Le code ci-dessous permet de créer un jeu de données contenant des informations supplémentaires sur sept hôpitaux (la population desservie et le niveau de soins disponible). Notez que le nom "Hôpital militaire" appartient à deux hôpitaux différents, l'un de niveau primaire desservant 10000 résidents et l'autre de niveau secondaire desservant 50280 résidents.  

```{r}
# Crée des informations sur les hôpitaux : 
hosp_info = data.frame(
  hosp_name     = c("central hospital", "military", "military", 
                    "port", "St. Mark's", "ignace", "sisters"),
  catchment_pop = c(1950280, 40500, 10000, 50280, 
                    12000, 5000, 4200),
  level         = c("Tertiary", "Secondary", "Primary", "Secondary",
                    "Secondary", "Primary", "Primary")
)
```

Voici le tableau ainsi produit :    

```{r message=FALSE, echo=F}
# display the hospital data as a table
DT::datatable(hosp_info, rownames = FALSE, options = list(pageLength = nrow(hosp_info)))
```





<!-- ======================================================= -->
### Nettoyage préliminaire {.unnumbered}

Les jointures "traditionnelles" (*i.e.* non-probabilistes) sont sensibles à la casse et nécessitent des correspondances exactes entre les valeurs des colonnes utilisées comme clef/identifiant. Pour démontrer certaines des étapes de nettoyage que vous pourriez avoir besoin de faire avant de joindre vos données, nous allons commencer par nettoyer et aligner les dataframe `linelist_mini` et `hosp_info`.  

**Identifier les différences**  

Le nom de l'hôpital étant notre identifiant/clef commun aux deux jeux de données, nous avons besoin que les valeurs de la colonne `hosp_name` dans le tableau `hosp_info` correspondent aux valeurs de la colonne `hospital` dans le tableau `linelist_mini`.  

Voici le dataframe `linelist_mini`, affiché avec la fonction **base** R `unique()` :  

```{r}
unique(linelist_mini$hospital)
```

... et voici les valeurs dans le dataframe `hosp_info` :    

```{r}
unique(hosp_info$hosp_name)
```

Il est clair que si certains hôpitaux sont présents dans les deux dataframes, leurs noms ne sont pas toujours orthographiés de la même manière.  

**Aligner les valeurs**  

Nettoyons les valeurs du jeu de données `hosp_info`. Comme expliqué dans le chapitre sur le [Nettoyage de données et fonctions essentielles](#cleaning_data), il est possible de recoder les valeurs à partir de critères logiques en utilisant la fonction `case_when()` de **dplyr**. Pour les quatre hôpitaux communs dans les deux dataframes, nous modifions les noms pour les aligner avec les noms dans le tableau `linelist_mini` (en ne touchant pas aux noms des autres hôpitaux grâce à l'argument `TRUE ~ hosp_name`).   

<span style="color: orange;">**_ATTENTION:_** Normalement on devrait créer une nouvelle colonne pour ce type de nettoyage (`hosp_name_clean` par exemple), mais pour mieux comprendre ce qui se passe lors des étapes suivantes, nous modifions directement la colonne contenant les données "brutes"</span>

```{r}
hosp_info <- hosp_info %>% 
  mutate(
    hosp_name = case_when(
      # critère                          # nouvelles valeur
      hosp_name == "military"          ~ "Military Hospital",
      hosp_name == "port"              ~ "Port Hospital",
      hosp_name == "St. Mark's"        ~ "St. Mark's Maternity Hospital (SMMH)",
      hosp_name == "central hospital"  ~ "Central Hospital",
      TRUE                             ~ hosp_name
      )
    )
```

Les noms des hôpitaux qui apparaissent dans les deux bases de données sont désormais identiques. Il y a deux hôpitaux dans `hosp_info` qui ne sont pas présents dans `linelist_mini`, nous les traiterons plus tard, lors de la jointure.  

```{r}
unique(hosp_info$hosp_name)
```

Avant une jointure, il est souvent rassurant de convertir une colonne tout en minuscules ou majuscules. Pour cela, on peut utiliser `mutate()` et une des colonnes de **stringr** (voir le chapitre [sur les chaînes de caractères](#character_strings)):  

`str_to_upper()`  
`str_to_lower()`  
`str_to_title()`  



<!-- ======================================================= -->
## Jointures à l'aide de **dplyr** { }

Le package **dplyr** offre plusieurs fonctions qui permettent d'effectuer des jointures différentes. **dplyr** est inclus dans le paquet **tidyverse**.  

Un grand merci à [https://github.com/gadenbuie](https://github.com/gadenbuie/tidyexplain/tree/master/images) pour les gifs informatifs !  



<!-- ======================================================= -->
### Syntaxe générale {.unnumbered}

Les fonctions de jointure peuvent s'utiliser seules pour unir deux dataframes et créer un nouveau dataframe, mais aussi au sein d'un enchaînement de commandes (pipe avec `%>%`) pour fusionner un dataframe dans un autre à la volée.  

Dans l'exemple ci-dessous, la fonction `left_join()` est utilisée de manière autonome pour créer un nouveau jeu de données (`joined_data`). Les arguments à l'entrée sont les dataframes à unir/fusionner/joindre (`df1` et `df2`). Le premier dataframe listé est le dataframe *de base*, et le deuxième dataframe listé est joint *à* celui-ci.  

Le troisième argument `by = ` précise quelle(s) colonne(s) sera utilisée pour faire la correspondance entre les lignes des deux dataframes (la clef). Si les noms de ces colonnes sont différents, fournissez-les dans un vecteur `c()` comme dans l'exemple ci-dessous, les identifiants communs sont dans la colonne `ID` dans `df1` et dans la colonne `identifier` dans `df2`.  
 

```{r, eval=F}
# Jointure basée sur les valeurs communes dans la colonne ID (df1) et la colonne "identifier" (df2)
joined_data <- left_join(df1, df2, 
                         by = c("ID" = "identifier"))
```

Si la ou les colonnes "clef" à le même nom dans les deux tableaux, alors leur nom peut juste être fourni directement, avec des guillemets :  

```{r, eval=F}
# Jointure basée sur les valeurs communes dans la colonne ID présente dans df1 et df2
joined_data <- left_join(df1, df2, 
                         by = "ID")
```

S'il y a besoin de plusieurs colonnes pour identifier de manière unique les observations (i.e. créer une clef primaire), on peut lister plusieurs colonnes dans un vecteur et le passer à `by`.
Dans cet exemple, les lignes des deux dataframes sont unies si les valeurs sont identiques dans les trois colonnes.  


```{r, eval=F}
# Jointure basée sur le prénom, le nom de famille et l'age : les lignes sont fusionnées si les valeurs sont alignées exactement
joined_data <- left_join(df1, df2, 
                         by = c("name"    = "firstname", 
                                "surname" = "lastname", 
                                "Age"     = "age"))
```


Les fonctions de jointure peuvent également être exécutées dans un enchaînement d'instructions (ou _pipe_). Cela modifiera le jeu de données qui est passée dans le pipe.  

Dans l'exemple ci-dessous, `df1` est pipé, `df2` lui est joint, et `df1` est ainsi modifié et redéfini.  

```{r eval=F}
df1 <- df1 %>%
  filter(date_onset < as.Date("2020-03-05")) %>%  # nettoyage divers
  left_join(df2, by = c("ID" = "identifier"))     # jointure de df2 à df1
```


<span style="color: orange;">**_ATTENTION:_** Les jointures respectent les majuscules/minuscules ! Il peut donc être utile de convertir les colonnes utilisées comme clefs en minuscules ou majuscules. Voir le chapitre [sur les chaînes de caractères](#character_strings)</span>



<!-- ======================================================= -->
### Jointures à gauche et droite {.unnumbered}  

**Une jointure à gauche ou droite est une opération très couramment utilisée pour ajouter des informations à un dataframe**, en particulier dans les analyses épidémiologiques. Les nouvelles informations sont ajoutées uniquement aux lignes qui existaient déjà dans le dataframe de "référence".  

*En utilisant ces jointures, l'ordre d'écriture des dataframes dans la commande est important*.  


* Dans une *jointure à gauche*, le *premier* dataframe écrit est utilisé comme "référence" à laquelle on adjoint les informations venant de l'autre table.  
* Dans une *jointure à droite*, le *second* dataframe est la référence à laquelle on rajoute les informations venant du premier dataframe.   

Plus précisement :  
* **Toutes les lignes présentes dans le dataframe de référence sont conservées**. Les informations contenues dans le dataframe secondaire sont adjointes au dataframe de référence *uniquement s'il existe une correspondance via la ou les colonnes d'identification/clefs*.    

* Les lignes du dataframe secondaire qui ne correspondent pas sont abandonnées.  

* Si plusieurs lignes du dataframe utilisé comme référence correspondent à une ligne dans le dataframe secondaire (many-to-one), les informations du dataframe secondaire sont ajoutées à *chaque ligne du dataframe de référence correspondantes*.  

* Si une ligne du dataframe de référence correspond à plusieurs lignes dans le dataframe secondaire (one-to-many), **toutes les combinaisons sont données**, ce qui signifie que *de nouvelles lignes sont ajoutées au dataframe de référence !*.  

Exemples animés de jointures gauche et droite ([source de l'image](https://github.com/gadenbuie/tidyexplain/tree/master/images))

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "left-join.gif"))
knitr::include_graphics(here::here("images", "right-join.gif"))
```

**Exemple**  

Voici le résultat d'un `left_join()` de `hosp_info` (dataframe secondaire, [voir ici](#joins_hosp_info)) *dans*/*vers* `linelist_mini` (dataframe de référence, [voir ici](#joins_llmini)). La `linelist_mini` originale a `nrow(linelist_mini)` lignes. La `linelist_mini` modifiée est affichée. On constate que :   

* Deux nouvelles colonnes, `catchment_pop` et `level` ont été ajoutées sur le côté gauche de `linelist_mini`.  

* Toutes les lignes originales du dataframe de référence `linelist_mini` ont été conservées.  

* Toutes les lignes originales de `linelist_mini` pour "Military Hospital" ont été dupliquées car elles correspondaient à *deux* lignes dans le dataframe secondaire, et donc les deux combinaisons ont été  retournées.  

* La colonne d'identifiant/clef de jointure du dataframe secondaire (`hosp_name`) a disparu car elle est redondante avec la colonne d'identifiant du dataframe de référence (`hospital`)   

* Lorsqu'une ligne du dataframe de gauche ne correspond à aucune du dataframe de droite (par exemple, lorsque `hospital` est "Autre" ou "Manquant"), les observations renvoyées dans les colonnes venant du dataframe de droite sont `NA`.  

* Les lignes du dataframe de droite qui ne correspondent pas au dataframe gauche (hôpitaux "sisters" et "ignace") ont été abandonnées.   


```{r, eval=F}
linelist_mini %>% 
  left_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  left_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 11))
```


#### Jointure à gauche ou jointure à droite ? {.unnumbered}  

Pour répondre à la question, il faut décider quel dataframe doit conserver toutes ses lignes, et l'utiliser comme dataframe de référence. Une *jonction à gauche* conserve toutes les lignes du premier dataframe écrit dans la commande, tandis qu'une *jonction à droite* conserve toutes les lignes du second dataframe.    


Les deux instructions ci-dessous retournent le même résultat : 10 lignes de `hosp_info` jointes *dans* un dataframe `linelist_mini`, mais elles utilisent des jointures différentes. Le résultat est que l'ordre des colonnes sera différent selon que `hosp_info` arrive "par la droite" (dans la jointure à gauche) ou arrive "par la gauche" (dans la jointure à droite). L'ordre des lignes peut également changer en conséquence. Néanmoins ces deux conséquences peuvent être traitées ultérieurement, en utilisant `select()` pour réordonner les colonnes ou `arrange()` pour trier les lignes.  


```{r, eval=F}
# On obtient le même jeu de données, mais avec l'ordre des colonnes et des lignes différent
left_join(linelist_mini, hosp_info, by = c("hospital" = "hosp_name"))
right_join(hosp_info, linelist_mini, by = c("hosp_name" = "hospital"))
```

Voici le résultat d'une fusion de `hosp_info` dans `linelist_mini` par une jointure à gauche (nouvelles colonnes ajoutées par la droite) :  


```{r message=FALSE, echo=F}
left_join(linelist_mini, hosp_info, 
          by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 11))
```

Voici le résultat de la fusion de `hosp_info` dans `linelist_mini` par une jointure à droite (nouvelles colonnes ajoutées par la gauche) :   


```{r message=FALSE, echo=F}
right_join(hosp_info, linelist_mini, 
           by = c("hosp_name" = "hospital")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 11))
```

Une autre chose à considérer est si la jointure est réalisée au sein d'une chaîne d'instructions pipées (`%>%`). Si le dataframe transmis par le pipe doit être utilisé comme référence, utilisez une jointure à gauche pour lui adjoindre de nouvelles données.  


<!-- ======================================================= -->
### Jointure complète {.unnumbered} 

**La jointure complète est la plus *inclusive* des jointures**, elle renvoie toutes les lignes des deux dataframes fusionnés.  

Si des lignes ne sont présentes que dans l'un des dataframes fusionnés (*i.e* aucune correspondance n'est trouvée), elles seront inclues dans le dataframe final (qui s'allonge donc), avec des `NA` pour combler les vides.  

Lorsque vous effectuez une jointure, **surveillez attentivement le nombre de colonnes et de lignes pour vérifier le nombre de lignes des dataframes en entrée, et du dataframe fusionné**. Cela vous permettra notamment de détecter des problèmes de correspondance dus à la sensibilité à la casse ou à des correspondances inexactes. 

Le dataframe de référence utilisé comme base est celui qui est écrit en premier dans la commande. Dans une jointure complète, Lequel des deux dataframes est écrit en premier n'affecte que l'ordre des lignes, l'ordre des colonnes et le nom des colonnes clef retenues.   


```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "full-join.gif"))
```

Exemple animé d'une jointure complète ([image source](https://github.com/gadenbuie/tidyexplain/tree/master/images))

**Exemple**  

Voici la sortie d'une  jointure complète du dataframe `hosp_info` (originellement ` nrow(hosp_info)` lignes, [voir ici](#joins_hosp_info))  *avec* `linelist_mini` (originellement ` nrow(linelist_mini)` lignes, [voir ici](#joins_llmini)). On constate que :    


* Toutes les lignes du dataframe de référence sont conservées (`linelist_mini`).   

* Les lignes du second dataframe qui n'ont pas de correspondance avec le premier dataframe sont conservées ("ignace" et "sisters"), et les valeurs des colonnes apportées par le dataframe de référence, `case_id` et `onset`, sont complétées par des valeurs manquantes.  

* De même, les lignes du dataframe de référence qui ne correspondent pas à la ligne secondaire ("Autre" et "Manquant") sont conservées, les colonnes secondaires `catchment_pop` et `level` étant remplies de valeurs manquantes.  

* Dans le cas d'une correspondance un-à-plusieurs ou plusieurs-à-un (par exemple, des lignes pour "Hôpital militaire"), toutes les combinaisons possibles sont retournées (ce qui allonge le dataframe final).  

* Seule la colonne d'identification du dataframe de référence est conservée (`hospital`).  



```{r, eval=F}
linelist_mini %>% 
  full_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  full_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 15))
```





<!-- ======================================================= -->
### Jointure interne {.unnumbered} 

**La jointure interne est la plus *restrictive* des jointures** : elle renvoie uniquement les lignes avec des correspondances dans les deux dataframes.  
Le nombre de lignes dans le dataframe de référence peut ainsi *diminuer*. Le choix du dataframe à passer en premier à la fonction n'aura pas d'impact sur les lignes conservées, mais affectera l'ordre des colonnes, l'ordre des lignes et clefs d'identification retenues.   


```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "inner-join.gif"))
```

Exemple animé d'une jointure complète : ([image source](https://github.com/gadenbuie/tidyexplain/tree/master/images))


**Exemple**  

Voici la sortie d'un `inner_join()` de la `linelist_mini` (référence) avec `hosp_info` (secondaire). On constate que :  

* Les lignes du dataframe de référence sans correspondance dans le second dataframe sont supprimées (lignes où `hospital` est "Missing" ou "Other").   
* De même, les lignes du dataframe secondaires qui n'ont pas de correspondance dans le dataframe de référence (lignes où `hosp_name` est "sisters" ou "ignace") sont supprimées.   
* Seule la colonne clef du dataframe de référence est conservée (`hospital`).  


```{r, eval=F}
linelist_mini %>% 
  inner_join(hosp_info, 
             by = c("hospital" = "hosp_name"))
```


```{r message=FALSE, echo=F}
linelist_mini %>% 
  inner_join(hosp_info, 
             by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, 
                options = list(pageLength = 12))
```




<!-- ======================================================= -->
### Semi jointure {.unnumbered} 

Les semi jointures sont des jointures dites "filtrantes", qui utilisent la correspondance avec un second dataframe pour *filtrer le dataframe de référence*. 

**La semi jointure garde toutes les observations du dataframe de référence qui ont une correspondance dans le dataframe secondaire**. Mais les colonnes du dataframe secondaire ne sont pas ajoutées et les lignes du dataframe de référence ne sont pas dupliquées s'il y a des correspondances multiples.  

Plus d'explications sur les semi-jointures  [ici](https://towardsdatascience.com/level-up-with-semi-joins-in-r-a068426096e0).  

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "semi-join.gif"))
```

Exemple animé d'une semi jointure ([image source](https://github.com/gadenbuie/tidyexplain/tree/master/images))

Voici un exemple qui retourne les lignes du dataframe `hosp_info` qui ont une correspondance dans le dataframe `linelist_mini` en utilisant le nom de l'hôpital comme clé de jointure/identifiant.  

```{r}
hosp_info %>% 
  semi_join(linelist_mini, 
            by = c("hosp_name" = "hospital"))
```



<!-- ======================================================= -->
### Anti-jointure `anti_join()` {.unnumbered} 

**L'anti-jointure est une autre type de jointure filtrante, qui, à l'opposé du semi-join, ne renvoie que les lignes du dataframe de référence qui n'ont PAS de correspondance dans le dataframe secondaire.**  

Plus de détails sur les jointures filtrantes [ici](https://towardsdatascience.com/level-up-with-semi-joins-in-r-a068426096e0).  

Voici quelques cas d'usage de l'anti-jointure : identifier les observations non présentes dans un autre dataframe, identifier les typos qui compliquent une jointure (se focaliser sur les observations qui auraient du correspondre), examiner les observations qui ont été exclues d'une jointure etc.  

**Comme pour les jointures à droite (`right_join()`) et à gauche (`left_join()`), l'ordre dans lequel sont passés les dataframe a de l'importance.** Dans les joins filtrants, on ne renvoie que les lignes présentes dans le dataframe de référence (écrit en premier), comme on peut le voir dans l'animation ci-dessous (la ligne 4, violette, du dataframe secondaire n'est pas retournée, alors qu'elle ne matche avec aucune ligne du dataframe de référence).  

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "anti-join.gif"))
```

Exemple animé d'une anti-jointure ([image source](https://github.com/gadenbuie/tidyexplain/tree/master/images))


#### Exemple simple d'anti-jointure  {.unnumbered}  

Un cas d'utilisation simple est de rechercher les hôpitaux dans le tableau `hosp_info` qui n'ont pas de cas présents dans le tableau `linelist_mini`. 
Nous rentrons `hosp_info` en premier, comme dataframe de référence, puis `linelist_mini`, la seconde table à comparer pour trouver les hôpitaux qui n'y sont pas présents.  


```{r, eval=F}
hosp_info %>% 
  anti_join(linelist_mini, by = c("hosp_name" = "hospital"))
```

```{r message=FALSE, echo=F}
hosp_info %>% 
  anti_join(linelist_mini, by = c("hosp_name" = "hospital")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 12))
```


#### Exemple d'anti-jointure plus complexe  {.unnumbered}  

Imaginons cette fois-ci que nous avons exécuté une jointure interne (`inner_join()`) entre les dataframes `linelist_mini` et `hosp_info`. Cette opération ne retourne qu'un sous-ensemble des lignes originales de `linelist_mini`, car certains hôpitaux ne sont pas présents dans `hosp_info`. 

```{r, eval=F}
linelist_mini %>% 
  inner_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  inner_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 8))
```

Nous pouvons utiliser une anti jointure pour inspecter les éléments de `linelist_mini` qui ont été exclus lors de la jointure interne, avec les mêmes paramètres (`linelist_mini` comme dataframe de référence).  


```{r, eval = F}
linelist_mini %>% 
  anti_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  anti_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 8))
```

Pour voir les lignes d'`hosp_info` exclues lors de la jointure interne, nous pourrions aussi exécuter une anti-jointure en utilisant `hosp_info` comme table de référence.



<!-- ======================================================= -->
## Apparienement probabiliste { }

Si l'on ne dispose pas d'un identifiant unique commun à tous les dataframes sur lequel se baser, il est possible d'utiliser un algorithme de correspondance probabiliste. Cet algorithme cherche des correspondances entre les observations sur la base de la **similarité** (par exemple, la distance entre les chaînes de caractères Jaro-Winkler ou la distance numérique). Nous illustrons ce concept ci-dessous à l'aide du paquet **fastLink**.  


**Charger les paquets**  

```{r}
pacman::p_load(
  tidyverse,      # manipulation de données et visualisation
  fastLink        # appariement d'observations
  )
```

Nous créons d'abord deux petits jeux de données d'exemple que nous utiliserons pour démontrer l'appariement/la correspondance probabiliste (`cases` et `test_results`) :  

```{r}
# Création des jeux de données

cases <- tribble(
  ~gender, ~first,      ~middle,     ~last,        ~yr,   ~mon, ~day, ~district,
  "M",     "Amir",      NA,          "Khan",       1989,  11,   22,   "River",
  "M",     "Anthony",   "B.",        "Smith",      1970, 09, 19,      "River", 
  "F",     "Marialisa", "Contreras", "Rodrigues",  1972, 04, 15,      "River",
  "F",     "Elizabeth", "Casteel",   "Chase",      1954, 03, 03,      "City",
  "M",     "Jose",      "Sanchez",   "Lopez",      1996, 01, 06,      "City",
  "F",     "Cassidy",   "Jones",      "Davis",     1980, 07, 20,      "City",
  "M",     "Michael",   "Murphy",     "O'Calaghan",1969, 04, 12,      "Rural", 
  "M",     "Oliver",    "Laurent",    "De Bordow" , 1971, 02, 04,     "River",
  "F",      "Blessing",  NA,          "Adebayo",   1955,  02, 14,     "Rural"
)

results <- tribble(
  ~gender,  ~first,     ~middle,     ~last,          ~yr, ~mon, ~day, ~district, ~result,
  "M",      "Amir",     NA,          "Khan",         1989, 11,   22,  "River", "positive",
  "M",      "Tony",   "B",         "Smith",          1970, 09,   19,  "River", "positive",
  "F",      "Maria",    "Contreras", "Rodriguez",    1972, 04,   15,  "Cty",   "negative",
  "F",      "Betty",    "Castel",   "Chase",        1954,  03,   30,  "City",  "positive",
  "F",      "Andrea",   NA,          "Kumaraswamy",  2001, 01,   05,  "Rural", "positive",      
  "F",      "Caroline", NA,          "Wang",         1988, 12,   11,  "Rural", "negative",
  "F",      "Trang",    NA,          "Nguyen",       1981, 06,   10,  "Rural", "positive",
  "M",      "Olivier" , "Laurent",   "De Bordeaux",  NA,   NA,   NA,  "River", "positive",
  "M",      "Mike",     "Murphy",    "O'Callaghan",  1969, 04,   12,  "Rural", "negative",
  "F",      "Cassidy",  "Jones",     "Davis",        1980, 07,   02,  "City",  "positive",
  "M",      "Mohammad", NA,          "Ali",          1942, 01,   17,  "City",  "negative",
  NA,       "Jose",     "Sanchez",   "Lopez",        1995, 01,   06,  "City",  "negative",
  "M",      "Abubakar", NA,          "Abullahi",     1960, 01,   01,  "River", "positive",
  "F",      "Maria",    "Salinas",   "Contreras",    1955, 03,   03,  "River", "positive"
  )

```


**Le dataframe `cases` a 9 observations** de patients attendant les résultats de leur test.  

```{r message=FALSE, echo=F}
# display the hospital data as a table
DT::datatable(cases, rownames = FALSE, options = list(pageLength = nrow(cases), scrollX=T), class = 'white-space: nowrap')
```


**Le dataframe `test_results`** a 14 observations et contient la colonne `result`, qui contient des informations que nous voudrions rapatrier dans le jeu de données `cases` en utilisant un algorithme probabiliste pour faire correspondre les observations.

```{r message=FALSE, echo=F}
# display the hospital data as a table
DT::datatable(results, rownames = FALSE, options = list(pageLength = nrow(results), scrollX=T), class = 'white-space: nowrap')
```

### Appariement probabiliste {.unnumbered}  

La fonction `fastLink()` du paquet **fastLink** peut être utilisée pour appliquer un algorithme probabiliste de correspondance. Voici quelques informations basiques, mais vous pouvez en savoir plus en tapant `?fastLink` dans la console.  

* Définir les deux dataframes à comparer grâce aux arguments `dfA = ` et `dfB = `.  
* Dans `varnames = `, indiquer les noms de toutes les colonnes à utiliser pour la comparaison. Ces colonnes doivent exister à la fois dans `dfA` et `dfB`.  
* Dans `stringdist.match = `, donner les colonnes sur lesquelles effectuer le calcul de la distance de similarité entre les chaînes de caractère (ce ou ces colonnes doivent être présentes dans  `varnames`).  
* Dans `numeric.match = `, donner les colonnes sur lesquelles calculer une mesure de distance numérique (ce ou ces colonnes doivent être présentes dans  `varnames`).  
* Les valeurs manquantes sont ignorées.  
* Par défaut, chaque ligne de l'un des deux dataframes est comparée à une ligne au maximum de l'autre dataframe. Si vous voulez voir toutes les correspondances évaluées, choisissez `dedupe.matches = FALSE`. La dé-duplication est faite en utilisant la technique de programmation linéaire de Winkler. 


*Astuce : divisez une colonne de date en trois colonnes numériques distinctes en utilisant `day()`, `month()`, et `year()` du package **lubridate***.  

Le seuil par défaut pour les correspondances est de 0.94 (`threshold.match = `) mais il peut être ajusté. Un seuil plus élevé peut produire plus de faux négatifs (des lignes qui ne correspondent pas alors qu'elles devraient correspondre) et un seuil plus bas peut produire plus de faux positifs.  

Ci-dessous, les données sont comparées sur la base de la distance de similarité entre les chaînes de caractères dans les colonnes du nom et du district, et sur la base de la distance numérique pour l'année, le mois et le jour de naissance. Un seuil de correspondance de 95% de probabilité est fixé.  


```{r, message=F, warning=F}
fl_output <- fastLink::fastLink(
  dfA = cases,
  dfB = results,
  varnames = c("gender", "first", "middle", "last", "yr", "mon", "day", "district"),
  stringdist.match = c("first", "middle", "last", "district"),
  numeric.match = c("yr", "mon", "day"),
  threshold.match = 0.95)
```


**Vérification des correspondance**  

L'objet `fl_output` contient la sortie de la fonction `fastLink()`. Cet objet est une liste (classe `list`) qui contient plusieurs dataframes détaillant les résultats de l'analyse des correspondances. Le dataframe `matches` contient les correspondances les plus probables entre `cases` et `results`. On peut y accéder avec la commande `fl_output$matches`. Ci-dessous, nous l'enregistrons sous le nom de `my_matches` pour faciliter son accès ultérieur.    

Le dataframe `my_matches` contient deux colonnes contenant les numéros de lignes/indices (aussi appelés "rownames") de `cases` ("inds.a") et de `results` ("inds.b"), représentant les meilleures correspondances. Si un numéro de ligne d'un dataframe est manquant, alors aucune correspondance n'a été trouvée dans l'autre dataframe au seuil de correspondance spécifié.    


```{r}
# print matches
my_matches <- fl_output$matches
my_matches
```

On observe que :  

* Les correspondances ont été trouvées malgré les légères différences dans l'orthographe des noms et les dates de naissance, c'est la beauté de ce type d'approche :  
  * "Tony B. Smith" correspond à "Anthony B. Smith".  
  * "Maria Rodriguez" correspond à "Marialisa Rodrigues".  
  * "Betty Chase" correspond à "Elizabeth Chase".  
  * "Olivier Laurent De Bordeaux" correspond à "Oliver Laurent De Bordow" (date de naissance manquante ignorée).  
* Une ligne de `cases` (pour "Blessing Adebayo", ligne 9) n'avait pas de bonne correspondance dans `results`, elle n'est donc pas présente dans `my_matches`.  


**Jointure basée sur les correspondances probabilistes**.  

Pour utiliser ces correspondances afin de joindre les `results` aux `cases`, une stratégie consiste à :  

1) Utiliser `left_join()` pour joindre `my_matches` à `cases` (en faisant correspondre les rownames dans `cases` à "inds.a" dans `my_matches`)  
2) Utiliser ensuite un autre `left_join()` pour joindre `results` à `cases` (en faisant correspondre les "inds.b" nouvellement acquis dans `cases` aux noms de domaine dans `results`).  

Avant les jointures, nous devons nettoyer les trois dataframes :  

* Les numéros de ligne ("rowname") de `dfA` et `dfB` doivent être convertis en une colonne.    
* Les deux colonnes de `my_matches` sont converties en chaînes de caractères, donc elles peuvent être jointes aux caractères rownames.  



```{r}
# Préparation des dataframes avant la jointure
#############################

# Covertir les numéros de lignes en colonne
cases_clean   <- cases %>% rownames_to_column()
results_clean <- results %>% rownames_to_column()  

# Convertir toutes les colonnes du dataframe des correspondances en texte pour pouvoire les joindre aux numéros de ligne
matches_clean <- my_matches %>%
  mutate(across(everything(), as.character))



# Joindre `clean_matches` à dfA, puis ajouer dfB
###################################
# la colonne "inds.b" est ajoutée à dfA
complete <- left_join(cases_clean, matches_clean, by = c("rowname" = "inds.a"))

# les colonnes de dfB sont rappatriées 
complete <- left_join(complete, results_clean, by = c("inds.b" = "rowname"))
```

Le dataframe `complete` ainsi crée contient *toutes* les colonnes de `cases` et de `results`. Beaucoup d'entre elles se retrouvent ajoutées avec les suffixes ".x" et ".y", parce que les noms des colonnes seraient dupliqués sinon.  


```{r message=FALSE, echo=F}
DT::datatable(complete, rownames = FALSE, options = list(pageLength = nrow(complete), scrollX=T), class = 'white-space: nowrap')
```

Pour obtenir seulement les 9 observations "originales" dans `cases` avec la ou les nouvelles colonnes de `results`, utiliser `select()` sur `results` avant les jointures, de sorte que e dataframe ne contienne que les rownames et les colonnes que vous voulez ajouter à `cases` (par exemple la colonne `result`).  


```{r}
cases_clean <- cases %>% rownames_to_column()

results_clean <- results %>%
  rownames_to_column() %>% 
  select(rowname, result)    # Sélectionner certaines colonnes

matches_clean <- my_matches %>%
  mutate(across(everything(), as.character))

# jointure
complete <- left_join(cases_clean, matches_clean, by = c("rowname" = "inds.a"))
complete <- left_join(complete, results_clean, by = c("inds.b" = "rowname"))
```


```{r message=FALSE, echo=F}
DT::datatable(complete, rownames = FALSE, options = list(pageLength = nrow(complete), scrollX=T), class = 'white-space: nowrap')
```

Si vous voulez ne garder que les lignes qui avec des correspondances dans l'un ou l'autre des des dataframe, vous pouvez utiliser les codes ci-dessous :  

```{r}
cases_matched <- cases[my_matches$inds.a,]  # Lignes de  `cases` qui matchent une ligne dans 'results'
results_matched <- results[my_matches$inds.b,]  # Lignes de 'results qui matchent une ligne dans `cases`
```

Ou pour ne voir que les lignes **sans correspondances** :  

```{r}
cases_not_matched <- cases[!rownames(cases) %in% my_matches$inds.a,]  # Lignes dans `cases` sans matchs dans `results`
results_not_matched <- results[!rownames(results) %in% my_matches$inds.b,]  # Lignes dans  `results` sans matchs dans `cases`
```


### Déduplication probabiliste {.unnumbered}  

La correspondance probabiliste peut également être utilisée pour dé-dupliquer un jeu de données. Voir la page sur la dé-duplication pour d'autres méthodes de dé-duplication.  

Ici, nous modifions le tableau `cases`, en ajoutant des lignes supplémentaires qui peuvent être des doublons de lignes existantes, et l'appelons `cases_dup`; voir "Tony" avec "Anthony", et "Marialisa Rodrigues" avec "Maria Rodriguez".  


```{r, echo=F}
## Ajout de duplicats
#cases_dup <- rbind(cases, cases[sample(1:nrow(cases), 3, replace = FALSE),])

cases_dup <- tribble(
  ~gender, ~first,      ~middle,     ~last,        ~yr,   ~mon, ~day, ~district,
  "M",     "Amir",      NA,          "Khan",       1989,  11,   22,   "River",
  "M",     "Anthony",   "B.",        "Smith",      1970, 09, 19,      "River", 
  "F",     "Marialisa", "Contreras", "Rodrigues",  1972, 04, 15,      "River",
  "F",     "Elizabeth", "Casteel",   "Chase",      1954, 03, 03,      "City",
  "M",     "Jose",      "Sanchez",   "Lopez",      1996, 01, 06,      "City",
  "F",     "Cassidy",   "Jones",      "Davis",     1980, 07, 20,      "City",
  "M",     "Michael",   "Murphy",     "O'Calaghan",1969, 04, 12,      "Rural", 
  "M",     "Oliver",    "Laurent",    "De Bordow" , 1971, 02, 04,     "River",
  "F",      "Blessing",  NA,          "Adebayo",   1955,  02, 14,     "Rural",
  "M",     "Tony",   "B.",        "Smith",         1970, 09, 19,      "River", 
  "F",     "Maria",  "Contreras", "Rodriguez",     1972, 04, 15,      "River",
)

```

```{r message=FALSE, echo=F}
DT::datatable(cases_dup, rownames = FALSE, options = list(pageLength = nrow(cases_dup)))
```

On peut désormais utiliser la fonction `fastLink()` comme précédemment, mais en comparant le jeu de données `cases_dup` à lui-même. Lorsque les dataframes fournis en argument sont identiques, la fonction suppose que vous voulez dé-dupliquer. 

Notez que nous ne spécifions pas `stringdist.match =` ou `numeric.match =` comme nous le faisions précédemment.  


```{r, message = F, warning = F}
## Utiliser fastLink sur le même jeu de données
dedupe_output <- fastLink(
  dfA = cases_dup,
  dfB = cases_dup,
  varnames = c("gender", "first", "middle", "last", "yr", "mon", "day", "district")
)
```

La fonction `getMatches()` permet d'examiner la sortie de `fastLink` pour rechercher les doublons potentiels.
Il faut fournir le dataframe d'origine à `dfA = ` *et* `dfB = `, ainsi que la sortie de  `fastLink()` à `fl.out = `.  

Note : `fl.out` doit nécessairement être de la classe `fastLink.dedupe`, ou en d'autres termes, le résultat de `fastLink()`.  


```{r}
## Executer getMatches()
cases_dedupe <- getMatches(
  dfA = cases_dup,
  dfB = cases_dup,
  fl.out = dedupe_output)
```


La colonne la plus à droite indique les identifiants des doublons. Ici les deux dernières lignes sont identifiées comme étant des doublons probables des lignes 2 et 3.  


```{r message=FALSE, echo=F}
DT::datatable(cases_dedupe, rownames = FALSE, options = list(pageLength = nrow(cases_dedupe)))
```

Pour obtenir les numéros de ligne des lignes qui sont potentiellement des doublons, il suffit de compter le nombre de lignes par valeur unique dans la colonne `dedupe.ids`, puis de filtrer pour ne garder que celles qui ont plus d'une ligne. Dans ce cas, nous obtenons laisse les lignes 2 et 3.  


```{r}
cases_dedupe %>% 
  count(dedupe.ids) %>% 
  filter(n > 1)
```

Pour inspecter les doublons, on peut utiliser le numéro de ligne pour obtenir la ligne complète :  

```{r}
# Afficher la ligne 2 et ses duplicats probables
cases_dedupe[cases_dedupe$dedupe.ids == 2,]   
```



## Assembler et aligner des dataframes

Une autre manière de combiner deux dataframes consiste à les assembler/concaténer/coller/aligner. On peut également considérer cette méthode comme un ajout de lignes ou de colonnes.  

Cette section explique également comment "aligner" l'ordre des lignes d'un dataframe sur celui d'un autre dataframe. Ce sujet est abordé ci-dessous dans la section consacrée à la liaison des colonnes. 


### Assembler verticallement {.unnumbered}

La fonction `bind_rows()` de **dplyr** permet de coller les lignes d'un dataframe à la suite d'un autre dataframe (verticalement, donc). Elle est très inclusive : toute colonne présente dans l'un ou l'autre des dataframes sera incluse dans la sortie. 

Quelques notes :  

* Contrairement à la version **base** de R `row.bind()`, **dplyr**'s `bind_rows()` n'exige pas que l'ordre des colonnes soit le même dans les deux dataframes à assembler. Du moment que les noms des colonnes sont orthographiés de manière identique, la fonction les alignera correctement.  

* Il est possible de fournir une chaîne de caractères à l'argument `.id = ` pour produire une  nouvelle colonne qui servira à identifier de quel dataframe chaque ligne provient à l'origine.  

* Il est possible d'utiliser `bind_rows()` sur une liste de dataframes possédant la même structure pour les combiner en un seul. Vous trouverez un exemple de cette action dans la page [Itération, boucles et listes](#iteration) qui importe plusieurs linelists avec le paquet **purrr**.  


Un exemple classique d'assemblage de dataframes est d'ajouter une ligne "total" à un tableau descriptif créé avec la fonction `summarise()` de **dplyr**. Ci-dessous, nous créons un tableau des nombres de cas et des valeurs médianes de CT par hôpital et y ajoutons une ligne de total.  

La fonction `summarise()` est utilisée sur des données groupées par hôpital pour retourner un dataframe récapitulatif par hôpital. Malheureusement, elle ne produit pas automatiquement une ligne de "totaux", donc nous devons la rajouter nous même. Nous l'obtenons en résumant *à nouveau* ces données, mais sans grouper par hôpital. Cela produit un deuxième dataframe d'une seule ligne, que nous concaténons ensuite au premier  pour obtenir le tableau final.  

Vous trouverez d'autres exemples de ce type dans les pages [Tableaux descriptifs](#descriptive_tables) et [Tableaux de présentation](#presentation_tables).  


```{r}
# Créer la table de base
###################
hosp_summary <- linelist %>% 
  group_by(hospital) %>%      # grouper les données par hôpital
  summarise(                  # Créer résumé :
    cases = n(),               # NNombre de lignes par hôpital
    ct_value_med = median(ct_blood, na.rm=T))     # Médiane du CT par hôpital
```

Voici à quoi ressemble `hosp_summary` :  

```{r message=FALSE, echo=F}
DT::datatable(hosp_summary, rownames = FALSE, options = list(pageLength = nrow(10)))
```

Maintenant nous créons un dataframe d'une seule ligne contenant les statistiques pour tous les hôpitaux (données _non groupées_) :   

```{r}
# Créer la ligne de totaux
###############
totals <- linelist %>% 
  summarise(
    cases = n(),                               # Nb lignes dataframe entier
    ct_value_med = median(ct_blood, na.rm=T))  # Médiane du CT
```

Ci-dessous, le dataframe `totals`. Remarquez qu'il n'y a que deux colonnes alors que `hosp_summary` en contenant trois. Ce n'est pas un problème.

```{r message=FALSE, echo=F}
DT::datatable(totals, rownames = FALSE, options = list(pageLength = nrow(10)))
```

Nous utilisons à présent `bind_rows()` pour assembler les deux dataframes :  

```{r}
# Combiner les deux dataframes ensemble
combined <- bind_rows(hosp_summary, totals)
```

Pour le résultat ci-dessous. Dans la dernière ligne, une valeur vide `NA` a été automatiquement insérée dans la colonne `hospital`, qui n'était pas dans `hosp_summary`. Comme expliqué dans la page [Tableaux de présentation](#presentation_tables), il est possible de "remplir" cette cellule avec "Total" en utilisant `replace_na()`.  


```{r message=FALSE, echo=F}
DT::datatable(combined, rownames = FALSE, options = list(pageLength = nrow(10)))
```



### Assembler des colonnes latéralement {.unnumbered}

De manière assez similaire, il existe une fonction `bind_cols()` dans **dplyr** qui combine deux dataframes latéralement. **En revanche, contrairement aux jointures, les lignes sont alignées les unes aux autres par position** : la ligne X du dataframe 1 sera alignée à la ligne X du dataframe 2.

Par exemple, nous allons assembler plusieurs tableaux récapitulatifs. Nous montrerons au passage comment réarranger l'ordre des lignes d'un dataframe pour qu'il corresponde à l'ordre dans un autre dataframe, à l'aide de la fonction `match()`.    

Ici, nous définissons `case_info` comme un dataframe récapitulatif des cas de la liste linéaire, par hôpital, avec le nombre de cas et le nombre de décès.


```{r}
# Information sur les cas 
case_info <- linelist %>% 
  group_by(hospital) %>% 
  summarise(
    cases = n(),
    deaths = sum(outcome == "Death", na.rm=T)
  )
```

```{r message=FALSE, echo=F}
DT::datatable(case_info, rownames = FALSE, options = list(pageLength = nrow(10)))
```

Nous définissons également `contact_fu`, un autre dataframe contenant des informations sur le pourcentage de contacts exposés ayant fait l'objet d'une enquête et d'un "suivi", toujours par hôpital.  

```{r}
contact_fu <- data.frame(
  hospital = c("St. Mark's Maternity Hospital (SMMH)", "Military Hospital", "Missing", "Central Hospital", "Port Hospital", "Other"),
  investigated = c("80%", "82%", NA, "78%", "64%", "55%"),
  per_fu = c("60%", "25%", NA, "20%", "75%", "80%")
)
```

```{r message=FALSE, echo=F}
DT::datatable(contact_fu, rownames = FALSE, options = list(pageLength = nrow(10)))
```

Notez que les hôpitaux sont les mêmes, mais dans un ordre différent dans les deux dataframes. La solution la plus simple serait d'utiliser un `left_join()` sur la colonne `hospital`, mais il est aussi possible d'utiliser `bind_cols()` avec une étape supplémentaire.  


#### Utiliser `match()` pour homogénéiser l'ordre des lignes {.unnumbered}  

Comme l'ordre des lignes est différent, un simple `bind_cols()` entraînerait une mauvaise correspondance des données. Pour résoudre ce problème, nous pouvons utiliser la fonction `match()` de **base** R pour aligner les lignes d'un dataframes dans le même ordre que dans un autre. Pour cette approche, nous supposons qu'il n'y a pas de doublons dans les dataframes.  

Lorsque nous utilisons `match()`, la syntaxe est `match(vecteur/colonne dans l'ordre désiré, colonne de dataframe à ordonner)`, où le premier argument est l'ordre souhaité (soit un vecteur autonome, soit une colonne d'un dataframe), et le second argument est la colonne du dataframe qui sera réordonnée. La sortie de `match()` est un vecteur de nombres représentant l'ordre correct des positions. Vous pouvez en savoir plus avec `?match`. 


```{r}
match(case_info$hospital, contact_fu$hospital)
```

On peut utiliser ce vecteur numérique pour réorganiser le dataframe : placez-le entre des crochets `[ ]` *avant la virgule*. Pour en savoir plus sur la syntaxe des crochets pour séléctionner les lignes et/ou colonnes d'un dataframe en **base** R, consultez la page [Bases de R](#rbasics). La commande ci-dessous crée un nouveau dataframe, défini comme l'ancien dataframe dans lequel les lignes sont ordonnées selon le tableau numérique ci-dessus. 

```{r}
contact_fu_aligned <- contact_fu[match(case_info$hospital, contact_fu$hospital),]
```


```{r message=FALSE, echo=F}
DT::datatable(contact_fu_aligned, rownames = FALSE, options = list(pageLength = nrow(10)))
```

Maintenant nous pouvons lier les colonnes du dataframe ensemble, avec l'ordre correct des lignes respecté. Notez que certaines colonnes sont dupliquées et devront être nettoyées avec `rename()`. Pour en savoir plus sur `bind_rows()`, rendez-vous [ici](https://dplyr.tidyverse.org/reference/bind.html).  


```{r}
bind_cols(case_info, contact_fu)
```

Une alternative **base** R à `bind_cols` est `cbind()`, qui effectue la même opération.  




<!-- ======================================================= -->
## Resources { }

La [page du tidyverse sur les jointures](https://dplyr.tidyverse.org/reference/join.html)  

Le [chapitre sur les données relationelles dans R for Data Science](https://r4ds.had.co.nz/relational-data.html)  

La [page de dplyr sur bind](https://dplyr.tidyverse.org/reference/bind.html) on binding  

Une vignette sur[fastLink](https://github.com/kosukeimai/fastLink) sur la page Github du paquet

Publication décrivant la méthodologie de [fastLink](https://imai.fas.harvard.edu/research/files/linkage.pdf)  

Publication décrivant le paquet[RecordLinkage](https://journal.r-project.org/archive/2010/RJ-2010-017/RJ-2010-017.pdf)




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/joining_matching.Rmd-->


# De-duplication {#deduplication}  

```{r, out.width=c("50%"), echo=F}
knitr::include_graphics(here::here("images", "deduplication.png"))
```

Cette page couvre les techniques de déduplication ci-dessous : 

1. Identifier et supprimer les lignes dupliquées  
2. Utiliser la fonction "slice" pour garder seulement certaines lignes (par exemple, min ou max) de chaque groupe de lignes.  
3. "Rolling-up", ou combinaison des valeurs de plusieurs lignes en une seule ligne.  


<!-- ======================================================= -->
## Préparation { }


### Importation des packages {.unnumbered}

Ces lignes de code importe les packages necessaire pour l'analyse. Dans ce guide, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le package si nécessaire *puis* l'importe pour l'utiliser. Vous pouvez également charger les packages installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les packages en R. 

```{r}
pacman::p_load(
  tidyverse,   # fonctions de déduplication, de regroupement et de slicing
  janitor,     # fonction de gestion des doublons 
  stringr)     # pour la recherche des caractères, peut être utilisé ensembler les valeurs
```

### Importer les données {.unnumbered}

Pour la démonstration, nous allons utiliser un ensemble de données exemplaire qui a été créé avec le code R ci-dessous.   

Les données sont des enregistrements des appels téléphoniques sur le COVID-19, y compris les appels avec des contacts et des cas. Les colonnes comprennent `recordID` (généré par ordinateur), `personID`, `name`, `date` de la rencontre, `time` de la rencontre, le `purpose` de la rencontre (soit pour un interview en tant que cas ou en tant que contact), et `symptoms_ever` (si la personne dans cette appel a déclaré avoir *jamais* eu des symptômes).   

Voici le code pour créer la base de données `obs` : 

```{r}
obs <- data.frame(
  recordID  = c(1,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18),
  personID  = c(1,1,2,2,3,2,4,5,6,7,2,1,3,3,4,5,5,7,8),
  name      = c("adam", "adam", "amrish", "amrish", "mariah", "amrish", "nikhil", "brian", "smita", "raquel", "amrish",
                "adam", "mariah", "mariah", "nikhil", "brian", "brian", "raquel", "natalie"),
  date      = c("1/1/2020", "1/1/2020", "2/1/2020", "2/1/2020", "5/1/2020", "5/1/2020", "5/1/2020", "5/1/2020", "5/1/2020","5/1/2020", "2/1/2020",
                "5/1/2020", "6/1/2020", "6/1/2020", "6/1/2020", "6/1/2020", "7/1/2020", "7/1/2020", "7/1/2020"),
  time      = c("09:00", "09:00", "14:20", "14:20", "12:00", "16:10", "13:01", "15:20", "14:20", "12:30", "10:24",
                "09:40", "07:25", "08:32", "15:36", "15:31", "07:59", "11:13", "17:12"),
  encounter = c(1,1,1,1,1,3,1,1,1,1,2,
                2,2,3,2,2,3,2,1),
  purpose   = c("contact", "contact", "contact", "contact", "case", "case", "contact", "contact", "contact", "contact", "contact",
                "case", "contact", "contact", "contact", "contact", "case", "contact", "case"),
  symptoms_ever = c(NA, NA, "No", "No", "No", "Yes", "Yes", "No", "Yes", NA, "Yes",
                    "No", "No", "No", "Yes", "Yes", "No","No", "No")) %>% 
  mutate(date = as.Date(date, format = "%d/%m/%Y"))
```


#### Voici le tableau de données {#dedup_data .unnumbered}  

Utilisez les boîtes de filtre au-dessous pour examiner les rencontres pour chaque personne.  

```{r message=FALSE, echo=F}
DT::datatable(obs, rownames = FALSE, filter = "top", options = list(pageLength = nrow(obs), scrollX=T), class = 'white-space: nowrap' )
```


Quelques éléments à noter lors de l'examen des données :  

* Les deux premiers enregistrements sont des doublons complets à 100%, y compris le `recordID` ( cela doit être un problème informatique !).  
* Les deux secondes lignes sont des doublons, dans toutes les colonnes, sauf pour le `recordID`.  
* Plusieurs personnes ont été contactées plusieurs fois par téléphone, à des dates et horaires différents, et en tant que contacts et/ou cas.  
* A chaque rencontre, il a été demandé à la personne si elle avait **jamais** eu des symptômes, et certaines de ces informations sont manquantes.  


Et voici un résumé de ces personnes et la raison de leurs rencontres, en utilisant `tabyl()` de **janitor** :  

```{r}
obs %>% 
  tabyl(name, purpose)
```
<!-- ======================================================= -->
## Deduplication { }


Cette section décrit comment examiner et supprimer les doublons dans un tableau de données. Elle montre également comment traiter les éléments dupliqués dans un vecteur.  


<!-- ======================================================= -->
### Examiner les lignes dupliquées {.unnumbered}  


Pour rapidement examiner les lignes qui ont été dupliquées, vous pouvez utiliser `get_dupes()` du package **janitor**. *Par défaut*, toutes les colonnes sont prises en compte lors de l'évaluation des duplications - les lignes retournées par la fonction sont des doublons à 100% en considérant les valeurs de *toutes* les colonnes.  

Dans le tableau de données `obs`, les deux premières lignes sont *100% dupliquées* - elles ont la même valeur dans chaque colonne (y compris la colonne `recordID`, qui est *supposée* être unique - cela doit être un problème informatique). Le tableau de données obtenu inclut automatiquement une nouvelle colonne `dupe_count` sur le côté droit, montrant le nombre de lignes avec cette combinaison de valeurs en double. 

```{r, eval=F}
# 100% duplicates across all columns
obs %>% 
  janitor::get_dupes()
```

```{r message=FALSE, echo=F}
obs %>% 
  janitor::get_dupes() %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = nrow(obs), scrollX=T), class = 'white-space: nowrap' )
```

Voir les [données originales](#dedup_data)  

Cependant, si nous décidons d'ignorer le `recordID`, les 3e et 4e lignes sont également des doublons entre eux. C'est-à-dire qu'elles ont les mêmes valeurs dans toutes les colonnes *sauf* pour `recordID`. Vous pouvez spécifier des colonnes spécifiques à ignorer dans la fonction en utilisant le symbole moins `-`.   

```{r, eval=F}
# Duplications lorsque la colonne recordID est exclue. 
obs %>% 
  janitor::get_dupes(-recordID)         # si multiples colonnes, les inclure dans c()
```

```{r message=FALSE, echo=F}
obs %>% 
  janitor::get_dupes(-recordID) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = nrow(obs), scrollX=T), class = 'white-space: nowrap' )
```

Vous pouvez également spécifier les colonnes à considérer. Ci-dessous, seules les lignes qui ont les mêmes valeurs dans les colonnes `name` et `purpose` sont retournées. Notez comment "amrish" a maintenant un `dupe_count` égal à 3 pour correspondre à ses trois rencontres "contact".    

*Défiler vers la gauche pour plus de lignes**  

```{r, eval=F}
# duplications basées sur les colonnes name et purpose uniquement
obs %>% 
  janitor::get_dupes(name, purpose)
```

```{r message=FALSE, echo=F}
obs %>% 
  janitor::get_dupes(name, purpose) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 7, scrollX=T), class = 'white-space: nowrap' )
```

Voir les [données originales](#dedup_data) 

Voir `?get_dupes` pour plus de details, ou consulter ceci [référence en ligne](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#explore-records-with-duplicated-values-for-specific-combinations-of-variables-with-get_dupes)  






<!-- ======================================================= -->
### Garder seulement les lignes uniques {.unnumbered}


Pour garder que les lignes uniques d'un tableau de données, utilisez `distinct()` de **dplyr** (comme démontré dans la page [CNettoyage de données et fonctions essentielles](#cleaning_data)). Les lignes qui sont dupliquées sont enlevées de sorte que seule la première ligne est retenue. Par défaut, la première ligne correspond au plus grand `rownumber` (ordre des lignes de haut en bas). Seules les lignes uniques sont retenues.  

Dans l'exemple ci-dessous, nous utilisons `distinct()` tel que la colonne `recordID` est exclue - ainsi **deux lignes dupliquées sont enlevées**. La première ligne (pour "adam") était dupliquée à 100% et a été enlevée. Par ailleurs, la troisième ligne (pour "amrish") était dupliquée dans chaque colonne *sauf* `recordID` (qui n'est pas considéré) donc a été supprimée. Le tableau de données `obs` est maintenant `nrow(obs)-2`, et non `nrow(obs)` lignes).  

*Défilez vers la gauche pour voir le tableau de données complet* 


```{r, eval=F}
# ajouté à une chaîne de pipes (par exemple, nettoyage de données)
obs %>% 
  distinct(across(-recordID), # réduit le tableau de données à seulement des lignes uniques (retient la première ligne de toute duplication)
           .keep_all = TRUE) 

# si en dehors des pipes, inclure les données comme premier argument  
# distinct(obs)
```

```{r message=FALSE, echo=F}
obs %>% 
  distinct(across(-recordID), # réduit le tableau de données à seulement des lignes uniques (retient la première ligne de toute duplication)
           .keep_all = TRUE) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 6, scrollX=T), class = 'white-space: nowrap' )
```

<span style="color: orange;">**_CAUTION:_** Si vous utilisez `distinct()` sur des données groupées, la fonction s'appliquera à chaque groupe. </span>


**Déduplication basée sur des colonnes spécifiques**  

Vous pouvez également spécifier des colonnes qui seront la base de la déduplication. Ainsi, la déduplication ne s'applique qu'aux lignes qui sont des duplications dans les colonnes spécifiées. A moins que vous ne définissiez `.keep_all = TRUE`, toutes les colonnes non mentionnées seront ignorées.    

Dans l'exemple ci-dessous, la déduplication ne s'applique qu'aux lignes qui ont des valeurs identiques pour les colonnes `name` et `purpose`. Ainsi, "brian" a seulement 2 lignes au lieu de 3 - son *premier* "contact" et son unique "case". Pour ajuster afin que la *dernière* rencontre de brian pour chaque "purpose" soit retenue, voir l'onglet "Slicing within groups".  

*Défilez vers la gauche pour voir le tableau de données complet*  

```{r, eval=F}
# ajouté à une chaîne de pipes (par exemple, nettoyage de données)
obs %>% 
  distinct(name, purpose, .keep_all = TRUE) %>%  # garder les lignes uniques par 'name' et par 'purpose', retient toutes les colonnes
  arrange(name)                                  # arranger pour faciliter la visualisation
```

```{r message=FALSE, echo=F}
obs %>% 
  distinct(name, purpose, .keep_all = TRUE) %>%  # garder les lignes uniques par 'name' et par 'purpose', retient toutes les colonnes
  arrange(name) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 6, scrollX=T), class = 'white-space: nowrap' )
```

Voir [données originales](#dedup_data).  

<!-- ======================================================= -->
### Dédupliquer les éléments d'un vecteur {.unnumbered} 


La fonction `duplicated()` de **base** R va évaluer un vecteur (colonne) et renvoie un vecteur logique de même longueur (TRUE/FALSE). La première fois qu'une valeur apparaît, elle renvoie FALSE (pas de duplication), et les fois suivantes, elle renvoie VRAI. Notez que `NA` est traité de la même façon que toute autre valeur. 

```{r}
x <- c(1, 1, 2, NA, NA, 4, 5, 4, 4, 1, 2)
duplicated(x)
```

Pour ne retourner que les éléments dupliqués, vous pouvez utiliser des parenthèses pour sous-titrer le vecteur original : 

```{r}
x[duplicated(x)]
```

Pour ne renvoyer que les éléments uniques, utilisez `unique()` de **base** R. Pour supprimer les `NA` de la sortie, mettez `na.omit()` dans `unique()`.  

```{r}
unique(x)           # alternativement, utilisez x[!duplicated(x)]
unique(na.omit(x))  # supprimez les NA 
```


<!-- ======================================================= -->
### Utilisant **base** R {.unnumbered}

**Pour retourner les lignes dupliquées**  

Dans **base** R, vous pouvez également voir quelles lignes sont dupliquées à 100% dans un tableau de données `df` avec la commande `duplicated(df)` ( retourne un vecteur logique des lignes).  

Ainsi, vous pouvez également utiliser le sous-groupe de base `[ ]` sur le tableau de données pour voir les lignes *dupliquées* avec `df[duplicated(df),]` (n'oubliez pas la virgule, qui signifie que vous voulez voir toutes les colonnes !) 

**Pour retourner les lignes uniques**  

Voir les notes ci-dessus. Pour voir les lignes *uniques*, ajoutez le négateur logique `!` devant la fonction `duplicated()` :  
`df[!duplicated(df),]`   


**Pour retourner les lignes qui sont des duplications de certaines colonnes seulement**.  

Sous-ensembler le `df` qui se trouve *dans la parenthèse de `duplicated()`, afin que cette fonction ne traite que certaines colonnes du `df`.  

Pour spécifier les colonnes, fournissez les numéros ou les noms des colonnes après une virgule (rappelez-vous, tout ceci est *dans* la fonction `duplicated()`).  

Assurez-vous de garder la virgule `,` *à l'extérieur* après la fonction `duplicated()` également ! 

Par exemple, pour évaluer seulement les colonnes 2 à 5 pour les doublons :  `df[!duplicated(df[, 2:5]),]``  
Pour évaluer seulement les colonnes `name` et `purpose` pour les doublons : `df[!duplicated(df[, c("name", "purpose)]),]`  





<!-- ======================================================= -->
## Slicing { }


Pour "slice" un tableau de données pour pouvoir appliquer un filtre sur les lignes par numéro/position de ligne. Cette fonction devient particulièrement utile si vous avez plusieurs lignes par groupe fonctionnel (par exemple, par "person") et que vous ne voulez retenir qu'une ou quelques-unes d'entre elles.  

La fonction de base `slice()` accepte des numéros et retourne les lignes dans ces positions. Si les numéros fournis sont positifs, seuls ceux-ci sont retournés. S'ils sont négatifs, ces lignes ne sont *pas* retournées. Les nombres doivent être soit tous positifs, soit tous négatifs. 

```{r}
obs %>% slice(4)  # retourne la 4e ligne
```

```{r}
obs %>% slice(c(2,4))  # retourne les lignes 2 et 4
#obs %>% slice(c(2:4))  # retourne les lignes 2 à 4
```


Voir [données originales](#dedup_data). 

Il existe plusieurs variations :  Celles-ci doivent être fournies avec une colonne et le nombre de lignes à retourner (à `n = `).  

* `slice_min()` et `slice_max()` ne gardent que la ou les lignes avec la ou les valeurs minimales ou maximales de la colonne spécifiée. Cela permet également de retourner le "min" et le "max" de facteurs ordonnés. 
* `slice_head()` et `slice_tail()` - Retient que la ou les *premières* ou *dernières* lignes.  
* `slice_sample()` - ne retenir qu'un échantillon aléatoire des lignes.   


```{r}
obs %>% slice_max(encounter, n = 1)  # Retourne les lignes avec le plus grand nombre de "encounter" 
```

Utilisez les arguments `n = ` ou `prop = ` pour spécifier le nombre ou la proportion de lignes à retenir. Si vous n'utilisez pas la fonction dans un pipe, fournissez d'abord l'argument data (par exemple, `slice(data, n = 2)`). Voir `?slice` pour plus d'informations. 

Autres arguments :   

`.order_by = ` utilisé dans `slice_min()` et `slice_max()` ceci est une colonne à ordonner par avant de "slice".  
`with_ties = ` TRUE par défaut, ce qui signifie que les liens sont retenus.  
`.preserve = ` FALSE par défaut. Si TRUE, alors la structure de regroupement est recalculée après le slicing.  
`weight_by = ` Optionnel, colonne numérique pour pondérer par ( un plus grand chiffre est plus probable d'être échantillonné).  
Aussi, `replace = ` pour savoir si l'échantillonnage est fait avec/sans remplacement.   

<span style="color: darkgreen;">**_TIP:_** En utilisant `slice_max()` et `slice_min()`, assurez-vous de spécifier/d'écrire `n = `  (e.g. `n = 2`, pas seulement `2`). Sinon, vous risquez d'obtenir une erreur: `Error: `...` is not empty.` </span>

<span style="color: black;">**_NOTE:_** Vous pouvez rencontrer la fonction [`top_n()`](https://dplyr.tidyverse.org/reference/top_n.html), qui a été remplacées par les fonctions `slice`.</span>

 


<!-- ======================================================= -->
### Slice avec les groupes  {.unnumbered}

Les fonctions `slice_*()` peuvent être très utiles si elles sont appliquées à un tableau de données groupées, puisque l'opération de slice est effectuée sur chaque groupe séparément. Utilisez la **fonction** `group_by()` en conjonction avec `slice()` pour regrouper les données et prendre une tranche de chaque groupe.  

Ceci est utile pour la déduplication si vous avez plusieurs lignes par personne mais que vous ne voulez retenir qu'une seule d'entre elles. Vous utilisez d'abord `group_by()` avec des colonnes clés qui sont les mêmes pour chaque personne, puis vous utilisez une fonction slice sur une colonne qui sera différente parmi les lignes groupées. 

Dans l'exemple ci-dessous, pour ne garder que la *dernière* rencontre *par personne*, nous regroupons les lignes par `name` et ensuite nous utilisons `slice_max()` avec `n = 1` sur la colonne `date`. Mais attention ! Pour appliquer une fonction comme `slice_max()` sur des dates, la colonne date doit être de la classe Date.   

Par défaut, les "liens" (par exemple la même date dans ce scénario) sont retenus, et nous aurions toujours plusieurs lignes pour certaines personnes (par exemple adam). Pour éviter cela, nous mettons `with_ties = FALSE`. Nous ne récupérons qu'une seule ligne par personne.  

<span style="color: orange;">**_CAUTION:_** Si utilisant `arrange()`, specifier `.by_group = TRUE` pour que les données soient organisées dans chaque groupe.</span>

<span style="color : red ;">**_DANGER:_** Si `with_ties = FALSE`, la première ligne d'une même égalité est conservée. Cela peut être déceptive. Voyez comment pour Mariah, elle a deux rencontres à sa dernière date (6 Jan) et la première (la plus ancienne) a été gardée. Il est probable que nous voulions garder la dernière rencontre de ce jour-là. Voyez comment " séparer " ces liens dans l'exemple suivant. </span> 




```{r, eval=F}
obs %>% 
  group_by(name) %>%       # regroupe les lignes par 'name'
  slice_max(date,          # retenir une ligne par groupe avec la valeur maximale de la date 
            n = 1,         # ne retenir que la ligne la plus élevée 
            with_ties = F) # s'il y a une égalité (de date), prenez le premier ligne
```

```{r message=FALSE, echo=F}
obs %>% 
  group_by(name) %>%       # regroupe les lignes par 'name'
  slice_max(date,          # retenir une ligne par groupe avec la valeur maximale de la date 
            n = 1,         # ne retenir que la ligne la plus élevée 
            with_ties = F) %>%  # s'il y a une égalité (de date), prenez le premier ligne
  DT::datatable(rownames = FALSE, options = list(pageLength = 8, scrollX=T), class = 'white-space: nowrap' )
```

Ci-dessus, par exemple, nous pouvons voir que seule la ligne d'Amrish du 5 janvier a été retenue, et que seule la ligne de Brian du 7 janvier a été retenue. Voir les [données originales](#dedup_data).   


**Séparation des égalités** 

Multiples lignes de slice peuvent être exécutées pour " séparer les égalités". Dans ce cas, si une personne a plusieurs rencontres à leur dernière *date*, la rencontre avec la dernière *heure* est retenue (`lubridate::hm()` est utilisé pour convertir les heures des caractères en une classe de temps triable).  
Notez comment maintenant, la seule ligne conservée pour "Mariah" le 6 janvier est la rencontre 3 de 08:32, et non la rencontre 2 de 07:25.  

```{r, eval=F}
# Exemple de multiple lignes de slice exécutées pour " séparer les égalités"
obs %>%
  group_by(name) %>%
  
  # PREMIEREMENT - slice par la dernière date
  slice_max(date, n = 1, with_ties = TRUE) %>% 
  
  # DEUXIÈMEMENT - s'il y a une égalité, sélectionner la ligne avec l'heure la plus tardive ; égalité interdite
  slice_max(lubridate::hm(time), n = 1, with_ties = FALSE)
```

```{r message=FALSE, echo=F}
# Exemple de multiple lignes de slice exécutées pour " séparer les égalités"
obs %>%
  group_by(name) %>%
  
  # FIRST - slice by latest date
  slice_max(date, n = 1, with_ties = TRUE) %>% 
  
  # DEUXIÈMEMENT - s'il y a une égalité, sélectionner la ligne avec l'heure la plus tardive ; égalité interdite
  slice_max(lubridate::hm(time), n = 1, with_ties = FALSE) %>% 
  
  DT::datatable(rownames = FALSE, options = list(pageLength = 8, scrollX=T), class = 'white-space: nowrap' )
```

*Dans l'exemple ci-dessus, il aurait également été possible de slice par le numéro de `encounter`, mais nous avons montré le slice sur `date` et `time` pour illustration.*  

<span style="color: darkgreen;">**_TIP:_** Pour utiliser `slice_max()` ou `slice_min()` sur une colonne e de "caractères", mutez-la en une classe de facteurs *ordonnée* !</span>.

Voir [données originales](#dedup_data).  


<!-- ======================================================= -->
### Retenir tous les lignes mais les marquer {.unnumbered}

Si vous voulez retenir tous les evenements mais n'en marquer que certains pour l'analyse, envisagez une approche en deux étapes en utilisant un numéro unique de recordID/encounter :   

1) Reduire/slice le tableau de données original pour n'avoir que les lignes à analyser. Sauvegardez/retenir ce tableau de données réduit.  
2) Dans le tableau de données original, marquez les lignes avec `case_when()`, selon que leur identifiant unique d'enregistrement (recordID dans cet exemple) est présent ou non dans le tableau de données réduit.    


```{r}
# 1. Definir les lignes de tableau de données à retenir pour l'analyse
obs_keep <- obs %>%
  group_by(name) %>%
  slice_max(encounter, n = 1, with_ties = FALSE) # ne garder que la dernière rencontre par personne


# 2. Marquer le tableau de données original
obs_marked <- obs %>%

  # Créer une nouvelle colonne dup_record
  mutate(dup_record = case_when(
    
    # si record est dans le tableau de données obs_keep
    recordID %in% obs_keep$recordID ~ "For analysis", 
    
    #tout le reste est marqué comme "Ignore" pour l'analyse
    TRUE                            ~ "Ignore"))

# imprimer
obs_marked
```


```{r, echo=F}
DT::datatable(obs_marked, rownames = FALSE, options = list(pageLength = 8, scrollX=T), class = 'white-space: nowrap' )
```

Voir [données originales](#dedup_data).   

<!-- ======================================================= -->
### Calcul de la complétude des lignes {.unnumbered} 

Créez une colonne qui contient une métrique pour la complétude des lignes (pas de valeurs manquantes). Cela peut être utile pour décider des lignes à prioriser par rapport aux autres lors de la déduplication.  

Dans cet exemple, les colonnes "clés" sur lesquelles vous voulez mesurer la complétude sont sauvegardées dans un vecteur de noms de colonnes.  

Ensuite, la nouvelle colonne `key_completeness` est créée avec `mutate()`. La nouvelle valeur dans chaque ligne est définie comme une fraction calculée : le nombre de valeurs non manquantes dans cette ligne parmi les colonnes clés, divisé par le nombre de colonnes clés.  

Cela fait appel à la fonction `rowSums()` de **base** R. On utilise également `.`, qui, dans le cadre d'un pipe, fait référence au tableau de données à ce point du pipe (dans ce cas, il est sous-ensemble avec les crochets `[]`).  

*Défiler vers la droite pour voir plus de lignes.*

```{r, eval=F}
# créer une colonne "complétude des variables clés".
# il s'agit de la *proportion* des colonnes désignées comme "key_cols" qui ont des valeurs non manquantes.

key_cols = c("personID", "name", "symptoms_ever")

obs %>% 
  mutate(key_completeness = rowSums(!is.na(.[,key_cols]))/length(key_cols)) 
```

```{r message=FALSE, echo=F}
key_cols = c("personID", "name", "symptoms_ever")

obs %>% 
  mutate(key_completeness = rowSums(!is.na(.[,key_cols]))/length(key_cols)) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Voir [données originales](#dedup_data).   




<!-- ======================================================= -->
## Réunir les valeurs de plusieurs lignes {#str_rollup}


Cette section décrit : 

1) Comment "réunir" les valeurs de plusieurs lignes en une seule ligne, avec quelques variations.  
2) Une fois les valeurs sont "réunit", comment remplacer/prioriser les valeurs dans chaque cellule.  

Cet onglet utilise le jeu de données d'exemple de l'onglet Préparation.  



<!-- ======================================================= -->
### Réunir les valeurs en une seule ligne {.unnumbered}  

L'exemple de code ci-dessous utilise `group_by()` et `summarise()` pour regrouper les lignes par personne, puis rassembler toutes les valeurs uniques dans les lignes groupées. Ainsi, vous obtenez un résumé de ligne par personne. Quelques notes :  
* Un suffixe est ajouté à toutes les nouvelles colonnes ("_roll" dans cet exemple).  
* Si vous ne voulez afficher que les valeurs uniques par cellule, enveloppez le `na.omit()` avec `unique()`.  
* `na.omit()` supprime les valeurs `NA`, mais si cela n'est pas souhaité, il peut être supprimé `paste0(.x)` ...  


```{r, eval=F}
# Réunir les valeurs en une seule ligne par groupe (par "personID") 
cases_rolled <- obs %>% 
  
  # créer des groupes par nom
  group_by(personID) %>% 
  
  # ordonner les lignes à l'intérieur de chaque groupe (par exemple par date)
  arrange(date, .by_group = TRUE) %>% 
  
  # Pour chaque colonne, rassemblez toutes les valeurs des lignes groupées, en les séparant par " ;".
  summarise(
    across(everything(),                           # appliquer à toutes les colonnes
           ~paste0(na.omit(.x), collapse = "; "))) # on définit une fonction qui combine les valeurs non-NA 
```

Le résultat est une ligne par groupe (`ID`), avec des entrées classées par date et assemblées. *Défiler vers la gauche pour voir plus de lignes*   

```{r message=FALSE, echo=F}
# Réunir les valeurs en une seule ligne par groupe (par "personID") 
obs %>% 
  
 # créer des groupes par nom
  group_by(personID) %>% 
  
  # ordonner les lignes à l'intérieur de chaque groupe (par exemple par date)
  arrange(date, .by_group = TRUE) %>% 
  
  # Pour chaque colonne, rassemblez toutes les valeurs des lignes groupées, en les séparant par " ;".
  summarise(
    across(everything(),                                 # appliquer à toutes les colonnes
           ~paste0(na.omit(.x), collapse = "; "))) %>%  # on définit une fonction qui combine les valeurs non-NA 

  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

Voir [données originales](#dedup_data).  


**Cette variation ne présente que des valeurs uniques:** 

```{r}
# Cette variation ne présente que des valeurs uniques 
cases_rolled <- obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                                   # appliquer à toutes les colonnes
           ~paste0(unique(na.omit(.x)), collapse = "; "))) # on définit une fonction qui combine les valeurs non-NA 
```

```{r message=FALSE, echo=F}
# Cette variation ne présente que des valeurs uniques 
obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                                        # appliquer à toutes les colonnes
           ~paste0(unique(na.omit(.x)), collapse = "; "))) %>%  # on définit une fonction qui combine les valeurs non-NA 

  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


**Cette variation ajoute un suffixe à chaque colonne.**  
Dans ce cas, "_roll" pour signifier qu'elle a été roulée : 

```{r, eval=F}
# Cette variation ajoute un suffixe à chaque colonne
cases_rolled <- obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                
           list(roll = ~paste0(na.omit(.x), collapse = "; ")))) # _roll est ajouté aux noms des colonnes
```

```{r message=FALSE, echo=F}
# afficher les données du linelist sous forme de tableau
# Variation - suffixe ajouté aux noms des colonnes 
obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                
           list(roll = ~paste0(na.omit(.x), collapse = "; ")))) %>%  # _roll est ajouté aux noms des colonnes
  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


<!-- ======================================================= -->
### Remplacer les valeurs/hiérarchie {.unnumbered} 


Si vous voulez ensuite évaluer toutes les valeurs reunit, et ne garder qu'une valeur spécifique (par exemple la "meilleure" ou la "valeur maximale"), vous pouvez utiliser `mutate()` sur les colonnes souhaitées, pour implémenter `case_when()`, qui utilise `str_detect()` du package **stringr** pour rechercher séquentiellement des séquences de caractères et remplacer le contenu de la cellule.  

```{r}
# CLEAN CASES
#############
cases_clean <- cases_rolled %>% 
    
    # nettoie les variables Yes-No-Unknown : remplace le texte par la valeur "la plus élevée" présente dans la séquence des caracteres
    mutate(across(c(contains("symptoms_ever")),                     # fonctionne sur les colonnes spécifiées (Y/N/U)
             list(mod = ~case_when(                                 # ajoute le suffixe "_mod" aux nouvelles cols ; implémente case_when() 
               
               str_detect(.x, "Yes")       ~ "Yes",                 # si "Yes" est détecté, alors la valeur de la cellule est convertie en Yes
               str_detect(.x, "No")        ~ "No",                  # # alors, si "No" est détecté, la valeur de la cellule est convertie en No
               str_detect(.x, "Unknown")   ~ "Unknown",             # alors, si "Unknown" est détecté, la valeur de la cellule est convertie en Unknown 
               TRUE                        ~ as.character(.x)))),   # alors, si quelque chose d'autre est retenu comme tel
      .keep = "unused")                                             # anciennes colonnes enlevées, ne laissant que des colonnes _mod
```


Maintenant vous pouvez voir dans la colonne `symptoms_ever` que si la personne a JAMAIS dit "Oui" aux symptômes, alors seul "Oui" est affiché.  

```{r message=FALSE, echo=F}
# afficher les données du linelist sous forme de tableau
DT::datatable(cases_clean, rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap')
```


Voir [données originales](#dedup_data).  


## Déduplication probabiliste  

Parfois, vous souhaitez identifier les doublons "probables" en vous basant sur la similarité (par exemple, la sequence des caractères "distance") entre plusieurs colonnes telles que le nom, l'âge, le sexe, la date de naissance, etc. Vous pouvez appliquer un algorithme de correspondance probabiliste pour identifier les doublons probables.   

Voir la page [Joindre des données](#joining_matching) pour une explication de cette méthode. La section sur l'Appariement Probabiliste contient un exemple d'application de ces algorithmes pour comparer un tableau de données à *soi-même*, effectuant ainsi une déduplication probabiliste.    



<!-- ======================================================= -->
## Ressources { }

La plupart des informations contenues dans cette page sont adaptées de ces ressources et des vignettes en ligne :  

[datanovia](https://www.datanovia.com/en/lessons/identify-and-remove-duplicate-data-in-r/)

[dplyr tidyverse reference](https://dplyr.tidyverse.org/reference/slice.html)  

[cran janitor vignette](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#explore-records-with-duplicated-values-for-specific-combinations-of-variables-with-get_dupes)  

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/deduplication.Rmd-->

# Itération, boucles et listes {#iteration}  

Les épidémiologistes sont souvent confrontés à la répétition d'analyses sur des sous-groupes tels que des pays, des districts ou des groupes d'âge. Ce ne sont là que quelques-unes des nombreuses situations impliquant l'itération. Le codage de vos opérations itératives à l'aide des approches ci-dessous vous aidera à effectuer ces tâches répétitives plus rapidement, à réduire les risques d'erreur et à réduire la longueur du code.  

Cette page présente deux approches des opérations itératives : l'utilisation de boucles *for* et l'utilisation du package **purrr**.  

1) Les boucles *for* permettent d'itérer le code sur une série d'entrées, mais sont moins courantes en R que dans d'autres langages de programmation. Néanmoins, nous les introduisons ici en tant qu'outil d'apprentissage et de référence.  
2) Le paquet **purrr** est l'approche **tidyverse** des opérations itératives - il fonctionne en "mappant" une fonction sur plusieurs entrées (valeurs, colonnes, ensembles de données, etc.).  

En cours de route, nous montrerons des exemples tels que :  

* L'importation et l'exportation de plusieurs fichiers  
* Création d'épicurves pour plusieurs juridictions  
* Exécution de tests T pour plusieurs colonnes dans un cadre de données.  

Dans la section **purrr** [section](#iter_purrr), nous fournirons également plusieurs exemples de création et de manipulation de `listes`.  



## Préparation { }
     
     
### Chargement des paquets {.unnumbered}  

Ce morceau de code montre le chargement des paquets nécessaires aux analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* le charge pour l'utiliser. Vous pouvez également charger les paquets installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les paquets R.  

```{r}
pacman::p_load(
     rio, # import/export
     here, # localisateur de fichiers
     purrr, # itération
     tidyverse, # gestion et visualisation des données
     grates
)
```


### Importer des données {.unnumbered}  

Nous importons le jeu de données des cas d'une épidémie d'Ebola simulée. Si vous voulez suivre le mouvement, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la linelist "propre"</a> (en tant que fichier .rds). Importez des données avec la fonction `import()` du paquet **rio** (elle gère de nombreux types de fichiers comme .xlsx, .csv, .rds - voir la page [Importation et exportation](#import_export) pour plus de détails).  

```{r, echo=F}
# Importez la liste de diffusion dans R
linelist <- rio::import(here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# Importez la liste de cas
linelist <- import("linelist_cleaned.rds")
```

Les 50 premières lignes de la linelist sont affichées ci-dessous.


```{r, message=FALSE, echo=F}
# affiche les données de la liste de diffusion sous forme de tableau
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space : nowrap')
```



<!-- ======================================================= -->

## *for loops* { }

### *for loops* en R {#iter_loops .unnumbered}  

Les *for loops* ne sont pas mis en avant dans R, mais sont courants dans d'autres langages de programmation. En tant que débutant, elles peuvent être utiles à l'apprentissage et à la pratique car elles sont plus faciles à "explorer", à "déboguer" et à comprendre exactement ce qui se passe à chaque itération, en particulier lorsque vous n'êtes pas encore à l'aise pour écrire vos propres fonctions.  

Vous pouvez passer rapidement des boucles *for* à l'itération avec des fonctions mappées avec **purrr** (voir [section ci-dessous](#iter_purrr)).  


### Composants principaux {.unnumbered}   

Une *boucle for* comporte trois éléments essentiels :  
     
1) la **séquence** d'éléments à parcourir par itération  
2) Les **opérations** à effectuer pour chaque élément de la séquence.  
3) le **contenu** des résultats (facultatif).  

La syntaxe de base est la suivante : `pour (élément dans la séquence) {faire des opérations avec l'élément}`. Notez les parenthèses et les accolades. Les résultats peuvent être imprimés sur la console, ou stockés dans un objet R conteneur.   

Voici un exemple simple de *boucle for*.   

```{r}
for (num in c(1,2,3,4,5)) { # la SEQUENCE est définie (numéros 1 à 5) et la boucle est ouverte avec "{"
  print(num + 2) # Les OPERATIONS (ajouter deux à chaque numéro de séquence et imprimer)
}                            # La boucle est fermée avec "}"                            
                             # Il n'y a pas de "conteneur" dans cet exemple.
```



### Séquence {.unnumbered} (sans numéro)  

Il s'agit de la partie "for" d'une *boucle for* - les opérations seront exécutées "pour" chaque élément de la séquence. La séquence peut être une série de valeurs (par exemple, des noms de juridictions, de maladies, des noms de colonnes, des éléments de listes, etc), ou bien une série de nombres consécutifs (par exemple, 1,2,3,4,5). Chaque approche a ses propres utilitaires, décrits ci-dessous.  

La structure de base d'une déclaration de séquence est `item in vector`.  

* Vous pouvez écrire n'importe quel caractère ou mot à la place de "item" (par exemple "i", "num", "hosp", "district", etc.). La valeur de cet "item" change à chaque itération de la boucle, en passant par chaque valeur du vecteur.  
* Le *vecteur* peut être constitué de valeurs de caractères, de noms de colonnes, ou peut-être d'une séquence de nombres - ce sont les valeurs qui changeront à chaque itération. Vous pouvez les utiliser dans les opérations *for loop* en utilisant le terme "item".  

**Exemple : séquence de valeurs de caractères**  

Dans cet exemple, une boucle est exécutée pour chaque valeur d'un vecteur de caractères prédéfini de noms d'hôpitaux.  

```{r}
# faire un vecteur des noms d'hôpitaux
hospital_names <- unique(linelist$hospital)
hospital_names # print
```

Nous avons choisi le terme `hosp` pour représenter les valeurs du vecteur `hospital_names`. Pour la première itération de la boucle, la valeur de `hosp` sera `hospital_names[[1]]`. Pour la deuxième boucle, elle sera `noms_hospitaliers[[2]]`. Et ainsi de suite...  

```{r, eval=F}
# une 'boucle for' avec une séquence de caractères

for (hosp in hospital_names){ # séquence
  
       # OPÉRATIONS ICI
  }
```

**Exemple : séquence de noms de colonnes**  
     
Il s'agit d'une variation de la séquence de caractères ci-dessus, dans laquelle les noms d'un objet R existant sont extraits et deviennent le vecteur. Par exemple, les noms des colonnes d'un cadre de données. De façon pratique, dans le code d'opérations de la *boucle for*, les noms de colonnes peuvent être utilisés pour *indexer* (sous-ensemble) leur cadre de données original.  

Ci-dessous, la séquence est constituée des `names()` (noms des colonnes) du cadre de données `linelist`. Notre nom d'"élément" est `col`, qui représentera chaque nom de colonne au fur et à mesure que les boucles se déroulent.  

Pour les besoins de l'exemple, nous incluons le code des opérations à l'intérieur de la boucle *for*, qui est exécutée pour chaque valeur de la séquence. Dans ce code, les valeurs de la séquence (noms de colonnes) sont utilisées pour *indexer* (sous-ensemble) `linelist`, une par une. Comme enseigné dans la page [bases de R](#rbasics), les doubles branchements `[[ ]]` sont utilisés pour sous-indexer. La colonne résultante est passée à `is.na()`, puis à `sum()` pour produire le nombre de valeurs manquantes dans la colonne. Le résultat est imprimé sur la console - un nombre pour chaque colonne.  

Une remarque sur l'indexation avec les noms de colonnes - lorsque vous faites référence à la colonne elle-même, *ne vous contentez pas d'écrire "col"!* `col` ne représente que le nom de la colonne en caractères! Pour faire référence à la colonne entière, vous devez utiliser le nom de la colonne comme *index* sur `linelist` via `linelist[[col]]`.  

```{r}
for (col in names(linelist)){ # La boucle est exécutée pour chaque colonne de la linelist ; le nom de la colonne est représenté par "col". 
  
  # Exemple de code d'opérations - impression du nombre de valeurs manquantes dans la colonne
  print(sum(is.na(linelist[[col]])))  # La linelist est indexée par la valeur actuelle de "col".
     
}
```



**Séquence de nombres**  
     
Dans cette approche, la séquence est une série de nombres consécutifs. Ainsi, la valeur de l'"item" n'est pas une valeur de caractère (par exemple, "Central Hospital" ou "date_onset") mais un nombre. Ceci est utile pour boucler des cadres de données, car vous pouvez utiliser le numéro de "item" dans la *boucle for* pour indexer le cadre de données par *numéro de ligne*.  

Par exemple, disons que vous souhaitez parcourir chaque ligne de votre cadre de données et extraire certaines informations. Vos "éléments" seraient des numéros de ligne numériques. Dans ce cas, les "éléments" sont souvent écrits sous la forme `i`.  

Le processus *for loop* pourrait s'expliquer par la phrase suivante: "pour chaque élément d'une séquence de nombres allant de 1 au nombre total de lignes de mon cadre de données, faire X". Pour la première itération de la boucle, la valeur de l'"élément" `i` sera 1. Pour la deuxième itération, `i` sera 2, etc.  

Voici à quoi ressemble la séquence en code: `for (i in 1:nrow(linelist)) {CODE DES OPERATIONS}` où `i` représente l'"élément" et `1:nrow(linelist)` produit une séquence de nombres consécutifs allant de 1 au nombre de lignes de `linelist`.  


```{r, eval=F}

for (i in 1:nrow(linelist)) { # utilisation sur un cadre de données
  # OPÉRATIONS ICI
}  

```

Si vous voulez que la séquence soit des nombres, mais que vous partez d'un vecteur (et non d'un cadre de données), utilisez le raccourci `seq_along()` pour retourner une séquence de nombres pour chaque élément du vecteur. Par exemple, `for (i in seq_along(hospital_names) {CODE D'OPÉRATIONS}`.  

Le code ci-dessous renvoie en fait des nombres, qui deviendront la valeur de `i` dans leur boucle respective.     

```{r}

seq_along(hospital_names) # utilisation sur un vecteur nommé

```

Un avantage de l'utilisation de nombres dans la séquence est qu'il est facile d'utiliser le nombre `i` pour indexer un *conteneur* qui stocke les sorties de la boucle. Il y a un exemple de ceci dans la section Opérations ci-dessous.  

### Opérations {.unnumbered}  

C'est le code entre les crochets `{ }` de la *boucle for*. Vous voulez que ce code soit exécuté pour chaque "élément" de la *séquence*. Par conséquent, faites attention à ce que chaque partie de votre code qui change en fonction de l'"item" soit correctement codée pour qu'elle change réellement ! Par exemple, n'oubliez pas d'utiliser `[[ ]]` pour l'indexation.  

Dans l'exemple ci-dessous, nous itérons à travers chaque ligne de la `linelist`. Les valeurs `gender` et `age` de chaque ligne sont collées ensemble et stockées dans le vecteur de caractères conteneur `cases_demographics`. Notez comment nous utilisons également l'indexation `[[i]]` pour enregistrer la sortie de la boucle à la bonne position dans le vecteur "conteneur".  

```{r}
# créer un conteneur pour stocker les résultats - un vecteur de caractères
cases_demographics <- vector(mode = "character", length = nrow(linelist))

# la boucle for
for (i in 1:nrow(linelist)){
  
  # OPERATIONS
  # extraire les valeurs de la linelist pour la ligne i, en utilisant les parenthèses pour l'indexation.
  row_gender <- linelist$gender[[i]]
  row_age <- linelist$age_years[[i]]    # n'oubliez pas d'indexer!
     
  # combinez sexe-âge et stockez dans un vecteur conteneur à l'emplacement indexé
  cases_demographics[[i]] <- str_c(row_gender, row_age, sep = ",") 

}  # fin de la boucle for


# affiche les 10 premières lignes du conteneur
head(cases_demographics, 10)
```


### Conteneur {.unnumbered}

Parfois, les résultats de votre *boucle for* seront imprimés sur la console ou dans le panneau Plots de RStudio. D'autres fois, vous voudrez stocker les résultats dans un "conteneur" pour une utilisation ultérieure. Ce conteneur peut être un vecteur, un cadre de données ou même une liste.  

Il est plus efficace de créer le conteneur pour les résultats *avant même* de commencer la *boucle for*. En pratique, cela signifie créer un vecteur, un cadre de données ou une liste vide. On peut les créer avec les fonctions `vector()` pour les vecteurs ou les listes, ou avec `matrix()` et `data.frame()` pour un cadre de données. 

**Vecteur vide**  

Utilisez `vector()` et spécifiez le `mode = ` en fonction de la classe attendue des objets que vous allez insérer - soit "double" (pour contenir des nombres), "caractère", ou "logique". Vous devez également définir la valeur `length = ` à l'avance. Il s'agit de la longueur de votre séquence *for loop*.  

Disons que vous voulez stocker le délai médian d'admission pour chaque hôpital. Vous utiliserez "double" et définirez la longueur comme étant le nombre de sorties attendues (le nombre d'hôpitaux uniques dans l'ensemble de données).  

```{r}
delays <- vector(
  mode = "double", # nous nous attendons à stocker des nombres
  length = length(unique(linelist$hospital))) # le nombre d'hôpitaux uniques dans l'ensemble de données
```

**Cadre de données vide  
     
Vous pouvez créer un cadre de données vide en spécifiant le nombre de lignes et de colonnes comme ceci :  
     
```{r, eval=F}
delays <- data.frame(matrix(ncol = 2, nrow = 3))
```


**Liste vide**  
     
Vous pouvez vouloir stocker certains graphiques créés par une *boucle for* dans une liste. Une liste est comme un vecteur, mais elle contient d'autres objets R qui peuvent être de différentes classes. Les éléments d'une liste peuvent être un nombre unique, un cadre de données, un vecteur et même une autre liste.  

Vous pouvez initialiser une liste vide en utilisant la même commande `vector()` que ci-dessus, mais avec `mode = "list"`. Spécifiez la longueur comme vous le souhaitez.  

```{r, eval=F}
plots <- vector(mode = "list", length = 16)
```




### Impression {.unnumbered}  

Notez que pour imprimer à partir d'une *boucle for*, vous aurez probablement besoin d'envelopper explicitement avec la fonction `print()`.  

Dans l'exemple ci-dessous, la séquence est un vecteur de caractères explicite, qui est utilisé pour sous-titrer la linelist par hôpital. Les résultats ne sont pas stockés dans un conteneur, mais sont imprimés sur la console avec la fonction `print()`.    

```{r}
for (hosp in hospital_names){ 
     hospital_cases <- linelist %>% filter(hospital == hosp)
     print(nrow(hospital_cases))
}
```


### Tester votre boucle for {.unnumbered}

Pour tester votre boucle, vous pouvez lancer une commande pour effectuer une affectation temporaire de l'"élément", comme `i <- 10` ou `hosp <- "Central Hospital"`. Faites cela *en dehors de la boucle*, puis exécutez votre code d'opérations uniquement (le code entre les accolades) pour voir si les résultats attendus sont produits.  




### Plots de bouclage {.unnumbered}

Pour rassembler les trois composants (conteneur, séquence et opérations), essayons de tracer une épicurve pour chaque hôpital (voir la page sur les [Courbes épidémiques](#epicurves)).  

Nous pouvons faire une belle épicurve de *tous* les cas par sexe en utilisant le paquet **incidence2** comme ci-dessous:  

```{r, warning=F, message=F}
# créer un objet 'incidence
outbreak <- incidence2::incidence(   
     x = linelist, # dataframe - linelist complet
     date_index = "date_onset", # colonne de date
     interval = "week", # aggregate counts weekly
     groups = "gender") # regroupe les valeurs par sexe
     #na_as_group = TRUE , deprecated incidence version 2.0.0

# tracer la courbe d'épidémie
ggplot(outbreak, # nom de l'objet d'incidence
        aes(x = date_index, #aesthetiques et axes
            y = count, 
            fill = gender), # couleur des barres par sexe
       color = "black"      # couleur de contour des barres
       ) +  
     geom_col() + 
     facet_wrap(~gender) +
     theme_bw() + 
     labs(title = "Outbreak of all cases", #titre
          x = "Counts", 
          y = "Date", 
          fill = "Gender", 
          color = "Gender")



```

Pour produire un graphique distinct pour les cas de chaque hôpital, nous pouvons placer ce code épicurve dans une *boucle for*. 

Tout d'abord, nous enregistrons un vecteur nommé des noms uniques des hôpitaux, `hospital_names`. La *boucle for* sera exécutée une fois pour chacun de ces noms : `for (hosp in hospital_names)`. A chaque itération de la *boucle for*, le nom de l'hôpital actuel du vecteur sera représenté par `hosp` pour être utilisé dans la boucle.  

Dans les opérations de la boucle, vous pouvez écrire du code R comme d'habitude, mais utiliser l'"élément" (`hosp` dans ce cas) en sachant que sa valeur va changer. Dans cette boucle :  
     
* Un `filter()` est appliqué à `linelist`, de telle sorte que la colonne `hospital` doit être égale à la valeur actuelle de `hosp`.  
* L'objet incidence est créé sur la liste de lignes filtrée.  
* Le graphique de l'hôpital actuel est créé, avec un titre auto-ajustable qui utilise `hosp`.  
* Le graphique de l'hôpital actuel est temporairement sauvegardé puis imprimé.  
* La boucle se répète ensuite avec l'hôpital suivant dans `hospital_names`.  

```{r, out.width='50%', message = F}
# fabrique un vecteur des noms d'hôpitaux
hospital_names <- unique(linelist$hospital)

# pour chaque nom ("hosp") dans hospital_names, créer et imprimer la courbe épi
for (hosp in hospital_names) {
     
     # créer un objet d'incidence spécifique à l'hôpital actuel
     outbreak_hosp <- incidence2::incidence(
          x = linelist %>% filter(hospital == hosp), # linelist est filtré sur l'hôpital actuel
          date_index = "date_onset",
          interval = "week", 
          groups = "gender",
          #na_as_group = TRUE , deprecated incidence version 2.0.0
     )
     
     
     # tracer la courbe d'épidémie

     plot_hosp <- ggplot(outbreak_hosp, # nom de l'objet d'incidence
                         aes(x = date_index, #aesthetiques et axes
                             y = count, 
                             fill = gender), # couleur des barres par sexe
                         color = "black"      # couleur de contour des barres
                         ) +  
          geom_col() + 
          facet_wrap(~gender) +
          theme_bw() + 
          labs(title = stringr::str_glue("Epidemic of cases admitted to {hosp}"), #titre
               x = "Counts", 
               y = "Date", 
               fill = "Gender", 
               color = "Gender")
     
     # Créez et enregistrez le graphique. Le titre s'ajuste automatiquement à l'hôpital actuel
    # plot_hosp <- plot(
#       outbreak_hosp,
#       fill = "gender",
#       color = "black",
#       title = stringr::str_glue("Epidemic of cases admitted to {hosp}")
#     )
     
     # imprimer le graphique pour l'hôpital actuel
     print(plot_hosp)
     
} # terminez la boucle for lorsqu'elle a été exécutée pour chaque hôpital dans hospital_names 
```



### Suivi de la progression d'une boucle {.unnumbered} 

Une boucle comportant de nombreuses itérations peut s'exécuter pendant plusieurs minutes, voire plusieurs heures. Ainsi, il peut être utile d'imprimer la progression dans la console R. L'instruction `if` ci-dessous peut être placée *dans* les opérations de la boucle pour imprimer chaque 100ème nombre. Il suffit de l'ajuster pour que `i` soit l'"élément" de votre boucle.  

```{r, eval=F}
# boucle avec code pour imprimer la progression toutes les 100 itérations
for (i in seq_len(nrow(linelist))){

  # imprimer la progression
  if(i %% 100==0){ # L'opérateur %% est le restant
    print(i)

}
```

<!-- ======================================================= -->
## **purrr** et listes {#iter_purrr}
     
Une autre approche des opérations itératives est le paquet **purrr** - c'est l'approche **tidyverse** de l'itération.  

Si vous devez effectuer la même tâche plusieurs fois, il est probablement utile de créer une solution généralisée que vous pouvez utiliser sur plusieurs entrées. Par exemple, produire des tracés pour plusieurs juridictions, ou importer et combiner de nombreux fichiers.  

Il y a aussi quelques autres avantages à **purrr** - vous pouvez l'utiliser avec des tubes `%>%`, il gère mieux les erreurs que les *boucles for* normales, et la syntaxe est assez propre et simple ! Si vous utilisez une *boucle for*, vous pouvez probablement le faire plus clairement et succinctement avec **purrr** ! 
   
Gardez à l'esprit que **purrr** est un outil de programmation *fonctionnel*. C'est-à-dire que les opérations qui doivent être appliquées de manière itérative sont regroupées dans des *fonctions*. Consultez la page [Écrire des fonctions](#writing_functions) pour apprendre à écrire vos propres fonctions.  

**purrr** est également presque entièrement basé sur des *listes* et des *vecteurs* - pensez-y comme si vous appliquiez une fonction à chaque élément de cette liste/ce vecteur !
     
### Chargement des paquets {.unnumbered}  
     
**purrr** fait partie de **tidyverse**, il n'y a donc pas besoin d'installer/charger un paquet séparé.  

```{r}
pacman::p_load(
     rio, # import/export
     here, # chemins de fichiers relatifs
     tidyverse, # gestion et visualisation des données
     writexl, # écriture d'un fichier Excel à feuilles multiples
     readxl # importer Excel avec plusieurs feuilles
)
```


### `map()` {.unnumbered}  

Une fonction essentielle de **purrr** est `map()`, qui "mappe" (applique) une fonction à chaque élément d'entrée d'une liste/vecteur que vous fournissez.  

La syntaxe de base est `map(.x = SEQUENCE, .f = FONCTION, AUTRES ARGUMENTS)`. Un peu plus en détail :  
     
* `.x = ` sont les *entrées* sur lesquelles la fonction `.f` sera appliquée de manière itérative - par exemple un vecteur de noms de juridiction, des colonnes dans un cadre de données, ou une liste de cadres de données.  
* `.f = ` est la *fonction* à appliquer à chaque élément de l'entrée `.x` - cela peut être une fonction comme `print()` qui existe déjà, ou une fonction personnalisée que vous définissez. La fonction est souvent écrite après un tilde `~` (détails ci-dessous). 

Quelques notes supplémentaires sur la syntaxe :  
     
* Si la fonction n'a pas besoin de spécifier d'autres arguments, elle peut être écrite sans parenthèses et sans tilde (par exemple, `.f = mean`). Pour fournir des arguments qui auront la même valeur à chaque itération, fournissez-les dans `map()` mais en dehors de l'argument `.f = `, comme le `na.rm = T` dans `map(.x = ma_liste, .f = mean, na.rm=T)`.  
* Vous pouvez utiliser `.x` (ou simplement `.`) * à l'intérieur* de la fonction `.f = ` comme substitut pour la valeur `.x` de cette itération.  
* Utilisez la syntaxe du tilde (`~`) pour avoir un meilleur contrôle sur la fonction - écrivez la fonction normalement avec des parenthèses, par exemple : `map(.x = ma_liste, .f = ~mean(., na.rm = T))`. Utilisez cette syntaxe particulièrement si la valeur d'un argument change à chaque itération, ou si c'est la valeur `.x` elle-même (voir les exemples ci-dessous)  


**Le résultat de l'utilisation de `map()` est une *liste* ** - une liste est une classe d'objets comme un vecteur mais dont les éléments peuvent être de classes différentes. Ainsi, une liste produite par `map()` peut contenir de nombreux cadres de données, ou de nombreux vecteurs, de nombreuses valeurs individuelles, ou même de nombreuses listes ! Il existe des versions alternatives de `map()` expliquées ci-dessous qui produisent d'autres types de sorties (par exemple, `map_dfr()` pour produire un cadre de données, `map_chr()` pour produire des vecteurs de caractères, et `map_dbl()` pour produire des vecteurs numériques).  

#### Exemple - Importer et combiner des feuilles Excel {#iter_combined .unnumbered}  

**Démontrons avec une tâche courante d'épidémiologiste:** - *Vous voulez importer un classeur Excel avec des données de cas, mais les données sont réparties sur différentes feuilles nommées dans le classeur. Comment importer et combiner efficacement les feuilles en un seul cadre de données ?  

Imaginons que l'on nous envoie le classeur Excel ci-dessous. Chaque feuille contient les cas d'un hôpital donné.  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "hospital_linelists_excel_sheets.png"))
```

Voici une approche qui utilise `map()` :  
     
1) `map()` la fonction `import()` pour qu'elle s'exécute pour chaque feuille Excel.  
2) Combinez les cadres de données importés en un seul en utilisant `bind_rows()`.  
3) En cours de route, conservez le nom de la feuille originale pour chaque ligne, en stockant cette information dans une nouvelle colonne du cadre de données final.  

Tout d'abord, nous devons extraire les noms des feuilles et les enregistrer. Nous fournissons le chemin du fichier du classeur Excel à la fonction `excel_sheets()` du package **readxl**, qui extrait les noms des feuilles. Nous les stockons dans un vecteur de caractères appelé `sheet_names`.  

```{r, echo=F}
sheet_names <- readxl::excel_sheets(here::here("data", "example", "hospital_linelists.xlsx"))

```

```{r, eval=F}
sheet_names <- readxl::excel_sheets("hospital_linelists.xlsx")
```

Voici les noms :  

```{r}
sheet_names
```

Maintenant que nous avons ce vecteur de noms, `map()` peut les fournir un par un à la fonction `import()`. Dans cet exemple, les `sheet_names` sont `.x` et `import()` est la fonction `.f`.  

Rappelez-vous de la page [Importation et exportation](#import_export) que lorsqu'il est utilisé sur des classeurs Excel, `import()` peut accepter l'argument `which = ` spécifiant la feuille à importer. Dans la fonction `.f` `import()`, nous fournissons `which = .x`, dont la valeur changera à chaque itération dans le vecteur `sheet_names` - d'abord "Central Hospital", puis "Military Hospital", etc.  

A noter - parce que nous avons utilisé `map()`, les données de chaque feuille Excel seront enregistrées comme un cadre de données séparé dans une liste. Nous voulons que chacun de ces éléments de liste (cadres de données) ait un *nom*, donc avant de passer `sheet_names` à `map()`, nous le passons à travers `set_names()` de **purrr**, ce qui garantit que chaque élément de liste reçoit le nom approprié.  

Nous enregistrons la liste de sortie comme `combined` (`combiné` en francais).  

```{r, echo=F}
combined <- sheet_names %>% 
  purrr::set_names() %>% 
  map(.f = ~import(here::here("data", "example", "hospital_linelists.xlsx"), which = .x))
```

```{r, eval=F}
combined <- sheet_names %>% 
  purrr::set_names() %>% 
  map(.f = ~import("hospital_linelists.xlsx", which = .x))
```

Lorsque nous inspectons la sortie, nous voyons que les données de chaque feuille Excel sont enregistrées dans la liste avec un nom. C'est bien, mais nous n'avons pas tout à fait terminé.  


```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "sheets_as_list.png"))
```

Enfin, nous utilisons la fonction `bind_rows()` (de **dplyr**) qui accepte la liste des cadres de données structurés de manière similaire et les combine en un seul cadre de données. Pour créer une nouvelle colonne à partir de l'élément de liste *names*, nous utilisons l'argument `.id = ` et lui fournissons le nom souhaité pour la nouvelle colonne.  

Voici la séquence complète des commandes :  

```{r, echo=F}
sheet_names <- readxl::excel_sheets(here::here("data", "example", "hospital_linelists.xlsx"))

combined <- sheet_names %>% 
  purrr::set_names() %>% 
  map(.f = ~import(here::here( "data", "example", "hospital_linelists.xlsx"), which = .x)) %>% 
  bind_rows(.id = "origin_sheet")
```


```{r, eval=F}
sheet_names <- readxl::excel_sheets("hospital_linelists.xlsx") # extrait les noms des feuilles
 
combined <- sheet_names %>% # commence avec les noms de feuilles
  purrr::set_names() %>% # définit leurs noms
  map(.f = ~import("hospital_linelists.xlsx", which = .x)) %>% # itère, importe, sauvegarde dans la liste
  bind_rows(.id = "origin_sheet") # combine la liste des cadres de données, en préservant l'origine dans une nouvelle colonne  
```

Et maintenant nous avons un cadre de données avec une colonne contenant la feuille d'origine !  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "sheets_as_df.png"))
```

Il existe des variantes de `map()` que vous devez connaître. Par exemple, `map_dfr()` renvoie un cadre de données, et non une liste. Ainsi, nous aurions pu l'utiliser pour la tâche ci-dessus et ne pas avoir à lier les rangées. Mais alors nous n'aurions pas été en mesure de capturer de quelle feuille (hôpital) chaque cas provenait.  

D'autres variations incluent `map_chr()`, `map_dbl()`. Ces fonctions sont très utiles pour deux raisons. Premièrement, elles convertissent automatiquement la sortie d'une fonction itérative en un vecteur (et non en une liste). Deuxièmement, elles peuvent contrôler explicitement la classe dans laquelle les données reviennent - vous vous assurez que vos données reviennent sous forme de vecteur de caractères avec `map_chr()`, ou de vecteur numérique avec `map_dbl()`. Nous y reviendrons plus tard dans cette section !

Les fonctions `map_at()` et `map_if()` sont aussi très utiles pour l'itération - elles vous permettent de spécifier quels éléments d'une liste vous devez itérer ! Elles fonctionnent en appliquant simplement un vecteur d'index/noms (dans le cas de `map_at()`) ou un test logique (dans le cas de `map_if()`).  

Prenons un exemple où nous ne voulons pas lire la première feuille de données de l'hôpital. Nous utilisons `map_at()` au lieu de `map()`, et spécifions l'argument `.at = ` à `c(-1)` ce qui signifie *ne pas* utiliser le premier élément de `.x`. Alternativement, vous pouvez fournir un vecteur de nombres positifs, ou de noms, à `.at = ` pour spécifier les éléments à utiliser.  

```{r, echo=F}
sheet_names <- readxl::excel_sheets(here::here("data", "example", "hospital_linelists.xlsx"))

combined <- sheet_names %>% 
     purrr::set_names() %>% 
     # exclure la première feuille
     map_at(.f = ~import(here::here("data", "example", "hospital_linelists.xlsx"), which = .x),
            .at = c(-1))
```


```{r, eval=F}
sheet-names <- readxl::excel_sheets("hospital_linelinest.xlsx")

combined <- sheet_names %>% 
     purrr::set_names() %>% 
     # exclure la première feuille
     map_at(.f = ~import( "hospital_linelists.xlsx", which = .x),
            .at = c(-1))
```

Notez que le nom de la première feuille apparaîtra toujours comme un élément de la liste de sortie - mais ce n'est qu'un nom à un seul caractère (pas un cadre de données). Vous devrez supprimer cet élément avant de lier les lignes. Nous verrons comment supprimer et modifier des éléments de liste dans une section ultérieure.  


### Diviser l'ensemble de données et exporter {.unnumbered}  

Ci-dessous, nous donnons un exemple de la façon de diviser un jeu de données en plusieurs parties, puis d'utiliser l'itération `map()` pour exporter chaque partie comme une feuille Excel séparée, ou comme un fichier CSV séparé.  

#### Diviser l'ensemble de données {.unnumbered}  

Disons que nous avons le cas complet `linelist` en tant que cadre de données, et que nous voulons maintenant créer une linelist séparée pour chaque hôpital et l'exporter comme un fichier CSV séparé. Ci-dessous, nous effectuons les étapes suivantes :  
     
Utilisez `group_split()` (de **dplyr**) pour diviser le cadre de données `linelist` par des valeurs uniques dans la colonne `hospital`. La sortie est une liste contenant un cadre de données par sous-ensemble hospitalier.  

```{r}
linelist_split <- linelist %>% 
     group_split(hospital)
```

Nous pouvons exécuter `View(linelist_split)` et voir que cette liste contient 6 cadres de données ("tibbles"), chacun représentant les cas d'un hôpital. 

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_linelist_split.png"))
```

Cependant, notez que les cadres de données de la liste n'ont pas de nom par défaut ! Nous voulons que chacun d'eux ait un nom, et que ce nom soit utilisé lors de l'enregistrement du fichier CSV.  

Une approche pour extraire les noms est d'utiliser `pull()` (de **dplyr**) pour extraire la colonne `hospital` de chaque cadre de données dans la liste. Ensuite, pour être sûr, nous convertissons les valeurs en caractères et utilisons ensuite `unique()` pour obtenir le nom de ce cadre de données particulier. Toutes ces étapes sont appliquées à chaque cadre de données via `map()`.  

```{r}
names(linelist_split) <- linelist_split %>% # Affectation aux noms des blocs de données listés 
     # Extrayez les noms en effectuant ce qui suit pour chaque cadre de données : 
     map(.f = ~pull(.x, hospital)) %>% # Extraire la colonne hôpital
     map(.f = ~as.character(.x)) %>% # Convertir en caractères, juste au cas où
     map(.f = ~unique(.x))                    # Prendre le nom unique de l'hôpital
```

Nous pouvons maintenant voir que chacun des éléments de la liste a un nom. On peut accéder à ces noms via `names(linelist_split)``.  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_linelist_split_named.png"))
```

```{r}
names(linelist_split)
```


##### Plus d'une colonne `group_split()` {.unnumbered}  

Si vous souhaitez diviser la linelist par *plusieurs colonnes de regroupement*, par exemple pour produire un sous-ensemble linelist par intersection de l'hôpital ET du sexe, vous aurez besoin d'une approche différente pour nommer les éléments de la liste. Cela implique de collecter les "clés de groupe" uniques en utilisant `group_keys()` de **dplyr** - elles sont retournées comme un cadre de données. Vous pouvez ensuite combiner les clés de groupe en valeurs avec `unite()` comme indiqué ci-dessous, et attribuer ces noms de conglomérat à `linelist_split`.  

```{r}
# divise la linelist par les combinaisons uniques hôpital-sexe
linelist_split <- linelist %>% 
     group_split(hospital, gender)

# extraire group_keys() sous forme de dataframe
groupings <- linelist %>% 
     group_by(hospital, gender) %>%       
     group_keys()

groupings # montre les groupements uniques 
```

Maintenant nous combinons les groupements ensemble, séparés par des tirets, et nous les assignons comme noms des éléments de la liste dans `linelist_split`. Cela prend quelques lignes supplémentaires car nous remplaçons `NA` par "Missing", nous utilisons `unite()` de **dplyr** pour combiner les valeurs des colonnes ensemble (séparées par des tirets), puis nous les convertissons en un vecteur sans nom pour qu'il puisse être utilisé comme noms de `linelist_split`.  

```{r, eval=F}
# Combinez en une seule valeur de nom 
names(linelist_split) <- groupings %>% 
     mutate(across(everything(), replace_na, "Missing")) %>% # Remplacer NA par "Missing" dans toutes les colonnes
     unite("combined", sep = "-") %>% # Réunit toutes les valeurs des colonnes en une seule
     setNames(NULL) %>% # d'unification de toutes les valeurs de colonnes en une seule 
     as_vector() %>% 
     as.list()
```



#### Exporter en tant que feuilles Excel {.unnumbered}  

Pour exporter les listes de lignes de l'hôpital comme *un classeur Excel avec une liste de lignes par feuille*, nous pouvons simplement fournir la liste nommée `linelist_split` à la fonction `write_xlsx()` du paquet **writexl**. Cela permet d'enregistrer un classeur Excel avec plusieurs feuilles. Les noms des éléments de la liste sont automatiquement appliqués comme noms de feuilles.  

```{r, eval=F}
linelist_split %>% 
     writexl::write_xlsx(path = here("data", "hospital_linelists.xlsx"))
```

Vous pouvez maintenant ouvrir le fichier Excel et voir que chaque hôpital a sa propre feuille.  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_export_sheets.png"))
```

#### Exportation en fichiers CSV {.unnumbered}  

C'est une commande un peu plus complexe, mais vous pouvez également exporter chaque liste de lignes spécifique à un hôpital sous forme de fichier CSV distinct, avec un nom de fichier spécifique à l'hôpital.  

Encore une fois, nous utilisons `map()` : nous prenons le vecteur des noms des éléments de la liste (montré ci-dessus) et utilisons `map()` pour les parcourir, en appliquant `export()` (du paquet **rio**, voir la page [Importation et exportation](#import_export)) sur le cadre de données dans la liste `linelist_split` qui a ce nom. Nous utilisons également le nom pour créer un nom de fichier unique. Voici comment cela fonctionne :  
     
* Nous commençons avec le vecteur de noms de caractères, passé à `map()` sous la forme `.x`.  
* La fonction `.f` est `export()`, qui requiert un cadre de données et un chemin de fichier pour l'écriture.  
* L'entrée `.x` (le nom de l'hôpital) est utilisée *dans `.f` pour extraire/indexer cet élément spécifique de la liste `linelist_split`. Il en résulte qu'un seul cadre de données à la fois est fourni à `export()`.  
* Par exemple, lorsque `map()` cherche "Military Hospital", alors `linelist_split[[.x]]` est en fait `linelist_split[["Military Hospital"]]`, retournant ainsi le deuxième élément de `linelist_split` - qui est tous les cas de Military Hospital.  
* Le chemin du fichier fourni à `export()` est dynamique grâce à l'utilisation de `str_glue()` (voir la page [Caractères et chaînes de caractères](#character_strings)) :  
     * `here()` est utilisé pour obtenir la base du chemin du fichier et spécifier le dossier "data" (notez les guillemets simples pour ne pas interrompre les guillemets doubles de `str_glue()`).  
* Puis une barre oblique `/`, et encore le `.x` qui imprime le nom de l'hôpital actuel pour rendre le fichier identifiable  
* Enfin, l'extension ".csv" que `export()` utilise pour créer un fichier CSV.  

```{r, eval=F, message = F, warning=F}
names(linelist_split) %>%
     map(.f = ~export(linelist_split[[.x]], file = str_glue("{here('data')}/{.x}.csv")))
```
Maintenant vous pouvez voir que chaque fichier est enregistré dans le dossier "data" du projet R "Epi_R_handbook" !  
     
```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_export_csv.png"))
```



### Fonctions personnalisées {.unnumbered}  

Vous pouvez créer votre propre fonction à fournir à `map()`.  

Disons que nous voulons créer des courbes épidémiques pour les cas de chaque hôpital. Pour faire cela en utilisant **purrr**, notre fonction `.f` peut être `ggplot()` et les extensions avec `+` comme d'habitude. Comme la sortie de `map()` est toujours une liste, les graphiques sont stockés dans une liste. Comme ce sont des tracés, ils peuvent être extraits et tracés avec la fonction `ggarrange()` du paquet **ggpubr** ( [documentation](https://rpkgs.datanovia.com/ggpubr/reference/ggarrange.html)).  


```{r, message = F, warning=F}

# charger le paquetage pour tracer les éléments d'une liste
pacman::p_load(ggpubr)

# cartographier le vecteur des 6 "noms" d'hôpitaux (créé précédemment)
# utiliser la fonction ggplot spécifiée
# la sortie est une liste avec 6 ggplots

hospital_names <- unique(linelist$hospital)

my_plots <- map(
  .x = hospital_names,
  .f = ~ggplot(data = linelist %>% filter(hospital == .x)) +
                geom_histogram(aes(x = date_onset)) +
                labs(title = .x)
)

# imprimer les ggplots (ils sont stockés dans une liste)
ggarrange(plotlist = my_plots, ncol = 2, nrow = 3)
```

Si ce code `map()` vous semble trop compliqué, vous pouvez obtenir le même résultat en enregistrant votre commande `ggplot()` spécifique comme une fonction personnalisée définie par l'utilisateur, par exemple nous pouvons la nommer `make_epicurve())`. Cette fonction est ensuite utilisée dans la fonction `map()`. `.x` sera itérativement remplacé par le nom de l'hôpital, et utilisé comme `hosp_name` dans la fonction `make_epicurve()`. Voir la page sur les [Fonctions d'écriture](#writing_functions).

```{r, eval=F}
# Créer une fonction
make_epicurve <- function(hosp_name){
  
  ggplot(data = linelist %>% filter(hospital == hosp_name)) +
    geom_histogram(aes(x = date_onset)) +
    theme_classic()+
    labs(title = hosp_name)
  
}
```

```{r, eval=F}
# cartographie
my_plots <- map(hospital_names, ~make_epicurve(hosp_name = .x))

# imprimer les ggplots (ils sont stockés dans une liste)
ggarrange(plotlist = my_plots, ncol = 2, nrow = 3)
```




### Mappage d'une fonction sur plusieurs colonnes {.unnumbered}  

Un autre cas d'utilisation courant est de mapper une fonction sur plusieurs colonnes. Ci-dessous, nous `mappons()` la fonction `t.test()` sur les colonnes numériques du cadre de données `linelist`, en comparant les valeurs numériques par sexe.  

Rappelez-vous de la page sur les [Tests statistiques simples](#stats_test) que `t.test()` peut prendre des entrées dans un format de formule, comme `t.test(colonne numérique ~ colonne binaire)`. Dans cet exemple, nous faisons ce qui suit :    
     
* Les colonnes numériques intéressantes sont sélectionnées dans `linelist` - elles deviennent les entrées `.x` de `map()`.  
* La fonction `t.test()` est fournie comme fonction `.f`, qui est appliquée à chaque colonne numérique.  
* Dans les parenthèses de `t.test()`:  
  * le premier `~` précède la fonction `.f` que `map()` va itérer sur `.x`  
  * Le `.x` représente la colonne courante fournie à la fonction `t.test()`.  
  * Le deuxième `~` fait partie de l'équation du test t décrit ci-dessus.  
  * La fonction `t.test()` attend une colonne binaire du côté droit de l'équation. Nous fournissons le vecteur `linelist$gender` indépendamment et statiquement (notez qu'il n'est pas inclus dans `select()`).  

`map()` retourne une liste, donc la sortie est une liste de résultats de tests t - un élément de liste pour chaque colonne numérique analysée.  

```{r}
# Les résultats sont enregistrés sous forme de liste
t.test_results <- linelist %>% 
  select(age, wt_kg, ht_cm, ct_blood, temp) %>% # Ne garder que certaines colonnes numériques pour les mapper entre elles
  map(.f = ~t.test(.x ~ linelist$gender))        # fonction t.test, avec équation NUMERIC ~ CATEGORICAL
```

Voici à quoi ressemble la liste `t.test_results` lorsqu'elle est ouverte (visualisée) dans RStudio. Nous avons mis en évidence les parties qui sont importantes pour les exemples de cette page.  

* Vous pouvez voir en haut que la liste entière est nommée `t.test_results` et a cinq éléments. Ces cinq éléments sont nommés `age`, `wt_km`, `ht_cm`, `ct_blood`, `temp` après chaque variable qui a été utilisée dans un test t avec `gender` de la `linelist`.  
* Chacun de ces cinq éléments sont eux-mêmes des listes, avec des éléments à l'intérieur comme `p.value` et `conf.int`. Certains de ces éléments, comme `p.value`, sont des nombres simples, tandis que d'autres, comme `estimate`, sont composés de deux éléments ou plus (`mean in group f` et `mean in group m`).  

```{r, out.height="150%", echo=F}
knitr::include_graphics(here::here("images", "purrr_ttest.png"))
```


Remarque : N'oubliez pas que si vous voulez appliquer une fonction à certaines colonnes seulement d'un cadre de données, vous pouvez aussi utiliser simplement `mutate()` et `across()`, comme expliqué dans la page [Nettoyage des données et fonctions de base](#cleaning_data). Vous trouverez ci-dessous un exemple d'application de `as.character()` aux seules colonnes "age". Notez l'emplacement des parenthèses et des virgules.  

```{r, eval=F}
# convertit les colonnes dont le nom contient "age" en classe Character
linelist <- linelist %>% 
  mutate(across(.cols = contains("age"), .fns = as.character))  
```


### Extraire des listes {.unnumbered}  

Comme `map()` produit une sortie de la classe List, nous allons passer un peu de temps à discuter de la façon d'extraire des données de listes en utilisant les fonctions **purrr** qui les accompagnent. Pour le démontrer, nous allons utiliser la liste `t.test_results` de la section précédente. C'est une liste de 5 listes - chacune des 5 listes contient les résultats d'un test t entre une colonne du cadre de données `linelist` et sa colonne binaire `gender`. Voir l'image dans la section ci-dessus pour un visuel de la structure de la liste.  

#### Noms des éléments {.unnumbered}  

Pour extraire les noms des éléments eux-mêmes, utilisez simplement `names()` de **base** R. Dans ce cas, nous utilisons `names()` sur `t.test_results` pour retourner les noms de chaque sous-liste, qui sont les noms des 5 variables qui ont eu des tests t effectués.  

```{r}
names(t.test_results)
```

#### Éléments par nom ou par position {.unnumbered}  

Pour extraire les éléments d'une liste par nom ou par position, vous pouvez utiliser des crochets "[[ ]]" comme décrit dans la page [bases de R](#rbasics). Ci-dessous, nous utilisons des doubles crochets pour indexer la liste `t.tests_results` et afficher le premier élément qui est le résultat du test t sur `age`.  

```{r}
t.test_results[[1]] # premier élément par position
t.test_results[[1]][ "p.value"] # retourne l'élément nommé "p.value" à partir du premier élément  
```

Cependant, nous allons démontrer ci-dessous l'utilisation des fonctions **purrr** simples et flexibles `map()` et `pluck()` pour obtenir les mêmes résultats.  

#### `pluck()` {.unnumbered}  

`pluck()` extrait les éléments par nom ou par position. Par exemple - pour extraire les résultats du test t pour l'âge, vous pouvez utiliser `pluck()` comme ceci :  

```{r}
t.test_results %>% 
  pluck("age") # Ou bien, utilisez pluck(1)
```

Indexez des niveaux plus profonds en spécifiant les autres niveaux avec des virgules. L'exemple ci-dessous extrait l'élément nommé "p.value" de la liste `age` dans la liste `t.test_results`. Vous pouvez également utiliser des nombres à la place des noms de caractères.    

```{r}
t.test_results %>% 
  pluck("age", "p.value")
```

Vous pouvez extraire ces éléments internes de *tous* les éléments de premier niveau en utilisant `map()` pour exécuter la fonction `pluck()` sur chaque élément de premier niveau. Par exemple, le code ci-dessous extrait les éléments "p.value" de toutes les listes de `t.test_results`. La liste des résultats du test t est le `.x` itéré, `pluck()` est la fonction `.f` itérée, et la valeur "p-value" est fournie à la fonction.     

```{r}
t.test_results %>%
  map(pluck, "p.value") # renvoie chaque valeur p
```

Comme autre alternative, `map()` offre un raccourci qui vous permet d'écrire le nom de l'élément entre guillemets, et il le récupérera. Si vous utilisez `map()`, la sortie sera une liste, alors que si vous utilisez `map_chr()`, ce sera un tableau de caractères nommé et si vous utilisez `map_dbl()`, ce sera un tableau numérique nommé.  

```{r}
t.test_results %>% 
  map_dbl("p.value") # renvoie les valeurs p sous la forme d'un tableau numérique nommé
```

Vous pouvez en savoir plus sur `pluck()` dans sa **purrr** [documentation](https://purrr.tidyverse.org/reference/pluck.html). Elle a une fonction sour `chuck()` qui retournera une erreur au lieu de NULL si un élément n'existe pas.  



### Convertir une liste en cadre de données {.unnumbered}  

Ceci est un sujet complexe - voir la section Ressources pour des tutoriels plus complets. Néanmoins, nous allons démontrer la conversion de la liste des résultats du test t en un cadre de données. Nous allons créer un cadre de données avec des colonnes pour la variable, sa valeur p, et les moyennes des deux groupes (hommes et femmes).  

Voici quelques-unes des nouvelles approches et fonctions qui seront utilisées :  

* La fonction `tibble()` sera utilisée pour créer un tibble (comme un cadre de données).  
  * Nous entourons la fonction `tibble()` de crochets `{ }` pour éviter que la totalité de `t.test_results` soit stockée dans la première colonne du tibble.  
* Dans `tibble()`, chaque colonne est créée explicitement, de façon similaire à la syntaxe de `mutate()` :  
  * Le `.` représente `t.test_results`
  * Pour créer une colonne avec les noms des variables t-test (les noms de chaque élément de la liste), nous utilisons `names()` comme décrit ci-dessus.  
  * Pour créer une colonne avec les valeurs p, nous utilisons `map_dbl()` comme décrit ci-dessus pour extraire les éléments `p.value` et les convertir en un tableau numérique.  

```{r}
t.test_results %>% {
  tibble(
    variables = names(.),
    p = map_dbl(., "p.value"))
  }
```

Mais maintenant, ajoutons des colonnes contenant les moyennes pour chaque groupe (hommes et femmes).  

Nous devrions extraire l'élément `estimate`, mais celui-ci contient en fait *deux* éléments (`mean in group f` et `mean in group m`). On ne peut donc pas le simplifier en vecteur avec `map_chr()` ou `map_dbl()`. A la place, nous utilisons `map()`, qui utilisé dans `tibble()` créera *une colonne de liste de classe dans le tibble* ! Oui, c'est possible !  

```{r}
t.test_results %>% 
  {tibble(
    variables = names(.),
    p = map_dbl(., "p.value"),
    means = map(., "estimation"))}
```

Une fois que vous avez cette colonne de liste, il existe plusieurs fonctions **tidyr** (faisant partie de **tidyverse**) qui vous aident à "rectangler" ou à "désimbriquer" ces colonnes de "liste imbriquée". Vous pouvez en savoir plus à leur sujet [ici](), ou en exécutant `vignette("rectangle")`. En bref:  

* `unnest_wider()` - donne à chaque élément d'une colonne de liste sa propre colonne.  
* `unnest_longer()` - donne à chaque élément d'une liste-colonne sa propre ligne
* `hoist()` - agit comme `unnest_wider()` mais vous spécifiez les éléments à dépiler.  

Ci-dessous, nous passons le tibble à `unnest_wider()` en spécifiant la colonne `means` du tibble (qui est une liste imbriquée). Le résultat est que `means` est remplacé par deux nouvelles colonnes, chacune reflétant les deux éléments qui étaient précédemment dans chaque cellule `means`.  

```{r}
t.test_results %>% 
  {tibble(
    variables = names(.),
    p = map_dbl(., "p.value"),
    means = map(., "estimate")
    )} %>% 
  unnest_wider(means)
```



### Jeter, conserver et compacter les listes {.unnumbered}  

Parce que le travail avec **purrr** implique si souvent des listes, nous allons explorer brièvement certaines fonctions **purrr** pour modifier les listes. Voir la section Ressources pour des tutoriels plus complets sur les fonctions **purrr**.    

* `list_modify()` a de nombreuses utilisations, dont l'une peut être de supprimer un élément de liste  
* `keep()` conserve les éléments spécifiés dans `.p = `, ou lorsqu'une fonction fournie dans `.p = ` évalue VRAI.  
* `discard()` supprime les éléments spécifiés dans `.p`, ou lorsqu'une fonction fournie à `.p = ` vaut VRAI.  
* `compact()` supprime tous les éléments vides.  

Voici quelques exemples utilisant la liste `combined` créée dans la section ci-dessus sur [l'utilisation de map() pour importer et combiner plusieurs fichiers](#iter_combined) (elle contient 6 cadres de données de listes de cas) :    

Les éléments peuvent être supprimés par leur nom avec `list_modify()` et en mettant le nom égal à `NULL`.  

```{r, eval=F}
combined %>% 
  list_modify("Central Hospital" = NULL) # Suppression d'un élément de liste par son nom
```

Vous pouvez également supprimer des éléments par critère, en fournissant une équation "prédicat" à `.p = ` (une équation qui évalue à VRAI ou FAUX). Placez un tilde `~` devant la fonction et utilisez `.x` pour représenter l'élément de la liste. En utilisant `keep()`, les éléments de la liste qui valent VRAI seront conservés. Inversement, si vous utilisez `discard()`, les éléments de la liste qui valent VRAI seront supprimés.  

```{r, eval=F}
# ne conserve que les éléments de liste de plus de 500 lignes
combined %>% 
  keep(.p = ~nrow(.x) > 500)  
```

Dans l'exemple ci-dessous, les éléments de liste sont éliminés si leur classe n'est pas un cadre de données.  

```{r, eval=F}
# Suppression des éléments de liste qui ne sont pas des cadres de données
combinws %>% 
  discard(.p = ~class(.x) != "data.frame")
```

Votre fonction prédicat peut également faire référence à des éléments/colonnes dans chaque élément de la liste. Par exemple, ci-dessous, les éléments de liste dont la moyenne de la colonne `ct_blood` est supérieure à 25 sont éliminés.  

```{r, eval=F}
# ne conserve que les éléments de liste dont la moyenne de la colonne ct_blood est supérieure à 25
combined %>% 
  discard(.p = ~mean(.x$ct_blood) > 25)  
```

Cette commande supprimerait tous les éléments de liste vides :  

```{r, eval=F}
# Supprime tous les éléments de liste vides
combined %>% 
  compact()
```



### `pmap()` {.unnumbered}

CETTE SECTION EST EN CONSTRUCTION  



## Fonctions d'application  

La famille de fonctions "apply" est une alternative **base** R à **purrr** pour les opérations itératives. Vous pouvez en savoir plus à leur sujet [ici](https://www.datacamp.com/community/tutorials/r-tutorial-apply-family). 


<!-- ======================================================= -->
## Resources { }

[for loops with Data Carpentry](https://datacarpentry.org/semester-biology/materials/for-loops-R/)  

The [R for Data Science page on iteration](https://r4ds.had.co.nz/iteration.html#iteration)  

[Vignette on write/read Excel files](https://martinctc.github.io/blog/vignette-write-and-read-multiple-excel-files-with-purrr/)  

A purrr [tutorial](https://jennybc.github.io/purrr-tutorial/index.html) by jennybc 

Another purrr [tutorial](http://www.rebeccabarter.com/blog/2019-08-19_purrr/) by Rebecca Barter  

A purrr [tutorial](http://zevross.com/blog/2019/06/11/the-power-of-three-purrr-poseful-iteration-in-r-with-map-pmap-and-imap/) on map, pmap, and imap  

[purrr cheatsheet](https://raw.githubusercontent.com/rstudio/cheatsheets/master/pngs/thumbnails/purrr-cheatsheet-thumbs.png)

[purrr tips and tricks](https://www.emilhvitfeldt.com/post/2018-01-08-purrr-tips-and-tricks/)

[keep and discard](https://hookedondata.org/going-off-the-map/#keep-and-discard)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/iteration.Rmd-->

# (PART) Analysis {.unnumbered}

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_analysis.Rmd-->

# Tableaux descriptifs {#descriptive_tables}

```{r out.width = c('75%'), fig.align='center', fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "descriptive_tables.png"))
```

Cette page montre l'utilisation de **janitor**, **dplyr**, **gtsummary**, **rstatix** et **extension/package R base** pour résumer des données et créer des tableaux avec des statistiques descriptives.

*Cette page explique comment* créer les tableaux de base, tandis que la page [Tableaux pour la présentation](#tables_presentation) explique comment les mettre en forme et les imprimer.

Chacun de ces packages présente des avantages et des inconvénients dans les domaines de la simplicité du code, de l'accessibilité des sorties, de la qualité des sorties imprimées. Utilisez cette page pour décider quelle approche convient à votre scénario.

Plusieurs choix s'offrent à vous lorsque vous produisez des tableaux de synthèse et des tableaux croisés. Parmi les facteurs à prendre en compte, la simplicité du code, les possibilités de personnalisation, la sortie que vous souhaitez obtenir ( affichée sur la console R, en tant que tableau de données, ou en tant que "bonne" image .png/.jpeg/.html), et la facilité de traitement ultérieur. Tenez compte des points ci-dessous pour choisir l'outil adapté à votre situation.

-   Utilisez la fonction `tabyl()` de **janitor** pour produire et " personnaliser " des tableaux et des tableaux croisés\

-   Utilisez la fonction `get_summary_stats()` de **rstatix** pour générer facilement des tableaux de données de synthèse de statistiques numériques pour plusieurs colonnes et/ou groupes\

-   Utilisez les fonctions `summarise()` et `count()` de **dplyr** pour des statistiques plus complexes, des sorties de tableaux de données ordonnées ou la préparation de données pour `ggplot()`\

-   Utilisez la fonction `tbl_summary()` de **gtsummary** pour produire des tableaux détaillés prêts à être publiés\

-   Utilisez la fonction `table()` de **extension/package R base** si vous n'avez pas accès aux packages ci-dessus

<!-- ======================================================= -->

## Préparation

### Chargement des packages {.unnumbered}

Ce bloc de code montre le chargement des packages nécessaires pour les analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le package si nécessaire *et* le charge pour utilisation. Vous pouvez également charger les packages installés avec `library()` de **extension/package R base**. Voir la page sur [Bases de R](#rbasics) pour plus d'informations sur les packages R.

```{r, warning=F, message=F}
pacman::p_load(
  rio,          # importation de fichier
  here,         # répertoire de fichiers
  skimr,        # obtenir un aperçu des données
  tidyverse,    # gestion de données + ggplot2 graphiques 
  gtsummary,    # sommaire des statistiques et tests
  rstatix,      # sommaire des statistiques et tests statistiques
  janitor,      # ajouter des totaux et des pourcentages à des tableaux
  scales,       # convertir facilement les proportions en pourcentages  
  flextable     # convertir les tableaux en belles images
  )
```

### Importer les données {.unnumbered}

Nous importons le jeu de données des cas d'une épidémie d'Ebola simulée. Si vous voulez suivre,<a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la linelist " nettoyé ".</a>(as .rds file). Importez vos données avec la fonction `import()` du package **rio** (elle accepte de nombreux types de fichiers comme .xlsx, .rds, .csv - voir la page [Importation et exportation](#import_export) pour plus de détails.

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

<!-- ======================================================= -->

## Explorer les données

### **skimr** package {.unnumbered}

En utilisant le package **skimr**, vous pouvez obtenir un aperçu détaillé et esthétique de chacune des variables de votre ensemble de données. Pour en savoir plus sur **skimr**, consultez sa [page github](https://github.com/ropensci/skimr).

Ci-dessous, la fonction `skim()` est appliquée à l'ensemble du tableau de données `linelist`. Un aperçu du tableau de données et un résumé de chaque colonne (par classe) est produit.

```{r eval=F}
## obtenir des informations sur chaque variable d'un jeu de données
skim(linelist)
```

```{r  echo=F}
# sparkline histograms not showing correctly, so avoiding them.
skim_without_charts(linelist)
```

Vous pouvez également utiliser la fonction `summary()`de **extension/package R base**, pour obtenir des informations sur un jeu de données entier, mais cette sortie peut être plus difficile à lire qu'en utilisant **skimr**. C'est pourquoi la sortie n'est pas montrée ci-dessous, afin de conserver de l'espace sur la page.

```{r, eval=F}
## obtenir des informations sur chaque colonne d'un jeu de données
summary(linelist)
```

### Statistiques sommaires {.unnumbered}

Vous pouvez utiliser les fonctions **extension/package R base** pour renvoyer des synthèses statistiques sur une colonne numérique. Vous pouvez retourner la plupart des synthèses statistiques utiles pour une colonne numérique en utilisant `summary()`, comme ci-dessous. Notez que le nom du tableau de données doit également être spécifié comme indiqué ci-dessous.

```{r}
summary(linelist$age_years)
```

Vous pouvez accéder à une partie spécifique et l'enregistrer avec les crochets d'indexation \[ \] :

```{r}
summary(linelist$age_years)[[2]]            # retourne uniquement le 2ème élément
# équivalent, alternative au précédent par nom d'élément
# summary(linelist$age_years)[["1st Qu."]]  
```

Vous pouvez renvoyer des statistiques individuelles avec des fonctions **extension/package R base** comme `max()`, `min()`, `median()`, `mean()`, `quantile()`, `sd()`, et `range()`. Consultez la page [Bases de R](#rbasics) pour obtenir une liste complète.

[***CAUTION:*** Si vos données contiennent des valeurs manquantes, R veut que vous le sachiez et retournera donc `NA`, sauf si vous spécifiez aux fonctions mathématiques ci-dessus que vous voulez que R ignore les valeurs manquantes, via l'argument `na.rm = TRUE`.]{style="color: orange;"}

Vous pouvez utiliser la fonction `get_summary_stats()` de **rstatix** pour retourner des synthèses statistiques *dans un format de tableau de données*. Cela peut être utile pour effectuer des opérations ultérieures ou des tracés sur les chiffres. Consultez la page [Tests statistiques simples](#stats_test) pour plus de détails sur le package **rstatix** et ses fonctions.

```{r}
linelist %>% 
  get_summary_stats(
    age, wt_kg, ht_cm, ct_blood, temp,  # colonnes à calculer pour
    type = "common")                    # sommaire des statistiques à retourner

```

## **janitor** package {#tbl_janitor}

Les packages **janitor** offrent la fonction `tabyl()` pour produire des tableaux et des tableaux croisés, qui peuvent être " améliorés " ou modifiés avec des fonctions d'aide pour afficher des pourcentages, des proportions, des comptes, etc.

Ci-dessous, nous envoyons le tableau de données `linelist` aux fonctions **janitor** et nous affichons le résultat. Si vous le souhaitez, vous pouvez également enregistrer les tableaux résultants avec l'opérateur d'affectation `<-`.

### Simple tabyl {.unnumbered}

L'utilisation par défaut de `tabyl()` sur une colonne spécifique produit les valeurs uniques, les nombres, et les "pourcentages" par colonne ( proportion en fait). Les proportions peuvent avoir plusieurs chiffres. Vous pouvez ajuster le nombre de décimales avec `adorn_rounding()` comme décrit ci-dessous.

```{r}
linelist %>% tabyl(age_cat)
```

Comme vous pouvez le voir ci-dessus, s'il y a des valeurs manquantes, elles s'affichent dans une ligne étiquetée `<NA>`. Vous pouvez les supprimer avec `show_na = FALSE`. S'il n'y a pas de valeurs manquantes, cette ligne n'apparaîtra pas. S'il y a des valeurs manquantes, toutes les proportions sont données à la fois brutes (dénominateur incluant les comptes `NA`) et "valide" (dénominateur excluant les comptes `NA`).

Si la colonne est un facteur de classe et que seuls certains niveaux sont présents dans vos données, tous les niveaux apparaîtront quand même dans le tableau. Vous pouvez supprimer cette fonctionnalité en spécifiant `show_missing_levels = FALSE`. Pour en savoir plus, consultez la page [Facteurs](#factors).

### Tableau croisé {.unnumbered}

Les chiffres des tableaux croisés sont obtenus en ajoutant une ou plusieurs colonnes supplémentaires dans `tabyl()`. Notez que maintenant, seuls les chiffres sont retournés - les proportions et les pourcentages peuvent être ajoutés avec les étapes supplémentaires montrées ci-dessous.

```{r}
linelist %>% tabyl(age_cat, gender)
```

### "Habillage" du tabyl {#tbl_adorn .unnumbered}

Utilisez les fonctions "adorn" de **janitor** pour ajouter des totaux ou convertir en proportions, en pourcentages, ou ajuster l'affichage. Souvent, vous ferez passer le tabyle par plusieurs de ces fonctions.

+--------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Fonction                 | Résultat                                                                                                                                                                                                   |
+==========================+============================================================================================================================================================================================================+
| `adorn_totals()`         | Ajoute les totaux (`où =` " ligne ", " colonne ", ou " les deux "). Définissez `nom =` pour "Total".                                                                                                       |
+--------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `adorn_percentages()`    | Convertir les nombres en proportions, avec `denominateur =` " ligne ", " colonne ", ou " tout ".                                                                                                           |
+--------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `adorn_pct_formatting()` | Convertit les proportions en pourcentages. Spécifiez `digits =`. Supprimez le symbole "%" avec `affix_sign = FALSE`.                                                                                       |
+--------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `adorn_rounding()`       | Pour arrondir les proportions à des positions `digits =`. Pour arrondir les pourcentages, utilisez `adorn_pct_formatting()` with `digits =`.                                                               |
+--------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `adorn_ns()`             | Ajoutez des nombres à un tableau de proportions ou de pourcentages. Indiquez `position =` "arrière" pour montrer les nombres entre parenthèses, ou "avant" pour mettre les pourcentages entre parenthèses. |
+--------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `adorn_title()`          | Ajouter une chaîne via les arguments `row_name =` et/ou `col_name =`.                                                                                                                                      |
+--------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

Faites attention à l'ordre dans lequel vous appliquez les fonctions ci-dessus. Voici quelques exemples.

Un simple tableau à sens unique avec des pourcentages au lieu des proportions par défaut.

```{r}
linelist %>%               # cas linelist
  tabyl(age_cat) %>%       # calculer les effectifs et les proportions par catégorie d'âge
  adorn_pct_formatting()   # convertir les proportions en pourcentages
```

Un tableau croisé avec une ligne totale et des pourcentages de ligne.

```{r}
linelist %>%                                  
  tabyl(age_cat, gender) %>%                  # comptage par âge et par sexe
  adorn_totals(where = "row") %>%             # ajouter une ligne totale
  adorn_percentages(denominator = "row") %>%  # convertir les comptages en proportions
  adorn_pct_formatting(digits = 1)            # convertir les proportions en pourcentages
```

Un tableau croisé ajusté de façon à ce que les nombres et les pourcentages soient affichés.

```{r}
linelist %>%                                  # cas linelist
  tabyl(age_cat, gender) %>%                  # croiser les comptages
  adorn_totals(where = "row") %>%             # ajouter une ligne de total
  adorn_percentages(denominator = "col") %>%  # convertir en proportions
  adorn_pct_formatting() %>%                  # convertir en pourcentages
  adorn_ns(position = "front") %>%            # afficher comme: "count (percent)"
  adorn_title(                                # ajuster les titres
    row_name = "Age Category",
    col_name = "Gender")
```

### Impression du tableau {.unnumbered}

Par défaut, le tableau s'affichera brute sur votre console R.

Vous pouvez également passer le tableau à **flextable** ou à un package similaire pour qu'il s'imprime comme une "jolie" image dans la visionneuse RStudio, qui peut être exportée en .png, .jpeg, .html, etc. Ce sujet est abordé à la page [Tableaux pour la présentation](#tables_presentation). Notez que si vous imprimez de cette manière en utilisant `adorn_titles()`, vous devez spécifier `placement = "combined"`.

```{r}
linelist %>%
  tabyl(age_cat, gender) %>% 
  adorn_totals(where = "col") %>% 
  adorn_percentages(denominator = "col") %>% 
  adorn_pct_formatting() %>% 
  adorn_ns(position = "front") %>% 
  adorn_title(
    row_name = "Age Category",
    col_name = "Gender",
    placement = "combined") %>% # c'est nécessaire pour afficher comme image
  flextable::flextable() %>%    # convertir en une belle image
  flextable::autofit()          # format à une ligne par rangée 

```

### Utiliser sur d'autres tables {.unnumbered}

Vous pouvez utiliser les fonctions `adorn_*()` de **janitor** sur d'autres tables, comme celles crées par `summarise()` et `count()` de **dplyr**, ou `table()` de **extension/package R base**. Il suffit de passer la table à la fonction **janitor** désirée. Par exemple :

```{r}
linelist %>% 
  count(hospital) %>%   # dplyr fonction
  adorn_totals()        # janitor fonction
```

### Enregistrer le tableau {.unnumbered}

Si vous convertissez le tableau en une " jolie " image avec un package comme **flextable**, vous pouvez l'enregistrer avec les fonctions de ce package - comme `save_as_html()`, `save_as_word()`, `save_as_ppt()`, et `save_as_image()` de **flextable** (comme discuté plus en détail dans la page [Tableaux de présentation](#tables_presentation)). Ci-dessous, le tableau est enregistré sous forme de document Word, dans lequel il peut être modifié manuellement.

```{r, eval=F}
linelist %>%
  tabyl(age_cat, gender) %>% 
  adorn_totals(where = "col") %>% 
  adorn_percentages(denominator = "col") %>% 
  adorn_pct_formatting() %>% 
  adorn_ns(position = "front") %>% 
  adorn_title(
    row_name = "Age Category",
    col_name = "Gender",
    placement = "combined") %>% 
  flextable::flextable() %>%                     # convertir en image
  flextable::autofit() %>%                       # assurer une seule ligne par rangée
  flextable::save_as_docx(path = "tabyl.docx")   # enregistrer en tant que document Word dans le chemin de fichier
```

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "tabyl_word.png"))
```

### Statistiques {#janitor_age_out_stats .unnumbered}

Vous pouvez appliquer des tests statistiques sur les tableaux, comme `chisq.test()` ou `fisher.test()` du package **stats**, comme indiqué ci-dessous. Notez que les valeurs manquantes ne sont pas autorisées, elles sont donc exclues du tableau avec `show_na = FALSE`

```{r, warning=F, message=F}
age_by_outcome <- linelist %>% 
  tabyl(age_cat, outcome, show_na = FALSE) 

chisq.test(age_by_outcome)
```

Consultez la page sur les [Tests statistiques simples](#stats_test) pour obtenir plus de code et de conseils sur les statistiques.

### Autres conseils {.unnumbered}

-   Incluez l'argument `na.rm = TRUE` pour exclure les valeurs manquantes de tous les calculs ci-dessus.\
-   Si vous appliquez des fonctions d'aide `adorn_*()` à des tables qui n'ont pas été crées par `tabyl()`, vous pouvez spécifier une ou plusieurs colonnes particulières auxquelles les appliquer comme `adorn_percentage(,,,c(cas,décès))` (spécifiez-les au 4ème argument non nommé). La syntaxe n'est pas simple. Pensez à utiliser `summarise()` à la place.\
-   Vous pouvez obtenir plus de détails dans le [janitor page](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html) et ce [tabyl vignette](https://cran.r-project.org/web/packages/janitor/vignettes/tabyls.html).

## **dplyr** package

**dplyr** fait partie des packages **tidyverse** et est un outil de gestion de données très courant. Créer des tableaux avec les fonctions **dplyr** `summarise()` et `count()` est une méthode très utile pour calculer des statistiques sommaires, résumer *par groupe*, ou passer des tableaux à `ggplot()`.

`summarise()` crée un *nouveau tableau de données récapitulatif*. Si les données sont *non groupées*, il renvoie un tableau de données à une ligne avec les statistiques récapitulatives spécifiées pour l'ensemble du tableau de données. Si les données sont *groupées*, le nouveau tableau de données aura une ligne par *groupe* (voir la page [Regroupement de données](#grouping_data)).

Entre les parenthèses de `summarise()`, vous indiquez le nom de chaque nouvelle colonne du résumé, suivi d'un signe égal et d'une fonction statistique à appliquer.

[***TIP:*** La fonction summarise fonctionne avec les orthographes britannique et américaine (`summarise()` et `summarize()`).]{style="color: darkgreen;"}

### Obtenir des nombres {.unnumbered}

La fonction la plus simple à appliquer dans `summarise()` est `n()`. Laissez les parenthèses vides pour compter le nombre de lignes.

```{r}
linelist %>%                 # commencer par une liste de lignes
  summarise(n_rows = n())    # retourne un nouveau tableau de données avec la colonne n_rows
```

Cela devient plus intéressant si nous avons regroupé les données au préalable.

```{r}
linelist %>% 
  group_by(age_cat) %>%     # regrouper les données par valeurs uniques dans une colonne age_cat
  summarise(n_rows = n())   # retourne le nombre de lignes *per group*
```

La commande ci-dessus peut être réduite en utilisant la fonction `count()` à la place. La fonction `count()` effectue les opérations suivantes :

1)  Regroupe les données selon les colonnes qui lui sont fournies.
2)  Les résume avec `n()` (en créant la colonne `n`).
3)  Dé-grouper les données

```{r}
linelist %>% 
  count(age_cat)
```

Vous pouvez changer le nom de la colonne des comptages de la valeur par défaut `n` à quelque chose d'autre en le spécifiant à `name =`.

Les résultats de la mise en tableau de deux colonnes de regroupement ou plus sont toujours renvoyés au format "long", avec les effectifs dans la colonne `n`. Consultez la page [Pivoter les données](#pivoting_data) pour en savoir plus sur les formats de données "long" et "large".

```{r}
linelist %>% 
  count(age_cat, outcome)
```

### Voir tous les niveaux {.unnumbered}

Si vous mettez en tableau une colonne de classe *facteur*, vous pouvez vous assurer que *tous* les niveaux sont affichés (et pas seulement les niveaux avec des valeurs dans les données) en ajoutant `.drop = FALSE` dans la commande `summarise()` ou `count()`.

Cette technique est utile pour standardiser vos tableaux/graphiques. Par exemple, si vous créez des chiffres pour plusieurs sous-groupes, ou si vous créez plusieurs fois le même chiffre pour des rapports de routine. Dans chacune de ces circonstances, la présence de valeurs dans les données peut fluctuer, mais vous pouvez définir des niveaux qui restent constants.

Consultez la page sur les [Facteurs](#factors) pour plus d'informations.

### Proportions {#tbl_dplyr_prop .unnumbered}

Les proportions peuvent être ajoutées en passant le tableau à `mutate()` pour créer une nouvelle colonne. Définissez la nouvelle colonne comme la colonne des comptages (`n` par défaut) divisée par la `sum()` de la colonne des comptages (ceci retournera une proportion).

Notez que dans ce cas, `sum()` dans la commande `mutate()` retournera la somme de la colonne entière `n` pour l'utiliser comme dénominateur de la proportion. Comme expliqué [dans la page Regroupement des données](#grouping_data), *si* `sum()` est utilisé dans des données *groupées* (par exemple, si la commande `mutate()` suit immédiatement une commande `group_by()`), il retournera les sommes *par groupe*. Comme indiqué juste au-dessus, `count()` termine ses actions en *dégroupant*. Ainsi, dans ce scénario, nous obtenons les proportions de la colonne entière.

Pour afficher facilement les pourcentages, vous pouvez inclure la proportion dans la fonction `percent()` du package **scales** (notez cette conversion en caractère de classe).

```{r}
age_summary <- linelist %>% 
  count(age_cat) %>%                     # grouper et compter par sexe (produit la colonne "n")
  mutate(                                # créer le pourcentage de la colonne - noter le dénominateur
    percent = scales::percent(n / sum(n))) 

# print
age_summary
```

Vous trouverez ci-dessous une méthode permettant de calculer les proportions *dans les groupes*. Elle repose sur l'application et la suppression sélectives de différents niveaux de regroupement des données. Tout d'abord, les données sont regroupées en fonction du `résultat` via `group_by()`. Ensuite, la fonction `count()` est appliquée. Cette fonction regroupe à nouveau les données par `age_cat` et retourne les résultats pour chaque combinaison `outcome`-`age-cat`. Il est important de noter qu'en terminant son processus, `count()` a également *dégroupé* le regroupement par `age_cat`, de sorte que le seul regroupement de données restant est le regroupement original par `outcome`. Ainsi, l'étape finale du calcul des proportions (dénominateur `sum(n)`) est toujours groupée par `outcome`.

```{r}
age_by_outcome <- linelist %>%                  # commencer par la linelist
  group_by(outcome) %>%                         # groupe par résultats
  count(age_cat) %>%                            # regrouper et compter par age_cat, puis supprimer le regroupement age_cat
  mutate(percent = scales::percent(n / sum(n))) # calculer le pourcentage - noter que le dénominateur est par groupe de résultats
```

```{r, echo=F}
DT::datatable(age_by_outcome, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```

### Plotting {.unnumbered}

Afficher un tableau "long" comme celui ci-dessus avec `ggplot()` est relativement simple. Les données sont naturellement au format "long", qui est naturellement accepté par `ggplot()`. Voir d'autres exemples dans les pages [ggplot basics](#ggplot_basics) et [Astuces de ggplot](#ggplot_tips).

```{r, warning=F, message=F}
linelist %>%                      # commencer par la linelist
  count(age_cat, outcome) %>%     # regrouper et présenter les comptages par deux colonnes
  ggplot()+                       # passer le nouveau tableau de données à ggplot
    geom_col(                     # créer un bar plot
      mapping = aes(   
        x = outcome,              # mappez le résultat sur l'axe des x
        fill = age_cat,           # mappe age_cat au remplissage
        y = n))                  # associer la colonne de comptage `n` à la hauteur
```

### Synthèse des statistiques {.unnumbered}

Un avantage majeur de **dplyr** et de `summarise()` est la possibilité de retourner des résumés statistiques plus avancés comme `median()`, `mean()`, `max()`, `min()`, `sd()` (écart-type), et les percentiles. Vous pouvez également utiliser `sum()` pour retourner le nombre de lignes qui répondent à certains critères logiques. Comme ci-dessus, ces sorties peuvent être produites pour l'ensemble du cadre de données, ou par groupe.

La syntaxe est la même - entre les parenthèses de `summarise()`, vous fournissez les noms de chaque nouvelle colonne de résumé, suivis d'un signe égal et d'une fonction statistique à appliquer. Dans la fonction statistique, donnez la ou les colonnes sur lesquelles vous voulez travailler et tous les arguments pertinents (par exemple `na.rm = TRUE` pour la plupart des fonctions mathématiques).

Vous pouvez également utiliser `sum()` pour retourner le nombre de lignes qui répondent à un critère logique. L'expression qu'il contient est comptée si elle vaut `TRUE`. Par exemple :

-   `sum(age_years < 18, na.rm=T)`\
-   `sum(gender == "male", na.rm=T)`\
-   `sum(response %in% c("Likely", "Very Likely"))`

Ci-dessous, les données `linelist` sont résumées pour décrire le délai en jours entre l'apparition des symptômes et l'admission à l'hôpital (colonne `days_onset_hosp`), par hôpital.

```{r}
summary_table <- linelist %>%                                        # commencez avec la linelist, enregistrez comme un nouvel objet
  group_by(hospital) %>%                                             # regrouper tous les calculs par hopital
  summarise(                                                         # seules les colonnes de résumé ci-dessous seront retournées
    cases       = n(),                                                # nombre de lignes par groupe
    delay_max   = max(days_onset_hosp, na.rm = T),                    # délai max
    delay_mean  = round(mean(days_onset_hosp, na.rm=T), digits = 1),  # délai moyen, arrondi
    delay_sd    = round(sd(days_onset_hosp, na.rm = T), digits = 1),  # Déviation standard des délais, arrondie
    delay_3     = sum(days_onset_hosp >= 3, na.rm = T),               # nombre de lignes avec un délai de 3 jours ou plus
    pct_delay_3 = scales::percent(delay_3 / cases)                    # convertir en pourcentage une colonne de délai précédemment définie
  )

summary_table  # Afficher
```

Quelques conseils :

-   Utilisez `sum()` avec une instruction logique pour "compter" les lignes qui répondent à certains critères (`==`).

-   Notez l'utilisation de `na.rm = TRUE` dans les fonctions mathématiques comme `sum()`, sinon `NA` sera retourné s'il y a des valeurs manquantes \`\`.

-   Utilisez la fonction `percent()` du package **scales** pour convertir facilement en pourcentages.

    -   Définissez `accuracy =` à 0,1 ou 0,01 pour garantir respectivement 1 ou 2 décimales.

-   Utilisez la fonction `round()` de **extension/package R base** pour spécifier les décimales.

-   Pour calculer ces statistiques sur l'ensemble des données, utilisez `summarise()` sans `group_by()`.

-   Vous pouvez créer des colonnes pour les besoins de calculs ultérieurs (par exemple, les dénominateurs) que vous supprimez éventuellement de votre cadre de données avec `select()`.

### Statistiques conditionnelles {.unnumbered}

Vous pouvez souhaiter renvoyer des *statistiques conditionnelles* - par exemple, le maximum de lignes qui répondent à certains critères. Pour ce faire, il suffit de subdiviser la colonne avec des parenthèses "\[ \]". L'exemple ci-dessous renvoie la température maximale pour les patients classés comme ayant ou n'ayant pas de fièvre. Attention cependant - il peut être plus approprié d'ajouter une autre colonne à la commande`group_by()`et`pivot_wider()\` (comme démontré [ci-dessous](#tbls_pivot_wider)).

```{r}
linelist %>% 
  group_by(hospital) %>% 
  summarise(
    max_temp_fvr = max(temp[fever == "yes"], na.rm = T),
    max_temp_no = max(temp[fever == "no"], na.rm = T)
  )
```

### Coller ensemble {.unnumbered}

La fonction `str_glue()` de **stringr** est utile pour combiner les valeurs de plusieurs colonnes en une nouvelle colonne. Dans ce contexte, elle est généralement utilisée *après* la commande `summarise()`.

Dans la page [Caractères et chaînes de caractères](#character_strings), diverses options pour combiner des colonnes sont discutées, notamment `unite()`, et `paste0()`. Dans ce cas d'utilisation, nous préconisons `str_glue()` parce qu'il est plus flexible que `unite()` et a une syntaxe plus simple que `paste0()`.

Ci-dessous, le tableau de données `summary_table` (créé plus haut) est modifié de telle sorte que les colonnes `delay_mean` et `delay_sd` sont combinées, la mise en forme entre parenthèses est ajoutée à la nouvelle colonne, et leurs anciennes colonnes respectives sont supprimées.

Ensuite, pour rendre le tableau plus présentable, une ligne de total est ajoutée avec `adorn_totals()` de **janitor** (qui ignore les colonnes non-numériques). Enfin, nous utilisons `select()` de **dplyr** pour réordonner et renommer les colonnes avec des noms plus appropriés.

Maintenant, vous pouvez passer à **flextable** et imprimer le tableau dans Word, .png, .jpeg, .html, Powerpoint, RMarkdown, etc. ! (voir la page [Tableaux pour la présentation](#tables_presentation)).

```{r}
summary_table %>% 
  mutate(delay = str_glue("{delay_mean} ({delay_sd})")) %>%  # fusionner et formater d'autres valeurs
  select(-c(delay_mean, delay_sd)) %>%                       # supprimer deux anciennes colonnes  
  adorn_totals(where = "row") %>%                            # ajouter la ligne totale
  select(                                                    # ordonner et renommer les colonnes
    "Hospital Name"   = hospital,
    "Cases"           = cases,
    "Max delay"       = delay_max,
    "Mean (sd)"       = delay,
    "Delay 3+ days"   = delay_3,
    "% delay 3+ days" = pct_delay_3
    )
```

#### Percentiles {.unnumbered}

*Les percentiles* et les quantiles dans **dplyr** méritent une mention spéciale. Pour retourner les quantiles, utilisez `quantile()` avec les valeurs par défaut ou spécifiez la ou les valeurs que vous souhaitez avec `probs =`.

```{r}
# obtenir les valeurs percentile par défaut de l'âge (0%, 25%, 50%, 75%, 100%)
linelist %>% 
  summarise(age_percentiles = quantile(age_years, na.rm = TRUE))

# obtenir des valeurs percentiles d'âge spécifiées manuellement (5%, 50%, 75%, 98%)
linelist %>% 
  summarise(
    age_percentiles = quantile(
      age_years,
      probs = c(.05, 0.5, 0.75, 0.98), 
      na.rm=TRUE)
    )
```

Si vous voulez retourner les quantiles *par groupe*, vous pouvez rencontrer des sorties longues et moins utiles si vous ajoutez simplement une autre colonne à `group_by()`. Donc, essayez plutôt cette approche - créez une colonne pour chaque niveau de quantile désiré.

```{r}
# obtenir des valeurs percentiles d'âge spécifiées manuellement (5%, 50%, 75%, 98%)
linelist %>% 
  group_by(hospital) %>% 
  summarise(
    p05 = quantile(age_years, probs = 0.05, na.rm=T),
    p50 = quantile(age_years, probs = 0.5, na.rm=T),
    p75 = quantile(age_years, probs = 0.75, na.rm=T),
    p98 = quantile(age_years, probs = 0.98, na.rm=T)
    )
```

Bien que **dplyr** `summarise()` offre certainement un contrôle plus précis, vous trouverez peut-être que toutes les synthèses statistiques dont vous avez besoin peuvent être produites avec `get_summary_stat()` du package **rstatix**. Si l'on opère sur des données groupées, if retournera 0%, 25%, 50%, 75%, et 100%. Si elle est appliquée à des données non groupées, vous pouvez spécifier les percentiles avec `probs = c(.05, .5, .75, .98)`.

```{r}
linelist %>% 
  group_by(hospital) %>% 
  rstatix::get_summary_stats(age, type = "quantile")
```

```{r}
linelist %>% 
  rstatix::get_summary_stats(age, type = "quantile")
```

### Synthèse des données agrégées {.unnumbered}

*Si vous commencez avec des données agrégées*, l'utilisation de `n()` renvoie le nombre de *lignes*, et non la somme des comptes agrégés. Pour obtenir la somme, utilisez `sum()` sur la colonne des comptages des données.

Par exemple, disons que vous commencez avec le tableau de données de comptage ci-dessous, appelé `linelist_agg` - il montre en format "long" le nombre de cas par résultat et par sexe.

Ci-dessous, nous créons cet exemple de tableau de données de comptage de `linelist` par résultat et par sexe (les valeurs manquantes sont supprimées pour plus de clarté).

```{r}
linelist_agg <- linelist %>% 
  drop_na(gender, outcome) %>% 
  count(outcome, gender)

linelist_agg
```

Pour additionner les valeurs (dans la colonne `n`) par groupe, vous pouvez utiliser `summarise()` mais définissez la nouvelle colonne égale à `sum(n, na.rm=T)`. Pour ajouter un élément de condition à l'opération de somme, vous pouvez utiliser la syntaxe des sous-ensembles \[ \] sur la colonne des comptages.

```{r}
linelist_agg %>% 
  group_by(outcome) %>% 
  summarise(
    total_cases  = sum(n, na.rm=T),
    male_cases   = sum(n[gender == "m"], na.rm=T),
    female_cases = sum(n[gender == "f"], na.rm=T))
```

### `across()` multiples colonnes {.unnumbered}

Vous pouvez utiliser `summarise()` sur plusieurs colonnes en utilisant `across()`. Cela vous facilite la tâche lorsque vous voulez calculer les mêmes statistiques pour plusieurs colonnes. Placez `across()` dans `summarise()` et spécifiez ce qui suit :

-   `.cols =` comme un vecteur de noms de colonnes `c()` ou des fonctions d'aide "tidyselect" (expliquées ci-dessous)\`\
-   `.fns =` la fonction à exécuter (sans parenthèses) - vous pouvez en fournir plusieurs dans une `list()`.

Ci-dessous, la fonction `mean()` est appliquée à plusieurs colonnes numériques. Un vecteur de colonnes est nommé explicitement dans `.cols =` et une seule fonction `mean` est spécifiée (sans parenthèses) dans `.fns =`. Tout argument supplémentaire pour la fonction (par exemple `na.rm=TRUE`) est fourni après `.fns =`, séparé par une virgule.

Il peut être difficile de respecter l'ordre des parenthèses et des virgules lorsqu'on utilise `across()`. N'oubliez pas qu'à l'intérieur de `across()`, vous devez inclure les colonnes, les fonctions et tous les arguments supplémentaires nécessaires aux fonctions.

```{r}
linelist %>% 
  group_by(outcome) %>% 
  summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm),  # colonnes
                   .fns = mean,                               # fonction
                   na.rm=T))                                  # arguments supplémentaires
```

Plusieurs fonctions peuvent être exécutées en même temps. Ci-dessous, les fonctions `mean` et `sd` sont fournies à `.fns =` dans une `list()`. Vous avez la possibilité de fournir des noms de caractères (par exemple "mean" et "sd") qui sont ajoutés dans les nouveaux noms de colonnes.

```{r}
linelist %>% 
  group_by(outcome) %>% 
  summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm), # colonnes
                   .fns = list("mean" = mean, "sd" = sd),    # fonctions multiples 
                   na.rm=T))                                 # arguments supplémentaires
```

Voici les fonctions d'aide "tidyselect" que vous pouvez fournir à `.cols =` pour sélectionner des colonnes :

-   `everything()` - toutes les autres colonnes non mentionnées\
-   `last_col()` - la dernière colonne\
-   `where()` - applique une fonction à toutes les colonnes et sélectionne celles qui sont VRAIES\
-   `starts_with()` - correspond à un préfixe spécifié. Exemple : `starts_with("date")`\
-   `ends_with()` - correspond à un suffixe spécifié. Exemple : \`ends_with("\_end")\`\`\
-   `contains()` - colonnes contenant une chaîne de caractères. Exemple : `contains("time")`\
-   `matches()` - pour appliquer une expression régulière (regex). Exemple : `contains("[pt]al")`\
-   `num_range()` -
-   `any_of()` - correspond si la colonne est nommée. Utile si le nom peut ne pas exister. Exemple : `any_of(date_onset, date_death, cardiac_arrest)`

Par exemple, pour retourner la moyenne de chaque colonne numérique, utilisez `where()` et fournissez la fonction `as.numeric()` (sans parenthèses). Tout cela reste dans la commande `across()`.

```{r}
linelist %>% 
  group_by(outcome) %>% 
  summarise(across(
    .cols = where(is.numeric),  # toutes les colonnes numériques dans le tableau de données
    .fns = mean,
    na.rm=T))
```

### Pivot élargi {#tbls_pivot_wider .unnumbered}

Si vous préférez votre tableau en format "large", vous pouvez le transformer en utilisant la fonction **tidyr** `pivot_wider()`. Vous devrez probablement renommer les colonnes avec `rename()`. Pour plus d'informations, consultez la page sur [le pivotement des données](#pivoting_data).

L'exemple ci-dessous commence avec la table "longue" `age_by_outcome` de la \[section proportions\](#tbl_dplyr_prop). Nous le créons à nouveau et le présentons à l'impression, pour plus de clarté :

```{r}
age_by_outcome <- linelist %>%                  # commencez par la linelist
  group_by(outcome) %>%                         # groupe par outcome 
  count(age_cat) %>%                            # regrouper et compter par age_cat, puis supprimer le regroupement age_cat
  mutate(percent = scales::percent(n / sum(n))) # calculer le pourcentage - noter que le dénominateur est par groupe de outcome
```

```{r, echo=F}
DT::datatable(age_by_outcome, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Pour effectuer un pivot plus large, nous créons les nouvelles colonnes à partir des *valeurs* de la colonne existante `age_cat` (en définissant `names_from = age_cat`). Nous spécifions également que les nouvelles valeurs de la table proviendront de la colonne existante `n`, avec `values_from = n`. Les colonnes non mentionnées dans notre commande de pivotement (`outcome`) resteront inchangées à l'extrême gauche.

```{r}
age_by_outcome %>% 
  select(-percent) %>%   # maintenir seulement compte pour la simplicité
  pivot_wider(names_from = age_cat, values_from = n)  
```

### Total de lignes {#tbl_dplyr_totals .unnumbered}

Lorsque `summarise()` opère sur des données groupées, il ne produit pas automatiquement des statistiques "totales". Ci-dessous, deux approches pour ajouter une ligne de total sont présentées :

#### **janitor**'s `adorn_totals()` {.unnumbered}

Si votre table consiste uniquement en des nombres ou des proportions/pourcentages qui peuvent être additionnés en un total, alors vous pouvez ajouter des totaux *sum* en utilisant `adorn_totals()` de **janitor** comme décrit dans la section ci-dessus. Notez que cette fonction ne peut additionner que les colonnes numériques - si vous voulez calculer d'autres statistiques totales, voyez l'approche suivante avec **dplyr**.

Ci-dessous, `linelist` est groupé par sexe et résumé dans un tableau qui décrit le nombre de cas dont l'issue est connue, les décès et les guéris. En passant le tableau à `adorn_totals()`, on ajoute une ligne de total en bas reflétant la somme de chaque colonne. Les autres fonctions `adorn_*()` ajustent l'affichage comme indiqué dans le code.

```{r}
linelist %>% 
  group_by(gender) %>%
  summarise(
    known_outcome = sum(!is.na(outcome)),           # Nombre de lignes dans le groupe où le outcome n'est pas manquant
    n_death  = sum(outcome == "Death", na.rm=T),    # Nombre de lignes dans le groupe où outcome est égale a Death.
    n_recover = sum(outcome == "Recover", na.rm=T), # Nombre de lignes dans le groupe où outcome est egale à Recovered.
  ) %>% 
  adorn_totals() %>%                                # Adorn total ligne (somme de chaque colonne numérique)
  adorn_percentages("col") %>%                      # Obtenir les proportions des colonnes
  adorn_pct_formatting() %>%                        # Convertir les proportions en pourcentages
  adorn_ns(position = "front")                      # Afficher les % et les comptes (avec les comptes en avant)
```

#### `summarise()` Sur "total" des données et ensuite `bind_rows()` {.unnumbered}

Si votre tableau est composé de données de synthèse statistiques telles que `median()`, `mean()`, etc, l'approche `adorn_totals()` présentée ci-dessus ne sera *pas* suffisante. Pour obtenir des données de synthèse pour l'ensemble des données, vous devez les calculer avec une commande séparée `summarise()` et ensuite lier les résultats au tableau de synthèse original. Pour faire la liaison, vous pouvez utiliser `bind_rows()` de **dplyr** comme décrit dans la page [Joining data](#joining_matching). Vous trouverez ci-dessous un exemple :

Vous pouvez faire un tableau de synthèse des résultats *par hôpital* avec `group_by()` et `summarise()` comme ceci :

```{r, warning=F, message=F}
by_hospital <- linelist %>% 
  filter(!is.na(outcome) & hospital != "Missing") %>%  # Supprimez les cas avec outcome ou  hôpital manquant
  group_by(hospital, outcome) %>%                      # Données du groupe
  summarise(                                           # Créer de nouvelles colonnes de résumé des indicateurs intéressants
    N = n(),                                            # Nombre de lignes par groupe d'hôpitaux et outcome     
    ct_value = median(ct_blood, na.rm=T))               # Valeur médiane du CT par groupe
  
by_hospital # Afficher la table
```

Pour obtenir les totaux, exécutez la même commande `summarise()` mais regroupez les données uniquement par résultat (et non par hôpital), comme ceci :

```{r}
totals <- linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Regroupés uniquement par outcome, et non par hôpital    
      summarise(
        N = n(),                                       # Ces statistiques sont maintenant par outcome seulement     
        ct_value = median(ct_blood, na.rm=T))

totals # print table
```

Nous pouvons lier ces deux tableaux ensemble. Notez que `by_hospital` a 4 colonnes alors que `totals` a 3 colonnes. En utilisant `bind_rows()`, les colonnes sont combinées par nom, et tout espace supplémentaire est rempli avec `NA` (par exemple les valeurs de la colonne `hospital` pour les deux nouvelles lignes `totals`). Après avoir lié les lignes, nous convertissons ces espaces vides en "Total" en utilisant `replace_na()` (voir la page [Nettoyage des données et des fonctions de base](#cleaning_data)).

```{r}
table_long <- bind_rows(by_hospital, totals) %>% 
  mutate(hospital = replace_na(hospital, "Total"))
```

Voici le nouveau tableau avec les lignes "Total" en bas.

```{r, message=FALSE, echo=F}
DT::datatable(table_long, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```

Ce tableau est dans un format "long", ce qui peut correspondre à ce que vous souhaitez. *En option*, vous pouvez *pivoter* ce tableau *plus large* pour le rendre plus lisible. Consultez la section sur le pivotement plus large ci-dessus, ainsi que la page [Pivoter les données](#pivoting_data). Vous pouvez également ajouter plus de colonnes, et les arranger joliment. Ce code se trouve ci-dessous.

```{r}
table_long %>% 
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Passer du long au large
    values_from = c(ct_value, N),                       # les nouvelles valeurs proviennent des colonnes ct et count
    names_from = outcome) %>%                           # les nouveaux noms de colonnes proviennent des outcomes
  mutate(                                              # Ajouter de nouvelles colonnes
    N_Known = N_Death + N_Recover,                               # nombre avec résultat connu
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # Pourcentage des cas qui sont morts (à une décimale près)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # Pourcentage de guérison (à une décimale près)
  select(                                              # Réorganiser les colonnes
    hospital, N_Known,                                   # Introduction de colonnes
    N_Recover, Pct_Recover, ct_value_Recover,            # Colonnes récupérées
    N_Death, Pct_Death, ct_value_Death)  %>%             # Colonnes de décès
  arrange(N_Known)                                  # Ranger les rangées du plus bas au plus haut (rangée totale en bas)

```

Et vous pouvez ensuite afficher ce tableau sous la forme d'une image. Vous trouverez ci-dessous le résultat imprimé avec **flextable**. Vous pouvez lire plus en détail cet exemple et la façon d'obtenir ce "joli" tableau sur la page [Tableaux pour la présentation](#tables_presentation).

```{r echo=FALSE, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}

linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds")) 

border_style = officer::fp_border(color="black", width=1)

pacman::p_load(
  rio,            # import/export
  here,           # file pathways
  flextable,      # make pretty images of tables 
  officer,        # helper functions for tables
  tidyverse)      # data management, summary, and visualization

table <- linelist %>% 
  # filter
  ########
  #filter(!is.na(outcome) & hospital != "Missing") %>%  # Remove cases with missing outcome or hospital
  
  # Get summary values per hospital-outcome group
  ###############################################
  group_by(hospital, outcome) %>%                      # Group data
  summarise(                                           # Create new summary columns of indicators of interest
    N = n(),                                            # Number of rows per hospital-outcome group     
    ct_value = median(ct_blood, na.rm=T)) %>%           # median CT value per group
  
  # add totals
  ############
  bind_rows(                                           # Bind the previous table with this mini-table of totals
    linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouped only by outcome, not by hospital    
      summarise(
        N = n(),                                       # Number of rows for whole dataset     
        ct_value = median(ct_blood, na.rm=T))) %>%     # Median CT for whole dataset
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivot from long to wide
    values_from = c(ct_value, N),                       # new values are from ct and count columns
    names_from = outcome) %>%                           # new column names are from outcomes
  mutate(                                              # Add new columns
    N_Known = N_Death + N_Recover,                               # number with known outcome
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # percent who recovered (to 1 decimal)
  select(                                              # Re-order columns
    hospital, N_Known,                                   # Intro columns
    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns
    N_Death, Pct_Death, ct_value_Death)  %>%             # Death columns
  arrange(N_Known) %>%                                 # Arrange rows from lowest to highest (Total row at bottom)

  # formatting
  ############
  flextable() %>% 
  add_header_row(
    top = TRUE,                # New header goes on top of existing header row
    values = c("Hospital",     # Header values for each column below
               "Total cases with known outcome", 
               "Recovered",    # This will be the top-level header for this and two next columns
               "",
               "",
               "Died",         # This will be the top-level header for this and two next columns
               "",             # Leave blank, as it will be merged with "Died"
               "")) %>% 
    set_header_labels(         # Rename the columns in original header row
      hospital = "", 
      N_Known = "",                  
      N_Recover = "Total",
      Pct_Recover = "% of cases",
      ct_value_Recover = "Median CT values",
      N_Death = "Total",
      Pct_Death = "% of cases",
      ct_value_Death = "Median CT values")  %>% 
  merge_at(i = 1, j = 3:5, part = "header") %>% # Horizontally merge columns 3 to 5 in new header row
  merge_at(i = 1, j = 6:8, part = "header") %>%  
  border_remove() %>%  
  theme_booktabs() %>% 
  vline(part = "all", j = 2, border = border_style) %>%   # at column 2 
  vline(part = "all", j = 5, border = border_style) %>%   # at column 5
  merge_at(i = 1:2, j = 1, part = "header") %>% 
  merge_at(i = 1:2, j = 2, part = "header") %>% 
  width(j=1, width = 2.7) %>% 
  width(j=2, width = 1.5) %>% 
  width(j=c(4,5,7,8), width = 1) %>% 
  flextable::align(., align = "center", j = c(2:8), part = "all") %>% 
  bg(., part = "body", bg = "gray95")  %>% 
  colformat_num(., j = c(4,7), digits = 1) %>% 
  bold(i = 1, bold = TRUE, part = "header") %>% 
  bold(i = 6, bold = TRUE, part = "body")


table
```

## **gtsummary** package {#tbl_gt}

Si vous voulez afficher vos synthèses statistiques dans un joli graphique, vous pouvez utiliser le package **gtsummary** et sa fonction `tbl_summary()`. Le code peut sembler complexe au début, mais les résultats sont très jolis et apparaissent dans votre panneau de visualisation RStudio sous forme d'image HTML. Lire une [vignette ici](http://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html).

Vous pouvez également ajouter les résultats des tests statistiques aux tableaux **gtsummary**. Ce processus est décrit dans la section **gtsummary** de la page [Tests statistiques simples](#stats_test).

Pour présenter `tbl_summary()`, nous allons d'abord montrer le comportement le plus basique, qui produit effectivement un grand et beau tableau. Ensuite, nous examinerons en détail comment faire des ajustements et des tableaux plus adaptés.

### Tableau de synthèse {.unnumbered}

Le comportement par défaut de `tbl_summary()` est assez incroyable - il prend les colonnes que vous fournissez et crée un tableau de synthèse en une seule commande. La fonction affiche les statistiques appropriées à la classe de la colonne : la médiane et l'écart inter-quartile (IQR) pour les colonnes numériques, et le nombre (%) pour les colonnes catégorielles. Les valeurs manquantes sont converties en "Inconnu". Des notes de bas de page sont ajoutées en bas de page pour expliquer les statistiques, tandis que le N total est affiché en haut de page.

```{r, warning=F, message=F}
linelist %>% 
  select(age_years, gender, outcome, fever, temp, hospital) %>%  # ne gardez que les colonnes d'intérêt
  tbl_summary()                                                  # defaut
```

### Ajustements {.unnumbered}

Nous allons maintenant expliquer le fonctionnement de la fonction et la manière de procéder aux ajustements. Les principaux arguments sont détaillés ci-dessous :

**`by =`**\
Vous pouvez stratifier votre tableau par une colonne (par exemple par `outcome`), créant ainsi un tableau à 2 dimensions.

**`statistic =`**\
Utilisez une équation pour spécifier les statistiques à afficher et comment les afficher. L'équation comporte deux côtés, séparés par un tilde `~`. Sur le côté droit, entre guillemets, se trouve l'affichage statistique souhaité, et sur la gauche se trouvent les colonnes auxquelles cet affichage s'appliquera.

-   Le côté droit de l'équation utilise la syntaxe de `str_glue()` de **stringr** (voir [Caractères et chaînes de caractères](#character_strings)), avec la chaîne d'affichage souhaitée entre guillemets et les statistiques elles-mêmes entre crochets. Vous pouvez inclure des statistiques comme "n" (pour les comptes), "N" (pour le dénominateur), "mean", "median", "sd", "max", "min", les percentiles comme "p##" comme "p25", ou le pourcentage du total comme "p". Voir `?tbl_summary` pour plus de détails.\
-   Pour le côté gauche de l'équation, vous pouvez spécifier les colonnes par leur nom (par exemple, `age` ou `c(age, gender)`) ou en utilisant des aides telles que `all_continuous()`, `all_categorical()`, `contains()`, `starts_with()`, etc.

Un exemple simple d'équation `statistic =` pourrait ressembler à ce qui suit, pour afficher uniquement la moyenne de la colonne `age_years` :

```{r}
linelist %>% 
  select(age_years) %>%         #  Ne gardez que les colonnes d'intérêt 
  tbl_summary(                  #  créer un tableau récapitulatif
    statistic = age_years ~ "{mean}") # Impression de la moyenne d'âge
```

Une équation un peu plus complexe pourrait ressembler à ""({min}, {max})"\`, incorporant les valeurs max et min entre parenthèses et séparées par une virgule :

```{r}
linelist %>% 
  select(age_years) %>%                       #  Ne gardez que les colonnes d'intérêt 
  tbl_summary(                                # créer un tableau résumé
    statistic = age_years ~ "({min}, {max})") # Impression des valeurs minimale et maximale de l'âge
```

Vous pouvez également différencier la syntaxe pour des colonnes ou des types de colonnes distincts. Dans l'exemple plus complexe ci-dessous, la valeur fournie à `statistc =` est une **liste** indiquant que pour toutes les colonnes continues, le tableau doit afficher la moyenne avec l'écart-type entre parenthèses, tandis que pour toutes les colonnes catégorielles, il doit afficher le n, le dénominateur et le pourcentage.

**`digits =`**\
Ajuste les chiffres et les arrondis. En option, il est possible de spécifier que cela ne concerne que les colonnes continues (comme ci-dessous).

**`label =`**\
Ajustez la façon dont le nom de la colonne doit être affiché. Fournissez le nom de la colonne et son étiquette souhaitée, séparés par un tilde. La valeur par défaut est le nom de la colonne.

**`missing_text =`**\
Ajustez la façon dont les valeurs manquantes sont affichées. La valeur par défaut est "Inconnu".

**`type =`**\
Permet de régler le nombre de niveaux de statistiques à afficher. La syntaxe est similaire à `statistic =` en ce sens que vous fournissez une équation avec des colonnes à gauche et une valeur à droite. Voici deux scénarios courants :

-   `type = all_categorical() ~ "categorical"` Force les colonnes dichotomiques (par exemple, `fièvre` oui/non) à montrer tous les niveaux au lieu de ne montrer que la ligne "oui"
-   `type = all_continuous() ~ "continuous2"` Permet des statistiques multi-lignes par variable, comme indiqué dans une section ultérieure.

Dans l'exemple ci-dessous, chacun de ces arguments est utilisé pour modifier le tableau de synthèse original :

```{r}
linelist %>% 
  select(age_years, gender, outcome, fever, temp, hospital) %>% #  Ne gardez que les colonnes d'intérêt 
  tbl_summary(     
    by = outcome,                                               # stratifier le tableau entier par résultat
    statistic = list(all_continuous() ~ "{mean} ({sd})",        # stats et format pour les colonnes continues
                     all_categorical() ~ "{n} / {N} ({p}%)"),   # stats et format pour les colonnes catégorielles
    digits = all_continuous() ~ 1,                              # arrondi pour les colonnes continues
    type   = all_categorical() ~ "categorical",                 # force l'affichage de tous les niveaux catégoriels
    label  = list(                                              # affichage des étiquettes pour les noms de colonnes
      outcome   ~ "Outcome",                           
      age_years ~ "Age (years)",
      gender    ~ "Gender",
      temp      ~ "Temperature",
      hospital  ~ "Hospital"),
    missing_text = "Missing"                                    # comment les valeurs manquantes doivent être affichées
  )
```

### Statistiques multi-lignes pour les variables continues {.unnumbered}

Si vous souhaitez afficher plusieurs lignes statistiques pour des variables continues, vous pouvez l'indiquer en définissant le `type =` à "continuous2". Vous pouvez combiner tous les éléments présentés précédemment dans un seul tableau en choisissant les statistiques que vous voulez afficher. Pour cela, vous devez indiquer à la fonction que vous voulez récupérer un tableau en entrant le type comme "continuous2". Le nombre de valeurs manquantes est indiqué comme "Inconnu".

```{r}
linelist %>% 
  select(age_years, temp) %>%                      # Ne garder que les colonnes d'intérêt
  tbl_summary(                                     # # créer un tableau résume
    type = all_continuous() ~ "continuous2",       # indique que vous voulez imprimer plusieurs statistiques 
    statistic = all_continuous() ~ c(
      "{mean} ({sd})",                             # ligne 1 : moyenne et SD
      "{median} ({p25}, {p75})",                   # ligne 2 : médiane et IQR
      "{min}, {max}")                              # ligne 3: min et max
    )
```

Il existe de nombreuses autres façons de modifier ces tableaux, notamment en ajoutant des valeurs p, en ajustant la couleur et les titres, etc. La plupart sont décrites dans la documentation (entrez `?tbl_summary` dans Console), et certaines sont données dans la section sur les [tests statistiques](https://epirhandbook.com/simple-statistical-tests.html).

## **extension/package R base**

Vous pouvez utiliser la fonction `table()` pour faire des tableaux et des tableaux croisés de colonnes. Contrairement aux options ci-dessus, vous devez spécifier le tableau de données chaque fois que vous faites référence à un nom de colonne, comme indiqué ci-dessous.

[***CAUTION:*** `Les valeurs`NA`(manquantes) ne seront **pas** affichées en tableau, à moins que vous n'incluiez l'argument`useNA = "always"\` (qui peut également être défini sur "no" ou "ifany").]{style="color: orange;"}

[***TIP:*** Vous pouvez utiliser le `%$%` de **magrittr** pour supprimer la répétition des enregistrements de données du tableaux dans les fonctions **base**. Par exemple, on pourrait écrire `linelist %$% table(outcome, useNA = "always")`.]{style="color: darkgreen;"}

```{r}
table(linelist$outcome, useNA = "always")
```

Plusieurs colonnes peuvent être croisées en les listant l'une après l'autre, séparées par des virgules. En option, vous pouvez attribuer à chaque colonne un "nom" comme `Outcome = linelist$outcome`.

```{r}
age_by_outcome <- table(linelist$age_cat, linelist$outcome, useNA = "always") # sauvegarder le tableau comme objet
age_by_outcome   # imprimer le tableau
```

### Proportions {.unnumbered}

Pour retourner les proportions, passez le tableau ci-dessus à la fonction `prop.table()`. Utilisez l'argument `margins =` pour spécifier si vous voulez que les proportions soient des lignes (1), des colonnes (2), ou du tableau entier (3). Pour plus de précisions, nous envoyons le tableau à la fonction `round()` de **base** R, en spécifiant 2 chiffres.

```{r}
# obtenir les proportions du tableau défini ci-dessus, par lignes, arrondies
prop.table(age_by_outcome, 1) %>% round(2)
```

### Totals {.unnumbered}

Pour ajouter les totaux des lignes et des colonnes, passez le tableau à `addmargins()`. Cela fonctionne à la fois pour les nombres et les proportions.

```{r}
addmargins(age_by_outcome)
```

### Convertir en tableau de données {.unnumbered}

Convertir un objet `table()` directement en tableau de données n'est pas simple. Une approche est démontrée ci-dessous :

1)  Créez la table, \*sans utiliser `useNA = "always"`. A la place, convertissez les valeurs `NA` en "(Missing)" avec `fct_explicit_na()` de **forcats**.\
2)  Ajoutez les totaux (facultatif) en utilisant \`addmargins()\`\`.
3)  Passez le tableau dans la fonction R **base** \`as.data.frame.matrix()\`\`.
4)  Passez le tableau dans la fonction **tibble** `rownames_to_column()`, en spécifiant le nom de la première colonne.
5)  Affichez, visualisez ou exportez comme vous le souhaitez. Dans cet exemple, nous utilisons `flextable()` du package **flextable** comme décrit dans la page [Tableaux pour la présentation](#tables_presentation). Cela permettra d'afficher dans le volet de visualisation de RStudio une jolie image HTML.

```{r, warning=F, message=F}
table(fct_explicit_na(linelist$age_cat), fct_explicit_na(linelist$outcome)) %>% 
  addmargins() %>% 
  as.data.frame.matrix() %>% 
  tibble::rownames_to_column(var = "Age Category") %>% 
  flextable::flextable()
```

<!-- ======================================================= -->

## Ressources

La plupart des informations contenues dans cette page sont issues de ces ressources et des sites Internet :

[gtsummary](http://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html)

[dplyr](https://dplyr.tidyverse.org/articles/grouping.html)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/tables_descriptive.Rmd-->

# Tests statistiques simples {#stats_test}


Cette page décrit comment réaliser des tests statistiques simples en utilisant **base** R, **rstatix** et **gtsummary**.  

* Test T  
* Test de Shapiro-Wilk  
* Test de la somme des rangs de Wilcoxon  
* Test de Kruskal-Wallis  
* Test du khi carré  
* Corrélations entre variables numériques  

... plusieurs d'autres tests peuvent être effectués, mais nous ne présentons que ceux qui sont les plus utilisés et nous fournissons des liens vers plus de documentation.  

Chacun des packages susmentionnés a des avantages et des désavantages :   

* Utilisez les fonctions de **base** R pour afficher les résultats statistiques dans la Console R.   
* Utilisez les fonctions de **rstatix** package pour afficher les résultats dans un tableau de données, ou si vous voulez que les tests soient effectués par groupe.  
* Utilisez **gtsummary** package si vous souhaitez produire des tableaux prêts à être publiés.    



<!-- ======================================================= -->
## Préparation {  }


### Importation des packages {.unnumbered}

Ce bloc de code montre l'importation des packages nécessaires pour les analyses. Dans ce manuel, nous soulignons la fonction `p_load()` de **pacman**, qui installe le package si nécessaire *et* l'importe pour utilisation. Vous pouvez aussi importer les packages déjà installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les packages R. 


```{r}
pacman::p_load(
  rio,          # pour importation des fichiers
  here,         # chemins de fichiers
  skimr,        # obtenir un aperçu des données
  tidyverse,    # gestion des données + graphiques ggplot2, 
  gtsummary,    # statistiques et tests sommaires
  rstatix,      # statistiques
  corrr,        # analyse de corrélation pour les variables numériques
  janitor,      # ajouter des totaux et des pourcentages à des tableaux
  flextable     # transformer les tableaux en HTML
  )
```

### Importation des données {.unnumbered}

Nous importons les données des cas d'une épidémie d'Ebola simulée. Si vous souhaitez suivre, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger le "clean" linelist</a> (as .rds file). Importez les données avec la fonction `import()` du package **rio** (cette fonction supporte de nombreux types de fichiers comme .xlsx, .csv, .rds - voir la page [Importation et exportation](#import_export) pour plus de détails).   


```{r, echo=F}
# importez linelist 
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# importez linelist 
linelist <- import("linelist_cleaned.rds")
```

Les 50 premières lignes de la liste des lignes sont affichées ci-dessous.

```{r, message=FALSE, echo=F}
# afficher les données linelist sous forme de tableau
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





## **Base** R {}

Vous pouvez utiliser les fonctions de **base** R pour effectuer des tests statistiques. Les commandes sont relativement simples et les résultats sont affichés dans la Console R pour une visualisation simple. Cependant, les résultats sont généralement des listes et sont donc plus difficiles à manipuler si vous souhaitez utiliser les résultats dans des opérations ultérieures. 

### Test T {.unnumbered} 

Un [t-test](https://en.wikipedia.org/wiki/Student%27s_t-test), aussi appelé "test t de Student", est généralement utilisé pour déterminer s'il existe une différence significative entre les moyennes d'une variable numérique entre deux groupes. Nous allons montrer ici quelle syntaxe utiliser pour effectuer ce test selon si les colonnes se trouvent dans le même tableau de données.

**Syntaxe 1:** Voici la syntaxe à utiliser lorsque les colonnes numériques et catégorielles se trouvent dans le même tableau de données. Fournissez la colonne numérique sur la gauche de l'équation et la colonne catégorielle sur la droite. Précisez le tableau de données à `data = `. Optionnellement, définissez`paired = TRUE`, et `conf.level = ` (0.95 par défaut), et `alternative = ` (soit "two.sided", "less", or "greater"). Entrez `?t.test` pour plus de détails.  

```{r}
## comparer l'âge moyen par groupe avec un test t.
t.test(age_years ~ gender, data = linelist)
```

**Syntaxe 2:** Vous pouvez comparer deux vecteurs numériques distincts en utilisant cette syntaxe alternative. Par exemple, si les deux colonnes se trouvent dans des tableau de données différents.  

```{r, eval=F}
t.test(df1$age_years, df2$age_years)
```

Vous pouvez aussi utiliser un test t pour déterminer si la moyenne d'un échantillon est significativement différente d'une valeur spécifique. Ici, nous effectuons un one-sample t-test avec une moyenne de population connue/hypothétique `mu = ` : 

```{r, eval=F}
t.test(linelist$age_years, mu = 45)
```

### Test de Shapiro-Wilk {.unnumbered}  

Le [Shapiro-Wilk test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test) peut être utilisé pour déterminer si un échantillon provient d'une population normalement distribuée (une hypothèse de nombreux autres tests et analyses, tels que le test t). Cependant, il ne peut être utilisé que sur un échantillon de 3 à 5000 observations. Pour des échantillons plus importants, un [quantile-quantile plot](https://ggplot2.tidyverse.org/reference/geom_qq.html) peut être utile.  


```{r, eval=F}
shapiro.test(linelist$age_years)
```

### Test de la somme des rangs de Wilcoxon {.unnumbered}

Le test de la somme des rangs de Wilcoxon, aussi appelé [test U de Mann-Whitney](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test), est souvent utilisé pour déterminer si deux échantillons numériques proviennent de la même distribution lorsque leurs populations ne sont pas normalement distribuées ou présentent une variance inégale.

```{r wilcox_base}

## comparer la distribution des âges par groupe de résultats avec un test de Wilcox.
wilcox.test(age_years ~ outcome, data = linelist)

```


### Test de Kruskal-Wallis {.unnumbered}


Le [test de Kruskal-Wallis](https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_one-way_analysis_of_variance) est une extension du test de la somme des rangs de Wilcoxon qui peut être utilisé pour tester les différences dans la distribution de plus de deux échantillons. Lorsque deux échantillons sont utilisés, ce test donne des résultats identiques à ceux du test de la somme des rangs de Wilcoxon.


```{r }

## comparer la distribution des âges par groupe de résultats avec un test de Kruskal-Wallis.
kruskal.test(age_years ~ outcome, linelist)

```

### Test du khi carré {.unnumbered} 

[Pearson's Chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test) est utilisé pour tester des différences significatives entre des groupes catégorielles. 

```{r}

## comparer les proportions dans chaque groupe avec un test de chi-carré
chisq.test(linelist$gender, linelist$outcome)

```



## Le **rstatix** package {}

Le package **rstatix** offre la possibilité d'exécuter des tests statistiques et de recueillir les résultats dans un cadre "pipe-friendly". Les résultats sont automatiquement intégrés dans un tableau de données afin que vous puissiez effectuer des opérations ultérieures sur les résultats. Il est aussi facile de regrouper les données transmises dans les fonctions, afin que les statistiques soient exécutées pour chaque groupe.  


### Statistiques sommaires {.unnumbered}  

La fonction `get_summary_stats()` est un moyen rapide de retourner des statistiques sommaires. Il suffit de passer vos données à cette fonction et de préciser les colonnes à analyser. Si aucune colonne n'est précisée, les statistiques sont calculées pour toutes les colonnes.

Par défaut, une gamme complète de statistiques sommaires est retournée : n, max, min, médiane, 25%ile, 75%ile, IQR, écart absolu médian (mad), moyenne, écart-type, erreur-type, et un intervalle de confiance de la moyenne. 


```{r}
linelist %>%
  rstatix::get_summary_stats(age, temp)
```

Vous pouvez préciser un sous-groupe de statistiques sommaires à retourner en fournissant l'une des valeurs suivantes à `type = ` : "full", "common", "robust", "five_number", "mean_sd", "mean_se", "mean_ci", "median_iqr", "median_mad", "quantile", "mean", "median", "min", "max".  

Elle peut également être utilisée avec des données groupées, de sorte qu'une ligne est renvoyée pour chaque variable de groupement :  

```{r}
linelist %>%
  group_by(hospital) %>%
  rstatix::get_summary_stats(age, temp, type = "common")
```

Vous pouvez aussi utiliser **rstatix** pour effectuer des tests statistiques :  

### Test T {.unnumbered}  

Utilisez une syntaxe de formule pour préciser les colonnes numériques et catégorielles :  

```{r}
linelist %>% 
  t_test(age_years ~ gender)
```

Ou utilisez `~ 1` et spécifiez `mu = ` pour un one-sample T-test. Cela peut aussi être fait par groupe.  

```{r}
linelist %>% 
  t_test(age_years ~ 1, mu = 30)
```

Si applicable, les tests statistiques peuvent être effectués par groupe, comme illustré ci-dessous :  

```{r}
linelist %>% 
  group_by(gender) %>% 
  t_test(age_years ~ 1, mu = 18)
```

### Test de Shapiro-Wilk {.unnumbered}  

Comme indiqué précédemment, la taille de l'échantillon doit être entre 3 et 5000.  

```{r}
linelist %>% 
  head(500) %>%            # les 500 premières lignes du case linelist, pour illustration seulement 
  shapiro_test(age_years)
```

### Test de la somme des rangs de Wilcoxon {.unnumbered}  

```{r}
linelist %>% 
  wilcox_test(age_years ~ gender)
```


### Test de Kruskal-Wallis {.unnumbered}  

Aussi appelé le test U de Mann-Whitney.  

```{r}
linelist %>% 
  kruskal_test(age_years ~ outcome)
```


### Test du khi carré {.unnumbered}  

La fonction de khi carré peut accepter un tableau, donc nous allons d'abord créer un tableau croisé. Il existe de plusieurs méthodes de créer un tableau croisé (voir [Tableaux descriptifs](#descriptive_tables)) mais ici nous utilisons `tabyl()` de **janitor** et nous supprimons la colonne la plus à gauche des labels de valeur avant de passer à `chisq_test()`. 

```{r}
linelist %>% 
  tabyl(gender, outcome) %>% 
  select(-1) %>% 
  chisq_test()

```

De nombreuses autres fonctions et tests statistiques peuvent être exécutés avec les fonctions de **rstatix**. Consultez la documentation de **rstatix** [online here](https://github.com/kassambara/rstatix) ou en entrant ?rstatix.  





## Le `gtsummary` package {#stats_gt}

Utilisez **gtsummary** si vous cherchez à ajouter les résultats d'un test statistique à un beau tableau qui a été créé avec ce package (comme décrit dans la section **gtsummary** de la page [Tableaux descriptifs](#descriptive_tables)).  

Effectuer des tests statistiques de comparaison avec `tbl_summary` se fait en ajoutant la fonction `add_p` à une table et en précisant le test à utiliser. Il est possible d'obtenir des valeurs p ajustées pour multiples tests en utilisant la fonction `add_q`. Exécutez `?tbl_summary` pour plus de détails. 

### Test du khi carré {.unnumbered}

Comparez les proportions d'une variable catégorielle dans deux groupes. Le test statistique par défaut pour `add_p()` lorsqu'il est appliqué à une variable catégorielle est d'effectuer un test d'indépendance du khi-carré avec correction de continuité, mais si le nombre d'appels attendus est inférieur à 5, alors un test exact de Fisher est utilisé.

```{r chi_gt}
linelist %>% 
  select(gender, outcome) %>%    # garder les variables d'intérêt
  tbl_summary(by = outcome) %>%  # produire un tableau sommaire et préciser la variable de groupement
  add_p()                        # préciser le test à effectuer
```


### Test T {.unnumbered} 

Comparez la différence de moyennes entre deux groupes de variables continues. 
Par exemple, comparez l'âge moyen selon le statut du patient. 

```{r ttest_gt}

linelist %>% 
  select(age_years, outcome) %>%             # garder les variables d'intérêt
  tbl_summary(                               # produire un tableau sommaire
    statistic = age_years ~ "{mean} ({sd})", # préciser quel statistique a afficher
    by = outcome) %>%                        # préciser la variable de groupement
  add_p(age_years ~ "t.test")                # préciser le test à effectuer


```

### Test de la somme des rangs de Wilcoxon {.unnumbered}

Comparez la distribution d'une variable continue dans deux groupes. La méthode par défaut 
est d'utiliser le test de la somme des rangs de Wilcoxon et la médiane (IQR) pour comparer deux groupes. 
Cependant, pour les données de distribution non normale ou la comparaison de plusieurs groupes, 
le test de Kruskal-wallis est plus approprié. 

```{r wilcox_gt}

linelist %>% 
  select(age_years, outcome) %>%                       # garder les variables d'intérêt
  tbl_summary(                                         # produire un tableau sommaire
    statistic = age_years ~ "{median} ({p25}, {p75})", # préciser quel statistique a afficher (ceci est par défaut et peut donc être supprimé)
    by = outcome) %>%                                  # préciser la variable de groupement
  add_p(age_years ~ "wilcox.test")                     # préciser le test à effectuer


```

### Test de Kruskal-Wallis {.unnumbered}

Comparer la distribution d'une variable continue dans deux ou plusieurs groupes, peu importe si les données sont normalement distribuées ou pas. 

```{r kruskal_gt}

linelist %>% 
  select(age_years, outcome) %>%                       # garder les variables d'intérêt
  tbl_summary(                                         # produire un tableau sommaire
    statistic = age_years ~ "{median} ({p25}, {p75})", # préciser quel statistique a afficher (ceci est par défaut et peut donc être supprimé)
    by = outcome) %>%                                  # préciser la variable de groupement
  add_p(age_years ~ "kruskal.test")                    # préciser le test à effectuer


```




<!-- ## `dplyr` package {} -->

<!-- Performing statistical tests in `dplyr` alone is very dense, again because it  -->
<!-- does not fit within the tidy-data framework. It requires using `purrr` to create -->
<!-- a list of dataframes for each of the subgroups you want to compare. See the page on [Iteration, loops, and lists] to learn about **purrr**.   -->

<!-- An easier alternative may be the `rstatix` package.  -->

<!-- ### T-tests {.unnumbered}  -->

<!-- ```{r ttest_dplyr} -->

<!-- linelist %>%  -->
<!--   ## only keep variables of interest -->
<!--   select(age, outcome) %>%  -->
<!--   ## drop those missing outcome  -->
<!--   filter(!is.na(outcome)) %>%  -->
<!--   ## specify the grouping variable -->
<!--   group_by(outcome) %>%  -->
<!--   ## create a subset of data for each group (as a list) -->
<!--   nest() %>%  -->
<!--   ## spread in to wide format -->
<!--   pivot_wider(names_from = outcome, values_from = data) %>%  -->
<!--   mutate( -->
<!--     ## calculate the mean age for the death group -->
<!--     Death_mean = map(Death, ~mean(.x$age, na.rm = TRUE)), -->
<!--     ## calculate the sd among dead  -->
<!--     Death_sd = map(Death, ~sd(.x$age, na.rm = TRUE)), -->
<!--     ## calculate the mean age for the recover group -->
<!--     Recover_mean = map(Recover, ~mean(.x$age, na.rm = TRUE)),  -->
<!--     ## calculate the sd among recovered  -->
<!--     Recover_sd = map(Recover, ~sd(.x$age, na.rm = TRUE)), -->
<!--     ## using both grouped data sets compare mean age with a t-test -->
<!--     ## keep only the p.value -->
<!--     t_test = map2(Death, Recover, ~t.test(.x$age, .y$age)$p.value) -->
<!--   ) %>%  -->
<!--   ## drop datasets  -->
<!--   select(-Death, -Recover) %>%  -->
<!--   ## return a dataset with the medians and p.value (drop missing) -->
<!--   unnest(cols = everything()) -->

<!-- ``` -->


<!-- ### Wilcoxon rank sum test {.unnumbered} -->

<!-- ```{r wilcox_dplyr} -->

<!-- linelist %>%  -->
<!--   ## only keep variables of interest -->
<!--   select(age, outcome) %>%  -->
<!--   ## drop those missing outcome  -->
<!--   filter(!is.na(outcome)) %>%  -->
<!--   ## specify the grouping variable -->
<!--   group_by(outcome) %>%  -->
<!--   ## create a subset of data for each group (as a list) -->
<!--   nest() %>%  -->
<!--   ## spread in to wide format -->
<!--   pivot_wider(names_from = outcome, values_from = data) %>%  -->
<!--   mutate( -->
<!--     ## calculate the median age for the death group -->
<!--     Death_median = map(Death, ~median(.x$age, na.rm = TRUE)), -->
<!--     ## calculate the sd among dead  -->
<!--     Death_iqr = map(Death, ~str_c( -->
<!--       quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE),  -->
<!--       collapse = ", " -->
<!--       )), -->
<!--     ## calculate the median age for the recover group -->
<!--     Recover_median = map(Recover, ~median(.x$age, na.rm = TRUE)),  -->
<!--     ## calculate the sd among recovered  -->
<!--     Recover_iqr = map(Recover, ~str_c( -->
<!--       quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE),  -->
<!--       collapse = ", " -->
<!--       )), -->
<!--     ## using both grouped data sets compare age distribution with a wilcox test -->
<!--     ## keep only the p.value -->
<!--     wilcox = map2(Death, Recover, ~wilcox.test(.x$age, .y$age)$p.value) -->
<!--   ) %>%  -->
<!--   ## drop datasets  -->
<!--   select(-Death, -Recover) %>%  -->
<!--   ## return a dataset with the medians and p.value (drop missing) -->
<!--   unnest(cols = everything()) -->

<!-- ``` -->

<!-- ### Kruskal-wallis test {.unnumbered} -->


<!-- ```{r kruskal_dplyr} -->

<!-- linelist %>%  -->
<!--   ## only keep variables of interest -->
<!--   select(age, outcome) %>%  -->
<!--   ## drop those missing outcome  -->
<!--   filter(!is.na(outcome)) %>%  -->
<!--   ## specify the grouping variable -->
<!--   group_by(outcome) %>%  -->
<!--   ## create a subset of data for each group (as a list) -->
<!--   nest() %>%  -->
<!--   ## spread in to wide format -->
<!--   pivot_wider(names_from = outcome, values_from = data) %>%  -->
<!--   mutate( -->
<!--     ## calculate the median age for the death group -->
<!--     Death_median = map(Death, ~median(.x$age, na.rm = TRUE)), -->
<!--     ## calculate the sd among dead  -->
<!--     Death_iqr = map(Death, ~str_c( -->
<!--       quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE),  -->
<!--       collapse = ", " -->
<!--       )), -->
<!--     ## calculate the median age for the recover group -->
<!--     Recover_median = map(Recover, ~median(.x$age, na.rm = TRUE)),  -->
<!--     ## calculate the sd among recovered  -->
<!--     Recover_iqr = map(Recover, ~str_c( -->
<!--       quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE),  -->
<!--       collapse = ", " -->
<!--       )), -->
<!--     ## using the original data set compare age distribution with a kruskal test -->
<!--     ## keep only the p.value -->
<!--     kruskal = kruskal.test(linelist$age, linelist$outcome)$p.value -->
<!--   ) %>%  -->
<!--   ## drop datasets  -->
<!--   select(-Death, -Recover) %>%  -->
<!--   ## return a dataset with the medians and p.value (drop missing) -->
<!--   unnest(cols = everything()) -->

<!-- ``` -->

<!-- ### Chi-squared test {.unnumbered}  -->


<!-- ```{r} -->
<!-- linelist %>%  -->
<!--   ## do everything by gender  -->
<!--   group_by(outcome) %>%  -->
<!--   ## count the variable of interest -->
<!--   count(gender) %>%  -->
<!--   ## calculate proportion  -->
<!--   ## note that the denominator here is the sum of each gender -->
<!--   mutate(percentage = n / sum(n) * 100) %>%  -->
<!--   pivot_wider(names_from = outcome, values_from = c(n, percentage)) %>%  -->
<!--   filter(!is.na(gender)) %>%  -->
<!--   mutate(pval = chisq.test(linelist$gender, linelist$outcome)$p.value) -->
<!-- ``` -->


<!-- ======================================================= -->

## Corrélations 

La corrélation entre les variables numériques peut être étudiée en utilisant le package **tidyverse**  
**corrr**. Il vous permet de calculer les corrélations en utilisant Pearson, Kendall
tau ou Spearman rho. Le package crée un tableau et dispose également d'une fonction pour 
pour tracer automatiquement les valeurs. 

```{r, warning=F, message=F}

correlation_tab <- linelist %>% 
  select(generation, age, ct_blood, days_onset_hosp, wt_kg, ht_cm) %>%   # garder les variables numeriques d'intérêt
  correlate()      # créer une table de corrélation (en utilisant le pearson par défaut)

correlation_tab    # afficher

## supprimer les entrées dupliquées (le tableau précédent est dupliqué) 
correlation_tab <- correlation_tab %>% 
  shave()

## voir le tableau de corrélation
correlation_tab

## graphique des corrélations 
rplot(correlation_tab)
```


<!-- ======================================================= -->

## Ressources {  }

La plupart des informations contenues dans cette page sont adaptées de ces ressources et vignettes disponibles en ligne :  

[gtsummary](http://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html)
[dplyr](https://dplyr.tidyverse.org/articles/grouping.html)
[corrr](https://corrr.tidymodels.org/articles/using-corrr.html)
[sthda correlation](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/stat_tests.Rmd-->

# Régression univariée et multivariable {#regression}

<!-- ======================================================= -->

Cette page montre comment utiliser des fonctions de régression **base** de R telles que `glm()` et le package **gtsummary** pour 
examiner les relations entre les variables (par exemple, les rapports de cotes, les rapports de risque et les
ratios de risque). Il utilise également des fonctions comme `tidy()` du package **broom** pour nettoyer les sorties de régression.  

1.  Univarié : tableaux deux par deux 
2.  Stratifié : estimations mantel-haenszel  
3.  Multivariable : sélection des variables, sélection du modèle, tableau final
4.  Forest graphe

Pour la régression des risques proportionnels de Cox, voir la page [Analyse d'enquête](#survival_analysis).




<span style="color: black;">**_NOTE:_** Nous utilisons le terme *multivariable* pour faire référence à une régression avec plusieurs variables explicatives. En effet, un modèle *multivarié* serait une régression avec plusieurs résultats - voir ceci [éditorial](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3518362/) pour plus de detail </span> 

<!-- ======================================================= -->

## Preparation {  }


### Chargement des packages {.unnumbered}

Ce bout de code montre le chargement des packages nécessaires aux analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` du package  **pacman**, qui installe le package si nécessaire *et* le charge pour l'utiliser. Vous pouvez également charger les packages installés avec `library()` de **base** R. Voir la page sur [R basics](#rbasics) pour plus d'informations sur les packages de R. 

 

```{r}
pacman::p_load(
  rio, # Importation du fichier
  here, # Localisation de fichiers
  tidyverse, # gestion des données + graphiques ggplot2, 
  stringr, # manipuler des chaînes de texte 
  purrr, # boucle sur les objets d'une manière ordonnée
  gtsummary, # statistiques et tests sommaires 
  broom, # met de l'ordre dans les résultats des régressions
  lmtest, # tests du rapport de vraisemblance
  parameters, # alternative pour mettre de l'ordre dans les résultats des régressions
  see # alternative pour visualiser les parcelles forestières
  )
```

### Importation de données {.unnumbered}

Nous importons le jeu de données des cas d'une épidémie d'Ebola simulée. Si vous voulez continuer dans le processus d'acquisition de données suivait ce lien, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquer pour téléchager le jeu de données  linelist "propre"</a> (as .rds file). Importez vos données avec la fonction `import()` du packages **rio** (elle accepte de nombreux types de fichiers comme .xlsx, .rds, .csv - voir la page [Importer et exporter des données](#import_export) pour plus de détails)..  


```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# importer la liste de lignes
linelist <- import("linelist_cleaned.rds")
```

Les 50 premières lignes de linelist sont affichées ci-dessous.

```{r, message=FALSE, echo=F}
# afficher les données linelist sous forme de tableau
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T) )
```

### Nettoyer les données {.unnumbered}

#### Stocker les variables explicatives {.unnumbered}  

Nous stockons les noms des colonnes explicatives sous la forme d'un vecteur de caractères. Il sera référencé plus tard.  

```{r}
## definir les variables d'interet 
explanatory_vars <- c("gender", "fever", "chills", "cough", "aches", "vomit")
```


#### Convertir en 1 et 0 {.non numéroté}   

Ci-dessous, nous convertissons les colonnes explicatives de "yes"/"no", "m"/"f", et "dead"/"alive" en **1 / 0**, pour se conformer avec les attentes des modèles de régression logistique. Pour faire cela efficacement, nous avons utilisé `across()` de **dplyr** pour transformer plusieurs colonnes en une seule fois. La fonction que nous appliquons à chaque colonne est `case_when()` (également **dplyr**) qui applique une logique pour convertir les valeurs spécifiées en 1 et 0. Voir les sections sur `across()` et `case_when()` dans la page [Nettoyage de données et fonctions essentielles](#cleaning_data)).  

Note : le "." ci-dessous représente la colonne qui est traitée par `across()` à ce moment-là.


```{r}
## convertir les  variables dichotomique   en  0/1 
linelist <- linelist %>%  
  mutate(across(                                      
    .cols = all_of(c(explanatory_vars, "outcome")),  ## pour chaque colonne listée et "résultat"
    .fns = ~case_when(                              
      . %in% c("m", "yes", "Death")   ~ 1,           ## recoder male, yes et death en 1
      . %in% c("f", "no",  "Recover") ~ 0,           ## female, no and recover en 0
      TRUE                            ~ NA_real_)    ## autre definir comme valeurs manquantes
    )
  )

       
      
```

#### Supprimer les lignes avec des valeurs manquantes {.unnumbered}  

Pour supprimer les lignes avec des valeurs manquantes, vous pouvez utiliser la fonction **tidyr** `drop_na()`. Cependant, nous ne voulons l'utiliser que pour les lignes qui ont des valeurs manquantes dans les colonnes qui nous intéressent.  

La première chose que nous devons faire est de nous assurer que notre vecteur `explanatory_vars` exlu la colonne `age` (`age` aurait produit une erreur dans l'opération précédente `case_when()`, qui ne concernait que les variables dichotomiques). Ensuite, nous envoyons la liste de lignes à `drop_na()` pour enlever toutes les lignes avec des valeurs manquantes dans la colonne `outcome` ou dans l'une des colonnes `explanatory_vars`.  

Avant d'exécuter le code, le nombre de lignes dans la `linelist` est ` nrow(linelist)`.  

```{r}
## ajout de la catégorie d'âge aux variables explicatives 
explanatory_vars <- c(explanatory_vars, "age_cat")

## supprimer les lignes avec des informations manquantes pour les variables d'intérêt 
linelist <- linelist %>% 
  drop_na(any_of(c("outcome", explanatory_vars)))

```

le nombre de lignes restant dans `linelist` est de ` nrow(linelist)`.  


<!-- ======================================================= -->

## Univarié {  }

Tout comme dans la page sur les [Tableaux descriptifs](#descriptive_tables), votre cas d'utilisation déterminera le package R que vous utiliserez. Nous vous présentons deux options pour effectuer une analyse univariée :  

* Utiliser les fonctions disponibles dans **base** R pour afficher rapidement les résultats sur la console. Utilisez le package **broom** pour mettre de l'ordre dans les résultats.  
* Utilisez le package **gtsummary** pour modéliser et obtenir des résultats prêts à être publiés.  



<!-- ======================================================= -->

### **base** R {.unnumbered}

#### Régression linéaire {.unnumbered}  

La fonction **base** R `lm()` effectue une régression linéaire, évaluant la relation entre une réponse numérique et des variables explicatives qui sont supposées avoir une relation linéaire.  

Fournissez l'équation sous forme de formule, avec les noms des colonnes de réponse et d'explication séparés par un tilde `~`. Spécifiez également l'ensemble de données à `data = `. Définissez les résultats du modèle comme un objet R, à utiliser ultérieurement.    

```{r lin_reg}
lm_results <- lm(ht_cm ~ age, data = linelist)
```

Vous pouvez ensuite exécuter `summary()` sur les résultats du modèle pour voir les coefficients (Estimations), la valeur P, les résidus, et d'autres mesures.  

```{r lin_reg_res}
summary(lm_results)
```

Vous pouvez également utiliser la fonction `tidy()` du package **broom** pour afficher les résultats dans un tableau. 
les résultats dans un tableau. Les résultats nous indiquent que pour chaque année de plus dans l'âge d'un individu, la taille augmente de de 3,5 cm, ce qui est statistiquement significatif. 

```{r lin_reg_res_tidy}
tidy(lm_results)
```

Vous pouvez également utiliser cette régression pour l'ajouter à un **ggplot**. 
d'abord prendre les points des données observées et la colonne predite à partir de la ligne ajustée dans un dataframe en utilisant la fonction `augment()` de **broom**. 

```{r lin_reg_res_plot}

## rassembler les points de régression et les données observées dans un seul ensemble de données
points <- augment(lm_results)

## creer un graphique  avec age comme   axe  des abscisses
ggplot(points, aes(x = age)) + 
  ## ajouter point pour l'ordonné
  geom_point(aes(y = ht_cm)) + 
  ## ajouter de la droite de régression linéaire
  geom_line(aes(y = .fitted), colour = "red")

```

Il est également possible d'ajouter une droite de régression linéaire simple dans **ggplot** 
en utilisant la fonction `geom_smooth()`. 

```{r geom_smooth}

## ajoute ta donnée dans le graphe
 ggplot(linelist, aes(x = age, y = ht_cm)) + 
  ## montrer les points
  geom_point() + 
  ## ajouter une regression linéaire
  geom_smooth(method = "lm", se = FALSE)
```

Consultez la section Ressources à la fin de ce chapitre pour obtenir des didacticiels plus détaillés.  


#### Régression logistique {.unnumbered}  

La fonction `glm()` du package **stats** (faisant partie de **base** R) est utilisée pour ajuster les modèles linéaires généralisés (GLM).  

`glm()` peut être utilisée pour la régression logistique univariée et multivariée (par exemple pour obtenir des Odds Ratios). Voici les parties principales :  

```{r, eval=F}
# arguments for glm()
glm(formula, family, data, weights, subset, ...)
```

* `formula = ` Le modèle est fourni à `glm()` sous forme d'équation, avec le résultat à gauche et les variables explicatives à droite d'un tilde `~`.  
* `family = ` Ceci détermine le type de modèle à exécuter. Pour la régression logistique, utilisez `famille = "binomiale"`, pour le poisson utilisez `famille = "poisson"`. D'autres exemples sont dans le tableau ci-dessous.  
* `data = ` Spécifiez votre dataframe 


Si nécessaire, vous pouvez également spécifier la fonction de lien via la syntaxe `family = familytype(link = "linkfunction"))`. Vous pouvez en savoir plus dans la documentation sur les autres familles et les arguments optionnels tels que `weights = ` et `subset = ` (`?glm`).  



Famille                 | Fonction de liaison par défaut 
-----------------------|-------------------------------------------  
`"binomial"` | `(lien = "logit")`  
`"gaussian"` | `(lien = "identity")`  
`"Gamma"` | `(lien = "inverse")`  
`"inverse.gaussian"` | `(link = "1/mu^2")`  
`"poisson"` | `(lien = "log")`  
`"quasi"` | `(lien = "identity", variance = "constant")`  
`"quasibinomial"` | `(lien = "logit")`  
`"quasipoisson"` | `(lien = "log")`  


Lorsque vous exécutez `glm()`, il est plus courant de sauvegarder les résultats comme un objet R nommé. Vous pouvez ensuite afficher les résultats sur votre console en utilisant `summary()` comme indiqué ci-dessous, ou effectuer d'autres opérations sur les résultats (par exemple, exponentiation).  

Si vous avez besoin d'exécuter une régression binomiale négative, vous pouvez utiliser le package **MASS** ; le `glm.nb()` utilise la même syntaxe que `glm()`. 
Pour une présentation des différentes régressions, consultez la [UCLA stats page](https://stats.idre.ucla.edu/other/dae/). 

#### Univarié `glm()` {.unnumbered}

Dans cet exemple, nous évaluons la relation entre différentes catégories d'âge et le résultat du décès (codé 1 dans la section Préparation). Nous présentons ci-dessous un modèle univarié de `outcome` par `age_cat`. Nous enregistrons la sortie du modèle sous le nom de `model` et nous l'affichons ensuite avec `summary()` sur la console. Notez que les estimations fournies sont les *log odds* et que le niveau de base est le premier niveau du facteur `age_cat` ("0-4").  

```{r}
model <- glm(outcome ~ age_cat, family = "binomial", data = linelist)
summary(model)
```

Pour modifier le niveau de base d'une variable donnée, assurez-vous que la colonne est  de classe facteur et déplacez le niveau désiré à la première position avec `fct_relevel()` (voir la page sur [Factors](#factors)). Par exemple, ci-dessous, nous prenons la colonne `age_cat` et définissons "20-29" comme niveau de base avant de passer le dataframe  modifié dans `glm()`.  

```{r}
linelist %>%
  mutate(age_cat = fct_relevel(age_cat, "20-29", after = 0)) %>% 
  glm(formula = outcome ~ age_cat, family = "binomial") %>% 
  summary()
```

#### Affichage des résultats {.unnumbered}

Pour la plupart des utilisations, plusieurs modifications doivent être apportées aux sorties ci-dessus. La fonction `tidy()` du package **broom** est pratique pour rendre les résultats du modèle lisibles et comprehensibles.  

Nous montrons ici comment combiner les sorties du modèle avec une table de comptage.  

1) Obtenez les estimations du logarithm de l'odd ratio *exponentiées* et les intervalles de confiance en passant le modèle à `tidy()` et en définissant `exponentiate = TRUE` et `conf.int = TRUE`.  

```{r odds_base_single}

model <- glm(outcome ~ age_cat, family = "binomial", data = linelist) %>% 
  tidy(exponentiate = TRUE, conf.int = TRUE) %>%        # exponentiée et  généré IC
  mutate(across(where(is.numeric), round, digits = 2))  # arrondir tous les colonnes numeriques
```

Voici la sortie du tibble `model` :  

```{r, message=FALSE, echo=F}
# afficher   linelist data comme un tableau
DT::datatable(model, rownames = FALSE, options = list(pageLength = nrow(model), scrollX=T), class = 'white-space: nowrap' )
```

2) Combinez les résultats de ces modèles avec un tableau de comptage. Ci-dessous, nous créons un tableau de comptage croisé avec la fonction `tabyl()` de **janitor**, comme indiqué dans la page [Tableaux descriptifs](#descriptive_tables).  

```{r}
counts_table <- linelist %>% 
  janitor::tabyl(age_cat, outcome)
```


<!-- * Grouper les lignes par résultat, et obtenir les effectifs par catégorie d'âge -->
<!-- * Effectuez un pivot plus large pour que les colonnes soient `age_cat`, `0`, et `1` -->
<!-- * Supprimez la ligne pour `NA` `age_cat`, si applicable, pour l'aligner avec les résultats du modèle -->

<!-- ```{r} -->
<!-- counts_table <- linelist %>% -->
<!-- filter(!is.na(outcome) & !is.na(age_cat)) %>% # s'assurer que outcome et age_cat sont présents -->
<!-- group_by(outcome) %>% # obtient les effectifs de la variable d'intérêt groupés par outcome -->
<!-- count(age_cat) %>% ## obtient le nombre ou les lignes par combinaisons uniques outcome-catégorie d'âge -->
<!-- pivot_wider(names_from = outcome, values_from = n) ## étaler les données dans un format large (comme dans un tableau croisé) -->

<!-- ``` -->


Voici à quoi ressemble ce dataframe `counts_table` :  

```{r, message=FALSE, echo=F}
# afficher  la donnée linelist comme un tableau
DT::datatable(counts_table, rownames = FALSE, options = list(pageLength = nrow(counts_table), scrollX=T), class = 'white-space: nowrap' )
```

Maintenant, nous pouvons lier les résultats de `counts_table` et de `model` ensemble horizontalement avec `bind_cols()` (**dplyr**). Rappelez-vous qu'avec `bind_cols()` les lignes des deux dataframes doivent être parfaitement alignées. Dans ce code, comme nous effectuons des liaisons dans une chaîne de commandes, nous utilisons `.` pour représenter l'objet `counts_table` lorsque nous le lions à `model`. Pour terminer le processus, nous utilisons `select()` pour choisir les colonnes souhaitées et leur ordre, et enfin nous appliquons la fonction **base** R `round()` sur toutes les colonnes numériques pour spécifier 2 décimales.  

```{r, message=F, warning=F}
combined <- counts_table %>%           # debutons avec un tableau de comptage
  bind_cols(., model) %>%              # combiner avec les sorties de la regression
  select(term, 2:3, estimate,          # selectionner and arranger les cols
         conf.low, conf.high, p.value) %>% 
  mutate(across(where(is.numeric), round, digits = 2)) ## arrondir à deux chiffres apres la virgule
```

Voici à quoi ressemble le dataframe combiné, affiché joliment comme une image avec une fonction de **flextable**. La section [Tableau pour la presentationn](#tables_presentation) explique comment personnaliser de tels tableaux avec **flextable**, ou vous pouvez utiliser de nombreux autres packages tels que **knitr** ou **GT**.  

```{r}
combined <- combined %>% 
  flextable::qflextable()
```


#### Mettre en Boucle plusieurs modèles univariés {.unnumbered}  

Nous présentons ci-dessous une méthode utilisant `glm()` et `tidy()` pour une approche plus simple, voir la section sur **gtsummary**.  

Pour exécuter les modèles sur plusieurs variables d'explicative afin de produire des odds ratios univariés (c'est-à-dire sans contrôle des autres variables), vous pouvez utiliser l'approche ci-dessous. Elle utilise `str_c()` de **stringr** pour créer des formules univariées (voir [Caractères et chaînes de caractères](#character_strings)), exécute la régression `glm()` sur chaque formule, passe chaque sortie `glm()` à `tidy()` et enfin rassemble toutes les sorties du modèle avec `bind_rows()` de **tidyr**. Cette approche utilise `map()` du packages **purrr** pour itérer - voir la page sur [Iteration, loops, and lists] pour plus d'informations sur cet outil.  

1) Créez un vecteur de noms de colonnes des variables explicatives. Nous l'avons déjà en tant que `explanatory_vars` dans la section Préparation de cette page.  

2) Utilisez `str_c()` pour créer plusieurs formules de chaîne, avec `outcome` à gauche, et un nom de colonne de `explanatory_vars` à droite. Le point `.` remplace le nom de la colonne dans `explanatory_vars`.  

```{r}
explanatory_vars %>% str_c("outcome ~ ", .)
```

3) Passez ces formules de chaîne à `map()` et définissez `~glm()` comme la fonction à appliquer à chaque entrée. Dans `glm()`, définissez la formule de régression comme `as.formula(.x)` où `.x` sera remplacé par la formule de chaîne définie dans l'étape précédente. `map()` bouclera sur chacune des formules en format  chaîne de caractères, en effectuant des régressions pour chacune d'entre elles.  

4) Les résultats de cette première `map()` sont passés à une seconde commande `map()`, qui applique `tidy()` aux résultats de la régression.  

5) Finalement, la sortie de la seconde commande `map()` (une liste de dataframe triés) est condensée avec `bind_rows()`, qui donne un dataframe avec tous les résultats univariés.  


```{r odds_base_multiple}

models <- explanatory_vars %>%       # commencer avec les variables d'interets
  str_c("outcome ~ ", .) %>%         # combiner chaque variable dans une formule ("outcome ~ variable of interest")
  
  # itérer à travers chaque formule univariée
  map(                               
    .f = ~glm(                       # passer les formules une par une à glm()
      formula = as.formula(.x),      # dans glm(), la formule de la chaîne de caractère est .x
      family = "binomial",           # spécifier le type de glm (logistique)
      data = linelist)) %>%          # jeu de données
  
  # mettre en ordre chacun des résultats de la régression glm ci-dessus
  map(
    .f = ~tidy(
      .x, 
      exponentiate = TRUE,           # exponentiation 
      conf.int = TRUE)) %>%          # retourne les intervalles de confiance
  
  # réduire la liste des résultats de la régression en un seul dataframe
  bind_rows() %>% 
  
  # arrondir tous les colonnes numeriques
  mutate(across(where(is.numeric), round, digits = 2))
```

Cette fois, l'objet final `models` est plus long car il représente maintenant les résultats combinés de plusieurs régressions univariées. Cliquez pour voir toutes les lignes de `model`.  

```{r, message=FALSE, echo=F}
# afficher   linelist comme tableau
DT::datatable(models, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Comme précédemment, nous pouvons créer une table des effectifs à partir de la `linelist` pour chaque variable explicative, la lier à `models`, et faire une belle table. Nous commençons par les variables, et nous les parcourons avec `map()`. Nous itérons à travers une fonction définie par l'utilisateur qui implique la création d'une table d'effectifs avec les fonctions **dplyr**. Ensuite, les résultats sont combinés et liés aux résultats du modèle `models`.  


```{r, warning=F, message=F}

## pour chaque variable explicative
univ_tab_base <- explanatory_vars %>% 
  map(.f = 
    ~{linelist %>%                ## debuter avec  linelist
        group_by(outcome) %>%     ## grouper le jeu de donnée par outcome
        count(.data[[.x]]) %>%    ## produire des comptages pour la variable d'intérêt
        pivot_wider(              ## étendre à un format large (comme dans un tableau croisé)
          names_from = outcome,
          values_from = n) %>% 
        drop_na(.data[[.x]]) %>%         ## éliminer les lignes avec des valeurs manquantes
        rename("variable" = .x) %>%      ## changer la colonne de la variable d'intérêt en "variable".
        mutate(variable = as.character(variable))} ## convertir en caractères, sinon les variables non-dichotomiques (catégorielles) apparaissent comme des facteurs et ne peuvent pas être fusionnées.
      ) %>% 
  
  ## Réduire la liste des sorties de comptage à un seul dataframe
  bind_rows() %>% 
  
  ## fusionner avec les sorties de la régression 
  bind_cols(., models) %>% 
  
  ## ne garder que les colonnes intéressées 
  select(term, 2:3, estimate, conf.low, conf.high, p.value) %>% 
  
  ## arrondir les décimales
  mutate(across(where(is.numeric), round, digits = 2))

```

Voici à quoi ressemble le dataframe. Voir la page sur les [Tableau pour la presentationn](#tables_presentation) pour des idées sur la façon de convertir ce tableau en une jolie sortie HTML (par exemple avec **flextable**).  

```{r, message=FALSE, echo=F}
# afficher les données de la liste des lignes sous forme de tableau
DT::datatable(univ_tab_base, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





<!-- ======================================================= -->

### **gtsummary** package {#reg_gt_uni .unnumbered}

Nous présentons ci-dessous l'utilisation de `tbl_uvregression()` du package **gtsummary**. Tout comme dans la page sur les [Tableaux descriptifs](#descriptive_tables), les fonctions **gtsummary** font un bon travail pour exécuter des statistiques *et* produire des résultats à usage professionnel. Cette fonction produit un tableau des résultats d'une régression univariée.  

Nous ne sélectionnons que les colonnes nécessaires de la `linelist` (les variables explicatives et la variable de résultat) et les introduisons dans `tbl_uvregression()`. Nous allons exécuter une régression univariée sur chacune des colonnes que nous avons définies comme `explanatory_vars` dans la section Préparation des données (sexe, fièvre, frissons, toux, courbatures, vomissements, et age_cat).  

Dans la fonction elle-même, nous fournissons la `method = ` comme `glm` (sans guillemets), la colonne `y = ` outcome (`outcome`), nous spécifions à `method.args = ` que nous voulons exécuter une régression logistique via `family = binomial`, et nous lui disons d'exponentiser les résultats.  

La sortie est en HTML et contient les comptes

```{r odds_gt, message=F, warning=F}

univ_tab <- linelist %>% 
  dplyr::select(explanatory_vars, outcome) %>% ## selectionner variables d'interet

  tbl_uvregression(                         ## produire un tableau univarié
    method = glm,                           ## définir la régression que l'on veut exécuter (modèle linéaire généralisé)
    y = outcome,                            ## définir la variable de résultat
    method.args = list(family = binomial),  ## définir le type de glm que l'on veut exécuter (logistique)
    exponentiate = TRUE                     ## exponentiez pour produire des odds ratios (plutôt que des odds logarithmiques)
  )

## visualiser le tableau des résultats univariés 
univ_tab
```

Vous pouvez apporter de nombreuses modifications à ce tableau, par exemple en ajustant les étiquettes de texte, en mettant en gras les lignes en fonction de leur valeur p, etc. Voir les didacticiels [ici](http://www.danieldsjoberg.com/gtsummary/articles/tbl_regression.html) et ailleurs en ligne.  



<!-- ======================================================= -->

## Stratifié { }

L'analyse stratifiée est actuellement en cours de développement pour **gtsummary**, 
cette page sera mise à jour en temps voulu. 




## Multivariable  

Pour l'analyse multivariable, nous présentons à nouveau deux approches :  

* `glm()` et `tidy()`.  
* package **gtsummary**.  

La methodologie est similaire pour chacune d'entre elles et seule la dernière étape, celle de l'élaboration d'un tableau final, est différente.


### Conduite multivariable {.unnumbered}  


Ici, nous utilisons `glm()` mais ajoutons plus de variables au côté droit de l'équation, séparées par des symboles plus (`+`). 


Pour exécuter le modèle avec toutes nos variables explicatives, nous devrions exécuter :  

```{r}
mv_reg <- glm(outcome ~ gender + fever + chills + cough + aches + vomit + age_cat, family = "binomial", data = linelist)

summary(mv_reg)
```

Si vous voulez inclure deux variables et une interaction entre elles, vous pouvez les séparer avec un astérisque `*` au lieu d'un `+`. Séparez-les par un deux-points `:` si vous ne spécifiez que l'interaction. Par exemple :  

```{r, eval=F}
glm(outcome ~ gender + age_cat * fever, family = "binomial", data = linelist)
```


*Optionnellement*, vous pouvez utiliser ce code pour exploiter le vecteur prédéfini des noms de colonnes et recréer la commande ci-dessus en utilisant `str_c()`. Cela peut être utile si les noms de vos variables explicatives changent, ou si vous ne voulez pas les taper à nouveau.  

```{r mv_regression}

## effectuer une régression avec toutes les variables d'intérêt 
mv_reg <- explanatory_vars %>%  ## commencer par un vecteur de noms de colonnes explicatives
  str_c(collapse = "+") %>%     ## combiner tous les noms des variables d'intérêt séparés par un plus
  str_c("outcome ~ ", .) %>%    ## combiner les noms des variables d'intérêt avec le résultat dans le style d'une formule
  glm(family = "binomial",      ## définir le type de glm comme logistique,
      data = linelist)          ## définir votre jeu de données
```


#### Construire le modèle {.unnumbered}  

Vous pouvez construire votre modèle étape par étape, en enregistrant plusieurs modèles qui incluent certaines variables explicatives. Vous pouvez comparer ces modèles avec des tests de rapport de vraisemblance en utilisant `lrtest()` du package **lmtest**, comme ci-dessous :  

<span style="color: black;">**_NOTE:_** L'utilisation de **base** `anova(model1, model2, test = "Chisq)` produit les mêmes résultats </span> 

```{r}
model1 <- glm(outcome ~ age_cat, family = "binomial", data = linelist)
model2 <- glm(outcome ~ age_cat + gender, family = "binomial", data = linelist)

lmtest::lrtest(model1, model2)
```

Une autre option consiste à prendre l'objet modèle et à appliquer la fonction `step()` du package **stats**. Spécifiez la direction de sélection des variables que vous souhaitez utiliser lors de la construction du modèle.      

```{r}
## choisir un modèle en utilisant la sélection avant basée sur l'AIC
## vous pouvez aussi faire "backward" ou "both" en ajustant la direction.
final_mv_reg <- mv_reg %>%
  step(direction = "forward", trace = FALSE)
```


Vous pouvez également désactiver la notation scientifique dans votre session R, pour plus de clarté :  

```{r}
options(scipen=999)
```

Comme décrit dans la section sur l'analyse univariée, nous passons la sortie du modèle à `tidy()` pour exponentialiser les probabilités logarithmiques et les IC. Enfin, nous arrondissons toutes les colonnes numériques à deux décimales. Faites défiler pour voir toutes les lignes.  

```{r mv_regression_base}

mv_tab_base <- final_mv_reg %>% 
  broom::tidy(exponentiate = TRUE, conf.int = TRUE) %>%  ## obtenir un tidy  dataframe d'estimations 
  mutate(across(where(is.numeric), round, digits = 2))          ## arrondir
```

Voici à quoi ressemble le dataframe obtenu : 

```{r, message=FALSE, echo=F}
DT::datatable(mv_tab_base, rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```





<!-- ======================================================= -->

### Combiner univarié et multivariable {.unnumbered}

#### Combinez avec **gtsummary** {.unnumbered}  

Le package **gtsummary** fournit la fonction `tbl_regression()`, qui prendra . les sorties d'une régression (`glm()` dans ce cas) et produira un joli tableau de synthèse. 
tableau récapitulatif. 

```{r mv_regression_gt}
## montrer le tableau des résultats de la régression finale 
mv_tab <- tbl_regression(final_mv_reg, exponentiate = TRUE)
```

Voyons le tableau :  

```{r}
mv_tab
```

Vous pouvez également combiner plusieurs tableaux de sortie différents produits par **gtsummary** avec 
la fonction `tbl_merge()`. Nous combinons maintenant les résultats multivariables avec les résultats *univariés* de **gtsummary** que nous avons créés [ci-dessus](#reg_gt_uni):  

```{r}
## combiner avec les résultats univariés 
tbl_merge(
  tbls = list(univ_tab, mv_tab),                          # combiner
  tab_spanner = c("**Univariate**", "**Multivariable**")) # definier les entetes des colonnes
```



#### Combiner avec **dplyr** {.unnumbered}  

Une autre façon de combiner les sorties univariées et multivariées de `glm()`/`tidy()` est d'utiliser les fonctions de jonction **dplyr**.  

* fusionner les résultats univariés de tout à l'heure (`univ_tab_base`, qui contient les comptages) avec les résultats multivariables triés `mv_tab_base`.  
* Utilisez `select()` pour ne garder que les colonnes que nous voulons, spécifier leur ordre, et les renommer.  
* Utilisez `round()` avec deux décimales sur toutes les colonnes qui sont classe Double  

```{r, warning=F, message=F}
## combiner des tableaux univariés et multivariés 
left_join(univ_tab_base, mv_tab_base, by = "term") %>% 
  ##choisir les colonnes et les renommer
  select( # nouveau nom = ancien nom
    "characteristic" = term, 
    "recovered"      = "0", 
    "dead"           = "1", 
    "univ_or"        = estimate.x, 
    "univ_ci_low"    = conf.low.x, 
    "univ_ci_high"   = conf.high.x,
    "univ_pval"      = p.value.x, 
    "mv_or"          = estimate.y, 
    "mvv_ci_low"     = conf.low.y, 
    "mv_ci_high"     = conf.high.y,
    "mv_pval"        = p.value.y 
  ) %>% 
  mutate(across(where(is.double), round, 2))   

```




<!-- ======================================================= -->

## Forest plot { }

Cette section montre comment produire un graphique avec les résultats de votre régression.
Il y a deux options, vous pouvez construire un graphique vous-même en utilisant **ggplot2** ou utiliser un méta-package appelé **easystats** (un package qui inclut plusieurs packages). 
méta-package appelé **easystats** (un package qui inclut plusieurs packages).  

Consultez la page sur [Les bases de ggplot](#ggplot_basics) si vous n'êtes pas familier avec le package de traçage **ggplot2**.  


<!-- ======================================================= -->

### **ggplot2** package {.unnumbered}

Vous pouvez construire un graphique forest avec `ggplot()` en traçant les éléments des résultats de la régression multivariable. Ajoutez les couches des tracés en utilisant ces "geoms" :  

* estimations avec `geom_point()`  
* intervalles de confiance avec "geom_errorbar()`".  
* une ligne verticale à OR = 1 avec `geom_vline()`.  

Avant de tracer un graphique, vous pouvez utiliser `fct_relevel()` du package **forcats** pour définir l'ordre des variables/niveaux sur l'axe des ordonnées. `ggplot()` peut les afficher dans l'ordre alpha-numérique, ce qui ne fonctionnerait pas bien pour ces valeurs de catégories d'âge ("30" apparaîtrait avant "5"). Voir la page sur les [facteurs](#factors) pour plus de détails.  

```{r ggplot_forest}

## enlever le terme intercept dans le resultats multivariables
mv_tab_base %>% 
  
  #definir l'odre d'apparition des niveaux  le long de l'axe y 
  mutate(term = fct_relevel(
    term,
    "vomit", "gender", "fever", "cough", "chills", "aches",
    "age_cat5-9", "age_cat10-14", "age_cat15-19", "age_cat20-29",
    "age_cat30-49", "age_cat50-69", "age_cat70+")) %>%
  
  # supprimer la ligne denommé "intercept" dans le graphique
  filter(term != "(Intercept)") %>% 
  
  ## concevoir un graphe avec la variable sur l'axe des y et l'estimation (OR) sur l'axe des x
  
  ggplot(aes(x = estimate, y = term)) +
  
  ## montrer  estimate  comme un point
  geom_point() + 
  
  ## ajouter une barre d'erreur pour les intervalles de confiance
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) + 
  
  ## montrer où OR = 1 est pour référence comme une ligne pointillée
  geom_vline(xintercept = 1, linetype = "dashed")
  
```


<!-- ======================================================= -->

### Packages **easystats** {.unnumbered}

Une alternative, si vous ne voulez pas le bon niveau de contrôle  que **ggplot2** fournit, est d'utiliser une combinaison des packages **easystats**.  

La fonction `model_parameters()` du package **parameters** fait l'équivalent de la fonction du package **broom**.
de la fonction `tidy()` du package **broom**. Le package **see** accepte alors ces sorties
et crée un graphique forest par défaut sous la forme d'un objet `ggplot()`. 

```{r easystats_forest}
pacman::p_load(easystats)

## supprimer le terme interception de vos résultats multivariables
final_mv_reg %>% 
  model_parameters(exponentiate = TRUE) %>% 
  plot()
  
```


<!-- ======================================================= -->

## Ressources { }

Le contenu de cette page a été alimenté par ces ressources et vignettes en ligne :  

[Linear regression in R](https://www.datacamp.com/community/tutorials/linear-regression-R)  

[gtsummary](http://www.danieldsjoberg.com/gtsummary/articles/tbl_regression.html)  

[UCLA stats page](https://stats.idre.ucla.edu/other/dae/)  

[sthda stepwise regression](http://www.sthda.com/english/articles/36-classification-methods-essentials/150-stepwise-logistic-regression-essentials-in-r/)   

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/regression.Rmd-->


# Données manquantes {#missing_data}

```{r, out.width=c("50%"), echo=F}
knitr::include_graphics(here::here("images", "missingness.png"))
knitr::include_graphics(here::here("images", "missingness_overview.png"))
```

Dans ce chapitre nous allons :  
1) Évaluer l'ampleur des données manquantes  
2) Filtrer les lignes en contenant des données manquantes  
3) Visualiser les données manquantes au cours du temps  
4) Gérer comment les `NA` apparaissent dans les graphes  
5) Imputer des données manquantes : MMCA, MA, MOP  



<!-- ======================================================= -->
## Étapes préliminaires { }

### Importation des paquets {.unnumbered}  

Ces lignes de code chargent les paquets nécessaires aux analyses. Dans ce guide, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *puis* l'importe pour l'utiliser. Vous pouvez également charger les paquets installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les paquets R.  

```{r}
pacman::p_load(
  rio,           # import des fichiers
  tidyverse,     # gestion des données + graphiques (ggplot2)
  naniar,        # bilan des données manquantes
  mice           # imputation
)
```


### Importation des données {.unnumbered}

Nous importons un jeu de données de cas d'une épidémie d'ébola fictive. Pour reproduire les étapes, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la linelist "propre"</a> (as .rds file). Importez vos données avec la fonction `import()` du paquet **rio** (elle accepte de nombreux types de fichiers comme .xlsx, .rds, .csv - voir la page [Importation et exportation des données](import_export) pour plus de détails).  

```{r, echo=F}
# importer la linelist dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# importer la linelist dans R
linelist <- import("linelist_cleaned.rds")
```

Les cinquantes premières lignes sont affichées ci-dessous :  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter = "top",
              options = list(pageLength = 5, scrollX = T), 
              class = 'white-space: nowrap' )
```


### Conversion des données manquantes lors de l'import {.unnumbered}  

Il faut être particulièrement attentif aux valeurs qui doivent être classifiées comme "manquantes" lors de l'import des données. Des données manquantes peuvent par exemple être indiquées par 99, 999, "Manquant", un espace vide (" ") ou des cellules vides (""). Vous pouvez les convertir en `NA` via la fonction d'importation des données.  
Pour plus de détails, consultez la page sur l'importation des [Données manquantes](#missing_data), car la syntaxe exacte varie selon le type de fichier.  


<!-- ======================================================= -->
## Valeurs manquantes dans R { }

Nous explorons ci-dessous les façons dont les données manquantes sont représentées et évaluées dans R.  

### `NA` {.unnumbered}  

En R, les valeurs manquantes sont représentées par un mot réservé (spécial) : `NA` (pour _"Non available"_). Notez que ce mot est tapé *sans* guillemets, et ne doit pas être confondu avec une chaîne de caractères "NA" (également une parole des Beatles de la chanson _Hey Jude_).  

Les données manquantes peuvent avoir été encodées de divers manières dans les données brutes, telles que "99", "Manquant", "Inconnu", une valeur de caractère vide "" qui ressemble à un "blanc", ou un espace simple " ". Tenez-en compte et réfléchissez à l'opportunité de [les convertir en `NA` pendant l'importation](#missing_data) ou pendant le nettoyage des données avec `na_if()`.  


A l'inverse, lors du nettoyage des données, il peut également être pertinent de convertir des `NA` en "Manquant" (ou autre) avec les fonctions `replace_na()` ou `fct_explicit_na()` dans le cas des facteurs.    


### `NA` et ses dérivés {.unnumbered}  

La plupart du temps, `NA` représente une valeur manquante et il n'y a pas besoin de se poser plus de questions que ça. Cependant, dans certaines circonstances, il peut y avoir besoin de *variations* de `NA` spécifiques à une classe d'objet (caractère, numérique, etc.). C'est rare, mais ça peut arriver.

Parmi ces cas rares, la création d'une nouvelle colonne avec la fonction **dplyr** `case_when()` est le plus commun. Comme décrit dans la page [Nettoyage des données et fonctions de base](#cleaning_data), cette fonction évalue chaque ligne du dataframe, détermine si les lignes répondent à des critères logiques spécifiés (partie droite du code), et attribue la nouvelle valeur correcte (partie gauche du code). *Important : toutes les valeurs du côté droit doivent être de la même classe*.  

```{r, eval=F}
linelist <- linelist %>% 
  
  # Créer une nouvelle colonne "age_years" à partir de la colonne "age"
  mutate(age_years = case_when(
    age_unit == "years"  ~ age,    # si l'unité est années => garder la valeur originale
    age_unit == "months" ~ age/12, # l'unité est en mois, diviser par 12
    is.na(age_unit)      ~ age,    # si l'unité est manquante, supposer que l'age est en années
    TRUE                 ~ NA_real_)) # sinon, définir age comme valeur manquante
```

Afin que toutes les valeurs spécifiées du côté droit des équations aient le même type, il faut utiliser des dérivés de `NA` avec un type connu. Si les autres valeurs de droite sont des chaines de caractères, on peut utiliser `NA_character_` ou envisager d'utiliser "Manquant" à la place. Si les valeurs sont toutes numériques, utiliser `NA_real_`. S'il s'agit de dates ou de valeurs logiques, on peut conserver `NA`.  

* `NA` - à utiliser pour les dates ou les booléens VRAI/FAUX  
* `NA_character_` - à utiliser pour les chaines de caractères   
* `NA_real_` - pour les valeurs numériques  



Encore une fois, il est peu probable que vous rencontriez ces variations, hors utilisation de `case_when()` pour créer une nouvelle colonne. Consultez la [documentation R sur NA](https://stat.ethz.ch/R-manual/R-devel/library/base/html/NA.html) pour plus d'informations. 



### `NULL` {.unnumbered}  

`NULL` est un autre mot réservée en R. C'est la représentation logique d'une déclaration qui n'est ni vraie ni fausse. Elle est retournée par des expressions ou des fonctions dont les valeurs sont indéfinies. En général, n'assignez pas NULL comme valeur, à moins d'écrire des fonctions ou peut-être une [**shiny** app](#shiny) pour retourner `NULL` dans des scénarios spécifiques.  

La nullité peut être évaluée avec `is.null()` et la conversion peut être faite avec `as.null()`.  

Voir cet [article de blog](https://www.r-bloggers.com/2010/04/r-na-vs-null/) sur la différence entre `NULL` et `NA`. 



### `NaN` {.unnumbered}  

Les valeurs *impossibles* sont représentées par le mot spécial `NaN`. Par exemple, R renvoi `NaN` si vous lui demandez de diviser 0 par 0. `NaN` peut être évalué avec `is.nan()`. Il existe également des fonctions complémentaires comme `is.infinite()` et `is.finite()`. 


### `Inf` {.unnumbered}  

`Inf` représente une valeur infinie, telle que l'on peut par exemple obtenir en divisant un nombre par zéro.  


### Exemples {.unnumbered}  

Pour comprendre comment ce type de valeurs peuvent affecter vos analyses, imaginons que vous avez un vecteur `z` qui contient ces valeurs : `z <- c(1, 22, NA, Inf, NaN, 5)`.  

Si vous voulez utiliser la fonction `max()` sur la colonne pour trouver la valeur la plus élevée, vous pouvez utiliser le `na.rm = TRUE` pour omettre le `NA` du calcul. Mais cela n'enlèvera pas les `Inf` et `NaN`, ce qui fait que le résultat retourné sera `Inf`. Pour résoudre ce problème, vous pouvez utiliser les crochets `[ ]` et `is.finite()` pour effectuer un sous-ensemble de sorte que seules les valeurs finies soient utilisées pour le calcul : `max(z[is.finite(z)])`.  


```{r, eval=F}
z <- c(1, 22, NA, Inf, NaN, 5)
max(z)                           # retourne NA
max(z, na.rm=T)                  # retourne Inf
max(z[is.finite(z)])             # retourne 22
```

Instruction R | Sortie
----------|--------------
`5 / 0` | `Inf`  
`0 / 0` | `NaN`  
`5 / NA` | `NA`  
`5 / Inf | `0`  
`NA - 5` | `NA`  
`Inf / 5` | `Inf`  
`class(NA)` | "logical"  
`class(NaN)` | "numeric"  
`class(Inf)` | "numeric"  
`class(NULL)` | "NULL"  

Un message d'avertissement que vous rencontrerez certainement est "NAs introduits par coercition". Cela peut se produire si vous tentez d'effectuer une conversion illégale, par exemple en insérant une chaîne caractères dans un vecteur qui contient des valeurs numériques.   

```{r}
as.numeric(c("10", "20", "thirty", "40"))
```

Note : `NULL` est ignoré dans un vecteur.  

```{r}
my_vector <- c(25, NA, 10, NULL)  # définit
my_vector                         # affiche
```


Note : tenter de calculer la variance sur une valeur unique retourne également un `NA`.  

```{r}
var(22)
```


<!-- ======================================================= -->
## Fonctions utiles { }

Voici quelques fonctions utiles en **base** R pour détecter et gérer les valeurs manquantes.


### `is.na()` et `!is.na()` {.unnumbered}  

`is.na()` permet d'identifier les valeurs manquantes. Pour identifier les valeurs non manquantes il suffit d'utiliser son opposé en ajoutant `!` devant l'instruction. Ces deux méthodes retournent une valeur logique (`TRUE` ou `FALSE`). Pour rappel, il est possible de sommer le vecteur résultant avec `sum()` pour compter le nombre de `TRUE`. Par exemple :  `sum(is.na(linelist$date_outcome))`.    


```{r}
my_vector <- c(1, 4, 56, NA, 5, NA, 22)
is.na(my_vector)
!is.na(my_vector)
sum(is.na(my_vector))
```


### `na.omit()` {.unnumbered}  

Appliquée à un dataframe, cette fonction de **base** R supprimera les lignes dont  *toutes* les valeurs sont manquantes. Appliquée à un vecteur, elle supprimera les valeurs `NA` de ce vecteur. Par exemple :   


```{r}
na.omit(my_vector)
```

### `drop_na()` {.unnumbered}  

Il s'agit d'une fonction de **tidyr** utile pour [nettoyer des données dans un pipeline](#cleaning_data). Si elle est exécutée sans argument, elle supprime également les lignes dont *toutes* les valeurs sont manquantes. Mais si des noms de colonnes sont spécifiés comme arguments, seules les lignes avec des valeurs manquantes dans ces colonnes seront supprimées.   

Note : on peut utiliser la syntaxe "tidyselect" pour spécifier les colonnes.  

```{r, eval=F}
linelist %>% 
  drop_na(case_id, date_onset, age) # omet les lignes contenant des valeurs manquantes dans une de ces colonnes au moins
```


### `na.rm = TRUE` {.unnumbered}  

Lorsque vous exécutez une fonction mathématique telle que `max()`, `min()`, `sum()` ou `mean()`, la valeur retournée est `NA` si des valeurs `NA` sont présentes dans les données. Ce comportement par défaut est intentionnel, afin que vous soyez alerté si l'une de vos données est manquante.  

Vous pouvez éviter cela en supprimant les valeurs manquantes du calcul. Pour ce faire, incluez l'argument `na.rm = TRUE` (le "rm" étant une abréviation de "remove").  


```{r}
my_vector <- c(1, 4, 56, NA, 5, NA, 22)

mean(my_vector)     

mean(my_vector, na.rm = TRUE)
```



<!-- ======================================================= -->
## Identifier les valeurs manquantes dans un dataframe { }

Le package **naniar** permet de détecter et de visualiser l'ampleur de la complétude des données (et donc de leur non-complétude) dans un tableau de données.  

```{r}
# installer et charger le paquet
pacman::p_load(naniar)
```

### Quantifier les données manquantes {.unnumbered}

La fonction `pct_miss()` permet de calculer le pourcentage de toutes les valeurs manquantes. La fonction `n_miss()` renvoi le nombre de valeurs manquantes.  


```{r}
# pourcentage de données manquantes sur TOUTES les valeurs du dataframe
pct_miss(linelist)
```

Les deux fonctions ci-dessous renvoient le pourcentage de lignes dont une valeur est manquante ou qui sont entièrement complètes.  

Note : `NA` signifie manquant, mais que ``""` ou `""` ne sont pas considérées comme des valeurs manquantes.  


```{r}
# Pourcentage des lignes avec au moins une valeur manquante
pct_miss_case(linelist)   # utiliser n_miss() pour le nombre de lignes
```

```{r}
# Pourcentage des lignes sans valeur manquante
pct_complete_case(linelist) # utiliser n_complete() pour le nombre
```


### Visualiser les données manquantes {.unnumbered}  

La fonction `gg_miss_var()` renvoi le nombre (ou %) de valeurs manquantes dans chaque colonne. Quelques notes :  

* Il est possible d'ajouter un nom de colonne (pas entre guillemets) à l'argument `facet = ` pour voir le graphique par groupe.  

* Les nombres sont affichés par défaut. Utilisez `show_pct = TRUE` pour voir les pourcentages, .  

* Il est possible d'ajouter des étiquettes d'axe et de titre comme pour un `ggplot()` normal avec `+ labs(...)`.  


```{r}
gg_miss_var(linelist, show_pct = TRUE)
```

Ici, les données sont passées à la fonction à l'aide d'un pipe `%>%`. L'argument `facet = ` est utilisé pour séparer les données par outcome.   

```{r}
linelist %>% 
  gg_miss_var(show_pct = TRUE, facet = outcome)
```


La fonction `vis_miss()` permet de visualiser le dataframe sous forme de carte thermique qui indique quelle valeur est manquante. Vous pouvez également `select()` certaines colonnes du cadre de données et ne fournir que ces colonnes à la fonction.  

```{r}
# Carte thermique de la complétude des données à l'échelle du dataframe
vis_miss(linelist)
```


### Explorer et visualiser les relations entre données manquantes {.unnumbered} 

Comment visualiser quelque chose qui n'existe pas ??? Par défaut, `ggplot()` n'affiche pas les points avec des valeurs manquantes dans les graphiques.  

Le package **naniar** propose une solution via la fonction `geom_miss_point()`. Lors de la création d'un nuage de points à partir de deux variables, les paires de valeurs dont l'une est manquante sont montrés en fixant les valeurs manquante à 10% plus bas que la valeur minimale de la colonne, et en les colorant différemment.  

Dans le nuage de points ci-dessous, les points rouges sont des enregistrements où la valeur d'une des deux colonne est présente mais où l'autre est manquante. Cela permet de visualiser la distribution des valeurs manquantes par rapport à celle des valeurs non manquantes.  


```{r}
ggplot(
  data = linelist,
  mapping = aes(x = age_years, y = temp)) +     
  geom_miss_point()
```

Pour évaluer les données manquantes dans un dataframe en *stratifiant par une autre colonne*, on peut utiliser la fonction `gg_miss_fct()`, qui retourne une carte thermique du pourcentage de valeurs manquantes dans le dataframe *pour chaque catégorie d'une autre variable* :  


```{r}
gg_miss_fct(linelist, age_cat5)
```

Cette fonction peut aussi être utilisée sur une colonne contenant des dates pour voir comment la complétude des données change au cours du temps :  

```{r}
gg_miss_fct(linelist, date_onset)
```




### Colonnes "fantômes" {.unnumbered}

**naniar** donne la possibilité de créer un jeu de données "fantôme" ("shadow matrix" en Anglais) pour aller plus loin dans l'étude de la distribution des données manquantes. Essentiellement, pour chaque colonne existante la fonction `bind_shadow()` crée une nouvelle colonne binaire contenant soit `NA`, soit `!NA` (pour "non `NA`"), et lie toutes ces nouvelles colonnes au jeu de données original avec l'appendice "_NA". Cela double le nombre de colonnes du jeu de données :  

```{r}
shadowed_linelist <- linelist %>% 
  bind_shadow()

names(shadowed_linelist)
```

Ces colonnes "fantômes" peuvent être utilisées pour visualiser la proportion de valeurs manquantes dans une colonne en fonction d'une autre colonne.  

Par exemple, le graphique ci-dessous montre la proportion de données manquantes dans la colonne `days_onset_hosp` (le nombre de jours entre l'apparition des symptômes et l'hospitalisation), en fonction de la `date_hospitalisation`. Ici on trace la densité de données manquantes et non manquantes (`color = `) en fonction de la date d'hospitalisation.  

Ce type de visualisation fonctionne mieux si la variable tracée en sur l'axe des abscisses est numérique ou temporelle.   


```{r, message = F}
ggplot(data = shadowed_linelist,   # dataframe augmenté avec les colonnes fantômes
  mapping = aes(x = date_hospitalisation, # colonne numérique ou date
                colour = age_years_NA)) + # colonne fantôme d'intérêt
  geom_density()                          # trace les courbes de densité
```

Les colonnes fantômes peuvent aussi être utilisé comme stratification dans des statistiques descriptives :  

```{r}
linelist %>%
  bind_shadow() %>%                # création des colonnes fantômes
  group_by(date_outcome_NA) %>%    # groupe par la colonne fantôme de date_outcome
  summarise(across(
    .cols = age_years,             # variable d'intérêt à résumer
    .fns = list("mean" = mean,     # statistiques
                "sd"  = sd,
                "var" = var,
                "min" = min,
                "max" = max),  
    na.rm = TRUE))                 # autres arguments des fonctions statistiques
```

**naniar** n'est pas le seul outil pour représenter la proportion de valeurs manquantes dans une colonne en fonction du temps. On peut aussi manuellement :     

1) Agréger les données dans une unité de temps pertinente (jours, semaines, etc.), en résumant la proportion d'observations avec `NA` (et toute autre valeur d'intérêt). 

2) Tracez la proportion de données manquantes comme une ligne en utilisant `ggplot()`.  

Dans l'exemple ci-dessous, nous ajoutons une nouvelle colonne pour la semaine à la linelist, regroupons les données par semaine, puis calculons le pourcentage des enregistrements de cette semaine où la valeur est manquante. (note : si vous voulez le % de 7 jours, le calcul sera légèrement différent).  


```{r}
outcome_missing <- linelist %>%
  mutate(week = lubridate::floor_date(date_onset, "week")) %>%   # crée colonne semaine
  group_by(week) %>%      # groupe les lignes par semaine
  summarise(              # pour chaque semaine, résumme : 
    n_obs = n(),          # nombre total d'observations
    outcome_missing = sum(is.na(outcome) | outcome == ""),  # nombre d'obs avec valeur manquante
    outcome_p_miss  = outcome_missing / n_obs,    # proportion d'obs avec valeur manquante
  
    outcome_dead    = sum(outcome == "Death", na.rm=T),     # nb de morts
    outcome_p_dead  = outcome_dead / n_obs) %>%             # prop morts
  
  tidyr::pivot_longer(-week, names_to = "statistic") %>%    # pivote toutes les colonnes sauf la semaine en format long
  filter(stringr::str_detect(statistic, "_p_"))   # garde uniquement les proportions
```

Ensuite, nous traçons la proportion de données manquantes par semaine, sous forme de ligne.  
Référez vous à la page [bases de ggplot](#ggplot_basics) si vous n'êtes pas familier avec le package **ggplot2**.  

```{r, message=F, warning=F}
ggplot(data = outcome_missing) +
    geom_line(
      mapping = aes(x = week, 
                    y = value, 
                    group = statistic, 
                    color = statistic),
      size = 2,
      stat = "identity") +
    labs(title = "Weekly outcomes",
         x = "Week",
         y = "Proportion of weekly records") + 
     scale_color_discrete(
       name = "",
       labels = c("Died", "Missing outcome")) +
    scale_y_continuous(breaks = c(seq(0, 1, 0.1))) +
  theme_minimal() +
  theme(legend.position = "bottom")
```





<!-- ======================================================= -->
## Utiliser des données avec des valeurs manquantes  


### Filtrer les lignes avec valeurs manquantes {.unnumbered}

La fonction `drop_na()` de **dplyr** permet de se débarrasser rapidement des lignes avec des valeurs manquantes.  

La linelist originale contient ` nrow(linelist)` lignes. La linelist sans lignes avec des valeurs manquantes contient moins de lignes :   

```{r}
linelist %>% 
  drop_na() %>%     # filtre les lignes sans aucune valeur manquante
  nrow()
```

On peut choisir de ne se débarrasser des lignes avec des valeurs manquantes que dans certaines colonnes :  

```{r}
linelist %>% 
  drop_na(date_onset) %>% # omet les lignes avec des valeurs manquantes dans date_onset
  nrow()
```

On peut passer plusieurs colonnes l'une après l'autre à la fonction, ou utiliser des [fonctions utilitaires de "tidyselect"](#clean_tidyselect):  

```{r}
linelist %>% 
  drop_na(contains("date")) %>% # omet lignes avec NA dans n'importe quelle colonne dont le nom contient "date"
  nrow()
```



<!-- ======================================================= -->
### Gérer les `NA` dans `ggplot()` {.unnumbered}

Il est souvent judicieux de signaler le nombre de valeurs exclues d'un graphique au lecteur du graphique. 

Dans `ggplot()`, la fonction `labs()`a un argument `caption = ` qui ajoute un texte de légende sous le graphique. On peut utiliser `str_glue()` du package **stringr** pour concaténer valeurs et chaînes de caractères ensemble dans une phrase qui s'ajuste automatiquement aux données (voir exemple ci-dessous).    


```{r, eval=F}
labs(
  title = "",
  y = "",
  x = "",
  caption  = stringr::str_glue(
  "n = {nrow(central_data)} du Central Hospital;
  {nrow(central_data %>% filter(is.na(date_onset)))} cas sans date de début des symptomes et non représentés"))  
```


Notes :   
* l'utilisation de `\n` pour aller à la ligne.   
* si plusieurs colonnes contribuent à ce que des valeurs ne soient pas affichées (par exemple, l'âge ou le sexe si ceux-ci sont reflétés dans le graphique), il faut également filtrer sur ces colonnes pour calculer correctement le nombre de valeurs non affichées. 
* on peut sauvegarder la chaîne de caractères en tant qu'objet dans des commandes antérieures à la commande `ggplot()`, et simplement la référencer dans la `str_glue()`.  


<!-- ======================================================= -->
### `NA` dans les facteurs {.unnumbered}

Si votre colonne d'intérêt est un facteur, utilisez `fct_explicit_na()` du package **forcats** pour convertir les valeurs `NA` en une chaîne de caractères (plus de détails dans la page [Facteurs](#factors). Par défaut, la nouvelle valeur est "(Missing)" mais cela peut être ajusté via l'argument `na_level =`.   

```{r}
pacman::p_load(forcats)   # charge le package

linelist <- linelist %>% 
  mutate(gender = fct_explicit_na(gender, na_level = "Missing"))

levels(linelist$gender)
```



<!-- ======================================================= -->
## Imputation { }


Lors de certaines analyses de données, il est nécessaire de "combler les lacunes" et d'imputer les données manquantes. En effet, s'il est souvent possible d'analyser un jeu de données après en avoir supprimé toutes les valeurs manquantes, cela peut néanmoins poser des problèmes à plusieurs égards. Voici deux exemples :  

1) Supprimer toutes les observations avec des valeurs manquantes, ou les variables avec beaucoup de données manquantes peut réduire considérablement la puissance et la capacité à effectuer certains types d'analyse. Par exemple, nous avons vu que seule une faible fraction des lignes de notre linelist ne comporte *aucune* donnée manquante. Si nous supprimions toutes les lignes contenant au moins une donnée manquante, nous perdrions beaucoup d'informations ! De plus, la plupart de nos variables comportent une certaine quantité de données manquantes - pour la plupart des analyses, il n'est probablement pas raisonnable de _toutes_ les éliminer.

2) Selon la raison pour laquelle vos données sont manquantes, l'analyse des données non manquantes seules peut conduire à des biais et des résultats trompeurs. Par exemple, nous avons vu que de nombreux patients ont des données manquantes dans les colonnes concernant des symptômes importants, comme la fièvre ou la toux. Il est possible que cette information n'ait pas été enregistrée pour les personnes qui ne paraissaient pas sévèrement malades. Dans ce cas, si nous supprimions simplement ces observations, nous exclurions une partie des patients en meilleure santé de notre analyse, ce qui pourrait vraiment biaiser les résultats.

In ne suffit pas seulement d'estimer la quantité de données manquantes, il est également capital de réfléchir à la *raison pour laquelle les données peuvent manquer*. Cela va guider vos choix quant à l'importance de l'imputation des données manquantes, ainsi que de la méthode d'imputation la plus appropriée à votre situation.


### Types de données manquantes {.unnumbered}

Voici les trois grands types de données manquantes, qui correspondent à des mécanismes différents de non-réponse :  

1) **Données manquantes de manière complètement aléatoire** (MMCA) (on trouvera souvent l'acronyme anglais MCAR, pour "Missing Completely at Random"). Dans ce cas, il n'y a pas de relation entre la probabilité de manquer et les autres variables de vos données (ou avec des variables non mesurées). **La probabilité d'être manquante est la même pour tous les cas**. C'est une situation rare. Néanmoins, si vous avez de bonnes raisons de penser que vos données sont MMCA, l'analyse des données non manquantes (sans imputation) ne faussera pas les résultats (malgré une possible perte de puissance).  


2) **Données manquantes aléatoirement** (MA, ou MAR en Anglais pour "Missing at Random". Ce nom est en fait un peu trompeur car MA signifie que les données sont manquantes de manière systématique et prévisible en fonction d'autres variables mesurées. Par exemple, dans notre cas, les docteurs auraient pu considérer que les patients présentant des frissons et des courbatures ont nécessairement de la fièvre, et n'ont pas pris leur température. Cela aboutit à des observations manquantes dans la colonne fièvre, aisément prévisibles grâce aux colonnes frissons et courbatures. Si c'est vrai, nous pourrions facilement prédire que chaque observation manquante avec des frissons et des courbatures a également de la fièvre et utiliser cette information pour imputer nos données manquantes. Dans la pratique, c'est souvent plus compliqué: si un patient présente à la fois des frissons et des courbatures, il est probable qu'il ait également de la fièvre, mais pas toujours. Les données MA sont prévisibles, mais la prédiction n'est jamais parfaite. Il s'agit d'un type très courant de données manquantes 


3) **Données manquantes par omission prévisible** (MOP) aussi appelées **Données manquantes non aléatoirement** (MNAR ou NMAR en Anglais, pour "Missing not at Random" ou "Not Missing at Random"). Dans ce cas, la probabilité qu'une valeur soit manquante n'est *PAS* systématique ou prévisible à l'aide des autres informations dont nous disposons, mais elle n'est pas non plus manquante au hasard. Les données manquent pour des raisons inconnues, sur lesquelles vous n'avez aucune information. La valeur de la variable manquante est liée à la raison pour laquelle elle est manquante. Par exemple, dans nos données, l'age du patient peut manquer parce que certains patients très âgés ne savent pas ou refusent de dire quel âge ils ont. Dans cette situation, les données manquantes sur l'âge sont liées à la valeur elle-même, ne sont donc pas aléatoires ni prévisibles sur la base des autres informations dont nous disposons. Ce mécanisme de non-réponse est _non-ignorable_, complexe et souvent, la meilleure façon d'y faire face est d'essayer de collecter plus de données ou d'informations sur la raison pour laquelle les données sont manquantes plutôt que de tenter de les imputer. 

 
En général, imputer des données MA est relativement simple, mais imputer des données MOP est complexe, difficile et souvent impossible. La plupart des méthodes d'imputation les plus répandues font l'hypothèse que les données sont de type MA.  


### Packages utiles {.unnumbered}

Voici un certain nombre de packages utiles pour l'imputation des données : **Mmisc**, **missForest** (qui utilise les forêts aléatoires pour imputer les données manquantes) et **mice** (Multivariate Imputation by Chained Equations). Dans cette section, nous nous focaliserons sur le paquet **mice**, qui met en œuvre diverses techniques. Le responsable du paquet **mice** a publié un [livre détaillé accessible en ligne gratuitement](https://stefvanbuuren.name/fimd/) sur l'imputation des données manquantes.  

Voici le code pour charger le paquetage **mice** :  

```{r}
pacman::p_load(mice)
```

### Imputation par la moyenne {.unnumbered}

Parfois, dans le cas d'analyses simples ou s'il y a de bonnes raisons de penser que que les données sont de type MA, il est possible de simplement remplacer les valeurs manquantes d'une variable par la moyenne de cette variable. Par exemple, nous pourrions avoir de bonnes raisons de penser que les mesures de température manquantes dans nos données étaient MA ou normales. Voici le code permettant de créer une nouvelle variable qui remplace les valeurs de température manquantes par la valeur de température moyenne de notre ensemble de données. 

Il faut rester prudent, car dans de nombreuses situations, le remplacement des données manquantes par la moyenne peut entraîner un biais.  


```{r}
linelist <- linelist %>%
  mutate(temp_replace_na_with_mean = replace_na(temp, mean(temp, na.rm = T)))
```

On peut procéder de la même manière pour remplacer des données catégoriques par une valeur spécifique. Dans nos données, imaginez que vous sachiez que toutes les observations pour lesquelles il manque une valeur de décharge (qui peut être "Décès" ou "Guéri") sont en fait des personnes décédées (remarque : ce n'est pas réellement vrai pour cet ensemble de données).


```{r}
linelist <- linelist %>%
  mutate(outcome_replace_na_with_death = replace_na(outcome, "Death"))
```


### Imputation par régression {.unnumbered}

Une méthode un peu plus avancée consiste à utiliser un modèle statistique pour prédire les valeurs manquantes et les remplacer. Par exemple, on pourrait imaginer utiliser une régression linéaire simple avec l'état de la fièvre et l'age pour prédire la température lorsque celle-ci est manquante. Dans la vie réelle, il vaut mieux utiliser des modèles plus avancés qu'une approche aussi simple.  


```{r, warning=F, message=F}
simple_temperature_model_fit <- lm(temp ~ fever + age_years, 
                                   data = linelist)

# Nous utilisons un modèle linéaire simple avec la température comme variable réponse pour prédire les valeurs de température manquantes
predictions_for_missing_temps <- predict(simple_temperature_model_fit,
                                         newdata = linelist %>%
                                              filter(is.na(temp))) 
```

On peut utiliser la même approche d'imputation par régression avec le package **mice** pour imputer les les observations de température manquantes :  

```{r}
model_dataset <- linelist %>%
  select(temp, fever, age_years)  

temp_imputed <- mice(model_dataset,
                            method = "norm.predict",
                            seed = 1,
                            m = 1,
                            print = FALSE)

temp_imputed_values <- temp_imputed$imp$temp
```


Il est possible d'utiliser des modèles plus avancés que la régression linéaire simple pour prédire les valeurs manquantes à l'aide d'autres variables. Par exemple, le package **missForest** utilise les forêts aléatoires pour prédire les valeurs des données manquantes.  

Quel que soit le modèle statistique utilisé pour modéliser les valeurs manquantes, il faut se rappeler que cette approche fonctionne bien avec des données MMCA, mais il faut être très prudent si vous pensez que vos données sont de type MA ou MOP.  

La qualité de l'imputation dépend de la qualité du modèle de prédiction et même avec un très bon modèle, la variabilité de vos données imputées peut être sous-estimée.  


### Report de la dernière observation et baseline {.unnumbered}

Lorsque l'on a des données longitudinales ou des séries temporelles, il est parfois pertinent d'utiliser des méthodes d'imputations basées sur le report de la dernière valeur connue (LOCF, pour "Last Observation Carried Forward") ou le report de la valeur "baseline" (BOCF pour "Baseline Observation Carried Forward"). Concrètement, il s'agit d'utiliser une valeur observée dans le passé et de l'utiliser comme remplacement des données manquantes. Dans le cas de l'imputation LOCF, si plusieurs valeurs sont manquantes à la suite, il faut remonter à la dernière observation non manquante pour ce patient.  

La fonction `fill()` du package **tidyr** peut être utilisée pour l'imputation LOCF et BOCF (mais d'autres packages tels que **HMISC**, **zoo**, et **data.table** peuvent aussi être utilisés). Pour illustrer la syntaxe de `fill()`, nous allons créer un simple ensemble de données de séries temporelles contenant le nombre de cas d'une maladie pour chaque trimestre des années 2000 et 2001. Cependant, la valeur de l'année pour les trimestres postérieurs à Q1 est manquante et nous devrons donc les imputer. La jonction `fill()` est également démontrée dans la page [Restructurer les données](pivoting_data).    


```{r}
# Création d'un jeu de données
disease <- tibble::tribble(
  ~quarter, ~year, ~cases,
  "Q1",    2000,    66013,
  "Q2",      NA,    69182,
  "Q3",      NA,    53175,
  "Q4",      NA,    21001,
  "Q1",    2001,    46036,
  "Q2",      NA,    58842,
  "Q3",      NA,    44568,
  "Q4",      NA,    50197)

# Imputation ds données manquantes pour l'année (vers le bas par défaut)
disease %>% fill(year)
```

**Note** : il faut que les données soient correctement triées avant d'utiliser la fonction `fill()`! Par défaut, la fonction `fill()` remplit les données _vers le bas_, mais il est possible d'imputer des valeurs dans différentes directions à l'aide du paramètre `.direction`. Si nous créons un jeu de données similaire où la valeur de l'année est enregistrée uniquement à la fin de l'année et manquante pour les trimestres précédents :  

```{r}
# Création d'un jeu de données
disease <- tibble::tribble(
  ~quarter, ~year, ~cases,
  "Q1",      NA,    66013,
  "Q2",      NA,    69182,
  "Q3",      NA,    53175,
  "Q4",    2000,    21001,
  "Q1",      NA,    46036,
  "Q2",      NA,    58842,
  "Q3",      NA,    44568,
  "Q4",    2001,    50197)

# Imputation des données de l'année "vers le haut"
disease %>% fill(year, .direction = "up")
```

Dans cet exemple, l'imputation avec les méthodes LOCF et BOCF sont clairement les solutions les plus adaptée. Néanmoins, dans des situations plus complexes, il peut être difficile de décider si ces méthodes sont appropriées ou non. Par exemple, vous pouvez avoir des valeurs de laboratoire manquantes pour un patient hospitalisé après le premier jour. Cela pourrait signifier que les valeurs de laboratoire n'ont pas changé... ou que le patient s'est rétabli et donc que ses valeurs seraient très différentes après le premier jour ! Utilisez ces méthodes avec prudence.  


### Imputation multiple {.unnumbered}

Nous n'avons pas la place ici de faire une explication détaillée de l'imputation multiple et de quand l'utiliser. Nous vous réferrons au [livre (en ligne et gratuit)](https://stefvanbuuren.name/fimd/) écrit par l'auteur du paquet **mice**  et ne présentons ici qu'une explication de base de la méthode :  

L'imputation multiple consiste à créer plusieurs jeux de données dans lesquels les valeurs manquantes sont imputées à des valeurs de données "plausibles". Dans chacun des jeux de données, chaque valeur imputée est tirée aléatoirement dans une distribution estimée (les données non manquantes restent, elles, intouchées), ce qui crée des jeux de données légèrement différents les uns des autres. La distribution utilisée d'où sont tirée les valeurs imputées vient ici encore d'un modèle statistique prédictif (**mice** propose de nombreuses options pour les méthodes de prédiction, notamment *Predictive Mean Matching*, *Régression logistique* et *Forêt aléatoire*), mais **mice** prend en charge de nombreux détails de la modélisation. Ensuite, l'analyse que vous aviez planifiée est effectuée sur chacun des jeux de données, et les paramètres estimés par les modèles sont ensuite poolés et leur variance estimée.  

Cette méthode fonctionne très bien pour réduire le biais dans les configurations MMCA et MA et permet souvent d'obtenir des estimations plus précises de l'erreur standard.  

Note : en fonction des données, on peut créer plus ou moins de jeux de données avec les données imputées. Le package **mice** fixe le nombre par défaut à 5.  

Voici un exemple d'application de l'imputation multiple pour prédire la température dans notre jeu de données de liste linéaire, en utilisant l'age et la présence/absence de fièvre :  

```{r}
# imputation des valurs manquantes pour notre jeu de données modèle, et création de 10 jeux de données imputés : 
multiple_imputation = mice(
  model_dataset,
  seed = 1,
  m = 10,
  print = FALSE) 

model_fit <- with(multiple_imputation, 
                  lm(temp ~ age_years + fever))

base::summary(mice::pool(model_fit))
```


Ici, nous avons utilisé la méthode d'imputation par défaut de **mice**, à savoir "Predictive Mean Matching". Nous avons ensuite utilisé ces jeux de données imputées pour estimer séparément, puis mettre en commun les résultats de régressions linéaires simples sur chacun de ces ensembles de données. 

Il existe de nombreux détails que nous avons survolés et de nombreux paramètres que vous pouvez ajuster pendant le processus d'imputation multiple en utilisant le package **mice**. Par exemple, vous n'aurez pas toujours des données numériques et vous devrez peut-être utiliser d'autres méthodes d'imputation (**mice** permet d'imputer de nombreux types de données, avec de nombreuses méthodes). Mais, pour une analyse plus robuste lorsque les données manquantes constituent un problème important, l'imputation multiple est une bonne solution qui ne demande pas toujours beaucoup plus de travail que l'analyse complète des cas.  



<!-- ======================================================= -->
## Resources { }

Vignette sur le [package **naniar**](https://cran.r-project.org/web/packages/naniar/vignettes/getting-started-w-naniar.html)

Galerie de [visualisation de données manquantes](https://cran.r-project.org/web/packages/naniar/vignettes/naniar-visualisation.html)

[Livre gratuit](https://stefvanbuuren.name/fimd/) sur l'imputation multiple par l'auteur et le gestionnaire du package **mice**
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/missing_data.Rmd-->

# Taux standardisés {#standardisation}  

Cette page vous montre deux façons de normaliser un résultat, tel que les hospitalisations ou la mortalité, en fonction de caractéristiques telles que l'âge et le sexe. 

* Utilisation du paquet **dsr**
* Utilisation du paquet **PHEindicatormethods**.  

Nous commençons par démontrer de manière extensive les processus de préparation/nettoyage/jonction des données, car cela est courant lorsqu'on combine des données de population provenant de plusieurs pays, des données de population standard, des décès, etc.  

## Vue d'ensemble  

Il existe deux manières principales de normaliser : la normalisation directe et la normalisation indirecte.
Supposons que nous voulions normaliser le taux de mortalité par âge et par sexe pour le pays A et le pays B, et comparer les taux normalisés entre ces pays.

* Pour une standardisation directe, vous devrez connaître le nombre de personnes à risque et le nombre de décès pour chaque strate d'âge et de sexe, pour le pays A et le pays B. Une strate dans notre exemple pourrait être les femmes âgées de 15 à 44 ans.  
* Pour une standardisation indirecte, il suffit de connaître le nombre total de décès et la structure d'âge et de sexe de chaque pays. Cette option est donc envisageable si les taux de mortalité ou les chiffres de population par âge et par sexe ne sont pas disponibles. La standardisation indirecte est en outre préférable en cas de petits effectifs par strate, car les estimations en standardisation directe seraient influencées par une variation d'échantillonnage importante. 

<!-- ======================================================= -->
## Préparation { }

Pour montrer comment se fait la standardisation, nous allons utiliser des comptages fictifs de population et de décès du pays A et du pays B, par âge (en catégories de 5 ans) et par sexe (femme, homme). Pour que les ensembles de données soient prêts à être utilisés, nous allons effectuer les étapes de préparation suivantes :  

1. Charger les paquets  
2. Charger les jeux de données  
3. Joignez les données de population et de décès des deux pays.
4. Pivoter plus longtemps pour qu'il y ait une ligne par strate âge-sexe.
5. Nettoyez la population de référence (population standard mondiale) et joignez-la aux données du pays.  

Dans votre scénario, vos données peuvent se présenter sous un format différent. Peut-être vos données sont-elles présentées par province, ville ou autre zone d'attraction. Vous avez peut-être une ligne pour chaque décès et des informations sur l'âge et le sexe pour chacun (ou une proportion importante) de ces décès. Dans ce cas, consultez les pages sur le [Travailler sur des données groupées](#grouping_data), [Pivoter les données](#pivoting_data), and [Tableaux descriptifs](#descriptive_tables) pour créer un ensemble de données avec des comptes d'événements et de population par strate âge-sexe.  

Nous avons également besoin d'une population de référence, la population standard. Pour les besoins de cet exercice, nous utiliserons la `world_standard_population_by_sex` (population standard mondiale par sexe). La population standard mondiale est basée sur les populations de 46 pays et a été développée en 1960. Il existe de nombreuses populations "standard" - à titre d'exemple, le site web de [NHS Scotland](https://www.opendata.nhs.scot/dataset/standard-populations) est assez informatif sur la population standard européenne, la population standard mondiale et la population standard écossaise. 

<!-- ======================================================= -->
### Chargement des paquets {.unnumbered}

Ce chunk de code montre le chargement des paquets nécessaires aux analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* le charge pour l'utiliser. Vous pouvez également charger les paquets installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les paquets R.  

```{r}
pacman::p_load(
     rio, # importer/exporter des données
     here, # localisation des fichiers
     tidyverse, # gestion et visualisation des données
     stringr, # nettoyage des caractères et des chaînes de caractères
     frailtypack, # nécessaire pour dsr, pour les modèles de frailty
     dsr, # standardiser les taux
     PHEindicatormethods) # alternative pour la standardisation des taux
```


<span style="color : orange ;">**ATTENTION:_** Si vous avez une version plus récente de R, le paquet **dsr** ne peut pas être téléchargé directement avec CRAN. Cependant, il est toujours disponible de l'archive CRAN. Vous pouvez installer et utiliser celui-ci. </span>

Pour les utilisateurs non-Mac :  

```{r, eval=F} 
packageurl <- "https://cran.r-project.org/src/contrib/Archive/dsr/dsr_0.2.2.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
```

```{r, eval=FALSE}
# Autre solution qui peut fonctionner
require(devtools)
devtools::install_version("dsr", version="0.2.2", repos="http:/cran.us.r.project.org")
```

Pour les utilisateurs de Mac :  

```{r, eval=FALSE}
require(devtools)
devtools::install_version("dsr", version="0.2.2", repos="https://mac.R-project.org")
```




### Charger les données de la population {.unnumbered}  

Voir la page [Télécharger le manuel et les données](#download_book_data) pour savoir comment télécharger tous les exemples de données du manuel. Vous pouvez importer les données de la page de normalisation directement dans R depuis notre dépôt Github en exécutant les commandes `import()` suivantes :  

```{r, eval=F}
# importer les données démographiques du pays A directement depuis Github
A_demo <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/country_demographics.csv")

# importer les décès pour le pays A directement depuis Github
A_deaths <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/deaths_countryA.csv")

# Importez les données démographiques pour le pays B directement depuis Github.
B_demo <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/country_demographics_2.csv")

# importer les décès pour le pays B directement depuis Github.
B_deaths <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/deaths_countryB.csv")

# Importez les données démographiques pour le pays B directement depuis Github.
standard_pop_data <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/world_standard_population_by_sex.csv")

```


Tout d'abord, nous chargeons les données démographiques (comptage des hommes et des femmes par catégorie d'âge de 5 ans) pour les deux pays que nous allons comparer, le "pays A" et le "pays B".  

```{r, echo=F}
# Pays A
A_demo <- rio::import(here::here("data", "standardization", "country_demographics.csv")) %>% 
     mutate(Country = "A") %>% 
     select(Country, everything()) %>% # re-arrangement
     mutate(age_cat5 = str_replace_all(age_cat5, "\\+", "")) # supprimer les symboles +
```

```{r, eval=F}
# Pays A
A_demo <- import("country_demographics.csv")
```

```{r message=FALSE, echo=F}
DT::datatable(A_demo, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


```{r, echo=F}
# Pays B
B_demo <- rio::import(here::here("data", "standardization", "country_demographics_2.csv")) %>% 
     mutate(Country = "B") %>% 
     select(Country, everything()) # réarrangement
```

```{r, eval=F}
# Pays B
B_demo <- import("country_demographics_2.csv")
```

```{r message=FALSE, echo=F}
DT::datatable(B_demo, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



### Chargement du nombre de morts {.unnumbered}  

De manière pratique, nous disposons également du nombre de décès survenus pendant la période qui nous intéresse, par âge et par sexe. Les chiffres de chaque pays sont dans un fichier séparé, comme indiqué ci-dessous.   

```{r, echo=F}
A_males <- c(224, 257, 251, 245, 334, 245, 154, 189, 334, 342, 565, 432, 543, 432, 245, 543, 234, 354) # pour les hommes du pays A
B_males <- c(34, 37, 51, 145, 434, 120, 100, 143, 307, 354, 463, 639, 706, 232, 275, 543, 234, 274) # pour les hommes du pays B
A_females <- c(194, 254, 232, 214, 316, 224, 163, 167, 354, 354, 463, 574, 493, 295, 175, 380, 177, 392) # pour les femmes du pays A
B_females <- c(54, 24, 32, 154, 276, 254, 123, 164, 254, 354, 453, 654, 435, 354, 165, 432, 287, 395) # pour les femmes du pays B

age_cat5 <- c("0-4", "5-9", "10-14", "15-19", "20-24", "25-29", "30-34", "35-39", "40-44",
                                                                                "45-49", "50-54", "55-59",
                                                                                "60-64", "65-69", "70-74",
                                                                                "75-79", "80-84", "85")
A_deaths <- data.frame(Country = "A", AgeCat = age_cat5, Male = A_males, Female = A_females)
B_deaths <- data.frame(Country = "B", AgeCat = age_cat5, Male = B_males, Female = B_females)
```

Décès dans le pays A
```{r message=FALSE, echo=F}
DT::datatable(A_deaths, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Décès dans le pays B

```{r message=FALSE, echo=F}
DT::datatable(B_deaths, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


```{r, echo=F}
rio::export(A_deaths, here::here("data", "standardization", "deaths_countryA.csv"))
rio::export(B_deaths, here::here("data", "standardization", "deaths_countryB.csv"))
```



### Nettoyer les populations et les décès {.unnumbered}  


Nous devons joindre et transformer ces données de la manière suivante :  

* Combiner les populations des pays en un seul ensemble de données et faire un pivot "long" pour que chaque strate âge-sexe soit une ligne.  
* Combiner le nombre de décès par pays dans un ensemble de données et faire pivoter "long" pour que chaque strate âge-sexe soit une ligne.  
* Joindre les décès aux populations  

Tout d'abord, nous combinons les ensembles de données sur les populations des pays, nous effectuons un pivot plus long et un nettoyage mineur. Voir la page [Pivoter les données](#pivoting_data) pour plus de détails.  

```{r}
pop_countries <- A_demo %>% # Commencez avec l'ensemble de données du pays A
     bind_rows(B_demo) %>% # lier les lignes, car les colonnes portent le même nom
     pivot_longer( # pivot plus long
          cols = c(m, f), # colonnes à combiner en une seule
          names_to = "Sex", # nom de la nouvelle colonne contenant la catégorie ("m" ou "f") 
          values_to = "Population") %>% # nom de la nouvelle colonne contenant les valeurs numériques pivotées
     mutate(Sex = recode(Sex, # re-code les valeurs pour plus de clarté
          "m" = "Male",
          "f" = "Female"))
```

Les données de population combinées ressemblent maintenant à ceci (cliquez pour voir les pays A et B) :  

```{r message=FALSE, echo=F}
DT::datatable(pop_countries, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Et maintenant, nous effectuons des opérations similaires sur les deux ensembles de données de décès.

```{r}
deaths_countries <- A_deaths %>% # Commencez avec l'ensemble de données des décès du pays A
     bind_rows(B_deaths) %>% # lier les lignes avec l'ensemble de données B, parce que les colonnes sont nommées de manière identique
     pivot_longer( # pivot plus long
          cols = c(Male, Female), # colonne à transformer en une seule
          names_to = "Sex", # nom de la nouvelle colonne contenant la catégorie ("m" ou "f") 
          values_to = "Deaths") %>% # nom pour la nouvelle colonne contenant les valeurs numériques pivotées
     rename(age_cat5 = AgeCat) # renomme pour plus de clarté
```

Les données de décès ressemblent maintenant à ceci, et contiennent les données des deux pays : 

```{r message=FALSE, echo=F}
DT::datatable(deaths_countries, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


Nous joignons maintenant les données de décès et de population sur la base des colonnes communes `Country`, `age_cat5`, et `Sex`. Cela ajoute la colonne `Deaths`.  

```{r}
country_data <- pop_countries %>% 
     left_join(deaths_countries, by = c("Country", "age_cat5", "Sex"))
```

Nous pouvons maintenant classer `Country`, `age_cat5`, et `Sex` comme facteurs et définir l'ordre des niveaux en utilisant la fonction `fct_relevel()` du paquet **forcats**, comme décrit dans la page sur [Facteurs](#factors). Notez que le classement des niveaux des facteurs ne change pas visiblement les données, mais la commande `arrange()` les trie par Pays, catégorie d'âge et sexe.  

```{r, warning=F, message=F}
country_data <- country_data %>% 
  mutate(
    Country = fct_relevel(Country, "A", "B"),
      
    Sex = fct_relevel(Sex, "Male", "Female"),
        
    age_cat5 = fct_relevel(
      age_cat5,
      "0-4", "5-9", "10-14", "15-19",
      "20-24", "25-29",  "30-34", "35-39",
      "40-44", "45-49", "50-54", "55-59",
      "60-64", "65-69", "70-74",
      "75-79", "80-84", "85")) %>% 
          
  arrange(Country, age_cat5, Sex)

```

```{r message=FALSE, echo=F}
DT::datatable(country_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

<span style="color : orange ;">**__ATTENTION:_** Si vous avez peu de décès par strate, envisagez d'utiliser des catégories de 10, ou 15 ans, au lieu de catégories de 5 ans pour l'âge.</span>




### Chargement de la population de référence {.unnumbered}  

Enfin, pour la standardisation directe, nous importons la population de référence (la "population standard" mondiale par sexe).

```{r, echo=F}
# Population de référence
standard_pop_data <- rio::import(here::here("data", "standardization", "world_standard_population_by_sex.csv")) %>% 
     rename(age_cat5 = AgeGroup)
```

```{r, eval=F}
# Population de référence
standard_pop_data <- import("world_standard_population_by_sex.csv")
```

```{r message=FALSE, echo=F}
DT::datatable(standard_pop_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
### Nettoyer la population de référence {.unnumbered}

Les valeurs des catégories d'âge dans les cadres de données `country_data` et `standard_pop_data` devront être alignées.  

Actuellement, les valeurs de la colonne `age_cat5` du cadre de données `standard_pop_data` contiennent le mot "years" et "plus", alors que celles du cadre de données `country_data` ne le font pas. Nous devrons faire correspondre les valeurs des catégories d'âge. Nous utilisons `str_replace_all()` du paquet **stringr**, comme décrit dans la page [Caractères et chaînes de caractères](#character_strings), pour remplacer ces motifs par des `""` sans espace.  

De plus, le paquet **dsr** s'attend à ce que dans la population standard, la colonne contenant les comptes soit appelée `"pop"`. Nous renommons donc cette colonne en conséquence.  

```{r}
# Suppression d'une chaîne spécifique des valeurs de la colonne
standard_pop_clean <- standard_pop_data %>%
     mutate(
          age_cat5 = str_replace_all(age_cat5, "years", ""), # supprime "year" (année)
          age_cat5 = str_replace_all(age_cat5, "plus", ""), # supprimez "plus".
          age_cat5 = str_replace_all(age_cat5, " ", "")) %>% # supprime l'espace " ".
     
     rename(pop = WorldStandardPopulation) # change le nom de la colonne en "pop", car cela est attendu par le paquet dsr
```

<span style="color : orange ;">**_CAUTION:_** Si vous essayez d'utiliser `str_replace_all()` pour supprimer un *symbole plus*, cela ne fonctionnera pas car c'est un symbole spécial. "Échappez" au spécial en mettant deux barres obliques inverses devant, comme dans `str_replace_call(column, "\\+", "")`. </span>

### Créer un jeu de données avec une population standard {#standard_all}  

Enfin, le package **PHEindicatormethods**, détaillé [ci-dessous](#standard_phe), attend les populations standards jointes aux événements et aux comptages de population du pays. Nous allons donc créer un jeu de données `all_data` à cet effet.  

```{r}
all_data <- left_join(country_data, standard_pop_clean, by=c("age_cat5", "Sex"))
```

Cet ensemble de données complet ressemble à ceci :  

```{r message=FALSE, echo=F}
DT::datatable(all_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
## **dsr** package { }
 
Nous démontrons ci-dessous le calcul et la comparaison de taux directement standardisés à l'aide du package **dsr**. Le package **dsr** vous permet de calculer et de comparer des taux directement standardisés (pas de taux indirectement standardisés !).
  
Dans la section Préparation des données, nous avons créé des jeux de données distincts pour le nombre de pays et la population standard :  

1) l'objet `country_data`, qui est un tableau de population avec le nombre de population et le nombre de décès par strate par pays  
2) l'objet `standard_pop_clean`, contenant le nombre de personnes par strate pour notre population de référence, la population standard mondiale.  

Nous utiliserons ces ensembles de données distincts pour l'approche **dsr**.  


<!-- ======================================================= -->
### Taux standardisés {.unnumbered}

Ci-dessous, nous calculons les taux par pays directement standardisés pour l'âge et le sexe. Nous utilisons la fonction `dsr()`. 

A noter - `dsr()` s'attend à un cadre de données pour les populations des pays et le nombre d'événements (décès), **et un autre cadre de données avec la population de référence**. Il s'attend également à ce que dans cette base de données de la population de référence, le nom de la colonne unité-temps soit "pop" (nous nous en sommes assurés dans la section Préparation des données).  

Il y a de nombreux arguments, comme annoté dans le code ci-dessous. Notamment, `event = ` est fixé à la colonne `Deaths`, et le `fu = ` ("follow-up") est fixé à la colonne `Population`. Nous définissons les sous-groupes de comparaison comme la colonne `Country` et nous standardisons sur la base de `age_cat5` et `Sex`. Ces deux dernières colonnes n'ont pas d'argument nommé particulier. Voir `?dsr` pour plus de détails. 

```{r, warning=F, message=F}
# Calculez les taux par pays directement standardisés pour l'âge et le sexe
mortality_rate <- dsr::dsr(
     data = country_data, # spécifier l'objet contenant le nombre de décès par strate
     event = Deaths, # colonne contenant le nombre de décès par strate 
     fu = Population, # colonne contenant le nombre de population par strate
     subgroup = Country, # unités que nous souhaitons comparer
     age_cat5, # autres colonnes - les taux seront standardisés par celles-ci
     Sex,
     refdata = standard_pop_clean, # cadre de données de la population de référence, avec une colonne appelée "pop".
     method = "gamma", # méthode pour calculer l'IC à 95%.
     sig = 0,95, # niveau de signification
     mp = 100000, # nous voulons les taux pour 100.000 habitants
     decimals = 2) # nombre de décimales)


# Imprimez la sortie sous la forme d'un joli tableau HTML
knitr::kable(mortality_rate) # Afficher le taux de mortalité avant et après la standardisation directe
```

Ci-dessus, nous voyons que même si le pays A avait un taux de mortalité brut plus faible que le pays B, il a un taux standardisé plus élevé après standardisation directe par âge et par sexe.




<!-- ======================================================= -->
### Ratios de taux standardisés {.unnumbered}

```{r,warning=F, message=F}
# Calculer le RR
mortality_rr <- dsr::dsrr(
     data = country_data, # spécifier l'objet contenant le nombre de décès par strate
     event = Deaths, # colonne contenant le nombre de décès par strate 
     fu = Population, # colonne contenant le nombre de population par strate
     subgroup = Country, # unités que nous souhaitons comparer
     age_cat5,
     Sex, # caractéristiques sur lesquelles nous aimerions nous standardiser 
     refdata = standard_pop_clean, # population de référence, avec des chiffres dans la colonne appelée pop
     refgroup = "B", # référence pour la comparaison
     estimate = "ratio", # type d'estimation
     sig = 0.95, # niveau de signification
     mp = 100000, # nous voulons des taux pour 100.000 habitants
     decimals = 2) # nombre de décimales

# Imprimer le tableau
knitr::kable(mortality_rr) 
```

Le taux de mortalité standardisé est 1,22 fois plus élevé dans le pays A que dans le pays B (IC 95 % 1.17-1.27).

<!-- ======================================================= -->
### Différence de taux standardisé {.unnumbered}

```{r, warning=F, message=F}
# Calculer RD
mortality_rd <- dsr::dsrr(
     data = country_data, # spécifier l'objet contenant le nombre de décès par strate
     event = Deaths, # colonne contenant le nombre de décès par strate 
     fu = Population, # colonne contenant le nombre de population par strate
     subgroup = Country, # unités que nous souhaitons comparer
     age_cat5, # caractéristiques sur lesquelles nous voulons nous standardiser
     Sex,                        
     refdata = standard_pop_clean, # population de référence, avec des chiffres dans la colonne appelée pop
     refgroup = "B", # référence pour la comparaison
     estimate = "difference", # type d'estimation
     sig = 0.95, # niveau de signification
     mp = 100000, # nous voulons des taux pour 100.000 habitants
     decimals = 2) # nombre de décimales

# Imprimer le tableau
knitr::kable(mortality_rd) 
```

Le pays A a 4.24 décès supplémentaires pour 100.000 habitants (IC 95% 3.24-5.24) par rapport au pays A.







<!-- ======================================================= -->
## **PHEindicatormethods** package {#standard_phe}

Une autre façon de calculer les taux standardisés est avec le paquet **PHEindicatormethods**. Ce package vous permet de calculer les taux standardisés directement et indirectement. Nous allons montrer les deux.  

Cette section utilisera le cadre de données `all_data` créé à la fin de la section Préparation. Ce cadre de données inclut les populations des pays, les événements de décès, et la population de référence standard mondiale. Vous pouvez le visualiser [ici](#standard_all).  



<!-- ======================================================= -->
### Taux directement standardisés {.unnumbered}

Ci-dessous, nous regroupons d'abord les données par Pays, puis nous les passons à la fonction `phe_dsr()` pour obtenir les taux directement standardisés par pays.

A noter - la population de référence (standard) peut être fournie comme une **colonne dans le cadre de données spécifique au pays** ou comme un **vecteur séparé**. Si elle est fournie dans le cadre de données spécifique au pays, vous devez définir `stdpoptype = "field"`. Si elle est fournie sous forme de vecteur, définissez `stdpoptype = "vector"`. Dans ce dernier cas, vous devez vous assurer que l'ordre des rangées par strate est similaire dans le cadre de données spécifique au pays et dans la population de référence, car les enregistrements seront appariés par position. Dans notre exemple ci-dessous, nous avons fourni la population de référence sous forme de colonne dans le cadre de données spécifique au pays.

Consultez l'aide de `?phe_dsr` ou les liens dans la section Références pour plus d'informations.  

```{r}
# Calculez les taux par pays directement normalisés pour l'âge et le sexe.
mortality_ds_rate_phe <- all_data %>%
     group_by(Country) %>%
     PHEindicatormethods::phe_dsr(
          x = Deaths, # colonne avec le nombre d'événements observés
          n = Population, # colonne avec les pops non standard pour chaque strate
          stdpop = pop, # populations standard pour chaque strate
          stdpoptype = "field")       # soit "vector" pour un vecteur autonome, soit "field" pour signifier que les populations std sont dans les données.  

# Imprimer le tableau
knitr::kable(mortality_ds_rate_phe)
```

<!-- ======================================================= -->
### Taux standardisés indirectement {#standard_indirect .unnumbered}

Pour la standardisation indirecte, vous avez besoin d'une population de référence avec le nombre de décès et le nombre de population par strate. Dans cet exemple, nous allons calculer les taux pour le pays A *en utilisant le pays B comme population de référence*, car la population de référence `standard_pop_clean` n'inclut pas le nombre de décès par strate. 

Ci-dessous, nous créons d'abord la population de référence du pays B. Ensuite, nous passons les données de mortalité et de population pour le pays A, nous les combinons avec la population de référence, et nous les passons à la fonction `phe_isr()`, pour obtenir des taux indirectement standardisés. Bien sûr, vous pouvez aussi faire l'inverse.

A noter - dans notre exemple ci-dessous, la population de référence est fournie comme un cadre de données séparé. Dans ce cas, nous nous assurons que les vecteurs `x = `, `n = `, `x_ref = ` et `n_ref = ` sont tous ordonnés par les mêmes valeurs de catégorie de standardisation (strate) que celles de notre cadre de données spécifique au pays, puisque les enregistrements seront appariés par position.

Consultez l'aide de `?phe_isr` (maintenant `calculate_ISRate` depuis dec 2022) ou les liens dans la section Références pour plus d'informations.  

```{r}
# Créez la population de référence
refpopCountryB <- country_data %>% 
  filter(Country == "B") 

# Calculer les taux pour le pays A indirectement standardisés par âge et sexe
mortality_is_rate_phe_A <- country_data %>%
     filter(Country == "A") %>%
     PHEindicatormethods::calculate_ISRate( #avant c'etait phe_isr()
          x = Deaths, # colonne avec le nombre d'événements observés
          n = Population, # colonne avec les pops non standard pour chaque strate
          x_ref = refpopCountryB$Deaths, # nombre de décès de référence pour chaque strate
          n_ref = refpopCountryB$Population) # population de référence pour chaque strate

# Imprimez le tableau
knitr::kable(mortality_is_rate_phe_A)
```

<!-- ======================================================= -->
## Ressources { }

Si vous souhaitez voir un autre exemple reproductible utilisant **dsr**, veuillez consulter [cette vignette]( https://mran.microsoft.com/snapshot/2020-02-12/web/packages/dsr/vignettes/dsr.html).  

Pour un autre exemple utilisant **PHEindicatormethods**, veuillez vous rendre sur [ce site Web](https://mran.microsoft.com/snapshot/2018-10-22/web/packages/PHEindicatormethods/vignettes/IntroductiontoPHEindicatormethods.html)  

Voir les **PHEindicatormethods** [fichier pdf de référence](https://cran.r-project.org/web/packages/PHEindicatormethods/PHEindicatormethods.pdf)  
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/standardization.Rmd-->



# Moyennes mobiles {#moving_average}  

```{r, out.width=c("100%"), echo=F}
knitr::include_graphics(here::here("images", "moving_avg_epicurve.png"))
```


Cette page va couvrir deux méthodes pour calculer et visualiser les moyennes mobiles :  

1) Calculer avec le paquet **slider**.  
2) Calculer *dans* une commande `ggplot()` avec le paquet **tidyquant**.  



<!-- ======================================================= -->
## Préparation { }

### Chargement des paquets {.unnumbered}

Ce morceau de code montre le chargement des paquets nécessaires aux analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* le charge pour l'utiliser. Vous pouvez également charger les paquets installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les paquets R. 


```{r}
pacman::p_load(
  tidyverse, # pour la gestion des données et le viz
  slider, # pour le calcul des moyennes mobiles
  tidyquant # pour le calcul des moyennes mobiles dans ggplot
)
```


### Importer des données {.unnumbered}

Nous importons le jeu de données des cas d'une épidémie d'Ebola simulée. Si vous voulez suivre, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la liste de lignes "propre"</a> (en tant que fichier .rds). Importez des données avec la fonction `import()` du paquet **rio** (elle gère de nombreux types de fichiers comme .xlsx, .csv, .rds - voir la page [Importation et exportation](#import_export) pour plus de détails).  


```{r, echo=F}
# Importez la liste de diffusion dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# Importez la liste de cas
linelist <- import("linelist_cleaned.xlsx")
```

Les 50 premières lignes de la linelist sont affichées ci-dessous.

```{r, message=FALSE, echo=F}
# affiche les données de la liste de diffusion sous forme de tableau
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


<!-- ======================================================= -->
## Calculer avec **slider** { }

**Utilisez cette approche pour calculer une moyenne mobile dans un cadre de données avant de tracer.**  

Le paquet **slider** fournit plusieurs fonctions de "fenêtre glissante" pour calculer des moyennes glissantes, des sommes cumulatives, des régressions glissantes, etc. Il traite un cadre de données comme un vecteur de lignes, permettant une itération par ligne sur un cadre de données.   

Voici quelques-unes des fonctions les plus courantes :  

* `slide_dbl()` - itère à travers une colonne *numérique* ("_dbl") en effectuant une opération utilisant une fenêtre glissante.  
  * `slide_sum()` - fonction de raccourci de la somme glissante pour `slide_dbl()`.  
  * `slide_mean()` - fonction de raccourci de la moyenne glissante pour `slide_dbl()`. 
* `slide_index_dbl()` - applique la fenêtre glissante sur une colonne numérique en utilisant une colonne séparée pour *indexer* la progression de la fenêtre (utile si la fenêtre est glissante par date et que certaines dates sont absentes).  
  * `slide_index_sum()` - fonction de raccourci de la somme roulante avec indexation.  
  * `slide_index_mean()` - fonction de raccourci de la moyenne mobile avec indexation.  
  
Le paquet **slider** possède de nombreuses autres fonctions qui sont couvertes dans la section Ressources de cette page. Nous abordons brièvement les plus courantes.  

**Arguments de base**  

* `.x`, le premier argument par défaut, est le vecteur sur lequel il faut itérer et auquel il faut appliquer la fonction.  
* `.i = ` pour les versions "index" des fonctions **slider** - fournir une colonne pour "indexer" le rouleau (voir section [ci-dessous](#roll_index))  
* `.f = `, le deuxième argument par défaut, soit :  
  * Une fonction, écrite sans parenthèses, comme `mean`, ou bien  
  * Une formule, qui sera convertie en fonction. Par exemple `~ .x - mean(.x)` retournera le résultat de la valeur courante moins la moyenne de la valeur de la fenêtre.  
  
* Pour plus de détails, voir ce [matériel de référence](https://davisvaughan.github.io/slider/reference/slide.html)



**Taille de la fenêtre**  

Spécifiez la taille de la fenêtre en utilisant soit `.before`, soit `.after`, soit les deux arguments :   

* `.before = ` - Fournir un nombre entier  
* `.after = ` - Fournir un nombre entier  
* `.complete = ` - Donnez-lui la valeur `TRUE` si vous voulez que le calcul soit effectué uniquement sur des fenêtres complètes.  

Par exemple, pour obtenir une fenêtre de 7 jours incluant la valeur actuelle et les six précédentes, utilisez `.before = 6`. Pour obtenir une fenêtre "centrée", donnez le même nombre à `.before = ` et `.after = `.    

Par défaut, `.complete = ` sera FAUX, donc si la fenêtre complète de lignes n'existe pas, les fonctions utiliseront les lignes disponibles pour effectuer le calcul. Si vous mettez la valeur TRUE, les calculs ne seront effectués que sur des fenêtres complètes.  

**Extension de la fenêtre**  

Pour réaliser des opérations *cumulatives*, définissez l'argument `.before =` à `Inf`. Ceci effectuera l'opération sur la valeur courante et toutes celles qui la précèdent.  





### Rouler par date {#roll_index .unnumbered}  

Le cas le plus probable d'utilisation d'un calcul glissant en épidémiologie appliquée est d'examiner une métrique *dans le temps*. Par exemple, une mesure continue de l'incidence des cas, basée sur le nombre de cas quotidiens. 

Si vous avez des séries temporelles propres avec des valeurs pour chaque date, vous pouvez utiliser `slide_dbl()`, comme démontré ici dans la page [Série chronologique et détection des épidémies](#time_series).  

Cependant, dans de nombreuses circonstances d'épidémiologie appliquée, vous pouvez avoir des dates absentes de vos données, où il n'y a aucun événement enregistré. Dans ces cas, il est préférable d'utiliser les versions "index" des fonctions **slider**.  


### Données indexées {.unnumbered}  

Ci-dessous, nous montrons un exemple d'utilisation de `slide_index_dbl()` sur la liste de cas. Disons que notre objectif est de calculer une incidence glissante sur 7 jours - la somme des cas utilisant une fenêtre glissante de 7 jours. Si vous cherchez un exemple de moyenne glissante, consultez la section ci-dessous sur le [roulement groupé](#roll_slider_group).    

Pour commencer, le jeu de données `daily_counts` est créé pour refléter le nombre de cas quotidiens de la `linelist`, tel que calculé avec `count()` de **dplyr**.  

```{r}
# créez un jeu de données des comptages quotidiens
daily_counts <- linelist %>% 
  count(date_hospitalisation, name = "new_cases")
```


Voici le cadre de données `daily_counts` - il y a ` nrow(daily_counts)` lignes, chaque jour est représenté par une ligne, mais surtout au début de l'épidémie *certains jours ne sont pas présents (il n'y avait pas de cas admis ces jours-là)*.  


```{r, echo=F}
DT::datatable(daily_counts, rownames = FALSE, options = list(pageLength = 6, scrollX=T) )
```



Il est crucial de reconnaître qu'une fonction de roulement standard (comme `slide_dbl()` utiliserait une fenêtre de 7 *lignes*, et non de 7 *jours*. Ainsi, s'il y a des dates absentes, certaines fenêtres s'étendront en fait sur plus de 7 jours calendaires !  

Une fenêtre déroulante "intelligente" peut être obtenue avec `slide_index_dbl()`. L'"index" signifie que la fonction utilise une colonne *séparée* comme "index" pour la fenêtre de roulement. La fenêtre n'est pas simplement basée sur les lignes du cadre de données.  

Si la colonne d'index est une date, vous avez la possibilité supplémentaire de spécifier l'étendue de la fenêtre à `.before = ` et/ou `.after = ` en unités de **lubridate** `days()` ou `months()`. Si vous faites ces choses, la fonction inclura les jours absents dans les fenêtres comme s'ils étaient là (comme des valeurs `NA`).  

Montrons une comparaison. Ci-dessous, nous calculons l'incidence des cas sur 7 jours glissants avec des fenêtres régulières et indexées.  


```{r}
rolling <- daily_counts %>% 
  mutate( # créer de nouvelles colonnes
    # Utiliser slide_dbl()
    ###################
    reg_7day = slide_dbl(
      new_cases, # calculer sur les new_cases
      .f = ~sum(.x, na.rm = T), # la fonction est sum() avec les valeurs manquantes supprimées
      .before = 6), # la fenêtre est le ROW et 6 ROWS précédents
    
    # Utilisation de slide_index_dbl()
    #########################
    indexed_7day = slide_index_dbl(
        new_cases, # calculer sur les new_cases
        .i = date_hospitalisation, # indexé avec date_onset 
        .f = ~sum(.x, na.rm = TRUE), # la fonction est sum() avec les valeurs manquantes supprimées
        .before = days(6))               # la fenêtre est le JOUR et les 6 JOURS précédents
    )

```

Observez comment, dans la colonne normale, pour les 7 premières lignes, le nombre augmente régulièrement *malgré le fait que les lignes ne sont pas à moins de 7 jours les unes des autres*! La colonne adjacente "indexée" tient compte de ces jours calendaires absents, de sorte que ses sommes sur 7 jours sont beaucoup plus faibles, du moins à cette période de l'épidémie où les cas sont plus espacés.  

```{r, echo=F}
DT::datatable(rolling, rownames = FALSE, options = list(pageLength = 12, scrollX=T) )
```



Vous pouvez maintenant tracer ces données avec `ggplot()` :  

```{r}
ggplot(data = rolling)+
  geom_line(mapping = aes(x = date_hospitalisation, y = indexed_7day), size = 1)
```




<!-- ### Roulage par mois {.non numéroté} -->

<!-- Si vous voulez calculer des statistiques par mois (par exemple, la somme, la moyenne, le maximum), vous pouvez le faire avec **dplyr** comme décrit dans la page [Regroupement des données]. Il suffit de créer une colonne "mois", de grouper les données, et d'exécuter vos calculs avec `summarise()`.   -->

<!-- Si toutefois vous souhaitez calculer des statistiques glissantes sur plusieurs mois (par exemple une fenêtre glissante de 2 mois), vous pouvez utiliser la fonction `slide_period()` de **slider**.   -->

<!-- ```{r} -->
<!-- monthly_mean = function(data){ -->
<!-- summarise(data, mean = mean(new_cases, na.rm=T)) -->
<!-- } -->

<!-- linelist %>% -->
<!-- count(date_hospitalisation, name = "new_cases") %>% -->>
<!-- mutate( -->
<!-- slide_period_dfr( -->
<!-- new_cases, -->
<!-- .i = date_hospitalisation, -->
<!-- .period = "mois", -->
<!-- .f = moyenne_mensuelle))  #~mean(.x, na.rm=T))) -->

<!-- #values_col = new_cases, -->
<!-- #index_col = date_hospitalisation -->
<!-- )) -->



<!-- ``` -->


### Rouler par groupe {#roll_slider_group .unnumbered}  

Si vous regroupez vos données avant d'utiliser une fonction **slider**, les fenêtres de glissement seront appliquées par groupe. Veillez à disposer vos lignes dans l'ordre souhaité *par groupe*.  

Chaque fois qu'un nouveau groupe commence, la fenêtre coulissante recommence. Par conséquent, une nuance à prendre en compte est que si vos données sont groupées *et* que vous avez défini `.complete = TRUE`, vous aurez des valeurs vides à chaque transition entre les groupes. Au fur et à mesure que la fonction se déplace vers le bas dans les lignes, chaque transition dans la colonne de regroupement redémarre l'accumulation de la taille minimale de la fenêtre pour permettre un calcul.  

Voir la page du manuel sur le [Regroupement des données](#grouping_data) pour plus de détails sur le regroupement des données.

Ci-dessous, nous comptons les cas de la linelist par date *et* par hôpital. Ensuite, nous classons les lignes par ordre croissant, d'abord par hôpital, puis par date. Ensuite, nous définissons `group_by()`. Nous pouvons alors créer notre nouvelle moyenne mobile. 


```{r}
grouped_roll <- linelist %>%

  count(hospital, date_hospitalisation, name = "new_cases") %>% 

  arrange(hospital, date_hospitalisation) %>% # arranger les lignes par hôpital puis par date
  
  group_by(hospital) %>% # groupage par hôpital 
    
  mutate( # moyenne mobile  
    mean_7day_hosp = slide_index_dbl(
      .x = new_cases, # le nombre de cas par jour d'hospitalisation
      .i = date_hospitalisation, # indice sur la date d'admission
      .f = mean, # utiliser mean()                   
      .before = days(6) # utilise le jour et les 6 jours précédents
      )
  )

```

Voici le nouvel ensemble de données :  

```{r, echo=F}
DT::datatable(grouped_roll, rownames = FALSE, options = list(pageLength = 12, scrollX=T) )
```


Nous pouvons maintenant tracer les moyennes mobiles, en affichant les données par groupe en spécifiant `~ hospital` à `facet_wrap()` dans `ggplot()`. Pour le plaisir, nous traçons deux géométries - un `geom_col()` montrant le nombre de cas quotidiens et un `geom_line()` montrant la moyenne mobile sur 7 jours.  


```{r, warning=F, message=F}
ggplot(data = grouped_roll)+
  geom_col( # Trace le nombre de cas de daly sous forme de barres grises
     mapping = aes(
      x = date_hospitalisation,
      y = new_cases),
    fill = "grey",
    width = 1)+
  geom_line(   # tracer la moyenne mobile sous forme de ligne colorée par hôpital
    mapping = aes(
      x = date_hospitalisation,
      y = mean_7day_hosp,
      color = hospital),
    size = 1)+
  facet_wrap(~hospital, ncol = 2)+ # créer des mini-plots par hôpital
  theme_classic()+ # simplifie le fond d'écran  
  theme(legend.position = "none")+ # supprimer la légende
  labs( # ajout d'étiquettes pour les graphiques
      title = "7-day rolling average of daily case incidence",
    x = "Date of admission",
    y = "Case incidence")
```


<span style="color : red ;">**ATTENTION:_** Si vous obtenez une erreur disant *"slide() was deprecated in tsibble 0.9.0 and is now defunct. Please use slider::slide() instead. "*, cela signifie que la fonction `slide()` du paquet **tsibble** masque la fonction `slide()` du paquet **slider**. Corrigez cela en spécifiant le package dans la commande, comme `slider::slide_dbl()`.</span>.




</p> <p>Vous pouvez regrouper les données avant d'utiliser une fonction **slider**. Par exemple, si vous voulez calculer la même somme glissante de 7 jours que ci-dessus, mais par hôpital. ci-dessus le délai moyen glissant entre l'apparition des symptômes et l'admission à l'hôpital (colonne `days_onset_hosp`).   -->

<!-- Vous pouvez regrouper les données par mois d'apparition des symptômes en utilisant `floor_date()` de **lubridate** comme décrit dans la page [Regroupement des données]. Ensuite, utilisez `slide_index_dbl()` comme précédemment mais définissez l'étendue de votre fenêtre en utilisant `months()` (également de **lubridate**).  -->

<!-- Si vous voulez une moyenne mobile par *mois*, vous pouvez utiliser **lubridate** pour regrouper les données par mois, puis appliquer `slide_index_dbl()` comme indiqué ci-dessous pour une moyenne mobile de trois mois : -->

<!-- ```{r} -->
<!-- months_delay <- linelist %>% -->
<!-- arrange(date_onset) %>% # drop rows missing date of onset -->
<!-- group_by(hospital) %>% -->
<!-- #group_by(month_onset = floor_date(date_onset, "month")) %>% # créer et grouper par mois d'apparition -->
<!-- mutate( -->
<!-- delay_7d = slide_index_dbl( -->
<!-- days_onset_hosp, # calculer la moyenne en fonction de la valeur dans la colonne new_cases -->
<!-- .i = date_onset, # la colonne index est date_onset, donc les dates non présentes sont incluses dans la fenêtre de 7 jours -->
<!-- .f = ~mean(.x, na.rm = TRUE), # la fonction est mean() avec les valeurs manquantes supprimées -->
<!-- .before = days(7)), -->

<!-- delay_month = slide_index_dbl( -->
<!-- days_onset_hosp, # calculer la moyenne en fonction de la valeur de la colonne new_cases -->
<!-- .i = date_onset, # la colonne index est date_onset, donc les dates non présentes sont incluses dans la fenêtre de 7 jours -->
<!-- .f = ~mean(.x, na.rm = TRUE), # la fonction est mean() avec les valeurs manquantes supprimées -->
<!-- .before = months(1))))               # la fenêtre est le mois et le mois antérieur -->


<!-- # la fenêtre est le mois et le mois précédent -->

<!-- ``` -->

<!-- ```{r} -->
<!-- ggplot(data = months_delay, mapping = aes(x = month_onset))+ -->
<!-- geom_line(mapping = aes(y = )) -->

<!-- ``` -->






<!-- ======================================================= -->
## Calculer avec **tidyquant** dans `ggplot()` { }

Le paquet **tidyquant** offre une autre approche du calcul des moyennes mobiles - cette fois-ci à partir *dans* une commande `ggplot()` elle-même.  

En dessous de la `linelist`, les données sont comptées par date d'apparition et sont représentées par une ligne fondue (`alpha` < 1). La ligne superposée est créée avec `geom_ma()` du paquet **tidyquant**, avec une fenêtre de 7 jours (`n = 7`) avec une couleur et une épaisseur spécifiées.  

Par défaut, `geom_ma()` utilise une moyenne mobile simple (`ma_fun = "SMA"`), mais d'autres types peuvent être spécifiés, tels que :  

* "EMA" - moyenne mobile exponentielle (plus de poids aux observations récentes)  
* "WMA" - moyenne mobile pondérée (`wts` sont utilisés pour pondérer les observations dans la moyenne mobile)  
* D'autres peuvent être trouvées dans la documentation de la fonction  

```{r}
linelist %>% 
  count(date_onset) %>% # compte les cas par jour
  drop_na(date_onset) %>% # Suppression des cas pour lesquels la date d'apparition est manquante
  ggplot(aes(x = date_onset, y = n))+ # démarrer ggplot
    geom_line( # tracer les valeurs brutes
      size = 1,
      alpha = 0.2 # ligne semi-transparente
      )+             
    tidyquant::geom_ma( # tracer la moyenne mobile
      n = 7,           
      size = 1,
      color = "blue")+ 
  theme_minimal() # fond simple
```

Voir cette [vignette](https://cran.r-project.org/web/packages/tidyquant/vignettes/TQ04-charting-with-tidyquant.html) pour plus de détails sur les options disponibles dans **tidyquant**.  


<!-- ## Régression par roulement -->

<!-- ```{r} -->
<!-- a <- linelist %>% -->
<!-- separate(time_admission, into = c("hour", "minute"), sep = " :") %>% -->
<!-- count(days_onset_hosp, hour) %>% -->
<!-- mutate(reg_admit_hour = slide(., ~lm(days_onset_hosp ~ hour), .before = 3, .complete = T)) %>% -->
<!-- mutate(coeff = reg_admit_hour[[1]]) -->

<!-- ggplot()+ -->
<!-- geom_point(aes(x = hour, y = days_onset_hosp)) -->

<!-- ``` -->

<!-- ```{r} -->
<!-- linelist %>% -->
<!-- mutate( -->

<!-- ) -->

<!-- ``` -->


<!-- ======================================================= -->
## Ressources { }


Voir la [vignette en ligne utile pour le paquet **slider**](https://cran.r-project.org/web/packages/slider/vignettes/slider.html).  

La page **slider** [github](https://github.com/DavisVaughan/slider)

Une **slider** [vignette](https://davisvaughan.github.io/slider/articles/slider.html)  

[Vignette tidyquant](https://cran.r-project.org/web/packages/tidyquant/vignettes/TQ04-charting-with-tidyquant.html)

Si votre cas d'utilisation exige que vous "passiez" les week-ends et même les jours fériés, vous aimerez peut-être le paquet **almanac**.

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/moving_average.Rmd-->

# Série temporelle et détection des épidémies {#time_series}  

<!-- ======================================================= -->
## Aperçu { }

Cet onglet démontre l'utilisation de plusieurs paquets pour l'analyse des séries temporelles. Il s'appuie principalement sur les paquets de la famille [**tidyverts**](https://tidyverts.org/) mais utilise également le paquet RECON [**trending**](https://github.com/reconhub/trending) pour ajuster des modèles qui sont plus appropriés à l'épidémiologie des maladies infectieuses. 

Notez que dans l'exemple ci-dessous, nous utilisons un ensemble de données provenant du paquet **surveillance**. sur Campylobacter en Allemagne (voir le [chapitre sur les données](#data_used), du manuel pour plus de détails). Cependant, si vous vouliez exécuter le même code sur un ensemble de données avec plusieurs pays ou d'autres strates, il y a un exemple de code pour cela dans le fichier [r4epis github repo](https://github.com/R4EPI/epitsa). 

Les sujets abordés sont les suivants :  

1.  Données de séries temporelles 
2.  Analyse descriptive 
3.  Régressions ajustées
4.  Relation de deux séries temporelles 
5.  Détection de l'épidémie
6.  Séries chronologiques interrompues


<!-- ======================================================= -->
## Préparation { }

### Paquets {.unnumbered}

Ce morceau de code montre le chargement des paquets nécessaires aux analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* le charge pour l'utiliser. Vous pouvez également charger des paquets avec `library()` depuis **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les paquets R.  

```{r load_packages}
pacman::p_load(rio, # Importation du fichier
               here, # Localisation de fichiers
               tidyverse, # gestion des données + graphiques ggplot2
               tsibble, # gère les ensembles de données de séries temporelles
               slider, # pour calculer les moyennes mobiles
               imputeTS, # pour remplir les valeurs manquantes
               feasts, # pour la décomposition des séries temporelles et l'autocorrélation
               forecast, # ajustement des termes sin et cosin aux données (note : doit être chargé après feasts)
               trending, # ajustement et évaluation des modèles 
               tmaptools, # pour obtenir des géocoordonnées (lon/lat) à partir de noms de lieux
               ecmwfr, # pour interagir avec l'API CDS de copernicus sateliate
               stars, # pour lire les fichiers .nc (données climatiques)
               units, # pour définir les unités de mesure (données climatiques)
               yardstick, # pour l'examen de la précision du modèle
               surveillance # pour la détection des aberrations
               )
``` 

### Chargement des données {.unnumbered}

Vous pouvez télécharger toutes les données utilisées dans ce manuel en suivant les instructions de la page [Télécharger le manuel et les données](#download_book_data).  

L'ensemble de données d'exemple utilisé dans cette section est le décompte hebdomadaire des cas de campylobacter signalés en Allemagne entre 2001 et 2011. <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/campylobacter_germany.xlsx' class='download-button'>
	Vous pouvez cliquer ici pour télécharger<span> ce fichier de données (.xlsx).</span></a>. 

Cet ensemble de données est une version réduite de l'ensemble de données disponible dans le paquet [**surveillance**](https://cran.r-project.org/web/packages/surveillance/) (pour plus de détails, chargez le paquet surveillance et voyez `?campyDE`)

Importez ces données avec la fonction `import()` du paquet **rio** (elle gère de nombreux types de fichiers comme .xlsx, .csv, .rds - voir la page [Importation et exportation](#import_export) pour plus de détails).

```{r read_data_hide, echo=F}
# Importez les comptes dans R
counts <- rio::import(here::here("data", "time_series", "campylobacter_germany.xlsx"))
```

```{r read_data_show, eval=F}
# Importez les comptes dans R
counts <- rio::import("campylobacter_germany.xlsx")
```

Les 10 premières lignes des comptages sont affichées ci-dessous.

```{r inspect_data, message=FALSE, echo=F}
# Affichez les données de comptage sous forme de tableau
DT::datatable(head(counts, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Nettoyer les données {.unnumbered}

Le code ci-dessous s'assure que la colonne de date est dans le format approprié. 
Pour cet onglet, nous utiliserons le package **tsibble** et donc la fonction `yearweek` sera utilisée pour créer une variable de semaine calendaire. Il existe plusieurs autres façons de le faire (voir la page [Manipuler les dates](#working_dates) pour plus de détails), mais pour les séries temporelles, il est préférable de rester dans un seul cadre (**tsibble**). 

```{r clean_data}

## s'assurer que la colonne date est dans le format approprié
counts$date <- as.Date(counts$date)

## créer une variable de semaine calendaire 
## adapter les définitions ISO des semaines commençant un lundi
counts <- counts %>% 
     mutate(epiweek = yearweek(date, week_start = 1))

```

### Télécharger les données climatiques {.unnumbered} 

Dans la partie *relation de deux séries temporelles* de cette page, nous allons comparer le nombre de cas de campylobacter aux données climatiques. 

Les données climatiques de n'importe quel endroit du monde peuvent être téléchargées à partir du satellite Copernicus de l'UE. Il ne s'agit pas de mesures exactes, mais de données basées sur un modèle (similaire à l'interpolation), mais l'avantage est une couverture horaire globale ainsi que des prévisions.  

Vous pouvez télécharger chacun de ces fichiers de données climatiques à partir de la page [Télécharger le manuel et les données](#download_book_data).  

Pour les besoins de la démonstration, nous allons présenter le code R permettant d'utiliser le paquet **ecmwfr** pour extraire ces données du magasin de données climatiques Copernicus Climate Data Store. Vous devrez créer un compte gratuit pour que cela fonctionne. Le site Web du paquet contient un [demo utile](https://github.com/bluegreen-labs/ecmwfr#use-copernicus-climate-data-store-cds) sur la manière de procéder. Vous trouverez ci-dessous un exemple de code expliquant comment procéder, une fois que vous les clés API appropriées. Vous devez remplacer les X ci-dessous par les identifiants de votre compte. Vous devrez télécharger une année de données à la fois, sinon le serveur s'arrête. 

Si vous n'êtes pas sûr des coordonnées d'un lieu pour lequel vous voulez télécharger des données, vous pouvez utiliser le paquet **tmaptools** pour extraire les coordonnées des cartes routières ouvertes. Une autre option est le paquet [**photon**](https://github.com/rCarto/photon)
mais il n'a pas encore été publié sur CRAN. **photon** fournit plus de données contextuelles lorsqu'il y a plusieurs correspondances pour votre recherche. 

```{r weather_data, eval = FALSE}

## récupérer les coordonnées de l'emplacement
coords <- geocode_OSM("Germany", geometry = "point")

## rassembler les long/lats dans un format pour les requêtes ERA-5 (bounding box) 
## (comme on ne veut qu'un seul point, on peut répéter les coordonnées)
request_coords <- str_glue_data(coords$coords, "{y}/{x}/{y}/{x}")


## Extraction des données modélisées à partir du satellite copernicus (réanalyse ERA-5)
## https://cds.climate.copernicus.eu/cdsapp#!/software/app-era5-explorer?tab=app
## https://github.com/bluegreen-labs/ecmwfr

## Configurer la clé pour les données météo 
wf_set_key(user = "XXXXX",
           key = "XXXXXXXXX-XXXX-XXXX-XXXX-XXXXXX-XXXXXXXXX",
           service = "cds") 

## Exécution pour chaque année d'intérêt (sinon le serveur s'arrête)
for (i in 2002:2011) {
  
  ## construire une requête 
  ## voir ici pour savoir comment faire : https://bluegreen-labs.github.io/ecmwfr/articles/cds_vignette.html#the-request-syntax
  ## changer la requête en une liste en utilisant le bouton addin ci-dessus (python to list)
  ## La cible est le nom du fichier de sortie ! !!
   request <- request <- list(
    product_type = "reanalysis",
    format = "netcdf",
    variable = c("2m_temperature", "total_precipitation"),
    year = c(i),
    month = c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12"),
    day = c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12",
            "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
            "25", "26", "27", "28", "29", "30", "31"),
    time = c("00:00", "01:00", "02:00", "03:00", "04:00", "05:00", "06:00", "07:00",
             "08:00", "09:00", "10:00", "11:00", "12:00", "13:00", "14:00", "15:00",
             "16:00", "17:00", "18:00", "19:00", "20:00", "21:00", "22:00", "23:00"),
    area = request_coords,
    dataset_short_name = "reanalysis-era5-single-levels",
    target = paste0("germany_weather", i, ".nc")
  )
  
  ### Télécharger le fichier et le stocker dans le répertoire de travail actuel.
  file <- wf_request(user = "XXXXX", # ID utilisateur (pour l'authentification)
                     request = request, # la requête
                     transfer = TRUE, # télécharger le fichier
                     path = here::here("data", "Weather")) ## chemin pour sauvegarder les données
  }

```

### Charger les données climatiques {.unnumbered}

Que vous ayez téléchargé les données climatiques via notre manuel ou que vous ayez utilisé le code ci-dessus, vous devriez maintenant avoir 10 ans de fichiers de données climatiques ".nc" stockés dans le même dossier sur votre ordinateur.  

Utilisez le code ci-dessous pour importer ces fichiers dans R avec le paquet **stars**. 

```{r read_climate, warning = FALSE, message = FALSE}

## définir le chemin vers le dossier météo 
file_paths <- list.files(
  here::here("data", "time_series", "weather"), # remplacer par votre propre chemin de fichier 
  full.names = TRUE)

## ne garder que ceux qui ont le nom courant d'intérêt 
file_paths <- file_paths[str_detect(file_paths, "germany")]

## lire tous les fichiers en tant qu'objet stars 
data <- stars::read_stars(file_paths)
```

Une fois que ces fichiers ont été importés en tant qu'objet `data`, nous allons les convertir en un cadre de données.  

```{r, warning=FALSE, message = FALSE}
## conversion en cadre de données 
temp_data <- as_tibble(data) %>% 
  ## ajouter des variables et corriger les unités
  mutate(
    ## créer une variable de semaine calendaire 
    epiweek = tsibble::yearweek(time), 
    ## créer une variable de date (début de la semaine calendaire)
    date = as.Date(epiweek),
    ## changer la température de kelvin en celsius
    t2m = set_units(t2m, celsius), 
    ## changer les précipitations de mètres en millimètres 
    tp = set_units(tp, mm)) %>% 
  ## regrouper par semaine (en gardant la date aussi)
  group_by(epiweek, date) %>% 
  ## obtenir la moyenne par semaine
  summarise(t2m = as.numeric(mean(t2m)), 
            tp = as.numeric(mean(tp)))

```




<!-- ======================================================= -->
## Données de séries temporelles { }

Il existe un certain nombre de paquets différents pour structurer et traiter les données de données de séries temporelles. Comme nous l'avons dit, nous nous concentrerons sur la famille de paquets **tidyverts** et nous utiliserons donc le paquet **tsibble** pour définir notre objet série temporelle. Avoir un ensemble de données défini comme un objet de série temporelle, il est beaucoup plus facile de structurer notre analyse. 

Pour ce faire, nous utilisons la fonction `tsibble()` et spécifions l'"index", c'est-à-dire la variable spécifiant l'unité de temps qui nous intéresse. Dans notre cas, il s'agit de la variable `epiweek`. 

Si nous avions un ensemble de données avec des comptages hebdomadaires par province, par exemple, nous pourrions également spécifier la variable de regroupement en utilisant l'argument `key = `. Cela nous permettrait d'effectuer une analyse pour chaque groupe. 


```{r ts_object}

## Définir un objet de série temporelle 
counts <- tsibble(counts, index = epiweek)

```

En regardant `class(counts)`, on constate qu'en plus d'être un cadre de données ordonné ("tbl_df", "tbl", "data.frame"), il possède les propriétés supplémentaires d'un cadre de données de série temporelle ("tbl_ts").

Vous pouvez jeter un coup d'oil rapide à vos données en utilisant **ggplot2**. Nous voyons sur le graphique que qu'il existe un modèle saisonnier clair, et qu'il n'y a pas de manques. Cependant, il semble y avoir un problème avec la déclaration au début de chaque année, dans la dernière semaine de l'année, puis augmentent pour la première semaine de l'année suivante. 

```{r basic_plot}

## tracer un graphique linéaire des cas par semaine
ggplot(counts, aes(x = epiweek, y = case)) + 
     geom_line()

```


<span style="color : red ;">**_ATTENTION:_** La plupart des ensembles de données ne sont pas aussi propres que cet exemple. Vous devrez vérifier les doublons et les valuers qui manques comme ci-dessous. </span>

<!-- ======================================================= -->
### Duplicates {.unnumbered}

**tsibble** n'autorise pas les observations en double. Ainsi, chaque ligne devra être
unique, ou unique au sein du groupe (variable `key`). Le paquet a quelques fonctions qui aident à identifier les doublons. Celles-ci incluent `are_duplicated()` qui vous donne un vecteur VRAI/FAUX indiquant si la ligne est un dupliqué, et `duplicates()` qui vous donne un cadre de données des lignes dupliquées. 

Voir la page sur [De-duplication](#deduplication) pour plus de détails sur la façon de sélectionner les lignes que vous voulez. 

```{r duplicates, eval = FALSE}

## obtient un vecteur de VRAI/FAUX si les lignes sont des doublons
are_duplicated(counts, index = epiweek) 

## Obtenez un cadre de données pour les lignes dupliquées. 
duplicates(counts, index = epiweek) 

```

<!-- ======================================================= -->
### Manques {.unnumbered}

Nous avons vu lors de notre brève inspection ci-dessus qu'il n'y a pas de manques, mais nous avons aussi vu qu'il semble y avoir un problème de retard de déclaration autour du nouvel an. Une façon de résoudre ce problème pourrait être de définir ces valeurs comme manquantes, puis d'imputer les valeurs. La forme la plus simple d'imputation de séries chronologiques consiste à tracer une ligne droite entre les dernières valeurs non manquantes et les valeurs manquantes.
Pour ce faire, nous utiliserons la fonction `na_interpolation()` du package **imputeTS**. 

Consultez la page [Données manquantes](#missing_data) pour connaître les autres options d'imputation.  

Une autre solution consisterait à calculer une moyenne mobile, pour essayer de pour tenter d'aplanir ces problèmes de déclaration apparents (voir la section suivante et la page sur les [Moyennes mobiles](#moving_average)). 

```{r missings}

## créez une variable avec les manques au lieu des semaines avec des problèmes de déclaration.
counts <- counts %>% 
     mutate(case_miss = if_else(
          ## si epiweek contient 52, 53, 1 ou 2
          str_detect(epiweek, "W51|W52|W53|W01|W02"), 
          ## alors définie comme manquante 
          NA_real_, 
          ## sinon, conservez la valeur dans le cas
          case
     ))

## alternativement interpoler les manquants par une tendance linéaire 
## entre deux points adjacents les plus proches
counts <- counts %>% 
  mutate(case_int = imputeTS::na_interpolation(case_miss)
         )

## pour vérifier quelles valeurs ont été imputées par rapport à l'original.
ggplot_na_imputations(counts$case_miss, counts$case_int) + 
  ## faire un graphique traditionnel (avec des axes noirs et un fond blanc)
  theme_classic()

```




<!-- ======================================================= -->
## Analyse descriptive { }



<!-- ======================================================= -->
### Moyennes mobiles {#timeseries_moving .unnumbered}

Si les données sont très bruyantes (les comptes sautent en haut et en bas), alors il peut être utile de calculer une moyenne mobile. Dans l'exemple ci-dessous, pour chaque semaine, nous calculons le nombre moyen de cas des quatre semaines précédentes. Cela permet de lisser les données, pour les rendre plus interprétables. Dans notre cas, cela n'apporte pas grand-chose.
Nous nous en tiendrons aux données interpolées pour la suite de l'analyse. Voir la page [Moyennes mobiles](#moving_average) pour plus de détails. 

```{r moving_averages}

## créer une variable de moyenne mobile (traite les manques)
counts <- counts %>% 
     ## créer la variable ma_4w 
     ## glisser sur chaque ligne de la variable case
     mutate(ma_4wk = slider::slide_dbl(case, 
                               ## pour chaque ligne, calculez le nom
                               ~ mean(.x, na.rm = TRUE),
                               ## utiliser les quatre semaines précédentes
                               .before = 4))

### faire une visualisation rapide de la différence 
ggplot(counts, aes(x = epiweek)) + 
     geom_line(aes(y = case)) + 
     geom_line(aes(y = ma_4wk), colour = "red")

```


<!-- ======================================================= -->
### Périodicité {.unnumbered}

Nous définissons ci-dessous une fonction personnalisée pour créer un périodogramme. Voir la page [ecrire les fonctions](#writing_functions) pour des informations sur la façon d'écrire des fonctions dans R.  

Tout d'abord, la fonction est définie. Ses arguments incluent un jeu de données avec une colonne `counts`, `start_week = ` qui est la première semaine du jeu de données, un nombre pour indiquer combien de périodes par an (par exemple 52, 12), et enfin le style de sortie (voir les détails dans le code ci-dessous).  


```{r periodogram}
## Arguments de la fonction
#####################
## x est un ensemble de données
## counts est une variable avec des données de comptage ou des taux dans x 
## start_week est la première semaine de votre série de données
## period est le nombre d'unités dans une année. 
## output indique si vous souhaitez renvoyer le périodogramme spectral ou les semaines de pointe.
  ## "périodogramme" ou "semaines".

# Définissez la fonction
periodogram <- function(x, 
                        counts, 
                        start_week = c(2002, 1), 
                        period = 52, 
                        output = "weeks") {
  

    ## s'assurer que ce n'est pas un tsibble, filtrer sur le projet et ne garder que les colonnes d'intérêt.
    prepare_data <- dplyr::as_tibble(x)
    
    # prepare_data <- prepare_data[prepare_data[[strata]] == j, ]
    prepare_data <- dplyr::select(prepare_data, {{counts}})
    
    ## créer une série temporelle "zoo" intermédiaire pour pouvoir l'utiliser avec spec.pgram
    zoo_cases <- zoo::zooreg(prepare_data, 
                             start = start_week, frequency = period)
    
    ## obtenir un périodogramme spectral n'utilisant pas la transformée de fourier rapide. 
    periodo <- spec.pgram(zoo_cases, fast = FALSE, plot = FALSE)
    
    ## retourner les semaines de pointe 
    periodo_weeks <- 1 / periodo$freq[order(-periodo$spec)] * period
    
    if (output == "weeks") {
      periodo_weeks
    } else {
      periodo
    }
    
}

## obtenir le périodogramme spectral pour extraire les semaines avec les fréquences les plus élevées 
## (vérification de la saisonnalité) 
periodo <- periodogram(counts, 
                       case_int, 
                       start_week = c(2002, 1),
                       output = "periodogram")

## Tirez le spectre et la fréquence dans un cadre de données pour le tracé.
periodo <- data.frame(periodo$freq, periodo$spec)

## Tracez un périodogramme montrant la périodicité la plus fréquente. 
ggplot(data = periodo, 
                aes(x = 1/(periodo.freq/52), y = log(periodo.spec))) + 
  geom_line() + 
  labs(x = "Period (weeks)", y = "Log(density)")


## obtenir un vecteur semaines dans l'ordre croissant 
peak_weeks <- periodogram(counts, 
                          case_int, 
                          start_week = c(2002, 1), 
                          output = "weeks")

```

<span style="color : black ;">**_ATTENTION:_** Il est possible d'utiliser les semaines ci-dessus pour les ajouter aux termes sin et cosinus, cependant nous utiliserons une fonction pour générer ces termes (voir la section régression ci-dessous) </span>.

<!-- ======================================================= -->
### Décomposition {.unnumbered}

La décomposition classique est utilisée pour décomposer une série temporelle en plusieurs parties, qui lorsqu'elles sont prises ensemble constituent le modèle que vous voyez. 

Ces différentes parties sont :  

* La tendance-cycle (la direction à long terme des données)  
* La saisonnalité (motifs répétitifs)  
* L'aléatoire (ce qui reste après avoir retiré la tendance et la saison).  


```{r decomposition, warning=F, message=F}

## Décomposition de l'ensemble de données counts 
counts %>% 
  ## en utilisant un modèle additif de décomposition classique
  model(classical_decomposition(case_int, type = "additive")) %>%
  ## extraire les informations importantes du modèle
  components() %>% 
  ## générer un graphique 
  autoplot()

```

<!-- ======================================================= -->
### Autocorrélation {.unnumbered}

L'autocorrélation vous renseigne sur la relation entre les comptes de chaque semaine et les semaines qui la précèdent (appelées décalages).  

En utilisant la fonction `ACF()`, nous pouvons produire un graphique qui nous montre un certain nombre de lignes pour la relation à différents décalages. Si le décalage est de 0 (x = 0), cette ligne sera toujours égale à 1, car elle montre la relation entre les deux. La première ligne illustrée ici (x = 1) montre la relation entre chaque observation et l'observation qui la précède (décalage de 1), la seconde montre la relation entre  chaque observation et l'avant-dernière (décalage de 2) et ainsi de suite jusqu'à un décalage de 52 qui montre la relation entre chaque observation et l'observation d'un an (52 semaines avant). 

L'utilisation de la fonction `PACF()` (pour l'autocorrélation partielle) montre le même type de relation, mais ajustée pour toutes les autres semaines intermédiaires. Ceci est moins informatif pour déterminer la périodicité. 

```{r autocorrelation}

## en utilisant l'ensemble de données counts
counts %>% 
  ## calculer l'autocorrélation en utilisant une année complète de lags
  ACF(case_int, lag_max = 52) %>% 
  ## afficher un graphique
  autoplot()

## en utilisant l'ensemble de données counts 
counts %>% 
  ## calculer l'autocorrélation partielle en utilisant une année complète de décalages
  PACF(case_int, lag_max = 52) %>% 
  ## Afficher un graphique
  autoplot()

```

Vous pouvez tester formellement l'hypothèse nulle d'indépendance d'une série temporelle (c'est à dire qu'elle n'est pas autocorrélée) en utilisant le test de Ljung-Box (dans le paquet **stats**). Une valeur p significative suggère qu'il existe une autocorrélation dans les données.

```{r ljung_box}

## Test d'indépendance 
Box.test(counts$case_int, type = "Ljung-Box")

```


<!-- ======================================================= -->
## Ajustement des régressions { }

Il est possible d'ajuster un grand nombre de régressions différentes à une série temporelle, Cependant, nous allons montrer ici comment ajuster une régression binomiale négative, c'est souvent la plus appropriée pour les données de comptage dans les maladies infectieuses. 

<!-- ======================================================= -->
### Termes de Fourier {.unnumbered}

Les termes de Fourier sont l'équivalent des courbes sin et cosin. La différence est que ceux-ci sont ajustés en fonction de la recherche de la combinaison de courbes la plus appropriée pour expliquervos données.  
Si vous n'ajustez qu'un seul terme de Fourier, cela équivaudrait à ajuster une courbe sin et un cosin pour le décalage le plus fréquent de votre périodogramme (dans notre cas 52 semaines). Nous utilisons la fonction `fourier()` du paquet **forecast**.  

Dans le code ci-dessous, nous assignons en utilisant le `$`, car `fourier()` renvoie deux colonnes (une pour le sin et une pour le cosin) et celles-ci sont ajoutées à l'ensemble de données sous forme de liste, appelée "fourier". Mais cette liste peut ensuite être utilisée comme une variable normale dans une régression. 

```{r fourier}

## ajout des termes de fourier en utilisant les variables epiweek et case_int
counts$fourier <- select(counts, epiweek, case_int) %>% 
  fourier(K = 1)
```

<!-- ======================================================= -->
### Binomiale négative {.unnumbered}

Il est possible d'ajuster des régressions en utilisant les fonctions **stats** ou **MASS** de base (par exemple, `lm()`, `glm()` et `glm.nb()`). Cependant, nous utiliserons celles du paquet **trending**, car cela permet de calculer les intervalles de confiance et de prédictionconfiance et les intervalles de prédiction appropriés (qui ne sont pas disponibles autrement). La syntaxe est la même, et vous spécifiez une variable de résultat puis un tilde (~) puis vous ajoutez vos diverses variables d'exposition d'intérêt séparées par un plus (+). 

L'autre différence est que nous définissons d'abord le modèle et ensuite `fit()` aux données. Ceci est utile car cela permet de comparer plusieurs modèles différents avec la même syntaxe. 

<span style="color : darkgreen ;">**_ATTENTION:_** Si vous vouliez utiliser des taux, plutôt que des comptes, vous pourriez inclure la variable population comme terme de décalage logarithmique, en ajoutant `offset(log(population)`. Vous auriez alors besoin de définir la population à 1, avant d'utiliser `predict()` afin de produire un taux. </span>

<span style="color : darkgreen ;">**_ATTENTION:_** Pour l'ajustement de modèles plus complexes tels que les modèles comme ARIMA ou prophète, voir le paquet [**fable**](https://fable.tidyverts.org/index.html).</span>

```{r nb_reg, warning = FALSE}

## définissez le modèle que vous voulez ajuster (binomiale négative). 
model <- glm_nb_model(
  ## définir le nombre de cas comme résultat d'intérêt
  case_int ~
    ## utiliser epiweek pour tenir compte de la tendance
    epiweek +
    ## utiliser les termes de fourier pour tenir compte de la saisonnalité
    fourier)

## ajustez votre modèle en utilisant le jeu de données de comptage
fitted_model <- trending::fit(model, data.frame(counts))

### calculer les intervalles de confiance et les intervalles de prédiction 
observed <- predict(fitted_model, simulate_pi = FALSE)

estimate_res <- data.frame(observed$result)

## Tracez votre régression 
ggplot(data = estimate_res, aes(x = epiweek)) + 
  ## ajouter une ligne pour l'estimation du modèle
  geom_line(aes(y = estimate),
            col = "red") + 
  ## ajouter une bande pour les intervalles de prédiction 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## ajouter une ligne pour le nombre de cas observés
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## faire un graphique traditionnel (avec des axes noirs et un fond blanc)
  theme_classic()


```

<!-- ======================================================= -->
### Résidus {.unnumbered}

Pour voir si notre modèle s'adapte bien aux données observées, nous devons examiner les résidus. Les résidus sont la différence entre les comptes observés et les comptes estimés à partir du modèle. Nous pourrions calculer cela simplement en utilisant `case_int - estimate`, mais la fonction `residuals()` l'extrait directement de la régression pour nous.

Ce que nous voyons ci-dessous, c'est que nous n'expliquons pas toutes les variations que nous pourrions expliquer avec le modèle. Il se peut que nous devions ajuster plus de termes de Fourier, et s'attaquer à l'amplitude. Cependant, pour cet exemple, nous allons laisser les choses telles quelles. Les graphiques montrent que notre modèle est moins bon dans les pics et les creux (lorsque les comptages sont les plus élevés et les plus bas) et qu'il est plus susceptible de sous-estimer les comptagees observés. 


```{r, warning=F, message=F}

## calculate the residuals 
estimate_res <- estimate_res %>% 
  mutate(resid = fitted_model$result[[1]]$residuals)

## are the residuals fairly constant over time (if not: outbreaks? change in practice?)
estimate_res %>%
  ggplot(aes(x = epiweek, y = resid)) +
  geom_line() +
  geom_point() + 
  labs(x = "epiweek", y = "Residuals")

## is there autocorelation in the residuals (is there a pattern to the error?)  
estimate_res %>% 
  as_tsibble(index = epiweek) %>% 
  ACF(resid, lag_max = 52) %>% 
  autoplot()

## are residuals normally distributed (are under or over estimating?)  
estimate_res %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 0.01) +
  geom_rug() +
  labs(y = "count") 
  
## compare observed counts to their residuals 
  ## should also be no pattern 
estimate_res %>%
  ggplot(aes(x = estimate, y = resid)) +
  geom_point() +
  labs(x = "Fitted", y = "Residuals")

## formally test autocorrelation of the residuals
## H0 is that residuals are from a white-noise series (i.e. random)
## test for independence 
## if p value significant then non-random
Box.test(estimate_res$resid, type = "Ljung-Box")

```


<!-- ======================================================= -->
## Relation entre deux séries temporelles { }

Nous examinons ici l'utilisation de données météorologiques (en particulier la température) pour expliquer le nombre de cas de campylobacter. 

<!-- ======================================================= -->
### Fusionner des ensembles de données {.unnumbered}

Nous pouvons fusionner nos ensembles de données à l'aide de la variable week. Pour plus d'informations sur la fusion, voir la section du manuel sur les [joindres](https://epirhandbook.com/joining-data.html).

```{r join}

## left join so that we only have the rows already existing in counts
## drop the date variable from temp_data (otherwise is duplicated)
counts <- left_join(counts, 
                    select(temp_data, -date),
                    by = "epiweek")

```

```{r basic_plot_bivar}

counts %>% 
  ## garder les variables qui nous intéressent 
  select(epiweek, case_int, t2m) %>% 
  ## Changez vos données en format long
  pivot_longer(
    ## utiliser epiweek comme clé
    !epiweek,
    ## déplacez les noms des colonnes vers la nouvelle colonne "measure".
    names_to = "measure", 
    ## déplacez les valeurs des cellules vers la nouvelle colonne "values".
    values_to = "value") %>% 
  ## créer un graphique avec l'ensemble de données ci-dessus
  ## Tracez l'epiweek sur l'axe des x et les valeurs (nombres/celsius) sur l'axe des y. 
  ggplot(aes(x = epiweek, y = value)) + 
    ## créez un graphique séparé pour les comptages tempérés et les comptages de cas. 
    ## laissez-les définir leurs propres axes y
    facet_grid(measure ~ ., scales = "free_y") +
    ## Tracez les deux comme une ligne
    geom_line()

```

<!-- ======================================================= -->
### Lags et corrélation croisée {.unnumbered}

Pour tester formellement quelles semaines sont les plus fortement liées entre les cas et la température. Nous pouvons utiliser la fonction de corrélation croisée (`CCF()`) du paquet **feasts**. Vous pouvez également visualiser (plutôt que d'utiliser `arrange`) en utilisant la fonction `autoplot()`. 

```{r cross_correlation, warning=FALSE}

counts %>% 
  ## calculer la corrélation croisée entre les comptages interpolés et la température
  CCF(case_int, t2m,
      ## fixer le délai maximum à 52 semaines
      lag_max = 52, 
      ## retourne le coefficient de corrélation 
      type = "correlation") %>% 
  ## arange dans l'ordre décroissant du coefficient de corrélation 
  ## montre les décalages les plus associés
  arrange(-ccf) %>% 
  ## montrer seulement les dix premiers 
  slice_head(n = 10)

```

Nous voyons qu'un décalage de 4 semaines est le plus fortement corrélé, donc nous créons une variable de température décalée à inclure dans notre régression. 

<span style="color : red ;">**_ATTENTION:_** Notez que les quatre premières semaines de nos données dans la variable de température décalée sont manquantes (`NA`) - car il n'y a pas quatre 
semaines précédentes pour obtenir des données. Afin d'utiliser cet ensemble de données avec la fonction `predict()` du paquet **trending**, nous devons utiliser l'argument `simulate_pi = FALSE` dans la fonction `predict()` plus bas. Si nous voulions utiliser l'option simulate, alors nous devons supprimer ces manques et les stocker comme un nouvel ensemble de données en ajoutant `drop_na(t2m_lag4)`. au morceau de code ci-dessous.</span>  
 

```{r lag_tempvar}

counts <- counts %>% 
  ## créer une nouvelle variable pour la température décalée de quatre semaines.
  mutate(t2m_lag4 = lag(t2m, n = 4))

```


<!-- ======================================================= -->
### Binomiale négative avec deux variables {.unnumbered}

Nous ajustons une régression binomiale négative comme nous l'avons fait précédemment. Cette fois, nous ajoutons la variable de température retardée de quatre semaines. 

<span style="color : orange ;">**_ATTENTION:_** Notez l'utilisation de `simulate_pi = FALSE`
dans l'argument `predict()`. Ceci est dû au fait que le comportement par défaut de **trending** 
est d'utiliser le paquet **ciTools** pour estimer un intervalle de prédiction. Cela ne fonctionne pas s'il y a des comptes `NA`, et produit également des intervalles plus granulaires. 
Voir `?trending::predict.trending_model_fit` pour plus de détails. </span>  

```{r nb_reg_bivar, warning = FALSE}

## définissez le modèle que vous voulez ajuster (binomial négatif). 
model <- glm_nb_model(
  ## définir le nombre de cas comme résultat d'intérêt
  case_int ~
    ## utiliser epiweek pour tenir compte de la tendance
    epiweek +
    ## utiliser les termes de fourier pour tenir compte de la saisonnalité
    fourier + 
    ## utiliser la température retardée de quatre semaines 
    t2m_lag4
    )

## ajustez votre modèle en utilisant l'ensemble de données de comptage
fitted_model <- trending::fit(model, data.frame(counts))

### calculer les intervalles de confiance et les intervalles de prédiction 
observed <- predict(fitted_model, simulate_pi = FALSE)

```


Pour étudier les termes individuels, nous pouvons extraire la régression binomiale négative originale du paquet **trending** en utilisant `get_fitted_model()` et la passer au fonction `tidy()` du paquetage **broom** pour récupérer les estimations exponentielles et lesintervalles de confiance associés.  

Ce que cela nous montre est que la température décalée, après avoir contrôlé la tendance et la saisonnalité, est similaire au nombre de cas (estimation ~ 1) et significativement associée. 
Cela suggère qu'il pourrait s'agir d'une bonne variable à utiliser pour prédire le nombre de cas futurs (les prévisions climatiques étant facilement accessibles).

```{r results_nb_reg_bivar}

fitted_model %>% 
  ## extraire la régression binomiale négative originale
  get_fitted_model() #%>% 
  ## obtenir un cadre de données ordonné des résultats
  #broom::tidy(exponentiate = TRUE, 
  #     conf.int = TRUE)
```

Une rapide inspection visuelle du modèle montre qu'il pourrait faire un meilleur travail pour d'estimer le nombre de cas observés. 

```{r plot_nb_reg_bivar, warning=F, message=F}

estimate_res <- data.frame(observed$result)
     
## Tracez votre régression 
ggplot(data = estimate_res, aes(x = epiweek)) + 
  ## ajouter une ligne pour l'estimation du modèle
  geom_line(aes(y = estimate),
            col = "red") + 
  ## ajouter une bande pour les intervalles de prédiction 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## ajouter une ligne pour le nombre de cas observés
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## faire un graphique traditionnel (avec des axes noirs et un fond blanc)
  theme_classic()


```


#### Résidus {.unnumbered}

Nous examinons à nouveau les résidus pour voir si notre modèle s'adapte bien aux données observées. Les résultats et l'interprétation sont ici similaires à ceux de la régression précédente, donc il est peut-être plus judicieux de s'en tenir au modèle plus simple sans température. 

```{r}
## calculer les résidus 
estimate_res <- estimate_res %>% 
  mutate(resid = case_int - estimate)

## les résidus sont-ils assez constants dans le temps (si non : épidémies ? changement de pratique ?)
estimate_res %>%
  ggplot(aes(x = epiweek, y = resid)) +
  geom_line() +
  geom_point() + 
  labs(x = "epiweek", y = "Residuals")

## Y a-t-il une autocorrélation dans les résidus (y a-t-il un modèle d'erreur ?) ?  
estimate_res %>% 
  as_tsibble(index = epiweek) %>% 
  ACF(resid, lag_max = 52) %>% 
  autoplot()

## les résidus sont-ils normalement distribués (y a-t-il sous-estimation ou surestimation ?)  
estimate_res %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 100) +
  geom_rug() +
  labs(y = "count") 
  
## comparer les comptages observés à leurs résidus 
  ## il ne devrait pas y avoir de modèle 
estimate_res %>%
  ggplot(aes(x = estimate, y = resid)) +
  geom_point() +
  labs(x = "Fitted", y = "Residuals")

## tester formellement l'autocorrélation des résidus
## H0 est que les résidus proviennent d'une série à bruit blanc (c'est-à-dire aléatoire)
## Test d'indépendance 
### si la valeur p est significative alors non aléatoire
Box.test(estimate_res$resid, type = "Ljung-Box")

```

<!-- ======================================================= -->
## Détection des épidémies { }

Nous allons démontrer ici deux méthodes (similaires) de détection des épidémies. 
La première s'appuie sur les sections précédentes. Nous utilisons le paquet **trending** pour ajuster les régressions aux années précédentes, et puisprédire ce que nous nous attendons à voir l'année suivante. Si les comptages observés sont supérieursce que nous attendons, cela pourrait suggérer qu'il y a une épidémie. La deuxième méthode est basée sur des principes similaires mais utilise le paquet **surveillance**,qui possède un certain nombre d'algorithmes différents pour la détection des aberrations.

<span style="color : orange ;">**_ATTENTION:_** Normalement, vous vous intéressez à l'année en cours (où vous ne connaissez que les comptages jusqu'à la semaine actuelle). Donc, dans cet exemple, nous prétendons être dans la semaine 39 de 2011.</span>

<!-- ======================================================= -->
### **tendance** paquet {.unnumbered}

Pour cette méthode, nous définissons une ligne de base (qui devrait généralement être d'environ 5 ans de données). Nous ajustons une régression aux données de base, puis nous l'utilisons pour prédire les estimations pour l'année suivante.

<!-- ======================================================= -->
#### Date limite { -}

Il est plus facile de définir vos dates à un endroit, puis de les utiliser dans le reste de votre code.

Ici nous définissons une date de début (quand nos observations ont commencé) et une date limite (la fin de notre période de référence - et le début de la période pour laquelle nous voulons prédire).Nous définissons également le nombre de semaines entre la date limite de la période de référence et la date de fin de la période pour laquelle nous sommes intéressés à prédire. 


<span style="color : black ;">**_ATTENTION:_** Dans cet exemple, nous prétendons être actuellement à la fin du mois de septembre 2011 ("2011 W39").</span>  

```{r cut_off}

## définir la date de début (quand les observations ont commencé)
start_date <- min(counts$epiweek)

## définir une semaine de coupure (fin de la ligne de base, début de la période de prédiction)
cut_off <- yearweek("2010-12-31")

## définir la dernière date qui nous intéresse (c'est-à-dire la fin de la prédiction)
end_date <- yearweek("2011-12-31")

## trouver combien de semaines dans la période (année) d'intérêt.
num_weeks <- as.numeric(end_date - cut_off)

```


<!-- ======================================================= -->
#### Ajoutez des lignes {.unnumbered}.

Pour pouvoir faire des prévisions dans un format tidyverse, nous devons avoir le bon nombre de lignes dans notre jeu de données, c'est-à-dire une ligne pour chaque semaine jusqu'à la date de fin définie ci-dessus. Le code ci-dessous vous permet d'ajouter ces lignes par une variable de regroupement - par exemple, si nous avons plusieurs pays dans un même ensemble de données, nous pouvons ajouter des lignes pour chacun d'entre eux. La fonction `group_by_key()` de **tsibble** nous permet d'effectuer ce regroupement, et ensuite de passer les données groupées aux fonctions **dplyr**, `group_modify()` et `add_row()`. Ensuite, nous spécifions la séquence des semaines entre une après la semaine maximale actuellement disponible dans les données et la semaine de fin. 

```{r add_rows}

## ajouter les semaines manquantes jusqu'à la fin de l'année 
counts <- counts %>%
  ## regrouper par région
  group_by_key() %>%
  ## pour chaque groupe, ajoutez les lignes à partir de la semaine d'épi la plus élevée jusqu'à la fin de l'année
  group_modify(~add_row(.,
                        epiweek = seq(max(.$epiweek) + 1, 
                                      end_date,
                                      by = 1)))

```



<!-- ======================================================= -->
#### Termes de Fourier {.unnumbered}

Nous devons redéfinir nos termes de Fourier, car nous voulons les adapter à la date de base uniquement, puis prédire (extrapoler) ces termes pour l'annee suivante. Pour ce faire, nous devons combiner deux listes de sortie de la fonction `fourier()` ensemble ; la première est pour les données de base, et la seconde prédit pour l'année qui nous intéresse (en définissant le paramètre `fourier()`). 

*N.b.* pour lier les lignes, nous devons utiliser `rbind()` (plutôt que tidyverse `bind_rows`) carles colonnes de fourier sont une liste (et ne sont donc pas nommées individuellement). 

```{r fourier_terms_pred}


## définir les termes de fourier (sincos) 
counts <- counts %>% 
  mutate(
    ## combiner les termes de fourier pour les semaines avant et après la date limite de 2010
    ## (nb. les termes de fourier de 2011 sont prédits)
    fourier = rbind(
      ## obtenir les termes de fourier pour les années précédentes
      fourier(
        ## garder uniquement les lignes avant 2011
        filter(counts, 
               epiweek <= cut_off), 
        ## inclure un ensemble de termes sin cos 
        K = 1
        ), 
      ## prédire les termes de fourier pour 2011 (en utilisant les données de base)
      fourier(
        ## garder uniquement les lignes avant 2011
        filter(counts, 
               epiweek <= cut_off),
        ## inclure un ensemble de termes sin cos 
        K = 1, 
        ## prédire 52 semaines à l'avance
        h = num_weeks
        )
      )
    )

```

<!-- ======================================================= -->
#### Diviser les données et ajuster la régression {.unnumbered}

Nous devons maintenant diviser notre ensemble de données en deux périodes : la période de base et la période de prédiction. Ceci est fait en utilisant la fonction **dplyr** `group_split()` après `group_by()`, et créera une liste avec deux cadres de données, un pour la période avant la coupure et un pour la période après la coupure. 

Nous utilisons ensuite la fonction `pluck()` du paquet **purrr** pour extraire les ensembles de données de la liste (ce qui équivaut à utiliser des crochets, par exemple `dat[[1]]`), et nous pouvons alors ajuster notre modèle aux données de base, et ensuite utiliser la fonction `predict()` pour nos données d'intérêt après la coupure.  

Consultez la page sur [l'itération, les boucles et les listes](#iteration) pour en savoir plus sur **purrr**.  

<span style="color : orange ;">**_ATTENTION:_** Notez l'utilisation de `simulate_pi = FALSE` dans l'argument `predict()`. Ceci est dû au fait que le comportement par défaut de **trending** est d'utiliser le paquet **ciTools** pour estimer un intervalle de prédiction. Cela ne fonctionne pas s'il y a des comptes `NA`, et produit également des intervalles plus granulaires. Voir `?trending::predict.trending_model_fit` pour plus de détails. </span>  

```{r forecast_regression, warning = FALSE}
# diviser les données pour l'ajustement et la prédiction
dat <- counts %>% 
  group_by(epiweek <= cut_off) %>%
  group_split()

## définir le modèle que vous voulez ajuster (binomial négatif) 
model <- glm_nb_model(
  ## définir le nombre de cas comme résultat d'intérêt
  case_int ~
    ## utiliser epiweek pour tenir compte de la tendance
    epiweek +
    ## utiliser les termes de fourier pour tenir compte de la saisonnalité
    fourier
)

# définir les données à utiliser pour l'ajustement et celles pour la prédiction.
fitting_data <- pluck(dat, 2)
pred_data <- pluck(dat, 1) %>% 
  select(case_int, epiweek, fourier)

# ajustement du modèle 
fitted_model <- trending::fit(model, data.frame(fitting_data))

# obtenir le confint et les estimations pour les données ajustées
observed <- fitted_model %>% 
  predict(simulate_pi = FALSE)

# prévoir avec les données que l'on veut prévoir avec 
forecast <- fitted_model %>% 
  predict(data.frame(pred_data), simulate_pi = FALSE)

## combiner les ensembles de données de base et prédits
observed <- bind_rows(observed$result, forecast$result)

```

Comme précédemment, nous pouvons visualiser notre modèle avec **ggplot**. Nous mettons en évidence les alertes avecpoints rouges pour les comptes observés au-dessus de l'intervalle de prédiction de 95 %. Cette fois, nous ajoutons également une ligne verticale pour indiquer quand la prévision commence. 

```{r forecast_plot}

## Tracez votre régression 
ggplot(data = observed, aes(x = epiweek)) + 
  ## ajoutez une ligne pour l'estimation du modèle
  geom_line(aes(y = estimate),
            col = "grey") + 
  ## ajouter une bande pour les intervalles de prédiction 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## ajouter une ligne pour le nombre de cas observés
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## Tracez des points pour les nombres observés supérieurs aux prévisions.
  geom_point(
    data = filter(observed, case_int > upper_pi), 
    aes(y = case_int), 
    color = "red", 
    size = 2) + 
  ## ajouter une ligne verticale et une étiquette pour montrer où la prévision a commencé
  geom_vline(
           xintercept = as.Date(cut_off), 
           linetype = "dashed") + 
  annotate(geom = "text", 
           label = "Forecast", 
           x = cut_off, 
           y = max(observed$upper_pi) - 250, 
           angle = 90, 
           vjust = 1
           ) + 
  ## faire un graphique traditionnel (avec des axes noirs et un fond blanc)
  theme_classic()
```



<!-- ======================================================= -->
#### Validation de la prédiction {.unnumbered}

Au-delà de l'inspection des résidus, il est important d'étudier la capacité de votre modèle à prédire les cas dans le futur. Cela vous donne une idée de la fiabilité de vos 
seuils d'alerte.  

La méthode traditionnelle de validation consiste à voir dans quelle mesure vous pouvez prédire l'année la plus récente avant l'année en cours (parce que vous ne pouvez pas prédire l'année en cours).Par exemple, dans notre ensemble de données, nous utiliserions les données de 2002 à 2009 pour prédire 2010, et ensuite voir si ces prédictions sont exactes. Ensuite, nous réajustons le modèle pour incluredonnées de 2010 et les utiliser pour prédire les comptages de 2011.  

Comme on peut le voir dans la figure ci-dessous, réalisée par *Hyndman et al* dans ["Forecasting principles and practice"](https://otexts.com/fpp3/). 

![](`r "https://otexts.com/fpp3/fpp_files/figure-html/traintest-1.png"`)*figure reproduite avec l'autorisation des auteurs*. 

L'inconvénient de cette méthode est que vous n'utilisez pas toutes les données dont vous disposez et que vous n'obtenez pas le modèle final que vous utilisez pour la prédiction. 

Une alternative consiste à utiliser une méthode appelée validation croisée. Dans ce scénario, vous passez en revue toutes les données disponibles pour ajuster plusieurs modèles afin de prédire un an à l'avance. Vous utilisez de plus en plus de données dans chaque modèle, comme le montre la figure ci-dessous tirée de la même [*Hyndman et al* texte]((https://otexts.com/fpp3/). Par exemple, le premier modèle utilise 2002 pour prédire 2003, le second utilise 2002 et 2003 pour prédire 2004, et ainsi de suite. 
![](`r "https://otexts.com/fpp2/fpp_files/figure-html/cv1-1.png"`)*figure reproduite avec l'autorisation des auteurs*.

Dans la figure ci-dessous, nous utilisons la fonction `map()` du paquet **purrr** pour boucler sur chaque ensemble de données. Nous mettons ensuite les estimations dans un seul ensemble de données et les fusionnons avec le nombre de cas original, pour utiliser le paquet **yardstick** pour calculer les mesures de précision. Nous calculons quatre mesures, notamment Erreur quadratique moyenne (RMSE), Erreur absolue moyenne (MAE) l'erreur absolue moyenne mise à l'échelle (MASE), l'erreur absolue moyenne en pourcentage (MAPE).

<span style="color : orange ;">**_CAUTION:_** Notez l'utilisation de `simulate_pi = FALSE` dans l'argument `predict()`. Ceci est dû au fait que le comportement par défaut de **trending** est d'utiliser le paquet **ciTools** pour estimer un intervalle de prédiction. Cela ne fonctionne pas s'il y a des comptes `NA`, et produit également des intervalles plus granulaires. Voir `?trending::predict.trending_model_fit` pour plus de détails. </span>  

```{r cross_validation, warning = FALSE}

## Validation croisée : prédire la ou les semaines à venir en fonction de la fenêtre glissante.

## élargissez vos données en les faisant glisser dans des fenêtres de 52 semaines (avant + après). 
## pour prédire les 52 semaines à venir
## (crée des chaînes d'observations de plus en plus longues - conserve les données plus anciennes)

## définir la fenêtre que l'on veut faire glisser
roll_window <- 52

## Définir les semaines à venir à prévoir 
weeks_ahead <- 52

## créer un ensemble de données répétitives, de plus en plus longues.
## étiqueter chaque ensemble de données avec un identifiant unique.
## utiliser seulement les cas avant l'année d'intérêt (i.e. 2011)
case_roll <- counts %>% 
  filter(epiweek < cut_off) %>% 
  ## Garder uniquement les variables de la semaine et du nombre de cas.
  select(epiweek, case_int) %>% 
    ## laisser tomber les x dernières observations 
    ## en fonction du nombre de semaines d'anticipation de la prévision. 
    ## (sinon ce sera une prévision réelle à "inconnu")
    slice(1 :(n() - weeks_ahead)) %>%
    as_tsibble(index = epiweek) %>% 
    ## reconduire chaque semaine dans x après les fenêtres pour créer l'ID de regroupement 
    ## en fonction de la fenêtre de roulement spécifiée
    stretch_tsibble(.init = roll_window, .step = 1) %>% 
  ## laisser tomber les deux premiers - car il n'y a pas de cas "avant".
  filter(.id > roll_window)


## pour chacun des ensembles de données uniques, exécutez le code ci-dessous
forecasts <- purrr::map(unique(case_roll$.id), 
                        function(i) {
  
  ## garder uniquement le pli courant en cours d'ajustement 
  mini_data <- filter(case_roll, .id == i) %>% 
    as_tibble()
  
  ## créer un ensemble de données vides pour la prévision sur 
  forecast_data <- tibble(
    epiweek = seq(max(mini_data$epiweek) + 1,
                  max(mini_data$epiweek) + weeks_ahead,
                  by = 1),
    case_int = rep.int(NA, weeks_ahead),
    .id = rep.int(i, weeks_ahead)
  )
  
  ## ajouter les données de prévision à l'original 
  mini_data <- bind_rows(mini_data, forecast_data)
  
  ## définir le cut off basé sur les dernières données de comptage non manquantes. 
  cv_cut_off <- mini_data %>% 
    ## ne garder que les lignes non manquantes
    drop_na(case_int) %>% 
    ## obtenir la dernière semaine
    summarise(max(epiweek)) %>% 
    ## extraire ce qui n'est pas dans un dataframe
    pull()
  
  ## Remettre mini_data dans un tsibble
  mini_data <- tsibble(mini_data, index = epiweek)
  
  ## définir les termes de fourier (sincos) 
  mini_data <- mini_data %>% 
    mutate(
    ## Combinez les termes de Fourier pour les semaines avant et après la date limite.
    fourier = rbind(
      ## obtenir les termes de fourier pour les années précédentes
      forecast::fourier(
        ### ne conserve que les lignes avant la date butoir
        filter(mini_data, 
               epiweek <= cv_cut_off), 
        ## inclure un ensemble de termes sin cos 
        K = 1
        ), 
      ## prédire les termes de fourier pour l'année suivante (en utilisant les données de base)
      fourier(
        ## conserver uniquement les lignes avant la coupure
        filter(mini_data, 
               epiweek <= cv_cut_off),
        ## inclure un ensemble de termes sin cos 
        K = 1, 
        ## prédire 52 semaines à l'avance
        h = weeks_ahead
        )
      )
    )
  
  
  # diviser les données pour l'ajustement et la prédiction
  dat <- mini_data %>% 
    group_by(epiweek <= cv_cut_off) %>%
    group_split()

  ## définir le modèle que vous voulez ajuster (binomiale négative) 
  model <- glm_nb_model(
    ## définir le nombre de cas comme résultat d'intérêt
    case_int ~
      ## utiliser epiweek pour tenir compte de la tendance
      epiweek +
      ## utiliser les termes de fourier pour tenir compte de la saisonnalité
      fourier
  )

  # définir les données à utiliser pour l'ajustement et celles pour la prédiction.
  fitting_data <- pluck(dat, 2)
  pred_data <- pluck(dat, 1)
  
  # ajuster le modèle 
  fitted_model <- trending::fit(model, data.frame(fitting_data))
  
  # prévoir avec les données que l'on veut prédire avec 
  forecasts <- fitted_model %>% 
    predict(data.frame(pred_data), simulate_pi = FALSE) 
 forecasts <- data.frame(forecasts$result[[1]]) %>% 
      ## garder seulement la semaine et l'estimation de la prévision
    select(epiweek, estimate)
    
  }
  )

## Transformer la liste en un cadre de données avec toutes les prévisions.
forecasts <- bind_rows(forecasts)

## joindre les prévisions aux données observées
forecasts <- left_join(forecasts, 
                       select(counts, epiweek, case_int),
                       by = "epiweek")

## en utilisant {yardstick} calculer les métriques
  ## RMSE : Root mean squared error (erreur quadratique moyenne)
  ## MAE : Erreur absolue moyenne	
  ## MASE : Mean absolute scaled error (erreur absolue moyenne mise à l'échelle)
  ## MAPE : Erreur absolue moyenne en pourcentage
model_metrics <- bind_rows(
  ## dans votre ensemble de données forcées, comparez les données observées aux données prédites.
  rmse(forecasts, case_int, estimate), 
  mae( forecasts, case_int, estimate),
  mase(forecasts, case_int, estimate),
  mape(forecasts, case_int, estimate),
  ) %>% 
  ### ne conserve que le type de métrique et sa sortie
  select(Metric = .metric, 
         Measure = .estimate) %>% 
  ## faire en sorte que le format soit large pour pouvoir lier les lignes ensuite
  pivot_wider(names_from = Metric, values_from = Measure)

## Retourner les métriques du modèle 
model_metrics

```


<!-- ======================================================= -->
### **surveillance** paquet {.unnumbered}

Dans cette section, nous utilisons le paquet **surveillance** pour créer des seuils d'alerte basés sur des algorithmes de détection d'épidémies. Il existe plusieurs méthodes différentes disponibles dans le paquet, mais nous nous concentrerons ici sur deux options. Pour plus de détails, voir ces articles sur [l'application](https://cran.r-project.org/web/packages/surveillance/vignettes/monitoringCounts.pdf) et [théorie](https://cran.r-project.org/web/packages/surveillance/vignettes/glrnb.pdf)
des alogirths utilisés. 

La première option utilise la méthode améliorée de Farrington. Celle-ci ajuste un glm binomial  négatif (y compris la tendance) et pondère à la baisse les épidémies passées (valeurs aberrantes) pour créer un niveau de seuil. 

La deuxième option utilise la méthode glrnb. Celle-ci ajuste également un glm binomial négatif 
binomiale négative, mais inclut la tendance et les termes de Fourier (elle est donc privilégiée ici). La régression est utilisée pour calculer la "moyenne de contrôle" (~valeurs ajustées) - elle utilise ensuite une statistique de rapport de vrai semblance généralisé calculé pour évaluer s'il y a un changement de la moyenne pour chaque semaine. Notez que le seuil pour chaque semaine prend en compte les semaines précédentes, de sorte que s'il y a un changement soutenu, une alarme sera déclenchée. (Notez également qu'après chaque alarme, l'algorithme est réinitialisé).

Afin de travailler avec le paquet **surveillance**, nous devons d'abord définir un objet "série temporelle de surveillance" (en utilisant la fonction `sts()`) pour s'intégrer dans le cadre. 

```{r surveillance_obj}

## Définir un objet de série temporelle de surveillance
## nb. vous pouvez inclure un dénominateur avec l'objet population (voir ?sts)
counts_sts <- sts(observed = counts$case_int[!is.na(counts$case_int)],
                  start = c(
                    ## sous-ensemble pour ne garder que l'année de start_date. 
                    as.numeric(str_sub(start_date, 1, 4)), 
                    ## sous-ensemble pour ne conserver que la semaine à partir de la date de départ
                    as.numeric(str_sub(start_date, 7, 8))), 
                  ## définir le type de données (dans ce cas, hebdomadaire)
                  freq = 52)

## définir la plage de semaines que vous voulez inclure (c'est-à-dire la période de prédiction)
## nb. l'objet sts ne compte que les observations sans leur attribuer un identifiant de semaine ou d'année. 
## d'année - nous utilisons donc nos données pour définir les observations appropriées.
weekrange <- cut_off - start_date

```

<!-- ======================================================= -->
#### Méthode Farrington {.unnumbered}

Nous définissons ensuite chacun de nos paramètres pour la méthode de Farrington dans une `liste`. 
Ensuite, nous exécutons l'algorithme en utilisant `farringtonFlexible()` et ensuite nous pouvons extraire le seuil d'une alerte en utilisant `farringtonmethod@upperbound` pour l'inclure dans notre données. Il est également possible d'extraire un VRAI/FAUX pour chaque semaine si elle a déclenché une alerte (au-dessus du seuil) en utilisant `farringtonmethod@alarm`. 

```{r farrington}

## définir le contrôle
ctrl <- list(
  ## définissez la période pour laquelle vous voulez un seuil (i.e. 2011)
  range = which(counts_sts@epoch > weekrange),
  b = 9, ## nombre d'années en arrière pour la ligne de base
  w = 2, ## taille de la fenêtre de roulement en semaines
  weightsThreshold = 2.58, ## repondération des épidémies passées (méthode noufaily améliorée - l'originale suggère 1)
  ## pastWeeksNotIncluded = 3, ## utilisation de toutes les semaines disponibles (noufaily suggère d'en éliminer 26)
  trend = TRUE,
  pThresholdTrend = 1, ## 0.05 normalement, mais 1 est conseillé dans la méthode améliorée (c'est-à-dire toujours garder)
  thresholdMethod = "nbPlugin",
  populationOffset = TRUE
  )

## appliquer la méthode flexible de Farrington
farringtonmethod <- farringtonFlexible(counts_sts, ctrl)

## créer une nouvelle variable dans le jeu de données original appelée threshold.
## contenant la limite supérieure de Farrington. 
## nb. ceci est seulement pour les semaines de 2011 (donc besoin de sous-ensembler les lignes)
counts[which(counts$epiweek >= cut_off & 
               !is.na(counts$case_int)),
              "threshold"] <- farringtonmethod@upperbound
```

Nous pouvons ensuite visualiser les résultats dans ggplot comme nous l'avons fait précédemment. 

```{r plot_farrington, warning=F, message=F}

ggplot(counts, aes(x = epiweek)) + 
  ## ajouter le nombre de cas observés sous forme de ligne
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## ajout de la limite supérieure de l'algorithme d'aberration
  geom_line(aes(y = threshold, colour = "Alert threshold"), 
            linetype = "dashed", 
            size = 1.5) +
  ## définir les couleurs
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Alert threshold" = "red")) + 
  ## faire un graphique traditionnel (avec des axes noirs et un fond blanc)
  theme_classic() + 
  ## supprimer le titre de la légende 
  theme(legend.title = element_blank())

```

<!-- ======================================================= -->
#### Méthode GLRNB {.unnumbered}

De même pour la méthode GLRNB, nous définissons chacun de nos paramètres pour le dans une `liste`, puis nous ajustons l'algorithme et extrayons les limites supérieures.

<span style="color : orange ;">**_ATTENTION:_** Cette méthode utilise la "force brute" (similaire au bootstrapping) pour calculer les seuils, donc peut prendre beaucoup de temps!</span>.

Voir la [vignette GLRNB](https://cran.r-project.org/web/packages/surveillance/vignettes/glrnb.pdf) pour plus de détails. 

```{r glrnb, warning = FALSE, message = FALSE}

## définir les options de contrôle
ctrl <- list(
  ## définir la période pour laquelle on veut un seuil (i.e. 2011)
  range = which(counts_sts@epoch > weekrange),
  mu0 = list(S = 1, ## nombre de termes de fourier (harmoniques) à inclure
  trend = TRUE, ## inclusion ou non de la tendance
  refit = FALSE), ## si l'on refit le modèle après chaque alarme
  ## cARL = seuil pour la statistique GLR (arbitraire)
     ## 3 ~ seuil intermédiaire pour minimiser les faux positifs
     ## 1 s'ajuste aux 99%PI de glm.nb - avec des changements après les pics (seuil abaissé pour l'alerte)
   c.ARL = 2,
   # thêta = log(1.5), ## équivaut à une augmentation de 50% des cas dans une épidémie
   ret = "cases" ## retourne la limite supérieure du seuil sous forme de nombre de cas
  )

## appliquer la méthode glrnb
glrnbmethod <- glrnb(counts_sts, control = ctrl, verbose = FALSE)

## créer une nouvelle variable dans l'ensemble de données original appelée threshold
## contenant la limite supérieure de glrnb. 
## nb. ceci est seulement pour les semaines de 2011 (donc besoin de sous-ensembler les lignes)
counts[which(counts$epiweek >= cut_off & 
               !is.na(counts$case_int)),
              "threshold_glrnb"] <- glrnbmethod@upperbound

```

Visualisez les sorties comme précédemment. 

```{r plot_glrnb, message=F, warning=F}

ggplot(counts, aes(x = epiweek)) + 
  ## ajouter le nombre de cas observés sous forme de ligne
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## ajout de la limite supérieure de l'algorithme d'aberration
  geom_line(aes(y = threshold_glrnb, color = "Alert threshold"), 
            linetype = "dashed", 
            size = 1.5) +
  ## définir les couleurs
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Alert threshold" = "red")) + 
  ## faire un graphique traditionnel (avec des axes noirs et un fond blanc)
  theme_classic() + 
  ## supprimer le titre de la légende 
  theme(legend.title = element_blank())

```

<!-- ======================================================= -->
## Série chronologique interrompue { }

Les séries chronologiques interrompues (également appelées régression segmentée ou analyse d'intervention), est souvent utilisée pour évaluer l'impact des vaccins sur l'incidence des maladies. Mais elle peut être utilisée pour évaluer l'impact d'un large éventail d'interventions ou d'introductions. Par exemple, des changements dans les procédures hospitalières ou l'introduction d'une nouvelle souche de maladie dans une population.  Dans cet exemple, nous supposerons qu'une nouvelle souche de Campylobacter a été introduite en Allemagne fin 2008, et nous verrons si cela affecte le nombre de cas. Nous utiliserons à nouveau la régression binomiale négative. Cette fois-ci, la régression sera divisée en deux parties, l'une avant l'intervention (ou l'introduction de la nouvelle souche ici) et une autre après (les périodes pré et post). Cela nous permet de calculer un ratio de taux d'incidence comparant les deux périodes. Une explication de l'équation pourrait rendre les choses plus claires (sinon, ignorez-la !).

La régression binomiale négative peut être définie comme suit : 

$$\log(Y_t)= Î²_0 + Î²_1 \times t+ Î²_2 \times Î´(t-t_0) + Î²_3\times(t-t_0 )^+ + log(pop_t) + e_t$$

Où :
$Y_t$ est le nombre de cas observés au temps $t$.  
$pop_t$ est la taille de la population en 100 000 au moment $t$ (non utilisé ici)  
$t_0$ est la dernière année de la pré-période (y compris la période de transition, le cas échéant).  
$Î'(x$ est la fonction indicatrice (elle vaut 0 si xâ¤0 et 1 si x>0)  
$(x)^+$ est l'opérateur de coupure (il vaut x si x>0 et 0 sinon)  
$e_t$ désigne le résidu 
Des termes supplémentaires, tendance et saison, peuvent être ajoutés si nécessaire. 

$Î²_2 \times Î'(t-t_0) + Î²_3\times(t-t_0 )^+$ est la partie linéaire généralisée de la post-période et est nulle dans la pré-période. Cela signifie que les estimations de $Î²_2$ et $Î²_3$ sont les effets de l'intervention. 

Nous devons recalculer les termes de fourier sans faire de prévision ici, car nous utiliserons toutes les données dont nous disposons (c'est-à-dire rétrospectivement). De plus, nous devons calculerles termes supplémentaires nécessaires à la régression. 

```{r define_terms_interrupted}

## ajouter les termes de fourier en utilisant les variabless epiweek et case_int
counts$fourier <- select(counts, epiweek, case_int) %>% 
  as_tsibble(index = epiweek) %>% 
  fourier(K = 1)

## définir la semaine d'intervention 
intervention_week <- yearweek("2008-12-31")

## définir les variables pour la régression 
counts <- counts %>% 
  mutate(
    ## correspond à t dans la formule
      ## nombre de semaines (on pourrait probablement aussi utiliser la variable epiweeks)
    # linear = row_number(epiweek), 
    ## correspond au delta(t-t0) dans la formule
      ## période de pré ou post intervention
    intervention = as.numeric(epiweek >= intervention_week), 
    ## correspond à (t-t0)^+ dans la formule
      ## nombre de semaines après l'intervention
      ## (choisir le plus grand nombre entre 0 et ce qui ressort du calcul)
    time_post = pmax(0, epiweek - intervention_week + 1))

```

Nous utilisons ensuite ces termes pour ajuster une régression binomiale négative, et produisons un tableau avec le pourcentage de changement. Cet exemple montre qu'il n'y a pas eu de changement significatif. 

<span style="color : orange ;">**_ATTENTION:_** Notez l'utilisation de `simulate_pi = FALSE` dans l'argument `predict()`. Ceci est dû au fait que le comportement par défaut de **trending** est d'utiliser le paquet **ciTools** pour estimer un intervalle de prédiction. Cela ne fonctionne pas s'il y a des comptes `NA`, et produit également des intervalles plus granulaires. Voir `?trending::predict.trending_model_fit` pour plus de détails. </span>  

```{r interrupted_regression, warning = FALSE}


## définissez le modèle que vous voulez ajuster (binomial négatif). 
model <- glm_nb_model(
  ## définir le nombre de cas comme résultat d'intérêt
  case_int ~
    ## utiliser epiweek pour tenir compte de la tendance
    epiweek +
    ## utiliser les termes fourier pour tenir compte de la saisonnalité
    fourier + 
    ## ajouter si dans la pré ou post-période 
    intervention + 
    ## ajouter le temps après l'intervention 
    time_post
    )

## ajustez votre modèle en utilisant l'ensemble de données de comptage
fitted_model <- trending::fit(model, data.frame(counts))

### calculer les intervalles de confiance et les intervalles de prédiction 
observed <- predict(fitted_model, simulate_pi = FALSE)


```


```{r table_regression, eval = FALSE, echo = TRUE}
## Afficher les estimations et le pourcentage de changement dans un tableau
fitted_model %>% 
  ## extraire la régression binomiale négative originale
  get_fitted_model() %>% 
  ## obtenir un cadre de données ordonné des résultats
  tidy(exponentiate = TRUE, 
       conf.int = TRUE) %>% ### ne conserve que les valeurs d'intervention 
  ### ne conserve que la valeur d'intervention 
  filter(term == "intervention") %>% 
  ## changer le IRR en pourcentage de changement pour l'estimation et les ICs 
  mutate(
    ## pour chacune des colonnes d'intérêt - créer une nouvelle colonne
    across(
      all_of(c("estimate", "conf.low", "conf.high")), 
      ## appliquer la formule pour calculer le pourcentage de changement
            .f = function(i) 100 * (i - 1), 
      ## ajouter un suffixe aux nouveaux noms de colonnes avec "_perc".
      .names = "{.col}_perc")
    ) %>% 
  ## ne garder (et renommer) que certaines colonnes 
  select( "IRR" = estimate, 
         "95%CI low" = conf.low, 
         "95%CI high" = conf.high,
         "Variation in percentag" = estimate_perc, 
         "95%CI low (perc)" = conf.low_perc, 
         "95%CI high (perc)" = conf.high_perc,
         "p-value" = p.value)
```

Comme précédemment, nous pouvons visualiser les résultats de la régression. 

```{r plot_interrupted}

estimate_res <- data.frame(observed$result)

ggplot(estimate_res, aes(x = epiweek)) + 
  ## ajouter le nombre de cas observés sous forme de ligne
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## ajout d'une ligne pour l'estimation du modèle
  geom_line(aes(y = estimate, col = "Estimate")) + 
  ## ajouter une bande pour les intervalles de prédiction 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## ajouter une ligne verticale et une étiquette pour montrer où la prévision a commencé
  geom_vline(
           xintercept = as.Date(intervention_week), 
           linetype = "dashed") + 
  annotate(geom = "text", 
           label = "Intervention", 
           x = intervention_week, 
           y = max(estimate_res$upper_pi), 
           angle = 90, 
           vjust = 1
           ) + 
  ## définir les couleurs
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Estimate" = "red")) + 
  ## faire un graphique traditionnel (avec des axes noirs et un fond blanc)
  theme_classic()

```


<!-- ======================================================= -->
## Ressources { }

[Prévision : principes et pratique - manuel](https://otexts.com/fpp3/)  
[Études de cas d'analyse de séries temporelles EPIET](https://github.com/EPIET/TimeSeriesAnalysis)  
[Cours de Penn State](https://online.stat.psu.edu/stat510/lesson/1) 
[Manuscrit du paquet de surveillance](https://www.jstatsoft.org/article/view/v070i10)

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/time_series.Rmd-->


#Modélisation des épidémies {#epidemic_models}  


<!-- ======================================================= -->
## Overview { }

Il existe un nombre croissant d'outils pour la modélisation des épidémies qui nous permettent de mener des analyses assez complexes avec un effort minimal.Cette section fournira une
aperçu sur la façon d'utiliser ces outils pour :

* estimer le nombre de reproduction effectif R<sub>t</sub> et les statistiques connexes.
  telles que le temps de doublement
* produire des projections à court terme de l'incidence future.

Il ne s'agit *pas* d'un aperçu des méthodologies et des méthodes statistiques qui sous-tendent ces outils. Veuillez donc vous référer à l'onglet Ressources pour des liens vers des
documents traitant de ce sujet. Assurez-vous d'avoir une bonne compréhension des
les méthodes avant d'utiliser ces outils ; cela vous permettra d'interpréter correctement
leurs résultats.

Voici un exemple de l'un des résultats que nous produirons dans cette section.

```{r out.width=c('100%', '100%'), fig.show='hold', echo=F, fig.width = 12, fig.height = 9, message=F, warning=F}

#if (!require("EpiNow2", quietly = TRUE))
#    remotes::install_github("epiforecasts/EpiNow2")

## installer et charger les paquets
pacman::p_load(tidyverse, EpiNow2, EpiEstim, here, incidence2, epicontacts, rio, projections)




## Chargement de la linelist
linelist <- import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

## générer des contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## générer des epicontacts
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)

## ## estimer le temps de génération gamma
## generation_time <- bootstrapped_dist_fit(
## get_pairwise(epic, "date_infection"),
## dist = "gamma",
## max_value = 20,
## bootstraps = 1
## )

## ## export pour la mise en cache
## export(
## generation_time,
## here("data/cache/epidemic_models/generation_time.rds")
## )

## importer le temps de génération mis en cache
generation_time <- import(here("data/cache/epidemic_models/generation_time.rds"))

## ## estimer la période d'incubation
## incubation_period <- bootstrapped_dist_fit(
## linelist$date_onset - linelist$date_infection,
## dist = "lognormal",
## max_value = 100,
## bootstraps = 1
## )

## ## export pour la mise en cache
## export(
### incubation_period,
## here("data/cache/epidemic_models/incubation_period.rds")
## )

## import période d'incubation en cache
incubation_period <- import(here("data/cache/epidemic_models/incubation_period.rds"))

## obtenir l'incidence à partir de la date d'apparition
cases <- linelist %>%
  group_by(date = date_onset) %>%
  summarise(confirm = n())

 ## exécuter epinow#
## epinow_res <- epinow(
## reported_cases = cases,
## generation_time = generation_times,
## delays = delay_opts(incubation_period),
## target_folder = here("data/cache/epidemic_models"),
## return_output = TRUE,
## output = "samples",
## verbose = TRUE,
## stan = stan_opts(samples = 750, chains = 4),
## horizon = 21
## )

## ## export pour la mise en cache
## export(
## epinow_res,
## here("data/cache/epidemic_models/epinow_res.rds")
## )

## importer les résultats epinow mis en cache
epinow_res <- import(here("data/cache/epidemic_models/epinow_res.rds"))

## tracer la figure de synthèse
plot(epinow_res)

```

<!-- ======================================================= -->
## Préparation { }

Nous allons utiliser deux méthodes et packages différents pour l'estimation R<sub>t</sub>,
à savoir **EpiNow** et **EpiEstim**, ainsi que le package **projections** pour la prévision de l'incidence des cas.  

Ce fragment de code montre le chargement des paquets nécessaires aux analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* le charge pour l'utiliser. Vous pouvez également charger les paquets installés avec `library()` de **base** R. Voir la page sur [R basics](#rbasics) pour plus d'informations sur les paquets R.  

	
```{r epidemic_models_packages, }
pacman::p_load(
   rio, # Importation de fichiers
   here, # Localisation de fichiers
   tidyverse, # Gestion des données + graphiques ggplot2
   epicontacts, # Analyse des réseaux de transmission
   EpiNow2, # Estimation de Rt
   EpiEstim, # Estimation Rt
   projections, # Projections d'incidence
   incidence2, # Traitement des données d'incidence
   epitrix, # Fonctions epi utiles
   distcrete # Distributions discrètes des délais
)
```
	
Nous utiliserons la linelist de cas nettoyée pour toutes les analyses de cette section. Si vous voulez suivre, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la linelist "propre"</a> (en tant que fichier .rds). Consultez la page [Télécharger le manuel et les données](#data_used) pour télécharger tous les exemples de données utilisés dans ce manuel.  

```{r, echo=F}
# Importez la liste de lignes dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r eval=F}
# Importez la liste de cas nettoyée
linelist <- import("linelist_cleaned.rds")
```


<!-- ======================================================= -->
## Estimation de R<sub>t</sub> { }

### EpiNow2 vs. EpiEstim {.unnumbered}

Le taux de reproduction R est une mesure de la transmissibilité d'une maladie, et est défini comme le nombre attendu de cas secondaires par cas infecté. Dans une population totalement sensible, cette valeur représente le nombre de reproduction de base, R<sub>0</sub>. Cependant, comme le nombre d'individus sensibles dans une population évolue au cours d'une épidémie ou d'une pandémie, et que diverses mesures de réponse sont mises en œuvre, la mesure la plus couramment utilisée de la transmissibilité est le taux de reproduction effectif, R<sub>t</sub> ; il est défini défini comme le nombre attendu de cas secondaires par cas infecté à un moment, _t_.

Le paquet **EpiNow2** fournit le cadre le plus sophistiqué pour l'estimation de R<sub>t</sub>. Il présente deux avantages essentiels par rapport à l'autre paquet couramment utilisé, **EpiEstim** :

* Il tient compte des délais de déclaration et peut donc estimer R<sub>t</sub> même lorsque les données récentes sont incomplètes.
* Il estime R<sub>t</sub> sur les _dates d'infection_ plutôt que sur les dates de début de déclaration, ce qui signifie que l'effet d'une intervention sera immédiatement reflété dans un changement de R<sub>t</sub>, plutôt qu'avec un delai.

Cependant, elle présente également deux inconvénients majeurs :

* Elle nécessite la connaissance de la distribution des temps de génération (c'est-à-dire la distribution des délais entre l'infection d'un cas primaire et d'un cas secondaire), la distribution de la période d'incubation (c'est-à-dire la distribution des délais entre l'infection et l'apparition des symptômes) et toute autre distribution de délai pertinente pour vos données (par exemple, si vous  avez des dates de déclaration, vous avez besoin de la distribution des délais entre l'apparition des symptômes et la déclaration, ou la période d'incubation). Bien que cela permette une estimation plus précise de R<sub>t</sub>, **EpiEstim** ne requiert que la distribution de l'intervalle sériel (c'est-à-dire la distribution des délais entre l'apparition des symptômes d'un cas primaire et d'un cas secondaire), qui peut être la seule distribution disponible pour vous.
* **EpiNow2** est significativement plus lent que **EpiEstim**, de manière anecdotique par un facteur de 100 à 1000 ! Par exemple, l'estimation de R<sub>t</sub> pour l'échantillon de foyers considéré dans cette section prend environ quatre heures (ceci a été exécuté pour un grand d'itérations pour garantir une grande précision et pourrait probablement être réduite si nécessaire) mais il n'en reste pas moins que l'algorithme est lent en général. Cela peut être irréalisable si vous mettez régulièrement à jour votre base de données pour R<sub>t</sub>.
  
Le paquet que vous choisirez d'utiliser dépendra donc des données, du temps et des ressources informatiques dont vous disposez.

### EpiNow2 {.unnumbered}

#### Estimation des distributions de retard {.nonnuméroté}

Les distributions de retard requises pour exécuter **EpiNow2** dépendent des données dont vous disposez. Essentiellement, vous devez être en mesure de décrire le délai entre la date d'infection à la date de l'événement que vous voulez utiliser pour estimer R<sub>t</sub>. Si
vous utilisez les dates d'apparition, il s'agit simplement de la distribution de la période d'incubation. Si vous utilisez les dates de déclaration, vous avez besoin du
délai entre l'infection et la déclaration. Comme il est peu probable que cette distribution soit connue directement, **EpiNow2** vous permet d'enchaîner plusieurs distributions de délai ; dans ce cas, le délai entre l'infection et la déclaration est le même.

Comme nous disposons des dates d'apparition des symptômes pour tous nos cas dans la liste d'exemples, nous n'aurons besoin que de la distribution de la période d'incubation pour déterminer le délai d'apparition des symptômes.Nous pouvons soit estimer cette distribution
à partir des données ou utiliser les valeurs de la littérature.

Une estimation de la période d'incubation d'Ebola dans la littérature (tirée de [cet article](https://www.nejm.org/doi/full/10.1056/nejmoa1411100)) avec une moyenne de 9,1, un écart-type de 7,3 et une valeur maximale de 30, serait spécifiée comme suit : 

```{r epidemic_models_incubation_literature, eval=F}
incubation_period_lit <- list(
  mean = log(9.1),
  mean_sd = log(0.1),
  sd = log(7.3),
  sd_sd = log(0.1),
  max = 30
)
```
Notez que **EpiNow2** exige que ces distributions de délais soient fournies sur une échelle **log** d'où l'appel `log` autour de chaque valeur (sauf le paramètre `max` qui doit être fourni sur une échelle naturelle). Les paramètres `mean_sd` et `sd_sd` définissent l'écart type des estimations de la moyenne. Comme ceux-ci ne sont pas connus dans ce cas, nous choisissons la valeur assez arbitraire de 0.1.

Dans cette analyse, nous estimons plutôt la distribution de la période d'incubation à partir de la linelist elle-même en utilisant la fonction `bootstrapped_dist_fit`, ce qui va
une distribution lognormale aux délais observés entre l'infection et l'apparition de la maladie.

```{r epidemic_models_incubation_estimate, eval=F}
## Estimation de la période d'incubation
incubation_period <- bootstrapped_dist_fit(
  linelist$date_onset - linelist$date_infection,
  dist = "lognormal",
  max_value = 100,
  bootstraps = 1
)
```

L'autre distribution dont nous avons besoin est le temps de génération. Comme nous avons des données sur les temps d'infection __et__ les liens de transmission, nous pouvons estimer cette
distribution à partir de la liste de liens en calculant le délai entre les temps d'infection
des paires infecteur-infecte. Pour ce faire, nous utilisons la fonction pratique `get_pairwise` du paquet **epicontacts**, qui nous permet de calculer les différences par paire des propriétés de la linelist entre les paires de transmission. Nous créons d'abord un objet epicontacts (voir la page [Chaînes de transmission](#transmission_chains) pour plus de détails) :

```{r epidemic_models_epicontacts, eval=F}
## générer des contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## générer un objet epicontacts
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)
```

Nous ajustons ensuite la différence de temps d'infection entre les paires de transmission,
calculée en utilisant `get_pairwise`, à une distribution gamma :

```{r epidemic_models_generation_estimate, eval=F}
## estimation du temps de génération gamma
generation_time <- bootstrapped_dist_fit(
  get_pairwise(epic, "date_infection"),
  dist = "gamma",
  max_value = 20,
  bootstraps = 1
)
```

#### Exécution de **EpiNow2** {.unnumbered}

Maintenant, il ne nous reste plus qu'à calculer l'incidence journalière à partir de la liste linéaire, ce que nous pouvons faire facilement avec les fonctions **dplyr** `group_by()` et `n()`. Notez que **EpiNow2** exige que les noms des colonnes soient `date` et `confirm`.

```{r epidemic_models_cases, eval=F}
## Obtenir l'incidence à partir des dates d'apparition
cases <- linelist %>%
  group_by(date = date_onset) %>%
  summarise(confirm = n())
```

Nous pouvons ensuite estimer R<sub>t</sub> en utilisant la fonction `epinow`. Quelques remarques sur les entrées :

* Nous pouvons fournir n'importe quel nombre de distributions de délais "enchaînés" à l'argument `delays`.
  Nous les insérons simplement à côté de l'objet `incubation_period` dans la fonction `delay_opts`.
* `return_output` permet de s'assurer que la sortie est retournée dans R et pas seulement
  un fichier.
* `verbose` spécifie que nous voulons une lecture de la progression.
* `horizon` indique pour combien de jours nous voulons projeter l'incidence future.
* Nous passons des options supplémentaires à l'argument `stan` pour spécifier combien de temps
  nous voulons exécuter l'inférence pour. L'augmentation de `samples` et de `chains` vous donnera une estimation plus précise qui caractérisera mieux l'incertitude. 
  
Cependant, l'exécution sera plus longue.

```{r epidemic_models_run_epinow, eval=F}
## exécuter epinow
epinow_res <- epinow(
  reported_cases = cases,
  generation_time = generation_time,
  delays = delay_opts(incubation_period),
  return_output = TRUE,
  verbose = TRUE,
  horizon = 21,
  stan = stan_opts(samples = 750, chains = 4)
)
```

#### Analyser les sorties {.unnumbered}

Une fois l'exécution du code terminée, nous pouvons tracer un résumé très facilement comme suit. Faites défiler l'image pour voir l'étendue complète.  


```{r out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F }
## Tracer la figure récapitulative
plot(epinow_res)
```

Nous pouvons également examiner diverses statistiques sommaires :

```{r epidemic_models_epinow_summary,}
## tableau récapitulatif
epinow_res$summary
```

Pour des analyses plus approfondies et des tracés personnalisés, vous pouvez accéder aux estimations quotidiennes résumées via `$estimates$summarised`. Nous allons convertir le tableau par défaut `data.table` en un `tibble` pour faciliter l'utilisation avec **dplyr**.

```{r epidemic_models_to_tibble, eval=F}
## extraire le résumé et le convertir en tibble
estimates <- as_tibble(epinow_res$estimates$summarised)
estimates
```

```{r epidemic_models_tibble_show, echo = F}
## montrer les résultats
estimates <- as_tibble(epinow_res$estimates$summarised)
DT::datatable(
  estimates,
  rownames = FALSE,
  filter = "top",
  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap'
)
```

A titre d'exemple, faisons un graphique du temps de doublement et de R<sub>t</sub>. Nous n'examinerons que les premiers mois de l'épidémie, lorsque R<sub>t</sub> est largement
supérieur à un, pour éviter de tracer des temps de doublement extrêmement élevés.

Nous utilisons la formule `log(2)/taux de croissance` pour calculer le temps de doublement à partir du taux de croissance estimé.

```{r epidemic_models_plot_epinow_cusotom, out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}

## faire des df larges pour le tracé de la médiane
df_wide <- estimates %>%
  filter(
    variable %in% c("growth_rate", "R"),
    date < as.Date("2014-09-01")
  ) %>%
  ## convertir les taux de croissance en temps de doublement
  mutate(
    across(
      c(median, lower_90:upper_90),
      ~ case_when(
        variable == "growth_rate" ~ log(2)/.x,
        TRUE ~ .x
      )
    ),
    ## renommer la variable pour refléter la transformation
    variable = replace(variable, variable == "growth_rate", "doubling_time")
  )

## créer des df longs pour le tracé des quantiles
df_long <- df_wide %>%
  ## ici, nous faisons correspondre les quantiles (par exemple, lower_90 à upper_90)
  pivot_longer(
    lower_90:upper_90,
    names_to = c(".value", "quantile"),
    names_pattern = "(.+)_(.+)"
  )

## créer un graphique
ggplot() +
  geom_ribbon(
    data = df_long,
    aes(x = date, ymin = lower, ymax = upper, alpha = quantile),
    color = NA
  ) +
  geom_line(
    data = df_wide,
    aes(x = date, y = median)
  ) +
  ## utiliser label_parsed pour permettre l'utilisation d'une étiquette en indice
  facet_wrap(
    ~ variable,
    ncol = 1,
    scales = "free_y",
    labeller = as_labeller(c(R = "R[t]", doubling_time = "Doubling~time"), label_parsed),
    strip.position = 'left'
  ) +
  ## définir manuellement la transparence des quantiles
  scale_alpha_manual(
    values = c(`20` = 0.7, `50` = 0.4, `90` = 0.2),
    labels = function(x) paste0(x, "%")
  ) +
  labs(
    x = NULL,
    y = NULL,
    alpha = "Credible\ninterval"
  ) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %d\n%Y"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    strip.background = element_blank(),
    strip.placement = 'outside'
  )

```

<!-- ======================================================= -->
### EpiEstim {.unnumbered}

Pour exécuter **EpiEstim**, nous devons fournir des données sur l'incidence journalière et spécifier l'intervalle sériel (c'est-à-dire la distribution des délais entre l'apparition des symptômes des cas primaires et secondaires). 

Les données d'incidence peuvent être fournies à **EpiEstim** sous la forme d'un vecteur, d'un cadre de données ou d'un objet `incidence` provenant du paquetage original **incidence**. Vous pouvez même faire la distinction entre les importations et les infections acquises localement ; voir la documentation de `?estimate_R` pour plus de détails.  

Nous allons créer l'entrée en utilisant **incidence2**. Voir la page sur [Epidemic curves](#epicurves) pour plus d'exemples avec le paquet **incidence2**. Comme il y a eu des mises à jour du paquet **incidence2** qui ne correspondent pas complètement à l'entrée attendue de `estimateR()`, quelques étapes supplémentaires mineures sont nécessaires. L'objet incidence consiste en un tibble avec des dates et leurs nombres de cas respectifs. Nous utilisons `complete()` de **tidyr** pour nous assurer que toutes les dates sont incluses (même celles sans cas), puis nous `rename()` les colonnes pour les aligner avec ce qui est attendu par `estimate_R()` dans une étape ultérieure.  

```{r epidemic_models_epiestim_incidence,}
## Obtenir l'incidence à partir de la date d'apparition
cases <- incidence2::incidence(linelist, date_index = "date_onset") %>% # obtient le nombre de cas par jour
  tidyr::complete(date_index = seq.Date( # s'assurer que toutes les dates sont représentées
    from = min(date_index, na.rm = T),
    to = max(date_index, na.rm=T),
    by = "day"),
    fill = list(count = 0)) %>% # convertit les comptes NA en 0
  rename(I = count, # renomme aux noms attendus par estimateR
         dates = date_index)
```

Le paquetage fournit plusieurs options pour spécifier l'intervalle sériel, dont les détails sont fournis dans la documentation de `?estimate_R`. 

#### Utiliser des estimations d'intervalles sériels issues de la littérature {.unnumbered}

En utilisant l'option `method = "parametric_si"`, nous pouvons spécifier manuellement la moyenne et l'écart type de l'intervalle sériel dans la littérature ou dans un objet `config` créé à l'aide de la fonction `make_config`. Nous utilisons une moyenne et un écart-type de 12.0 et 5.2, respectivement, définis dans [cet article](https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-014-0196-0) :

```{r epidemic_models_epiestim_config,}
## créer config
config_lit <- make_config(
  mean_si = 12.0,
  std_si = 5.2
)
```

Nous pouvons ensuite estimer R<sub>t</sub> avec la fonction `estimate_R` :

```{r epidemic_models_epiestim_lit, warning = FALSE}

cases <- cases %>% 
     filter(!is.na(date))


#créer un cadre de données pour la fonction estimate_R()
cases_incidence <- data.frame(dates = seq.Date(from = min(cases$dates),
                               to = max(cases$dates), 
                               by = 1))

cases_incidence <- left_join(cases_incidence, cases) %>% 
     select(dates, I) %>% 
     mutate(I = ifelse(is.na(I), 0, I))


epiestim_res_lit <- estimate_R(
  incid = cases_incidence,
  method = "parametric_si",
  config = config_lit
)
```

et tracer un résumé des résultats :

```{r epidemic_models_epiestim_lit_plot, warning = FALSE}
plot(epiestim_res_lit)
```

#### Utilisation d'estimations d'intervalles en série à partir des données {.unnumbered}

Comme nous avons des données sur les dates d'apparition des symptômes _et_ les liens de transmission, nous pouvons également estimer l'intervalle sériel à partir de la liste de liens en calculant le délai entre les dates d'apparition des symptômes des paires infecteur-infecté. 
Comme nous l'avons fait dans la section **EpiNow2** nous allons utiliser la fonction `get_pairwise` du paquet **epicontacts** qui nous permet de calculer les différences par paires des propriétés de la liste de liens entre les paires de transmission. Nous créons d'abord un objet epicontacts (voir la page [Chaînes de transmission](#transmission_chains) pour plus de détails) :

```{r epidemic_models_epicontacts_epiestim, eval=F}
## générer des contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## générer un objet epicontacts
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)
```

Nous ajustons ensuite la différence de dates d'apparition entre les paires de transmissions, calculée en utilisant `get_pairwise`, à une distribution gamma. Nous utilisons l'outil pratique `fit_disc_gamma` du paquet **epitrix** pour cette procédure d'ajustement, car nous avons besoin d'une distribution _discrète_.

```{r epidemic_models_incubation_estimate_epiestim, warning = FALSE}
## Estimation de l'intervalle sériel gamma
serial_interval <- fit_disc_gamma(get_pairwise(epic, "date_onset"))
```

Nous passons ensuite ces informations à l'objet `config`, exécutons **EpiEstim** et traçons les résultats :

```{r epidemic_models_epiestim_emp, warning = FALSE}
## faire le config
config_emp <- make_config(
  mean_si = serial_interval$mu,
  std_si = serial_interval$sd
)

## Exécuter epiestim
epiestim_res_emp <- estimate_R(
  incid = cases_incidence,
  method = "parametric_si",
  config = config_emp
)

## tracer les résultats
plot(epiestim_res_emp)
```

#### Spécification des fenêtres temporelles d'estimation {.unnumbered}

Ces options par défaut fournissent une estimation hebdomadaire glissante et peuvent servir d'avertissement si vous estimez R<sub>t</sub> trop tôt dans l'épidémie pour une estimation précise.Vous pouvez changer cela en fixant une date de début ultérieure pour l'estimation de R<sub>t</sub>, comme indiqué ci-dessous. 

Malheureusement, **EpiEstim** n'offre qu'une façon très maladroite de spécifier ces temps d'estimation, en ce sens que vous devez fournir un vecteur d'entiers __ se référant aux dates de début et de fin de chaque fenêtre temporelle.

```{r epidemic_models_epiestim_config_late,}

## définir un vecteur de dates commençant le 1er juin
start_dates <- seq.Date(
  as.Date("2014-06-01"),
  max(cases$dates) - 7,
  by = 1
) %>%
  ## soustraire la date de départ pour la convertir en numérique
  `-`(min(cases$dates)) %>%
  ## convertir en entier
  as.integer()

## ajouter six jours pour une fenêtre glissante d'une semaine
end_dates <- start_dates + 6
  
## faire la configuration
config_partial <- make_config(
  mean_si = 12.0,
  std_si = 5.2,
  t_start = start_dates,
  t_end = end_dates
)
```
Maintenant, nous réexécutons **EpiEstim** et nous pouvons voir que les estimations ne commencent qu'à partir de juin :

```{r epidemic_models_epiestim_config_late_run,}

## exécuter epiestim
epiestim_res_partial <- estimate_R(
  incid = cases_incidence,
  method = "parametric_si",
  config = config_partial
)

## tracer les résultats
plot(epiestim_res_partial)

```

#### Analyser les sorties {.unnumbered}

Les principales sorties sont accessibles via `$R`. A titre d'exemple, nous allons créer un graphe de R<sub>t</sub> et une mesure de "potentiel de transmission" donnée par le produit de
R<sub>t</sub> et du nombre de cas signalés ce jour-là ; cela représente le
nombre attendu de cas dans la prochaine génération d'infection.

```{r epidemic_models_epiestim_plot_full, out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}

## créer un cadre de données large pour la médiane
df_wide <- epiestim_res_lit$R %>%
  rename_all(clean_labels) %>%
  rename(
    lower_95_r = quantile_0_025_r,
    lower_90_r = quantile_0_05_r,
    lower_50_r = quantile_0_25_r,
    upper_50_r = quantile_0_75_r,
    upper_90_r = quantile_0_95_r,
    upper_95_r = quantile_0_975_r,
    ) %>%
  mutate(
    ## extraire la date médiane de t_start et t_end
    dates = epiestim_res_emp$dates[round(map2_dbl(t_start, t_end, median))],
    var = "R[t]"
  ) %>%
  ## fusionner les données d'incidence quotidienne
  left_join(cases, "dates") %>%
  ## calculer le risque pour toutes les estimations r
  mutate(
    across(
      lower_95_r:upper_95_r,
      ~ .x*I,
      .names = "{str_replace(.col, '_r', '_risk')}"
    )
  ) %>%
  ## séparer les estimations de r et les estimations de risque
  pivot_longer(
    contains("median"),
    names_to = c(".value", "variable"),
    names_pattern = "(.+)_(.+)"
  ) %>%
  ## Assigner des niveaux de facteurs
  mutate(variable = factor(variable, c("risk", "r")))

## créer un cadre de données long à partir des quantiles
df_long <- df_wide %>%
  select(-variable, -median) %>%
  ## séparer les estimations de r/risque et les niveaux de quantile
  pivot_longer(
    contains(c("lower", "upper")),
    names_to = c(".value", "quantile", "variable"),
    names_pattern = "(.+)_(.+)_(.+)"
  ) %>%
  mutate(variable = factor(variable, c("risk", "r")))

## créer un graphique
ggplot() +
  geom_ribbon(
    data = df_long,
    aes(x = dates, ymin = lower, ymax = upper, alpha = quantile),
    color = NA
  ) +
  geom_line(
    data = df_wide,
    aes(x = dates, y = median),
    alpha = 0.2
  ) +
  ## utiliser label_parsed pour permettre l'utilisation d'une étiquette en indice
  facet_wrap(
    ~ variable,
    ncol = 1,
    scales = "free_y",
    labeller = as_labeller(c(r = "R[t]", risk = "Transmission~potential"), label_parsed),
    strip.position = 'left' 
  ) +
  ## définir manuellement la transparence des quantiles
  scale_alpha_manual(
    values = c(`50` = 0.7, `90` = 0.4, `95` = 0.2),
    labels = function(x) paste0(x, "%")
  ) +
  labs(
    x = NULL,
    y = NULL,
    alpha = "Credible\ninterval"
  ) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %d\n%Y"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    strip.background = element_blank(),
    strip.placement = 'outside' 
  )
  
```

<!-- ======================================================= -->
## Projection de l'incidence { }

### EpiNow2 {.unnumbered}

En plus de l'estimation de R<sub>t</sub>, **EpiNow2** permet également la prévision de
R<sub>t</sub> et les projections du nombre de cas par l'intégration avec le paquet **EpiSoon** sous le capot. Tout ce que vous avez à faire est de spécifier l'argument `horizon` dans votre appel de fonction `epinow`, indiquant le nombre de jours que vous voulez projeter dans le futur ; voir la section **EpiNow2** sous la rubrique "Estimation
R<sub>t</sub>" pour plus de détails sur la façon de mettre en place **EpiNow2**. Dans cette
section, nous allons simplement tracer les sorties de cette analyse, stockées dans le fichier
l'objet `epinow_res`.

```{r epidemic_models_episoon, out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}

## définir la date minimale pour le tracé
min_date <- as.Date("2015-03-01")

## extraire les estimations résumées
estimates <- as_tibble(epinow_res$estimates$summarised)

## extraire les données brutes sur l'incidence des cas
observations <- as_tibble(epinow_res$estimates$observations) %>%
  filter(date > min_date)

## extraire les estimations prévisionnelles du nombre de cas
df_wide <- estimates %>%
  filter(
    variable == "reported_cases",
    type == "forecast",
    date > min_date
  )

## convertir en un format encore plus long pour le tracé des quantiles
df_long <- df_wide %>%
  ## ici nous faisons correspondre les quantiles (par exemple, lower_90 à upper_90)
  pivot_longer(
    lower_90:upper_90,
    names_to = c(".value", "quantile"),
    names_pattern = "(.+)_(.+)"
  )

## créer un graphique
ggplot() +
  geom_histogram(
    data = observations,
    aes(x = date, y = confirm),
    stat = 'identity',
    binwidth = 1
  ) +
  geom_ribbon(
    data = df_long,
    aes(x = date, ymin = lower, ymax = upper, alpha = quantile),
    color = NA
  ) +
  geom_line(
    data = df_wide,
    aes(x = date, y = median)
  ) +
  geom_vline(xintercept = min(df_long$date), linetype = 2) +
  ## Définir manuellement la transparence des quantiles
  scale_alpha_manual(
    values = c(`20` = 0.7, `50` = 0.4, `90` = 0.2),
    labels = function(x) paste0(x, "%")
  ) +
  labs(
    x = NULL,
    y = "Daily reported cases",
    alpha = "Credible\ninterval"
  ) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %d\n%Y"
  ) +
    theme_minimal(base_size = 14)

```

### projections {.unnumbered}

Le paquet **projections** développé par RECON permet de faire très facilement des prévisions d'incidence à court terme, ne nécessitant que la connaissance du nombre de reproduction effectif de reproduction R<sub>t</sub> et de l'intervalle de série. Nous verrons ici comment utiliser des estimations d'intervalle sériel de la littérature et comment utiliser nos propres estimations de la liste de diffusion.

#### Utiliser les estimations d'intervalles sériels de la littérature {.unnumbered}

**projections** nécessite une distribution d'intervalle série discrétisée de la classe `distcrete` du paquet **distcrete**. Nous utiliserons une distribution gamma avec une moyenne de 12,0 et un écart-type de 5,2 définie dans [cet article](https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-014-0196-0). Pour convertir ces valeurs en paramètres de forme et d'échelle requis pour une distribution gamma. nous utiliserons la fonction `gamma_mucv2shapescale` du paquet **epitrix**.


```{r epidemic_models_projections_distcrete,}

## obtenir les paramètres de forme et d'échelle à partir du mu moyen et du coefficient de
## variation (par exemple, le rapport entre l'écart type et la moyenne).
shapescale <- epitrix::gamma_mucv2shapescale(mu = 12.0, cv = 5.2/12)

## fabriquer un objet distcrete
serial_interval_lit <- distcrete::distcrete(
  name = "gamma",
  interval = 1,
  shape = shapescale$shape,
  scale = shapescale$scale
)

```

Voici une vérification rapide pour s'assurer que l'intervalle de série est correct. Nous accédons à la densité de la distribution gamma que nous venons de définir par `$d`, ce qui revient à appeler `dgamma` :

```{r epidemic_models_projections_distcrete_plot,}

## vérifiez que l'intervalle série est correct
qplot(
  x = 0:50, y = serial_interval_lit$d(0:50), geom = "area",
  xlab = "Serial interval", ylab = "Density"
)

```

#### Utilisation des estimations d'intervalles sériels à partir des données {.unnumbered}

Comme nous avons des données sur les dates d'apparition des symptômes _et_ les liens de transmission, nous pouvons également estimer l'intervalle sériel à partir de la liste de liens en calculant le délai entre les dates d'apparition des symptômes des paires infecteur-infecté. Comme nous l'avons fait dans la section **EpiNow2**, nous allons utiliser la fonction `get_pairwise` du paquet **epicontacts** qui nous permet de calculer les différences par paires des propriétés de la liste de liens entre les paires de transmission. Nous créons d'abord un objet epicontacts (voir la page [Chaînes de transmission](#transmission_chains) pour plus de détails) :

```{r epidemic_models_epicontacts_projections, eval=F}
## générer des contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## générer un objet epicontacts
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)
```

Nous ajustons ensuite la différence de dates d'apparition entre les paires de transmissions, calculée avec `get_pairwise`, à une distribution gamma. Nous utilisons l'outil pratique `fit_disc_gamma` du paquet **epitrix** pour cette procédure d'ajustement, car nous avons besoin d'une distribution _discrète_.

```{r epidemic_models_incubation_estimate_projections, warning = FALSE}
## Estimation de l'intervalle sériel gamma
serial_interval <- fit_disc_gamma(get_pairwise(epic, "date_onset"))

## inspecter l'estimation
serial_interval[c("mu", "sd")]
```

#### Projection de l'incidence {.unnumbered}

Pour projeter l'incidence future, nous devons fournir l'incidence historique sous la forme d'un objet `incidence`, ainsi qu'un échantillon de valeurs R<sub>t</sub> plausibles. Nous générerons ces valeurs en utilisant les estimations R<sub>t</sub> générées par **EpiEstim** dans la section précédente (sous "Estimation de la valeur de R<sub>t</sub>") et stockées dans l'objet `epiestim_res_emp`. Dans le code ci-dessous nous extrayons les estimations de la moyenne et de l'écart type de R<sub>t</sub> pour la dernière fenêtre de temps de l'épidémie (en utilisant la fonction `tail` pour accéder au dernier élément d'un vecteur), et nous simulons 1000 valeurs à partir d'une distribution gamma en utilisant `rgamma`. Vous pouvez également fournir votre propre vecteur de valeurs R<sub>t</sub> que vous souhaitez utiliser pour les projections.

```{r epidemic_models_projection_setup, warning = FALSE}

## créer un objet d'incidence à partir des dates d'apparition des symptômes
inc <- incidence::incidence(linelist$date_onset)

## extraire les valeurs r plausibles de l'estimation la plus récente
mean_r <- tail(epiestim_res_emp$R$`Mean(R)`, 1)
sd_r <- tail(epiestim_res_emp$R$`Std(R)`, 1)
shapescale <- gamma_mucv2shapescale(mu = mean_r, cv = sd_r/mean_r)
plausible_r <- rgamma(1000, shape = shapescale$shape, scale = shapescale$scale)

## vérifier la distribution
qplot(x = plausible_r, geom = "histogram", xlab = expression(R[t]), ylab = "Counts")

```

Nous utilisons ensuite la fonction `project()` pour effectuer la prévision réelle. Nous spécifions le nombre de jours pour lesquels nous voulons faire une projection via les arguments `n_days`, et nous spécifions le nombre de simulations en utilisant les arguments `n_sim`.

```{r epidemic_models_make_projection,}

## faire une projection
proj <- project(
  x = inc,
  R = plausible_r,
  si = serial_interval$distribution,
  n_days = 21,
  n_sim = 1000
)

```

Nous pouvons alors facilement tracer l'incidence et les projections en utilisant les fonctions `plot()` et `add_projections()`. On peut facilement sous-évaluer l'objet `incidence` pour ne montrer que les cas les plus récents en utilisant l'opérateur de crochets.

```{r epidemic_models_plot_projection, out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}

## Tracer l'incidence et les projections
plot(inc[inc$dates > as.Date("2015-03-01")]) %>%
  add_projections(proj)

```

Vous pouvez également extraire facilement les estimations brutes du nombre de cas quotidiens en convertissant la sortie en un cadre de données.

```{r epidemic_models_projection_df, eval=F, warning = FALSE}
## convertir en cadre de données pour les données brutes
proj_df <- as.data.frame(proj)
proj_df
```

```{r epidemic_models_projection_dt, echo = F}

## convertir en cadre de données pour les données brutes
proj_df <- as.data.frame(proj)

## sortie de la table de données
DT::datatable(
  proj_df[1:11],
  rownames = FALSE,
  filter = "top",
  options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap'
)

```


<!-- ======================================================= -->
## Ressources { }

[Voici un article](https://www.sciencedirect.com/science/article/pii/S1755436519300350) qui décrit la méthodologie mise en œuvre dans **EpiEstim**.
[Voici un article](https://wellcomeopenresearch.org/articles/5-112/v1) décrivant la méthodologie mise en œuvre dans **EpiNow**.
[Voici un article](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008409) décrivant  diverses considérations méthodologiques et pratiques pour l'estimation de R<sub>t</sub>.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/epidemic_models.Rmd-->

---
editor_options: 
  markdown: 
    wrap: 72
---

# Suivi des contacts {#contact_tracing}

Cette page présente une analyse descriptive de données de recherche de contacts, en ajoutant quelques considérations et approches clés uniques à ce type de données.

Cette page fait référence à un grand nombre des compétences de base en matière de gestion et de visualisation des données R abordées dans d'autres pages (par exemple, le nettoyage des données, le pivotement, les tableaux, les analyses de séries chronologiques), mais nous mettrons en évidence des exemples spécifiques au suivi des contacts qui ont été utiles pour la prise de décision opérationnelle. Il s'agit par exemple de la visualisation des données de suivi de la recherche de contacts dans le temps ou dans des zones géographiques, ou de la production de tableaux d'indicateurs de performance clés (KPI) propres pour les superviseurs de la recherche de contacts.

Pour la démonstration, nous utiliserons un échantillon de données de suivi des contacts provenant de la plateforme [Go.Data](https://www.who.int/tools/godata). Les principes abordés ici s'appliquent aux données de suivi des contacts provenant d'autres plates-formes. Il se peut que vous deviez simplement suivre différentes étapes de pré-traitement des données en fonction de la structure de vos données.

Vous pouvez en savoir plus sur le projet Go.Data sur le [site de documentation Github](<https://worldhealthorganization.github.io/godata/>) ou la [communauté de pratique](https://community-godata.who.int/).

## Préparation

### Chargement de packages {.unnumbered}

Ce bout de code montre le chargement des packages nécessaires aux analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le package si nécessaire *et* le charger pour l'utiliser. Vous pouvez aussi charger les packages installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les packages R.

```{r, message = F}
pacman::p_load(
  rio,          # importation de données 
  here,         # chemins d'accès relatifs aux fichiers 
  janitor,      # nettoyage des données et tableaux
  lubridate,    # travailler avec des dates
  epikit,       # fonction age_categories()
  apyramid,     # age pyramids
  tidyverse,    # manipulation et visualisation des données
  RColorBrewer, # colour palettes
  formattable,  # fancy tables
  kableExtra    # formatage des tableaux
)
```

### Importation de données {.unnumbered}

Nous allons importer des jeux de données exemple de contacts, et de leur"suivi". Ces données ont été récupérées et non imbriquées à partir de l'API Go.Data et stockées dans des fichiers ".rds".

Vous pouvez télécharger tous les exemples de données pour ce manuel à partir de la page [Télécharger le manuel et les données](#download_book_data).

Si vous souhaitez télécharger les exemples de données de suivi des contacts spécifiques à cette page, utilisez les trois liens de téléchargement ci-dessous :

<a href='https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/cases_clean.rds?raw=true' class='download-button'> Cliquer pour télécharger données sur les investigations des cas (.rds file) </a>

<a href='https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/contacts_clean.rds?raw=true' class='download-button'> Cliquer pour télécharger les données d'enregistrement de contacts (.rds file) </a>

<a href='https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/followups_clean.rds?raw=true' class='download-button'> Cliquer pour télécharger les données de suivi des contacts (.rds file) </a>

<!-- ```{r out.width = "100%", fig.align = "center", echo=F} -->

<!-- knitr::include_graphics(here::here("images", "godata_api_github.png")) -->

<!-- ``` -->

Dans leur forme originale dans les fichiers téléchargeables, les données reflètent les données fournies par l'API Go.Data (en savoir plus sur [APIs here](#import_api)). À titre d'exemple, nous allons nettoyer les données pour les rendre plus faciles à lire sur cette page. Si vous utilisez une instance Go.Data, vous pouvez consulter les instructions complètes sur la façon de récupérer vos données [here](https://github.com/WorldHealthOrganization/godata/tree/master/analytics/r-reporting).

Ci-dessous, les jeux de données sont importés à l'aide de la fonction `import()` du package **rio**. Voir la page [Importation et exportation](#import_export) pour les différentes manières d'importer des données. Nous utilisons `here()` pour spécifier le chemin du fichier - vous devez fournir le chemin du fichier spécifique à votre ordinateur. Nous utilisons ensuite `select()` pour sélectionner seulement certaines colonnes des données, afin de simplifier pour les besoins de la démonstration.

#### Données des cas {.unnumbered}

Ces données sont un tableau des cas, et des informations les concernant.

```{r}
cases <- import(here("data", "godata", "cases_clean.rds")) %>% 
  select(case_id, firstName, lastName, gender, age, age_class,
         occupation, classification, was_contact, hospitalization_typeid)
```

Voici les `nrow(cases)` cas :

```{r, message=FALSE, echo=F}
DT::datatable(cases, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Données sur les contacts {.unnumbered}

Ces données sont un tableau de tous les contacts et des informations les concernant. Là encore, vous pouvez fournir votre propre chemin de fichier. Après l'importation, nous effectuons quelques étapes préliminaires de nettoyage des données, notamment :

-   Définir age_class comme facteur et inverser l'ordre des niveaux pour que les plus jeunes soient les premiers.\
-   Sélectionner seulement certaines colonnes, en renommant l'une d'entre elles.\
-   Attribuer artificiellement les lignes dont le 2 niveau d'administration est manquant à "Djembe", pour améliorer la clarté de certains exemples de visualisation.

```{r}
contacts <- import(here("data", "godata", "contacts_clean.rds")) %>% 
  mutate(age_class = forcats::fct_rev(age_class)) %>% 
  select(contact_id, contact_status, firstName, lastName, gender, age,
         age_class, occupation, date_of_reporting, date_of_data_entry,
         date_of_last_exposure = date_of_last_contact,
         date_of_followup_start, date_of_followup_end, risk_level, was_case, admin_2_name) %>% 
  mutate(admin_2_name = replace_na(admin_2_name, "Djembe"))
```

Voici les `nrow(contacts)` lignes de le `contacts` dataframe:

```{r, message=FALSE, echo=F}
DT::datatable(contacts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Données de suivi {.unnumbered}

Ces données sont des enregistrements des interactions de "suivi" avec les contacts. Chaque contact est censé avoir une rencontre chaque jour pendant 14 jours après son exposition.

Nous importons et effectuons quelques étapes de nettoyage. Nous sélectionnons certaines colonnes, et convertissons également une colonne de caractères en toutes les valeurs minuscules.

```{r}
followups <- rio::import(here::here("data", "godata", "followups_clean.rds")) %>% 
  select(contact_id, followup_status, followup_number,
         date_of_followup, admin_2_name, admin_1_name) %>% 
  mutate(followup_status = str_to_lower(followup_status))
```

Voici les 50 premières lignes de la base de données `nrow(followups)`-row `followups` (chaque ligne est une interaction de suivi, avec le statut du suivi dans la colonne `followup_status`) :

```{r, message=FALSE, echo=F}
DT::datatable(head(followups, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Données de relations {.unnumbered}

Ici, nous importons des données montrant la relation entre les cas et les contacts. Nous sélectionnons certaines colonnes à afficher.

```{r}
relationships <- rio::import(here::here("data", "godata", "relationships_clean.rds")) %>% 
  select(source_visualid, source_gender, source_age, date_of_last_contact,
         date_of_data_entry, target_visualid, target_gender,
         target_age, exposure_type)
```

Vous trouverez ci-dessous les 50 premières lignes du jeu de données `relations`, qui enregistre toutes les relations entre les cas et les contacts.

```{r, message=FALSE, echo=F}
DT::datatable(head(relationships, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

## Analyses descriptives

Vous pouvez utiliser les techniques abordées dans d'autres pages de ce manuel pour effectuer des analyses descriptives de vos cas, de vos contacts et de leurs relations. Vous trouverez ci-dessous quelques exemples

### Démographie {.unnumbered}

Comme le montre la page consacrée aux [Pyramides démographiques](#age_pyramid), vous pouvez visualiser la répartition par âge et par sexe (nous utilisons ici le package **apyramide**).

#### Age et sexe des contacts {.unnumbered}

La pyramide ci-dessous compare la répartition par âge des contacts, par sexe. Notez que les contacts dont l'âge est manquant sont inclus dans leur propre barre en haut. Vous pouvez modifier ce comportement par défaut, mais envisagez alors d'indiquer le nombre de contacts manquants dans une légende.

```{r, warning=F, message=F}
apyramid::age_pyramid(
  data = contacts,                                   # utiliser la base de données des contacts
  age_group = "age_class",                           # colonne d'âge catégorielle
  split_by = "gender") +                             # genre pour les moitiés de la pyramide
  labs(
    fill = "Gender",                                 # titre de la légende
    title = "Age/Sex Pyramid of COVID-19 contacts")+ # titre du graphique
  theme_minimal()                                    # un fond simple
```

Avec la structure de données Go.Data, le jeu de données `relations` contient les âges des cas et des contacts, vous pourriez donc utiliser ce jeu de données et créer une pyramide des âges montrant les différences entre ces deux groupes de personnes. Le tableau de données `relations` sera modifié pour transformer les colonnes d'âge numériques en catégories (voir la page [Nettoyage des données et fonctions de base](#cleaning_data)). Nous faisons également pivoter le tableau de données pour faciliter le traçage avec **ggplot2** (voir [Pivoter les données](#pivoting_data)).

```{r}
relation_age <- relationships %>% 
  select(source_age, target_age) %>% 
  transmute(        # transmute est comme mutate() mais supprime toutes les autres colonnes non mentionnées
    source_age_class = epikit::age_categories(source_age, breakers = seq(0, 80, 5)),
    target_age_class = epikit::age_categories(target_age, breakers = seq(0, 80, 5)),
    ) %>% 
  pivot_longer(cols = contains("class"), names_to = "category", values_to = "age_class")  # pivotement plus long


relation_age
```

Maintenant nous pouvons tracer cet ensemble de données transformées avec `age_pyramid()` comme avant, mais en remplaçant `gender` par `category` (contact, ou cas).

```{r, warning=F, message=F}
apyramid::age_pyramid(
  data = relation_age,                               # utiliser un ensemble de données de relations modifiées
  age_group = "age_class",                           # colonne d'âge catégorielle
  split_by = "category") +                           # par cas et contacts
  scale_fill_manual(
    values = c("orange", "purple"),                  # pour spécifier les couleurs ET les étiquettes
    labels = c("Case", "Contact"))+
  labs(
    fill = "Legend",                                           # titre de la légende
    title = "Pyramides demographiques de cas et contacts de COVID-19")+ # titre du graph
  theme_minimal()                                              # fond simple
```

Nous pouvons également visualiser d'autres caractéristiques telles que la répartition par profession (par exemple, sous la forme d'un diagramme circulaire).

```{r, warning=F, message=F}
# Clean dataset and get counts by occupation
occ_plot_data <- cases %>% 
  mutate(occupation = forcats::fct_explicit_na(occupation),  # faire des valeurs manquantes NA une catégorie
         occupation = forcats::fct_infreq(occupation)) %>% # ordonner les niveaux de facteurs par ordre de fréquence
  count(occupation)                                          # obtenir des chiffres par profession
  
# Make pie chart
ggplot(data = occ_plot_data, mapping = aes(x = "", y = n, fill = occupation))+
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0) +
  labs(
    fill = "Occupation",
    title = "Occupation connue des cas de covid-19")+
  theme_minimal() +                    
  theme(axis.line = element_blank(),
        axis.title = element_blank(),
        axis.text = element_blank())
```

### Contacts par cas {.unnumbered}

Le nombre de contacts par cas peut être une unité de mesure importante pour évaluer la qualité du dénombrement des contacts et la conformité de la population à la réponse de santé publique.

En fonction de votre structure de données, cela peut être évalué avec un ensemble de données qui contient tous les cas et les contacts. Dans les ensembles de données de Go.Data, les liens entre les cas ("sources") et les contacts ("cibles") sont stockés dans le jeu de données `relationships`.

Dans cet ensemble de données, chaque ligne est un contact, et le cas source est listé dans la ligne. Aucun contact n'a de relations avec plusieurs affaires. multiples, mais si c'est le cas, vous devrez peut-être en tenir compte avant de faire le graphique (et de les explorer aussi !).

Nous commençons par compter le nombre de lignes (contacts) par cas source. Ceci est enregistré comme un tableau de données.

```{r}
contacts_per_case <- relationships %>% 
  count(source_visualid)

contacts_per_case
```

Nous utilisons `geom_histogram()` pour représenter ces données sous forme d'histogramme.

```{r, warning=F, message=F}
ggplot(data = contacts_per_case)+        # commencer avec le tableau de données créé ci-dessus
  geom_histogram(mapping = aes(x = n))+  # afficher l'histogramme du nombre de contacts par cas
  scale_y_continuous(expand = c(0,0))+   # supprimer l'espace excédentaire en dessous de 0 sur l'axe des ordonnées
  theme_light()+                         # simplifier le fond
  labs(
    title = "Number of contacts per case",
    y = "Cases",
    x = "Contacts per case"
  )
  

```

## Suivi de contacts

Les données de recherche des contacts contiennent souvent des données de "suivi", qui enregistrent les résultats des contrôles quotidiens des symptômes des personnes en quarantaine. L'analyse de ces données permet d'orienter la stratégie de réponse, d'identifier les contacts susceptibles d'être perdus pour le suivi ou de développer la maladie.

### Nettoyage de données {.unnumbered}

Ces données peuvent exister sous différents formats. Elles peuvent exister sous la forme d'une large avec une ligne par contact et une colonne par "jour" de suivi. jour" de suivi. Voir [Pivoter les données](#pivoting_data) pour obtenir des descriptions des données "longues" et données "larges" et comment faire pivoter des données plus larges ou plus longues.

Dans notre exemple Go.Data, ces données sont stockées dans le tableau `followups`, qui est dans un format "long" avec une ligne par interaction de suivi. Les 50 premières lignes ressemblent à ceci :

```{r, message=FALSE, echo=FALSE}
# display the first 50 rows of contact linelist data as a table
DT::datatable(head(followups, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

[***ATTENTION:*** Méfiez-vous des doublons lorsque vous traitez des données de suivi, car il peut y avoir plusieurs suivis erronés le même  jour pour un contact donné. Cela peut sembler être une erreur mais reflète la réalité - par exemple, un agent de recherche de contacts pourrait soumettre un formulaire de suivi en début de journée alors qu'il n'a pas pu joindre le contact, et soumettre un second formulaire lorsqu'il a été joint par la suite. La façon dont vous souhaitez traiter les doublons dépend du contexte opérationnel. - veillez simplement à documenter clairement votre approche.]{style="color : orange ;"}

Voyons combien d'instances de lignes "en double" nous avons :

```{r}
followups %>% 
  count(contact_id, date_of_followup) %>%   # obtenir des jours de suivi uniques
  filter(n > 1)                             # afficher les enregistrements où le nombre est supérieur à 1 
```

Dans notre exemple de données, les seuls enregistrements auxquels cela s'applique sont ceux auxquels il manque un ID ! Nous pouvons les supprimer. Mais, pour les besoins de la démonstration, nous allons montrer les étapes de la déduplication afin qu'il n'y ait qu'un seul encoutrement de suivi par personne et par jour. Voir la page sur [déduplication](#deduplication) pour plus de détails. Nous supposerons que l'enregistrement de rencontre le plus récent est le bon. Nous profitons également de l'occasion pour nettoyer la colonne `followup_number` (le "jour" du suivi qui devrait qui devrait être compris entre 1 et 14).

```{r, warning=F, message=F}
followups_clean <- followups %>%
  
  # Enlever les doublons
  group_by(contact_id, date_of_followup) %>%        # grouper les lignes par jour de suivi
  arrange(contact_id, desc(date_of_followup)) %>%   # organiser les lignes, par jour de suivi, par date de suivi (le plus récent en haut)
  slice_head() %>%                                  # ne conserver que la première ligne par id contact
  ungroup() %>% 
  
  # Autres nettoyages
  mutate(followup_number = replace(followup_number, followup_number > 14, NA)) %>% # nettoyer des données erronées
  drop_na(contact_id)                               # supprimer les id_contact dont les données sont manquantes
```

Pour chaque rencontre de suivi, nous avons un statut de suivi (tel que si la rencontre a eu lieu et, le cas échéant, si le contact a eu des symptômes ou non). Pour voir toutes les valeurs, nous pouvons exécuter un rapide `tabyl()` (de **janitor**) ou `table()` (de **base** R) (voir [Tableaux descriptifs](#descriptive_tables)) par `followup_status` pour voir la fréquence de chacun des résultats.

Dans cet ensemble de données, "vu_not_ok" signifie "vu avec des symptômes", et "vu_ok" signifie "vu sans symptômes".

```{r}
followups_clean %>% 
  tabyl(followup_status)
```

### Graphe dans le temps {.unnumbered}

Comme les données de dates sont continues, nous utiliserons un histogramme pour les représenter avec `date_du_suivi` assigné à l'axe des abscisses. Nous pouvons obtenir un histogramme "empilé" en spécifiant un argument `fill =` dans `aes()`, que nous assignons à la colonne `followup_status`. Par conséquent, vous pouvez définir le titre de la légende en utilisant l'argument `fill =` de `labs()`.

On constate que les contacts ont été identifiés par vagues (correspondant vraisemblablement aux vagues épidémiques de cas), et que vl'achèvement du suivi ne semble pas s'être amélioré au cours de l'épidémie.

```{r, warning=F, message=F}
ggplot(data = followups_clean)+
  geom_histogram(mapping = aes(x = date_of_followup, fill = followup_status)) +
  scale_fill_discrete(drop = FALSE)+   # Afficher tous les niveaux de facteurs (followup_status) dans la légende, même ceux qui ne sont pas utilisés.
  theme_classic() +
  labs(
    x = "",
    y = "Number of contacts",
    title = "Daily Contact Followup Status",
    fill = "Followup Status",
    subtitle = str_glue("Data as of {max(followups$date_of_followup, na.rm=T)}"))   # sous-titres dynamiques
  
```

[***ATTENTION:*** Si vous préparez de nombreux graphiques (par exemple pour plusieurs juridictions), vous voudrez que les légendes apparaissent de manière identique, même si les données sont plus ou moins complètes ou composées. Il peut y avoir des graphiques pour lesquels tous les statuts de suivi ne sont pas présents dans les données, mais vous voulez quand même que ces catégories apparaissent dans les légendes. Dans les ggplots (comme ci-dessus), vous pouvez spécifier l'argument `drop = FALSE` de la fonction `scale_fill_discrete()`. Dans les tableaux, utilisez `tabyl()` qui montre les comptes pour tous les niveaux de facteurs, ou si vous utilisez `count()` de **dplyr** ajoutez l'argument `.drop = FALSE` pour inclure les comptes pour tous les niveaux de facteurs.]{style="color : orange ;"}

### Suivi quotidien individuel {.unnumbered}

Si votre épidémie est suffisamment petite, vous voudrez peut-être voir chaque contact individuellement et voir son statut au cours de son suivi. Heureusement, cet tableau de données `followups` contient déjà une colonne avec le le "numéro" du jour du suivi (1-14). Si cette colonne n'existe pas dans vos données, vous pouvez la créer en calculant la différence entre la date de la dernière rencontre et la date à laquelle le suivi devait commencer pour le contact.

Un mécanisme de visualisation pratique (si le nombre de cas n'est pas trop important) peut être un diagramme de dispersion, réalisé avec `geom_tile()`. Voir plus de détails dans la page [heat plot](#heatmaps).

```{r, warning=F, message=F}
ggplot(data = followups_clean)+
  geom_tile(mapping = aes(x = followup_number, y = contact_id, fill = followup_status),
            color = "grey")+       # lignes grises
  scale_fill_manual( values = c("yellow", "grey", "orange", "darkred", "darkgreen"))+
  theme_minimal()+
  scale_x_continuous(breaks = seq(from = 1, to = 14, by = 1))
```

### Analyse par groupe {.unnumbered}

Ces données de suivi sont peut-être consultées journellement ou hebdomadairement pour la prise de décision opérationnelle. Vous souhaitez peut-être des désagrégations plus significatives par zone géographique ou par équipe de suivi des contacts. Nous pouvons le faire en ajustant les colonnes fournies à `group_by()`.

```{r, warning=F, message=F}

plot_by_region <- followups_clean %>%                                        # commencer par l'ensemble de données de suivi
  count(admin_1_name, admin_2_name, followup_status) %>%   # obtenir les chiffres par région-statut unique (crée la colonne 'n' avec les chiffres)
  
  # begin ggplot()
  ggplot(                                         # commencer le ggplot
    mapping = aes(x = reorder(admin_2_name, n),     # réorganiser les facteurs administratifs en fonction des valeurs numériques de la colonne 'n'.
                  y = n,                            # hauteur de la barre de la colonne 'n'.
                  fill = followup_status,           # colorer les barres empilées en fonction de leur statut
                  label = n))+                      # passer à geom_label()              
  geom_col()+                                     # barres empilées, cartographie obtenue au-dessus
  geom_text(                                      # ajouter du texte, cartographie obtenue à partir de la version précédente
    size = 3,                                         
    position = position_stack(vjust = 0.5), 
    color = "white",           
    check_overlap = TRUE,
    fontface = "bold")+
  coord_flip()+
  labs(
    x = "",
    y = "Number of contacts",
    title = "Contact Followup Status, by Region",
    fill = "Followup Status",
    subtitle = str_glue("Data as of {max(followups_clean$date_of_followup, na.rm=T)}")) +
  theme_classic()+                                                                      # Simplifier le fond
  facet_wrap(~admin_1_name, strip.position = "right", scales = "free_y", ncol = 1)      # introduire les facettes 

plot_by_region
```

<!-- If this was disaggregated by contact tracer, perhaps we would want to add a threshold line to display total # contacts that normally one person or area/team can handle, and how the current workload compares. We just do this by using `geom_hline()` function. -->

<!-- ```{r, warning=F, message=F} -->

<!-- plot_by_region +  -->

<!--      geom_hline(aes(yintercept=25), color="#C70039", linetype = "dashed") # fictitious threshold at 25 contacts -->

<!-- ``` -->

## Tableaux KPI

Il existe un certain nombre d'indicateurs clés de performance ( KPI) qui peuvent être calculés et suivis à différents niveaux de désagrégation et sur différentes périodes de temps afin de contrôler les performances de la recherche de contacts. Une fois que vous maîtrisez les calculs et le format de base du tableau, il est assez facile d'intervertir les différents KPI.

Il existe de nombreuses sources de KPI pour le suivi des contacts, comme celle de [ResolveToSaveLives.org](https://contacttracingplaybook.resolvetosavelives.org/checklists/metrics). La majeure partie du travail consistera à parcourir votre structure de données et à réfléchir à tous les critères d'inclusion/exclusion. Nous présentons quelques exemples ci-dessous, en utilisant la structure de métadonnées de Go.Data :

+-------------+-------------+---------------------+-------------+
| Catégorie   | Indicateur  | Go.Data Numérateur  | Go.Data D   |
|             |             |                     | énominateur |
+=============+=============+=====================+=============+
| Indicateur  | \% cas      | NOMBRE DE `case_id` | NOMBRE DE   |
| de          | interviewé  | OU                  | `case_id`   |
| processus   | et isololé  | (`                  |             |
| -Rapidité   | dans les    | da te_of_reporting` |             |
| du Suivi de | 24h du cas  | -                   |             |
| contact     | rapport     | `da                 |             |
|             |             | te _of_data_entry`) |             |
|             |             | \< 1 jour et        |             |
|             |             | (`is                |             |
|             |             | ol ation_startdate` |             |
|             |             | -                   |             |
|             |             | `da                 |             |
|             |             | te _of_data_entry`) |             |
|             |             | \< 1 jour           |             |
+-------------+-------------+---------------------+-------------+
| Indicateur  | \% contacts | NOMBRE DE           | NOMBRE DE   |
| de          | notifié et  | `contact_id` OÙ     | `c          |
| processus   | mis en      | `followup_status`   |  ontact_id` |
| -Rapidité   | quarantaine | == "SEEN_NOT_OK" OR |             |
| du Suivi de | dans 24h d' | "SEEN_OK" ET        |             |
| contact     | élicitation | `d ate_of_followup` |             |
|             |             | -                   |             |
|             |             | `                   |             |
|             |             | da te_of_reporting` |             |
|             |             | \< 1 jour           |             |
+-------------+-------------+---------------------+-------------+
| Indicateur  | \% nouveaux | NOMBRE DE `case_id` | NOMBRE DE   |
| de          | symptômes   | OÙ                  | `case_id`   |
| processus - | cas testés  | (`                  |             |
| Complétude  | et          | da te_of_reporting` |             |
| des tests   | interviewé  | - `date_of_onset`)  |             |
|             | dans les 3  | \< =3 jours         |             |
|             | jours de    |                     |             |
|             | début de    |                     |             |
|             | symptôme    |                     |             |
+-------------+-------------+---------------------+-------------+
| Indicateur  | \% nouveaux | NOMBRE DE `case_id` | NOMBRE DE   |
| de résultat | cas parmi   | OÙ `was_contact` == | `case_id`   |
| - Globale   | les         | "TRUE"              |             |
| ment        | contacts    |                     |             |
|             | listés      |                     |             |
+-------------+-------------+---------------------+-------------+

Nous vous proposons ci-dessous un exemple de création d'un tableau visuel pour afficher le suivi des contacts dans les différentes zones d'administration. À la fin, nous le convertirons en tableau de présentation avec le package **formattable** (mais vous pouvez utiliser d'autres packages comme **flextable** - voir [Tableaux de présentation](#tables_presentation)).

La manière de créer un tel tableau dépend de la structure de vos données de suivi des contacts. Utilisez la page [Tableaux descriptifs](#descriptive_tables) pour apprendre à résumer les données à l'aide des fonctions **dplyr**.

Nous allons créer une table qui sera dynamique et changera au fur et à mesure que les données changeront. Pour rendre les résultats intéressants, nous allons définir une "date de rapport" pour nous permettre de simuler l'exécution du tableau à un certain jour (nous choisissons le 10 juin 2020). Les données sont filtrées à cette date.

```{r, warning=F, message=F}
# Définissez "Date du rapport" pour simuler l'exécution du rapport avec des données "à partir de" cette date.
report_date <- as.Date("2020-06-10")

# Créez des données de suivi pour refléter la date du rapport.
table_data <- followups_clean %>% 
  filter(date_of_followup <= report_date)
```

Maintenant, sur la base de notre structure de données, nous allons faire ce qui suit :

1)  Commencez par les données `followups` et résumez-les pour inclure, pour chaque contact unique :\

-   La date du dernier enregistrement (quel que soit le statut du suivi).\
-   La date de la dernière suivi où le contact a été "vu"\
-   le statut du suivi lors de cette dernière suivi (par exemple, avec des symptômes, sans symptômes)\

2)  Joignez ces données aux données des contacts, qui contiennent d'autres informations telles que le statut général du contact, la date de la dernière exposition à un cas, etc. Nous allons également calculer des indicateurs intéressants pour chaque contact, comme le nombre de jours depuis la dernière exposition.\
3)  Nous regroupons les données de contact élargies par région géographique (`admin_2_name`) et calculons des statistiques sommaires par région\
4)  Enfin, nous mettons en forme le tableau pour qu'il soit bien présenté.

Tout d'abord, nous résumons les données de suivi pour obtenir les informations qui nous intéressent :

```{r, warning=F, message=F}
followup_info <- table_data %>% 
  group_by(contact_id) %>% 
  summarise(
    date_last_record   = max(date_of_followup, na.rm=T),
    date_last_seen     = max(date_of_followup[followup_status %in% c("seen_ok", "seen_not_ok")], na.rm=T),
    status_last_record = followup_status[which(date_of_followup == date_last_record)]) %>% 
  ungroup()
```

Voici à quoi ressemblent ces données :

```{r, echo=F}
DT::datatable(followup_info, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```

Maintenant, nous allons ajouter ces informations à l'ensemble de données `contacts`, et calculer quelques colonnes supplémentaires.

```{r}
contacts_info <- followup_info %>% 
  right_join(contacts, by = "contact_id") %>% 
  mutate(
    database_date       = max(date_last_record, na.rm=T),
    days_since_seen     = database_date - date_last_seen,
    days_since_exposure = database_date - date_of_last_exposure
    )
```

Voici à quoi ressemblent ces données. Il faut noter la colonne `contacts` à droite, et la nouvelle colonne calculée à l'extrême droite.

```{r, echo=F}
DT::datatable(contacts_info, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```

Ensuite, nous résumons les données sur les contacts par région, afin d'obtenir un tableaux de synthèse des colonnes de statistiques.

```{r}
contacts_table <- contacts_info %>% 
  
  group_by(`Admin 2` = admin_2_name) %>%
  
  summarise(
    `Registered contacts` = n(),
    `Active contacts`     = sum(contact_status == "UNDER_FOLLOW_UP", na.rm=T),
    `In first week`       = sum(days_since_exposure < 8, na.rm=T),
    `In second week`      = sum(days_since_exposure >= 8 & days_since_exposure < 15, na.rm=T),
    `Became case`         = sum(contact_status == "BECAME_CASE", na.rm=T),
    `Lost to follow up`   = sum(days_since_seen >= 3, na.rm=T),
    `Never seen`          = sum(is.na(date_last_seen)),
    `Followed up - signs` = sum(status_last_record == "Seen_not_ok" & date_last_record == database_date, na.rm=T),
    `Followed up - no signs` = sum(status_last_record == "Seen_ok" & date_last_record == database_date, na.rm=T),
    `Not Followed up`     = sum(
      (status_last_record == "NOT_ATTEMPTED" | status_last_record == "NOT_PERFORMED") &
        date_last_record == database_date, na.rm=T)) %>% 
    
  arrange(desc(`Registered contacts`))

```

```{r, echo=F}
DT::datatable(contacts_table, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```

Et maintenant, nous appliquons le style des paquets **formattable** et **knitr**. y compris une note de pied de page qui indique la date "en date du".

```{r}
contacts_table %>%
  mutate(
    `Admin 2` = formatter("span", style = ~ formattable::style(
      color = ifelse(`Admin 2` == NA, "red", "grey"),
      font.weight = "bold",font.style = "italic"))(`Admin 2`),
    `Followed up - signs`= color_tile("white", "orange")(`Followed up - signs`),
    `Followed up - no signs`= color_tile("white", "#A0E2BD")(`Followed up - no signs`),
    `Became case`= color_tile("white", "grey")(`Became case`),
    `Lost to follow up`= color_tile("white", "grey")(`Lost to follow up`), 
    `Never seen`= color_tile("white", "red")(`Never seen`),
    `Active contacts` = color_tile("white", "#81A4CE")(`Active contacts`)
  ) %>%
  kable("html", escape = F, align =c("l","c","c","c","c","c","c","c","c","c","c")) %>%
  kable_styling("hover", full_width = FALSE) %>%
  add_header_above(c(" " = 3, 
                     "Of contacts currently under follow up" = 5,
                     "Status of last visit" = 3)) %>% 
  kableExtra::footnote(general = str_glue("Data are current to {format(report_date, '%b %d %Y')}"))

```

## Matrices de transmission

Comme indiqué sur la page [Heat plots](#heatmaps), vous pouvez créer une matrice de "qui a infecté qui" en utilisant `geom_tile()`.

Lorsque de nouveaux contacts sont créés, Go.Data stocke ces informations de liens dans le lien `relationships` de l'API ; et nous pouvons voir les 50 premières lignes de cet ensemble de données ci-dessous. Cela signifie que nous pouvons créer un diagramme de chaleur avec relativement peu d'étapes étant donné que chaque contact est déjà joint à son cas source.

```{r, warning=F, message=F, echo=F}
# afficher les 50 premières lignes de données de relations sous forme de tableau
DT::datatable(head(relationships, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Comme nous l'avons fait ci-dessus pour la pyramide des âges comparant les cas et les contacts, nous pouvons sélectionner les quelques variables dont nous avons besoin et créer des colonnes avec des groupes d'âge catégoriques pour les sources (cas) et les cibles (contacts).

```{r}
heatmap_ages <- relationships %>% 
  select(source_age, target_age) %>% 
  mutate(                              # transmute est comme mutate() mais supprime toutes les autres colonnes
    source_age_class = epikit::age_categories(source_age, breakers = seq(0, 80, 5)),
    target_age_class = epikit::age_categories(target_age, breakers = seq(0, 80, 5))) 
```

Comme décrit précédemment, nous créons des tableaux croisés ;

```{r, warning=F, message=FALSE}

cross_tab <- table(
  source_cases = heatmap_ages$source_age_class,
  target_cases = heatmap_ages$target_age_class)

cross_tab
```

convertir en format long avec des proportions ;

```{r, warning=FALSE, message=FALSE}

long_prop <- data.frame(prop.table(cross_tab))

```

et créer une carte géographique pour l'âge.

```{r, warning=F, message=F}

ggplot(data = long_prop)+       # utiliser des données longues, avec des proportions comme Freq
  geom_tile(                    # visualisez-le en tuiles
    aes(
      x = target_cases,         # l'axe des x est l'âge du cas
      y = source_cases,     # l'axe y est l'âge de l'infecteur
      fill = Freq))+            # La couleur de la tuile est la colonne Freq dans les données
  scale_fill_gradient(          # ajuster la couleur de remplissage des tuiles
    low = "blue",
    high = "orange")+
  theme(axis.text.x = element_text(angle = 90))+
  labs(                         # labels
    x = "Target case age",
    y = "Source case age",
    title = "Who infected whom",
    subtitle = "Frequency matrix of transmission events",
    fill = "Proportion of all\ntranmsission events"     # legend title
  )

```

## Resources

<https://github.com/WorldHealthOrganization/godata/tree/master/analytics/r-reporting>

<https://worldhealthorganization.github.io/godata/>

<https://community-godata.who.int/>
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/contact_tracing.Rmd-->


# Analyse d'enquête {#survey_analysis}  

<!-- ======================================================= -->
## Aperçu { }

Cette page démontre l'utilisation de plusieurs packages pour l'analyse d'enquêtes. 



La plupart des paquets R d'enquête reposent sur le paquet [**survey**](https://cran.r-project.org/web/packages/survey/index.html) pour effectuer des analyses pondérées. 
Nous utiliserons **survey** ainsi que [**srvyr**](https://cran.r-project.org/web/packages/srvyr/index.html) (une enveloppe pour **survey** permettant un codage de type tidyverse) et [**gtsummary**](https://cran.r-project.org/web/packages/gtsummary/index.html) (une enveloppe pour **survey** permettant de créer des tableaux prêts à être publiés).Bien que le paquet original **survey** ne permette pas le codage de style tidyverse, tidyverse, il présente l'avantage supplémentaire d'autoriser les modèles linéaires généralisés pondérés par les pondérés par les enquêtes (qui seront ajoutés à cette page à une date ultérieure). Nous allons également démontrer l'utilisation d'une fonction du paquet [**sitrep**](https://github.com/R4EPI/sitrep) pour créer des poids d'échantillonnage (*n.b* ce paquet n'est pas encore sur CRAN, mais peut être installé à partir de github).


La plupart de cette page est basée sur le travail effectué pour le projet ["R4Epis"](https://r4epis.netlify.app/) ; pour le code détaillé et les modèles R-markdown, voir la page github ["R4Epis"] (https://github.com/R4EPI/sitrep). Une partie du code basé sur le paquet **survey** est basé sur les premières versions de [Études de cas EPIET](https://github.com/EPIET/RapidAssessmentSurveys).

Pour l'instant, cette page ne traite pas du calcul de la taille des échantillons ou de l'échantillonnage. Pour un calculateur de taille d'échantillon simple à utiliser, voir [OpenEpi](https://www.openepi.com/Menu/OE_Menu.htm). La page [GIS basics](https://epirhandbook.com/gis-basics.html) du manuel comportera éventuellement une section sur l'échantillonnage aléatoire spatial, et cette page comportera éventuellement une section sur les cadres d'échantillonnage. Cette page contiendra également une section sur les bases de sondage ainsi que sur le calcul de la taille des échantillons. 

1.  Données d'enquête 
2.  Temps d'observation 
3.  Pondération 
4.  Objets de la conception de l'enquête
5.  Analyse descriptive 
6.  Proportions pondérées
7.  Taux pondérés 


<!-- ======================================================= -->
## Préparation { }

### Paquets {.unnumbered}

Ce morceau de code montre le chargement des paquets nécessaires aux analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* le charge pour l'utiliser. Vous pouvez également charger des paquets avec `library()` depuis **base** R. Voir la page sur [R basics] pour plus d'informations sur les paquets R.  
Ici, nous démontrons également l'utilisation de la fonction `p_load_gh()` de **pacman** pour installer et charger un paquet de github qui n'a pas encore été publié sur CRAN. 

```{r}

## charger des paquets depuis CRAN
pacman::p_load(rio, # Importation du fichier
               here, # Localisation de fichiers
               tidyverse, # gestion des données + graphiques ggplot2
               tsibble, # gère les ensembles de données de séries temporelles
               survey, # pour les fonctions d'enquête
               srvyr, # wrapper dplyr pour le paquet d'enquête
               gtsummary, # wrapper pour le paquet d'enquête pour produire des tableaux
               apyramid, # un paquet dédié à la création de pyramides des âges
               patchwork, # pour combiner des ggplots
               ggforce # pour les tracés alluviaux/sankey
               ) 

## charger les paquets de github pour le temps d'observation / fonctions de pondération
if(!require(sitrep)){
   remotes::install_github("r4epi/sitrep")
   library(sitrep)
}




``` 

### Charger les données {.unnumbered}

L'ensemble de données d'exemple utilisé dans cette section :

- des données fictives d'enquête de mortalité.
- comptes de population fictifs pour la zone d'enquête. 
- Dictionnaire de données pour les données fictives de l'enquête sur la mortalité. 


Ceci est basé sur l'enquête pré-approuvée par le comité d'examen éthique de MSF OCA. Le site données fictives ont été produites dans le cadre du projet ["R4Epis"](https://r4epis.netlify.app/). 
Tout ceci est basé sur les données collectées à l'aide de [KoboToolbox](https://www.kobotoolbox.org/), qui est un logiciel de collecte de données basé sur [Open Data Kit](https://opendatakit.org/).

Kobo vous permet d'exporter à la fois les données collectées et le dictionnaire de données pour cet ensemble de données. Nous vous recommandons vivement de le faire, car cela simplifie le nettoyage des données et est utile pour rechercher des variables/questions. 


<span style="color : darkgreen ;">**_TIP:_** Le dictionnaire de données de Kobo comporte des noms de variables dans la colonne "nom" de la feuille d'enquête. Les valeurs possibles pour chaque variable sont spécifiées dans la feuille de choix. Dans la feuille de choix, "name" a la valeur abrégée et les colonnes "label::english" et "label::french" ont les versions longues appropriées. L'utilisation de la fonction `msf_dict_survey()` du paquet **epidict** pour importer un fichier Excel du dictionnaire Kobo sera reformaté pour vous afin de pouvoir l'utiliser facilement pour le recodage.  </span>

<span style="color : orange ;">**_CAUTION:_** Le jeu de données d'exemple n'est pas le même comme un export (comme dans Kobo vous exportez différents niveaux de questionnaire individuellement). 
- voir la section sur les données d'enquête ci-dessous pour fusionner les différents niveaux.</span>


Le jeu de données est importé à l'aide de la fonction `import()` du paquet **rio**. Consultez la page [Importation et exportation](#import_export) pour connaître les différentes façons d'importer des données.

```{r echo = FALSE}
# Importez l'enquête dans R
survey_data <- rio::import(here::here("data", "surveys", "survey_data.xlsx"))

# Importez le dictionnaire dans R
survey_dict <- rio::import(here::here("data", "surveys", "survey_dict.xlsx")) 

# Importez la population dans R 
population <- rio::import(here::here("data", "surveys", "population.xlsx"))
```

```{r eval = FALSE}
# Importez les données d'enquête
survey_data <- rio::import("survey_data.xlsx")

# Importez le dictionnaire dans R
survey_dict <- rio::import("survey_dict.xlsx") 
```

Les 10 premières lignes de l'enquête sont affichées ci-dessous.

```{r, message = FALSE, echo = FALSE}
# Affichez les données de l'enquête sous forme de tableau
DT::datatable(head(survey_data, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space : nowrap' )
```

Nous voulons également importer les données sur la population d'échantillonnage afin de pouvoir produire des pondérations appropriées. Ces données peuvent se présenter sous différents formats. Cependant, nous vous suggérons de les présenter comme suit (vous pouvez simplement les saisir dans un excel). 


```{r read_data_pop_show, eval = FALSE}
# Importez les données de la population
population <- rio::import("population.xlsx")
```

Les 10 premières lignes de l'enquête sont affichées ci-dessous.

```{r message=FALSE, echo=F}
# Affichez les données de l'enquête sous forme de tableau
DT::datatable(head(population, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space : nowrap' )
```

Pour les enquêtes en grappes, vous pouvez souhaiter ajouter des poids d'enquête au niveau de la grappe. Vous pouvez lire ces données comme ci-dessus. 
Alternativement, s'il n'y a que quelques comptages, ils peuvent être entrés comme suit dans un tibble. Dans tous les cas, vous aurez besoin d'avoir une colonne avec un identifiant de grappe qui correspondant à vos données d'enquête, et une autre colonne avec le nombre de ménages dans chaque grappe.

```{r cluster_counts}

## définir le nombre de ménages dans chaque cluster
cluster_counts <- tibble(cluster = c("village_1", "village_2", "village_3", "village_4", 
                                     "village_5", "village_6", "village_7", "village_8",
                                     "village_9", "village_10"), 
                         households = c(700, 400, 600, 500, 300, 
                                        800, 700, 400, 500, 500))

```

### Nettoyer les données {.unnumbered}

L'exemple ci-dessous permet de s'assurer que la colonne date est au bon format. Il existe plusieurs autres façons de procéder (voir la page [Travailler avec des dates](#working_dates) pour plus de détails), mais l'utilisation du dictionnaire pour définir les dates est rapide et facile. 

Nous créons également une variable de groupe d'âge en utilisant la fonction `age_categories()` de **epikit** - voir la section [Nettoyage de données et fonctions essentielles](#cleaning_data) pour plus de détails. De plus, nous créons une variable de caractère définissant dans quel district se trouvent les différents clusters. 

Enfin, nous recodons toutes les variables oui/non en variables VRAI/FAUX, sinon elles ne peuvent pas être utilisées par les fonctions de proportion **survey**. 

```{r nettoyage}

## sélectionne les noms de variables de date dans le dictionnaire 
DATEVARS <- survey_dict %>% 
  filter(type == "date") %>% 
  filter(name %in% names(survey_data)) %>% 
  ## filtre pour correspondre aux noms des colonnes de vos données
  pull(name) # sélectionne les variables de date
  
## changer en dates 
survey_data <- survey_data %>%
  mutate(across(all_of(DATEVARS), as.Date))


## ajouter ceux dont l'âge est uniquement en mois à la variable année (diviser par douze).
survey_data <- survey_data %>% 
  mutate(age_years = if_else(is.na(age_years), 
                             age_months / 12, 
                             age_years))

## définir la variable groupe d'âge
survey_data <- survey_data %>% 
     mutate(age_group = age_categories(age_years, 
                                    breakers = c(0, 3, 15, 30, 45)
                                    ))


## créer une variable caractère basée sur les groupes d'une autre variable 
survey_data <- survey_data %>% 
  mutate(health_district = case_when(
    cluster_number %in% c(1:5) ~ "district_a", 
    TRUE ~ "district_b"
  ))


## sélectionner les noms de variables oui/non dans le dictionnaire 
YNVARS <- survey_dict %>% 
  filter(type == "yn") %>% 
  filter(name %in% names(survey_data)) %>% 
  ## filtre pour correspondre aux noms des colonnes de vos données
  pull(name) # select yn vars
  
## changer en dates 
survey_data <- survey_data %>%
  mutate(across(all_of(YNVARS), 
                str_detect, 
                pattern = "yes"))

```



<!-- ======================================================= -->
## Données d'enquête { }

Il existe de nombreux plans d'échantillonnage différents qui peuvent être utilisés pour les enquêtes. Ici, nous allons démontrer le code pour : 
- Stratifié 
- en grappes 
- Stratifié et grappe 

Comme décrit ci-dessus (en fonction de la façon dont vous concevez votre questionnaire), les données de chaque niveau seront exportées comme un ensemble de données séparé de Kobo. Dans notre exemple, il y a
un niveau pour les ménages et un niveau pour les individus au sein de ces ménages. 

Ces deux niveaux sont liés par un identifiant unique. Pour un ensemble de données Kobo, cette variable est "_index" au niveau du ménage, qui correspond à "_parent_index" au niveau de l'individu.
Cela créera de nouvelles lignes pour le ménage avec chaque individu correspondant, voir la section du manuel sur [Joindre des données](#joining_matching) pour plus de détails. 

```{r merge_data_levels, eval = FALSE}

## Joignez les données des individus et des ménages pour former un ensemble de données complet.
survey_data <- left_join(survey_data_hh, 
                         survey_data_indiv,
                         by = c("_index" = "_parent_index"))


## créer un identifiant unique en combinant les index des deux niveaux 
survey_data <- survey_data %>% 
     mutate(uid = str_glue("{index}_{index_y}"))

```

<!-- ======================================================= -->
## Temps d'observation { }

Pour les enquêtes de mortalité, nous voulons maintenant savoir combien de temps chaque individu a été présent dans l'emplacement afin de pouvoir calculer un taux de mortalité approprié pour notre période d'intérêt. Ceci n'est pas pertinent pour toutes les enquêtes, mais particulièrement pour les enquêtes de mortalité, car elles sont fréquemment menées auprès de populations mobiles ou déplacées.

Pour ce faire, nous définissons d'abord notre période d'intérêt, également connue sous le nom de période de rappel (c'est-à-dire le moment où l'enquête est menée). Nous pouvons ensuite utiliser cette période pour définir des dates inappropriées comme manquantes, c'est-à-dire que si des décès sont signalés en dehors de la période d'intérêt. 

```{r recall_period}

## Définit le début/la fin de la période de rappel.
## peut être changé en variables de date provenant de l'ensemble de données 
## (par exemple, date d'arrivée et date du questionnaire)
survey_data <- survey_data %>% 
  mutate(recall_start = as.Date("2018-01-01"), 
         recall_end = as.Date("2018-05-01")
  )


# Définir les dates inappropriées à NA sur la base de règles 
## par exemple, arrivées avant le début, départs après la fin.
survey_data <- survey_data %>%
      mutate(
           arrived_date = if_else(arrived_date < recall_start, 
                                 as.Date(NA),
                                  arrived_date),
           birthday_date = if_else(birthday_date < recall_start,
                                  as.Date(NA),
                                  birthday_date),
           left_date = if_else(left_date > recall_end,
                              as.Date(NA),
                               left_date),
           death_date = if_else(death_date > recall_end,
                               as.Date(NA),
                               death_date)
           )

```


Nous pouvons ensuite utiliser nos variables de date pour définir les dates de début et de fin pour chaque individu. Nous pouvons utiliser la fonction `find_start_date()` de **sitrep** pour affiner les causes des les dates et ensuite utiliser cela pour calculer la différence entre les jours (personne-temps). 

Date de début : 
L'événement d'arrivée approprié le plus tôt dans votre période de rappel. Soit le début de votre période de rappel (que vous définissez à l'avance), soit une date après le début de la période de rappel, le cas échéant (par exemple, les arrivées ou les naissances).

Date de fin : 
Soit la fin de votre période de rappel, soit une date antérieure à la fin du rappel. 

```{r observation_time}

## Créer de nouvelles variables pour les dates/causes de début et de fin.
survey_data <- survey_data %>% 
     ## choisir la date la plus ancienne saisie dans l'enquête.
     ## à partir des naissances, des arrivées dans les ménages et des arrivées dans les camps. 
     find_start_date("birthday_date",
                  "arrived_date",
                  period_start = "recall_start",
                  period_end = "recall_end",
                  datecol = "startdate",
                  datereason = "startcause"
                 ) %>%
     ## choisir la date la plus ancienne saisie dans l'enquête
     ## à partir des départs du camp, des décès et de la fin de l'étude
     find_end_date("left_date",
                "death_date",
                period_start = "recall_start",
                period_end = "recall_end",
                datecol = "enddate",
                datereason = "endcause" 
               )


## étiqueter ceux qui étaient présents au début/à la fin (sauf les naissances/décès)
survey_data <- survey_data %>% 
     mutate(
       ## remplir la date de début pour qu'elle corresponde au début de la période de rappel (pour ceux qui sont vides) 
       startdate = if_else(is.na(startdate), recall_start, startdate), 
       ## définir la cause de début comme présente au début si elle est égale à la période de rappel 
       ## sauf si elle est égale à la date de naissance 
       startcause = if_else(startdate == recall_start & startcause != "birthday_date",
                              "Present at start", startcause), 
       ## remplir la date de fin pour qu'elle corresponde à la fin de la période de rappel (pour ceux qui sont vides) 
       enddate = if_else(is.na(enddate), recall_end, enddate), 
       ## définir la cause de fin comme étant présente à la fin si égale à la fin de rappel 
       ## sauf si elle est égale à la date de décès
       endcause = if_else(enddate == recall_end & endcause != "death_date", 
                            "Present at end", endcause))


## Définir la durée d'observation en jours
survey_data <- survey_data %>% 
  mutate(obstime = as.numeric(enddate - startdate))

```


<!-- ======================================================= -->
## Pondération { }

Il est important d'éliminer les observations erronées avant d'ajouter les poids de l'enquête. Par exemple, si vous avez des observations avec un temps d'observation négatif, vous devrez les vérifier (vous pouvez le faire avec la fonction `assert_positive_timespan()` de **sitrep**. Une autre chose est si vous voulez supprimer les lignes vides (par exemple avec `drop_na(uid)`) ou supprimer les doublons (voir la section du manuel sur la [déduplication](#deduplication) pour plus de détails). Ceux qui n'ont pas de consentement doivent aussi être supprimés. 

Dans cet exemple, nous filtrons les cas que nous voulons supprimer et les stockons dans un cadre de données séparé - de cette façon, nous pouvons décrire ceux qui ont été exclus de l'enquête. Nous utilisons ensuite la fonction `anti_join()` de **dplyr** pour supprimer ces cas exclus de nos données d'enquête. 

<span style="color : red ;">**_DANGER:_** Vous ne pouvez pas avoir de valeurs manquantes dans votre variable de poids, ni dans aucune des variables pertinentes pour le plan de sondage (par exemple, les variables d'âge, de sexe, de strates ou de grappes).</span>  

```{r remove_unused_data}

## stockez les cas que vous abandonnez afin de pouvoir les décrire (par exemple, non-consentant. 
## ou mauvais village/cluster)
dropped <- survey_data %>% 
  filter(!consent | is.na(startdate) | is.na(enddate) | village_name == "other")

## utiliser les cas abandonnés pour supprimer les lignes inutilisées de l'ensemble des données de l'enquête.  
survey_data <- anti_join(survey_data, dropped, by = names(dropped))

```

Comme mentionné ci-dessus, nous montrons comment ajouter des poids pour trois plans d'étude différents (stratifié, en grappe et en grappe stratifié). Ceux-ci nécessitent des informations sur la population source et/ou les grappes enquêtées. Nous utiliserons le code de la grappe stratifiée pour cet exemple, mais utilisez celui qui est le plus approprié à votre plan d'étude.


```{r survey_weights}

# stratified ------------------------------------------------------------------
# Créez une variable appelée "surv_weight_strata".
# contient les poids pour chaque individu - par groupe d'âge, sexe et district sanitaire.
survey_data <- add_weights_strata(x = survey_data,
                                         p = population,
                                         surv_weight = "surv_weight_strata",
                                         surv_weight_ID = "surv_weight_ID_strata",
                                         age_group, sex, health_district)

## cluster ---------------------------------------------------------------------

# obtient le nombre de personnes d'individus interrogés par ménage
# ajoute une variable avec les comptes de la variable index du ménage (parent)
survey_data <- survey_data %>%
  add_count(index, name = "interviewed")


## crée des poids de cluster
survey_data <- add_weights_cluster(x = survey_data,
                                          cl = cluster_counts,
                                          eligible = member_number,
                                          interviewed = interviewed,
                                          cluster_x = village_name,
                                          cluster_cl = cluster,
                                          household_x = index,
                                          household_cl = households,
                                          surv_weight = "surv_weight_cluster",
                                          surv_weight_ID = "surv_weight_ID_cluster",
                                          ignore_cluster = FALSE,
                                          ignore_household = FALSE)


# stratifié et cluster ------------------------------------------------------
# créer un poids d'enquête pour la grappe et les strates
survey_data <- survey_data %>%
  mutate(surv_weight_cluster_strata = surv_weight_strata * surv_weight_cluster)

```


<!-- ======================================================= -->
## Objets de conception d'enquête { }

Créez un objet d'enquête en fonction de la conception de votre étude. Utilisé de la même manière que les cadres de données pour calculer les proportions de poids, etc. Assurez-vous que toutes les variables nécessaires sont créées avant cela. 

Il y a quatre options, commentez celles que vous n'utilisez pas : 
- aléatoire simple 
- Stratifié 
- en grappe 
- Grappe stratifiée

Pour ce modèle, nous supposerons que nous regroupons les enquêtes dans deux strates distinctes (districts sanitaires A et B). Pour obtenir des estimations globales, nous devons donc combiner les poids des grappes et des strates. 

Comme nous l'avons mentionné précédemment, il existe deux paquets disponibles pour ce faire. Le classique est **survey** et il existe un paquetage appelé **srvyr** qui crée des objets et des fonctions adaptés à tidyverse. Nous ferons la démonstration des deux, mais notez que la plupart du code de ce chapitre utilisera des objets basés sur **srvyr**. La seule exception est que le paquet **gtsummary** n'accepte que les objets **survey**. 

### Paquet **Survey  

Le paquet **survey** utilise effectivement le codage **base** *R*, et il n'est donc pas possible d'utiliser les pipes (`%>%`) ou d'autres syntaxes **dplyr**. Avec le paquetage **survey**, nous utilisons la fonction `svydesign()` pour définir un objet d'enquête avec les clusters, les poids et les strates appropriés. 

<span style="color : black ;">**_NOTE:_** nous devons utiliser le tilde (`~`) devant les variables, ceci parce que le package utilise la syntaxe **base** *R* d'affectation des variables basée sur des formules. </span>

```{r survey_design}

# aléatoire simple ---------------------------------------------------------------
base_survey_design_simple <- svydesign(ids = ~1, # 1 pour aucun id de grappe
                   weights = NULL, # aucun poids ajouté
                   strata = NULL, # l'échantillonnage est simple (pas de strates)
                   data = survey_data # doit spécifier l'ensemble de données
                  )

## stratified ------------------------------------------------------------------
base_survey_design_strata <- svydesign(ids = ~1, # 1 pour aucun id de cluster
                   weights = ~surv_weight_strata, # variable de poids créée ci-dessus
                   strata = ~health_district, # l'échantillonnage a été stratifié par district
                   data = survey_data # il faut spécifier l'ensemble de données
                  )

# cluster ---------------------------------------------------------------------
base_survey_design_cluster <- svydesign(ids = ~village_name, # ids de cluster
                   weights = ~surv_weight_cluster, # variable de poids créée ci-dessus
                   strata = NULL, # l'échantillonnage était simple (pas de strates)
                   data = survey_data # il faut spécifier l'ensemble de données
                  )

# cluster stratifié ----------------------------------------------------------
base_survey_design <- svydesign(ids = ~village_name, # ids de cluster
                   weights = ~surv_weight_cluster_strata, # variable de poids créée ci-dessus
                   strata = ~health_district, # l'échantillonnage a été stratifié par district
                   data = survey_data # doit spécifier l'ensemble de données
                  )
```



### Paquet **Srvyr  

Avec le paquet **srvyr**, nous pouvons utiliser la fonction `as_survey_design()`, qui a les mêmes arguments que ci-dessus, mais autorise les tubes (`%>%`), et nous n'avons donc pas besoin d'utiliser le tilde (`%>%`). 

```{r survey_design_srvyr}
## aléatoire simple ---------------------------------------------------------------
survey_design_simple <- survey_data %>% 
  as_survey_design(ids = 1, # 1 pour aucun id de grappe 
                   weights = NULL, # Aucun poids ajouté
                   strata = NULL # l'échantillonnage était simple (pas de strates)
                  )
## stratified ------------------------------------------------------------------
survey_design_strata <- survey_data %>%
  as_survey_design(ids = 1, # 1 pour aucun id de cluster
                   weights = surv_weight_strata, # variable de poids créée ci-dessus
                   strata = health_district # l'échantillonnage a été stratifié par district
                  )
## cluster ---------------------------------------------------------------------
survey_design_cluster <- survey_data %>%
  as_survey_design(ids = village_name, # ids de la grappe
                   weights = surv_weight_cluster, # variable de poids créée ci-dessus
                   strata = NULL # l'échantillonnage était simple (pas de strates)
                  )

## cluster stratifié ----------------------------------------------------------
survey_design <- survey_data %>%
  as_survey_design(ids = village_name, # ids de la grappe
                   weights = surv_weight_cluster_strata, # variable de poids créée ci-dessus
                   strata = health_district # l'échantillonnage a été stratifié par district
                  )
```

<!-- ======================================================= -->
## Analyse descriptive { }

L'analyse descriptive et la visualisation de base sont traitées en détail dans d'autres chapitres du manuel, nous ne nous y attarderons donc pas ici. Pour plus de détails, voir les chapitres sur les [tableaux descriptifs](#descriptive_tables), [les tests statistiques](#stats_test), [les tableaux de présentation](#tables_presentation), [les bases du ggplot] (#ggplot_basics) et [rapports R markdown](#rmarkdown). 

Dans cette section, nous allons nous concentrer sur la manière d'étudier le biais dans votre échantillon et de le visualiser. Nous nous pencherons également sur la visualisation du flux de population dans le cadre d'une enquête à l'aide de diagrammes alluviaux/sankey.  

En général, vous devriez envisager d'inclure les analyses descriptives suivantes :  

- Le nombre final de grappes, de ménages et d'individus inclus.  
- Nombre d'individus exclus et les raisons de cette exclusion 
- Nombre médian (fourchette) de ménages par grappe et d'individus par ménage. 


### Biais d'échantillonnage 

Comparez les proportions dans chaque groupe d'âge entre votre échantillon et la population source. Ceci est important pour pouvoir mettre en évidence un éventuel biais d'échantillonnage. Vous pouvez également répéter cette opération en examinant les distributions par sexe. 

Notez que ces p-values ne sont qu'indicatives, et qu'une discussion descriptive (ou une visualisation avec les pyramides d'âge ci-dessous) des distributions dans votre échantillon d'étude par rapport à la population source est plus importante que le test binomial lui-même. Cela est dû au fait que l'augmentation de la taille de l'échantillon conduira le plus souvent à des différences qui peuvent ne pas être pertinentes après la pondération de vos données.

```{r descriptive_sampling_bias, warning = FALSE}

## dénombrements et saillies de la population étudiée.
ag <- survey_data %>% 
  group_by(age_group) %>% 
  drop_na(age_group) %>% 
  tally() %>% 
  mutate(proportion = n / sum(n), 
         n_total = sum(n))

## comptes et props de la population source
propcount <- population %>% 
  group_by(age_group) %>%
    tally(population) %>%
    mutate(proportion = n / sum(n))

## lier ensemble les colonnes de deux tables, regrouper par âge, et effectuer un 
## un test binomial pour voir si n/total est significativement différent de la population
## proportion.
  ## Le suffixe ajoute ici du texte à la fin des colonnes dans chacun des deux ensembles de données.
left_join(ag, propcount, by = "age_group", suffix = c("", "_pop")) %>%
  group_by(age_group) %>%
  ## broom::tidy(binom.test()) crée une trame de données à partir du test binomial et ## ajoutera les variables p.p. à la trame de données.
  ## ajoutera les variables p.value, parameter, conf.low, conf.high, method, and
  ## alternative. Nous n'utiliserons que p.value ici. Vous pouvez inclure d'autres
  ## d'autres colonnes si vous souhaitez faire état des intervalles de confiance.
  mutate(binom = list(broom::tidy(binom.test(n, n_total, proportion_pop)))) %>%
  unnest(cols = c(binom)) %>% # important pour l'expansion du cadre de données binom.test
  mutate(proportion_pop = proportion_pop * 100) %>%
  ## Ajustement des valeurs de p pour corriger les faux positifs. 
  ## (parce que l'on teste plusieurs groupes d'âge). Cela ne fera une 
  ## une différence que si vous avez plusieurs catégories d'âge
  mutate(p.value = p.adjust(p.value, method = "holm")) %>%
                      
  ## N'affichez que les valeurs de p supérieures à 0,001 (celles qui sont inférieures sont signalées comme <0,001)
  mutate(p.value = ifelse(p.value < 0.001, 
                          "<0.001", 
                          as.character(round(p.value, 3)))) %>% 
  
  ## renommez les colonnes de manière appropriée
  select(
    "Groupe d'âge" = age_group,
    "Population étudiée (n)" = n,
    "Population étudiée (%)" = proportion,
    "Population source (n)" = n_pop,
    "Population source (%)" = proportion_pop,
    "P-value" = p.value
  )
```



### Pyramides démographiques 

Les pyramides démographiques (ou âge-sexe) sont un moyen facile de visualiser la distribution dans la population de votre enquête. Il est également intéressant de créer des [tableaux descriptifs](#descriptive_tables) et le sexe par strates d'enquête. Nous allons démontrer l'utilisation du paquet **apyramide** car il permet de pondérées en utilisant notre objet de conception d'enquête créé ci-dessus. Autres options pour créer [pyramides démographiques](#age_pyramid) sont traitées en détail dans ce chapitre du manuel. Nous utiliserons également une fonction wrapper de **apyramid** appelée `age_pyramid()` qui permet de gagner quelque lignes de code pour produire un graphique avec des proportions. 

Comme pour le test binomial formel de différence, vu plus haut dans la section sur le biais d'échantillonnage, nous sommes intéressés ici à visualiser si notre population échantillonnée est sensiblement différente de la population source et si la pondération corrige cette différence. Pour ce faire, nous allons utiliser le paquet **patchwork** pour montrer nos visualisations **ggplot** côte à côte ; pour plus de détails, voir la section sur la combinaison de tracés dans le chapitre du manuel [Astuces de ggplot](#ggplot_tips) du manuel. Nous allons visualiser notre population source, notre population d'enquête non pondérée et notre population d'enquête pondérée. Vous pouvez également envisager de visualiser chaque strate de votre enquête, pars exemple ici, en utilisant l'argument `stack_by = "health_district"` (voir `?plot_age_pyramid` pour plus de détails). 

<span style="color : black ;">**_NOTE:_** Les axes x et y sont inversés dans les pyramides </span>.

```{r weighted_age_pyramid, warning = FALSE, message = FALSE, fig.show = "hold", fig.width = 15}

## définir les limites et les étiquettes de l'axe des x ---------------------------------------------
## (mettez à jour ces chiffres pour qu'ils correspondent aux valeurs de votre graphique)
max_prop <- 35 ## choisissez la plus haute proportion que vous voulez montrer 
step <- 5 # choisissez l'espace que vous voulez entre les étiquettes. 

## cette partie définit le vecteur en utilisant les nombres ci-dessus avec des ruptures d'axe.
breaks <- c(
    seq(max_prop/100 * -1, 0 - step/100, step/100), 
    0, 
    seq(0 + step / 100, max_prop/100, step/100)
    )

## cette partie définit le vecteur en utilisant les nombres ci-dessus avec les limites de l'axe
limits <- c(max_prop/100 * -1, max_prop/100)

## Cette partie définit le vecteur en utilisant les nombres ci-dessus avec les étiquettes d'axe.
labels <- c(
      seq(max_prop, step, -step), 
      0, 
      seq(step, max_prop, step)
    )


## créer des graphiques individuellement --------------------------------------------------

## tracer la population source 
## nb : cette population doit être réduite à la population globale (c'est-à-dire en enlevant les districts de santé).
source_population <- population %>%
  ## s'assurer que l'âge et le sexe sont des facteurs
  mutate(age_group = factor(age_group, 
                            levels = c("0-2", 
                                       "3-14", 
                                       "15-29",
                                       "30-44", 
                                       "45+")), 
         sex = factor(sex)) %>% 
  group_by(age_group, sex) %>% 
  ## additionner les comptes pour chaque district de santé 
  summarise(population = sum(population)) %>% 
  ## supprimer le regroupement pour pouvoir calculer la proportion globale
  ungroup() %>% 
  mutate(proportion = population / sum(population)) %>% 
  ## tracer la pyramide 
  age_pyramid(
            age_group = age_group, 
            split_by = sex, 
            count = proportion, 
            proportional = TRUE) +
  ## Afficher uniquement le libellé de l'axe des y (sinon répété dans les trois graphiques)
  labs(title = "Population source", 
       y = "", 
       x = "Groupe d'âge (années)") + 
  ## rendre l'axe des x identique pour tous les graphiques 
  scale_y_continuous(breaks = breaks, 
    limits = limits, 
    labels = labels)
  
  
## Tracez l'échantillon de population non pondéré 
sample_population <- age_pyramid(survey_data, 
                 age_group = "age_group", 
                 split_by = "sex",
                 proportion = TRUE) + 
  ## Afficher uniquement le libellé de l'axe des x (sinon répété dans les trois graphiques)
  labs(title = "Population échantillon non pondérée", 
       y = "Proportion (%)", 
       x = "") + 
  ## rendre l'axe des x identique pour tous les graphiques 
  scale_y_continuous(breaks = breaks, 
    limits = limits, 
    labels = labels)


## tracer la population de l'échantillon pondéré 
weighted_population <- survey_design %>% 
  ## S'assurer que les variables sont des facteurs
  mutate(age_group = factor(age_group), 
         sex = factor(sex)) %>%
  age_pyramid(
    age_group = "age_group",
    split_by = "sex", 
    proportion = TRUE) +
  ## Afficher uniquement le libellé de l'axe des x (sinon répété dans les trois graphiques)
  labs(title = "Echantillon de population pondéré", 
       y = "", 
       x = "") + 
  ## rendre l'axe des x identique pour tous les graphiques 
  scale_y_continuous(breaks = breaks, 
    limits = limits, 
    labels = labels)

## combine les trois tracés ----------------------------------------------------
## Combinez trois tracés côte à côte en utilisant + 
source_population + sample_population + weighted_population + 
  ## ne montrer qu'une seule légende et définir le thème 
  ## notez l'utilisation de & pour combiner le thème avec plot_layout()
  plot_layout(guides = "collect") & 
  theme(legend.position = "bottom", # déplace la légende vers le bas
        legend.title = element_blank(), # supprimer le titre
        text = element_text(size = 18), # change la taille du texte
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1) # tourner le texte de l'axe x
       )
```


### Diagramme alluvial/sankey

La visualisation des points de départ et des résultats pour les individus peut être très utile pour obtenir une vue d'ensemble. L'application est évidente pour les populations mobiles, mais il existe de nombreuses autres applications telles que les cohortes ou toute autre situation
où il y a des transitions d'états pour les individus. Ces diagrammes ont plusieurs noms différents, y compris alluvial, sankey et ensembles parallèles - les détails sont dans le chapitre du manuel consacré aux [diagrammes et graphiques](#diagrams). 


```{r visualise_population_flow}

## résumer les données
flow_table <- survey_data %>%
  count(startcause, endcause, sex) %>% # obtenir des comptages 
  gather_set_data(x = c("startcause", "endcause")) # changer de format pour le tracé



## Tracez votre ensemble de données 
  ## sur l'axe des x, les causes de début et de fin.
  ## gather_set_data génère un ID pour chaque combinaison possible.
  ## La division par y donne les combinaisons possibles de début et de fin.
  ## la valeur n donne les comptes (peut aussi être changée en proportion).
ggplot(flow_table, aes(x, id = id, split = y, value = n)) +
  ## colorer les lignes par sexe 
  geom_parallel_sets(aes(fill = sex), alpha = 0.5, axis.width = 0.2) + ### remplir les cases d'étiquettes en gris.
  ## remplir les cases d'étiquettes en gris
  geom_parallel_sets_axes(axis.width = 0.15, fill = "grey80", color = "grey80") + ## changer la couleur du texte et l'angle de l'étiquette.
  ## changer la couleur et l'angle du texte (doit être ajusté)
  geom_parallel_sets_labels(color = "black", angle = 0, size = 5) + ## ajuster la couleur et l'angle du texte (doit être ajusté)
  ## suppression des étiquettes d'axe
  theme_void()+
  theme(legend.position = "bottom")

```


<!-- ======================================================= -->
## Proportions pondérées { }

Cette section détaillera comment produire des tableaux pour les effectifs et les proportions pondérés, avec les intervalles de confiance associés et l'effet de plan. Il existe quatre options différentes utilisant les fonctions des paquets suivants : **survey**, **srvyr**, **sitrep** et **gtsummary**. Pour un codage minimal permettant de produire un tableau standard de style épidémiologique, nous recommandons la fonction **sitrep** - qui est un wrapper pour le code **srvyr** ; notez cependant que ce n'est pas encore sur CRAN et que cela peut changer dans le futur. Autrement, le code **survey** est susceptible d'être le plus stable à long terme, alors que **srvyr** s'intégrera le mieux dans les flux de travail de tidyverse. Bien que les fonctions **gtsummary** ont beaucoup de potentiel, elles semblent expérimentales et incomplètes au moment de la rédaction. 


### Paquet **Survey 

Nous pouvons utiliser la fonction `svyciprop()` de **survey** pour obtenir des proportions pondérées et les intervalles de confiance à 95% qui les accompagnent. Il est intéressant de noter que `svyprop()` ne semble accepter que les variables comprises entre 0 et 1 (ou VRAI/FAUX), donc les variables catégorielles ne fonctionneront pas.

<span style="color : black ;">**_NOTE:_** Les fonctions de **survey** acceptent également les objets de conception **srvyr**, mais ici nous avons utilisé l'objet de conception **survey** juste pour la cohérence </span>.


```{r survey_props}

## produire des comptes pondérés 
svytable(~died, base_survey_design)

## produit des proportions pondérées
svyciprop(~died, base_survey_design, na.rm = T)

## obtenir l'effet du plan de sondage 
svymean(~died, base_survey_design, na.rm = T, deff = T) %>% 
  deff()

```

Nous pouvons combiner les fonctions de **survey** présentées ci-dessus en une fonction que nous définissons nous-mêmes ci-dessous, appelée `svy_prop` ; et nous pouvons alors utiliser cette fonction avec `map()` du paquetage purrr pour itérer sur plusieurs variables et créer un tableau. Voir le chapitre du manuel [itération](#iteration) pour plus de détails sur **purrr**. 

```{r survey_prop_fun}
# Définissez la fonction permettant de calculer les effectifs pondérés, les proportions, l'IC et l'effet de plan.
# x est la variable entre guillemets 
# design est votre objet de conception d'enquête

svy_prop <- function(design, x) {
  
  ## mettre la variable d'intérêt dans une formule 
  form <- as.formula(paste0( "~" , x))
  ## garder seulement la colonne VRAIE des comptes de svytable
  weighted_counts <- svytable(form, design)[[2]]
  ## calculer les proportions (multiplier par 100 pour obtenir des pourcentages).
  weighted_props <- svyciprop(form, design, na.rm = TRUE) * 100
  ## extraire les intervalles de confiance et les multiplier pour obtenir des pourcentages.
  weighted_confint <- confint(weighted_props) * 100
  ## utiliser svymean pour calculer l'effet de plan et ne garder que la colonne TRUE.
  design_eff <- deff(svymean(form, design, na.rm = TRUE, deff = TRUE))[[TRUE]]
  
  ## combiner en un seul cadre de données
  full_table <- cbind(
    "Variable" = x,
    "Count" = weighted_counts,
    "Proportion" = weighted_props,
    weighted_confint, 
    "Design effect" = design_eff
    )
  
  ## Retourner le tableau sous forme de cadre de données
  full_table <- data.frame(full_table, 
             ## supprimer les noms de variables des lignes (c'est une colonne séparée maintenant)
             row.names = NULL)
  
  ## Remplacer les valeurs numériques par des valeurs numériques
  full_table[ , 2:6] <- as.numeric(full_table[ , 2:6])
  
  ## Retourner le cadre de données
  full_table
}

## itérer sur plusieurs variables pour créer un tableau 
purrr::map(
  ## définir les variables d'intérêt
  c("left", "died", "arrived"), 
  ## déclarer la fonction utilisée et les arguments pour cette fonction (design)
  svy_prop, design = base_survey_design) %>% 
  ## réduire la liste à un seul cadre de données
  bind_rows() %>% 
  ## round 
  mutate(across(where(is.numeric), round, digits = 1))

```



### Paquet **Srvyr 

Avec **srvyr**, nous pouvons utiliser la syntaxe **dplyr** pour créer une table. Notez que la méthode fonction `survey_mean()` est utilisée et que l'argument de proportion est spécifié, ainsi que également que la même fonction est utilisée pour calculer l'effet de plan. Ceci est dû au fait que **srvyr** englobe les deux fonctions du paquetage **survey**, `svyciprop()` et `svymean()`, qui sont utilisées dans la section ci-dessus. 

<span style="color : black ;">**_NOTE:_** Il ne semble pas non plus possible d'obtenir des proportions à partir de variables catégorielles en utilisant **srvyr**, si vous en avez besoin, consultez la section ci-dessous utilisant **sitrep** </span>.

```{r srvyr_prop}

## utiliser l'objet de conception srvyr
survey_design %>% 
  summarise(
    ## produire les comptes pondérés 
    counts = survey_total(died), 
    ## produire les proportions pondérées et les intervalles de confiance 
    ## multiplier par 100 pour obtenir un pourcentage 
    props = survey_mean(died, 
                        proportion = TRUE, 
                        vartype = "ci") * 100, 
    ## produire l'effet de plan 
    deff = survey_mean(died, deff = TRUE)) %>% 
  ## conserver uniquement les lignes d'intérêt
  ## (supprimez les erreurs standard et répétez le calcul des proportions)
  select(counts, props, props_low, props_upp, deff_deff)

```

Là encore, nous pourrions écrire une fonction pour itérer sur plusieurs variables en utilisant le paquet **purrr**. Voir le chapitre du manuel [itération](#iteration) pour plus de détails sur **purrr**. 

```{r srvyr_prop_fun}

# Définit la fonction permettant de calculer les effectifs pondérés, les proportions, l'IC et l'effet du plan de sondage.
# design est l'objet de votre plan de sondage
# x est la variable entre guillemets 


srvyr_prop <- function(design, x) {
  
  summarise(
    ## utiliser l'objet du plan de sondage
    design, 
    ## produire les comptes pondérés 
    counts = survey_total(.data[[x]]), 
    ## produire les proportions pondérées et les intervalles de confiance 
    ## multiplier par 100 pour obtenir un pourcentage 
    props = survey_mean(.data[[x]], 
                        proportion = TRUE, 
                        vartype = "ci") * 100, 
    ## produire l'effet de plan 
    deff = survey_mean(.data[[x]], deff = TRUE)) %>% 
  ## ajouter le nom de la variable
  mutate(variable = x) %>% 
  ## ne conserve que les lignes d'intérêt
  ## (supprimez les erreurs standard et répétez le calcul des proportions)
  select(variable, counts, props, props_low, props_upp, deff_deff)
  
}
  

## itérer sur plusieurs variables pour créer un tableau 
purrr::map(
  ## définir les variables d'intérêt
  c("left", "died", "arrived"), 
  ## déclarer la fonction utilisée et les arguments pour cette fonction (design)
  ~srvyr_prop(.x, design = survey_design)) %>% 
  ## réduire la liste à un seul cadre de données
  bind_rows()
  

```



### Paquet **Sitrep 

La fonction `tab_survey()` de **sitrep** est une enveloppe pour **srvyr**, vous permettant de créer des tableaux pondérés avec un codage minimal. Elle vous permet également de calculer proportions pondérées pour les variables catégorielles. 

```{r sitrep_props}

## utilisation de l'objet survey design
survey_design %>% 
  ## passe les noms des variables d'intérêt sans les citer
  tab_survey(arrived, left, died, education_level,
             deff = TRUE, # calculer l'effet du plan de sondage
             pretty = TRUE # fusionner la proportion et le 95%CI
             )

```



### Paquet **Gtsummary

Avec **gtsummary**, il ne semble pas y avoir de fonctions intégrées pour ajouter des intervalles de confiance ou l'effet de plan. Ici nous montrons comment définir une fonction pour ajouter des intervalles de confiance et ensuite ajouter des intervalles de confiance à une table **gtsummary** créée en utilisant la fonction `tbl_svysummary()`.  


```{r gtsummary_table}


confidence_intervals <- function(data, variable, by, ...) {
  
  ## extraire les intervalles de confiance et les multiplier pour obtenir des pourcentages.
  props <- svyciprop(as.formula(paste0( "~" , variable)),
              data, na.rm = TRUE)
  
  ## Extraire les intervalles de confiance 
  as.numeric(confint(props) * 100) %>% ### rendre numérique et multiplier pour le pourcentage
    round(., digits = 1) %>% ## arrondir à un chiffre
    c(.) %>% ## extraire les chiffres de la matrice
    paste0(., collapse = "-") ## combine en un seul caractère
}

## utiliser l'objet de conception du paquet d'enquêtes
tbl_svysummary(base_survey_design, 
               include = c(arrived, left, died), ## définir les variables à inclure
               statistic = list(everything() ~ c("{n} ({p}%)"))) %>% ## définir les statistiques d'intérêt
  add_n() %>% ## ajoute le total pondéré 
  add_stat(fns = everything() ~ confidence_intervals) %>% ## ajouter les ICs
  ## modifier les en-têtes de colonnes
  modify_header(
    list(
      n ~ "**Total pondéré (N)**",
      stat_0 ~ "**Compte pondéré**",
      add_stat_1 ~ "**95%CI**"
    )
    )

```



<!-- ======================================================= -->
## Ratios pondérés { }

De même, pour les ratios pondérés (comme pour les ratios de mortalité), vous pouvez utiliser le paquetage **survey** ou le paquet **srvyr**. Vous pouvez également écrire des fonctions (similaires à celles ci-dessus) pour itérer sur plusieurs variables. Vous pourriez également créer une fonction pour **gtsummary** comme ci-dessus mais actuellement, elle n'a pas de fonctionnalité intégrée. 


### Paquet **Survey 

```{r survey_ratio}

ratio <- svyratio(~died, 
         denominator = ~obstime, 
         design = base_survey_design)

ci <- confint(ratio)

cbind(
  ratio$ratio * 10000, 
  ci * 10000
)

```


### **Paquet SRVYR** (paquet) 

```{r srvyr_ratio}

survey_design %>% 
  ### ratio d'enquête utilisé pour tenir compte du temps d'observation 
  summarise(
    mortality = survey_ratio(
      as.numeric(died) * 10000, 
      obstime, 
      vartype = "ci")
    )

```




<!-- ======================================================= -->
## Ressources { }

[Page des statistiques de l'UCLA](https://stats.idre.ucla.edu/r/seminars/survey-data-analysis-with-r/)  

[Analyse des données d'enquête gratuite](http://asdfree.com/)  

[paquet srvyr](http://gdfe.co/srvyr/)  

[paquet gtsummary](http://www.danieldsjoberg.com/gtsummary/reference/index.html) 

[Études de cas de l'enquête EPIET](https://github.com/EPIET/RapidAssessmentSurveys)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/survey_analysis.Rmd-->


# Analyse de survie {#survival_analysis}  


```{r out.width = c('75%'), fig.align='center', fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "survival_analysis.png"))
```

<!-- ======================================================= -->
## Aperçu {}


*L'analyse de survie* s'attache à décrire pour un individu ou un groupe d'individus donné, un point d'événement défini appelé **_l'échec_** (apparition d'une maladie, guérison d'une maladie, décès, rechute après réponse à un traitement...) qui survient après une période de temps appelée **_le temps d'échec_** (ou **_le temps de suivi_** dans les études de cohorte/population) pendant laquelle les individus sont observés. Pour déterminer le temps d'échec, il est alors nécessaire de définir un temps d'origine (qui peut être la date d'inclusion, la date du diagnostic...). 

La cible d'inférence de l'analyse de survie est alors le temps entre une origine et un événement. Dans la recherche médicale actuelle, elle est largement utilisée dans les études cliniques pour évaluer l'effet d'un traitement par exemple, ou en épidémiologie du cancer pour évaluer une grande variété de mesures de survie au cancer. 


Elle s'exprime généralement par la **_probabilité de survie_** qui est la probabilité que l'événement d'intérêt ne se soit pas produit avant une durée t.


**_Censure_** : La censure se produit lorsqu'à la fin du suivi, certains des individus n'ont pas eu l'événement d'intérêt, et donc leur temps réel jusqu'à l'événement est inconnu. Nous nous concentrerons principalement sur la censure à droite ici, mais pour plus de détails sur la censure et l'analyse de survie en général, vous pouvez consulter les références. 


```{r echo=F, eval=F, out.width = "80%", out.height="80%", fig.align = "center"}
 
#Ajouter une figure à partir des chunks suivants pour la dernière version de la page
#N'oubliez pas de sauvegarder la figure de sortie dans "images".
# knitr::include_graphics(here::here("images", "survanalysis.png"))

```  

<!-- ======================================================= -->
## Préparation { }

### Chargement des paquets {.unnumbered}  

Pour effectuer des analyses de survie dans R, un des paquets les plus utilisés est le paquet **survival**. Nous l'installons d'abord et le chargeons ensuite, ainsi que les autres paquets qui seront utilisés dans cette section :

Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* le charge pour l'utiliser. Vous pouvez aussi charger les paquets installés avec `library()` de **base** R. Voir la page sur [R basics] pour plus d'informations sur les paquets R.  

```{r, echo=F, message=FALSE, warning=FALSE}

# installer/charger les différents paquets nécessaires à cette page
pacman::p_load(
  rio, # importation de données  
  here, # chemins d'accès aux fichiers relatifs  
  janitor, # tabulations
  SemiCompRisks, # exemples d'ensembles de données et outils avancés pour travailler avec des données de Risques Semi-Compétents
  tidyverse, # manipulation et visualisation des données
  Epi, # analyses statistiques dans Epi
  survival, # analyse de survie
  survminer # analyse de survie : courbes KM avancées
)


```


Cette page explore les analyses de survie en utilisant la linelist utilisée dans la plupart des pages précédentes et sur laquelle nous appliquons quelques changements pour avoir des données de survie correctes.


### Importation du jeu de données {.unnumbered}  

Nous importons le jeu de données des cas d'une épidémie d'Ebola simulée. Si vous voulez suivre le mouvement, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la linelist "propre"</a> (en tant que fichier .rds). Importez des données avec la fonction `import()` du paquet **rio** (elle gère de nombreux types de fichiers comme .xlsx, .csv, .rds - voir la page [Importation et exportation](#import_export) pour plus de détails).  

```{r echo=F}
# import linelist
linelist_case_data <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r eval=F}
# importation de linelist
linelist_case_data <- rio::import("linelist_cleaned.rds")
```

### Gestion et transformation des données {.unnumbered}

En bref, les données de survie peuvent être décrites comme ayant les trois caractéristiques suivantes :

1) la variable dépendante ou réponse est le temps d'attente jusqu'à l'occurrence d'un événement bien défini,
2) les observations sont censurées, en ce sens que pour certaines unités, l'événement d'intérêt ne s'est pas produit au moment où les données sont analysées, et 
3) il existe des prédicteurs ou des variables explicatives dont nous souhaitons évaluer ou contrôler l'effet sur le temps d'attente. 

Ainsi, nous allons créer les différentes variables nécessaires pour respecter cette structure et effectuer l'analyse de survie.

Nous définissons

- un nouveau cadre de données `linelist_surv` pour cette analyse  
- notre événement d'intérêt comme étant le "décès" (donc notre probabilité de survie sera la probabilité d'être en vie après un certain temps après le moment d'origine),
- le temps de suivi (`futime`) comme le temps entre le moment de l'apparition et le moment du résultat *en jours*,
- les patients censurés comme ceux qui se sont rétablis ou pour lesquels le résultat final n'est pas connu, c'est-à-dire que l'événement "décès" n'a pas été observé (`event=0`).

<span style="color : orange ;">**_CAUTION:_** Puisque dans une étude de cohorte réelle, l'information sur le moment de l'origine et la fin du suivi est connue étant donné que les individus sont observés, nous éliminerons les observations où la date d'apparition ou la date de l'issue est inconnue. De même, les cas où la date d'apparition est postérieure à la date de l'issue seront supprimés car ils sont considérés comme erronés.</span>

<span style="color : darkgreen ;">**_TIP:_** Étant donné que le filtrage sur une date supérieure à (>) ou inférieure à (<) peut supprimer les lignes avec des valeurs manquantes, l'application du filtre sur les mauvaises dates supprimera également les lignes avec des dates manquantes.</span>

Nous utilisons ensuite `case_when()` pour créer une colonne `age_cat_small` dans laquelle il n'y a que 3 catégories d'âge.

```{r }
#Créer une nouvelle donnée appelée linelist_surv à partir de la donnée linelist_case_data.

linelist_surv <- linelist_case_data %>% 
     
  dplyr::filter(
       # supprimez les observations dont la date d'apparition ou la date d'issue est erronée ou manquante.
       date_outcome > date_onset) %>% 
  
  dplyr::mutate(
       # créer la var événement qui vaut 1 si le patient est décédé et 0 s'il a été censuré à droite
       event = ifelse(is.na(outcome) | outcome == "Recover", 0, 1), 
    
       # créer la var sur le temps de suivi en jours
       futime = as.double(date_outcome - date_onset), 
    
       # créer une nouvelle variable de catégorie d'âge avec seulement 3 niveaux de strates
       age_cat_small = dplyr::case_when( 
            age_years < 5 ~ "0-4",
            age_years >= 5 & age_years < 20 ~ "5-19",
            age_years >= 20 ~ "20+"),
       
       # l'étape précédente a créé la var age_cat_small en tant que caractère.
       # maintenant le convertir en facteur et spécifier les niveaux.
       # Notez que les valeurs NA restent des NA et ne sont pas mises dans un niveau "inconnu" par exemple,
       # puisque dans les prochaines analyses, elles devront être supprimées.
       age_cat_small = fct_relevel(age_cat_small, "0-4", "5-19", "20+")
       )
```


<span style="color : darkgreen ;">**_TIP:_** Nous pouvons vérifier les nouvelles colonnes que nous avons créées en faisant un résumé sur le `futime` et un tableau croisé entre `event` et `outcome` à partir duquel il a été créé. Outre cette vérification, c'est une bonne habitude de communiquer la durée médiane de suivi lors de l'interprétation des résultats de l'analyse de survie.</span>

```{r }

summary(linelist_surv$futime)

# croiser les tableaux de la nouvelle var événement et de la var résultat à partir de laquelle elle a été créée.
# pour s'assurer que le code a fait ce qu'il était censé faire.
linelist_surv %>% 
  tabyl(outcome, event)
```

Maintenant, nous croisons la nouvelle var age_cat_small et l'ancienne col age_cat pour nous assurer que les affectations sont correctes.  

```{r}
linelist_surv %>% 
  tabyl(age_cat_small, age_cat)
```

Maintenant, nous examinons les 10 premières observations des données `linelist_surv` en regardant des variables spécifiques (y compris celles nouvellement créées).  


```{r}
linelist_surv %>% 
  select(case_id, age_cat_small, date_onset, date_outcome, outcome, event, futime) %>% 
  head(10)
```

Nous pouvons aussi croiser les colonnes `age_cat_small` et `gender` pour avoir plus de détails sur la distribution de cette nouvelle colonne par sexe. Nous utilisons `tabyl()` et les fonctions *adorn* de **janitor** comme décrit dans la page [Descriptive tables]. 

<!-- Pour cela, nous utilisons la fonction `stat.table()` du paquet **Epi**. -->

```{r}

linelist_surv %>% 
  tabyl(gender, age_cat_small, show_na = F) %>% 
  adorn_totals(where = "both") %>% 
  adorn_percentages() %>% 
  adorn_pct_formatting() %>% 
  adorn_ns(position = "front")

```

<!-- Epi::stat.table( -->
<!-- #donner les variables pour le tableau croisé -->
<!-- list( -->
<!-- sexe, -->
<!-- age_cat_small -->
<!-- ), -->

<!-- #précisez la fonction que vous voulez appeler (mean,count..) -->
<!-- list( -->
<!-- count(), -->
<!-- pourcentage(âge_cat_petit) -->
<!-- ), -->

<!-- #add margins -->
<!-- margins=T, -->

<!-- #data used -->
<!-- data = linelist_surv -->
<!-- ) -->

<!-- ``` -->


<!-- ======================================================= -->
## Bases de l'analyse de survie {}


### Construction d'un objet de type surv {.unnumbered}

Nous allons d'abord utiliser `Surv()` de **survival** pour construire un objet de type survie à partir des colonnes de temps de suivi et d'événement.  

Le résultat d'une telle étape est de produire un objet de type *Surv* qui condense les informations de temps et si l'événement d'intérêt (le décès) a été observé. Cet objet sera finalement utilisé dans le côté droit des formules de modèle suivantes (voir [documentation](https://cran.r-project.org/web/packages/survival/vignettes/survival.pdf)).  


```{r survobj }
# Utilisez la syntaxe Suv() pour les données censurées à droite
survobj <- Surv(time = linelist_surv$futime,
                event = linelist_surv$event)
```

<!-- ```{r} -->
<!-- survobj <- with(linelist_surv, -->

<!-- survie::Surv(futime, event) -->

<!-- ) -->
<!-- ``` -->


Pour revoir, voici les 10 premières lignes des données `linelist_surv`, en ne visualisant que certaines colonnes importantes.  

```{r}
linelist_surv %>% 
  select(case_id, date_onset, date_outcome, futime, outcome, event) %>% 
  head(10)
```

Et voici les 10 premiers éléments de `survobj`. Il s'imprime essentiellement comme un vecteur de temps de suivi, avec "+" pour représenter si une observation a été censurée à droite. Voyez comment les chiffres s'alignent au-dessus et en dessous.  

```{r}
#imprimez les 50 premiers éléments du vecteur pour voir comment il se présente
head(survobj, 10)
```


### Exécution des analyses initiales {.unnumbered}

Nous commençons ensuite notre analyse en utilisant la fonction `survfit()` pour produire un objet *survfit*, qui s'adapte aux calculs par défaut pour les estimations **_Kaplan Meier_** (KM) de la courbe de survie globale (marginale), qui sont en fait une fonction échelon avec des sauts aux moments des événements observés. L'objet final *survfit* contient une ou plusieurs courbes de survie et est créé en utilisant l'objet *Surv* comme variable de réponse dans la formule du modèle.  

<span style="color : black ;">**_NOTE:_** L'estimation de Kaplan-Meier est une estimation non paramétrique du maximum de vraisemblance (MLE) de la fonction de survie. (voir les ressources pour plus d'informations).</span>

Le résumé de cet objet *survfit* donnera ce que l'on appelle une *table de survie*. Pour chaque pas de temps du suivi (`temps`) où un événement s'est produit (par ordre croissant) :  

* le nombre de personnes qui étaient à risque de développer l'événement (les personnes qui n'ont pas encore eu l'événement ou qui ont été censurées : `n.risk`)  
* ceux qui ont développé l'événement (`n.event`)  
* et à partir de ce qui précède : la probabilité de *ne pas* développer l'événement (probabilité de ne pas mourir, ou de survivre au-delà de ce moment spécifique).  
* enfin, l'erreur standard et l'intervalle de confiance pour cette probabilité sont dérivés et affichés.  

Nous ajustons les estimations de la GC en utilisant la formule où l'objet "survobj" précédemment survécu est la variable de réponse. "~ 1" précise que nous exécutons le modèle pour la survie globale.  

```{r fit}
# ajuster les estimations KM en utilisant une formule où l'objet Surv "survobj" est la variable de réponse.
# "~ 1" signifie que nous exécutons le modèle pour la survie globale.  
linelistsurv_fit <- survival::survfit(survobj ~ 1)

#imprimez son résumé pour plus de détails
summary(linelistsurv_fit)

```


En utilisant `summary()`, nous pouvons ajouter l'option `times` et spécifier certaines heures auxquelles nous voulons voir les informations de survie. 

```{r print_spec_times}

#imprime son résumé à des moments précis
summary(linelistsurv_fit, times = c(5,10,20,30,60))

```


Nous pouvons également utiliser la fonction `print()`. L'argument `print.rmean = TRUE` permet d'obtenir le temps de survie moyen et son erreur standard (se).

<span style="color : black ;">**_NOTE:_** La durée moyenne de survie restreinte (RMST) est une mesure de survie spécifique de plus en plus utilisée dans l'analyse de survie des cancers et qui est souvent définie comme l'aire sous la courbe de survie, étant donné que nous observons les patients jusqu'au temps restreint T (plus de détails dans la section Ressources).


```{r, mean_survtime}
# Imprimez l'objet linelistsurv_fit avec le temps de survie moyen et son se. 
print(linelistsurv_fit, print.rmean = TRUE)

```


<span style="color : darkgreen ;">**_TIP:_** Nous pouvons créer *l'objet surv* directement dans la fonction `survfit()` et économiser une ligne de code. Cela ressemblera alors à : `linelistsurv_quick <- survfit(Surv(futime, event) ~ 1, data=linelist_surv)`.</span>


### Risque cumulé {.unnumbered}  

Outre la fonction `summary()`, nous pouvons également utiliser la fonction `str()` qui donne plus de détails sur la structure de l'objet `survfit()`. Il s'agit d'une liste de 16 éléments.  

Parmi ces éléments, il y en a un important : `cumhaz`, qui est un vecteur numérique. Il pourrait être tracé pour permettre de montrer le **_danger cumulatif_**, le **_danger_** étant le **_taux instantané d'occurrence de l'événement_** (voir références).

```{r fit_struct}

str(linelistsurv_fit)

```

<!-- ======================================================= -->
### Tracer les courbes de Kaplan-Meir {.unnumbered}

Une fois les estimations KM ajustées, nous pouvons visualiser la probabilité d'être en vie à un moment donné en utilisant la fonction de base `plot()` qui dessine la "courbe de Kaplan-Meier". En d'autres termes, la courbe ci-dessous est une illustration classique de l'expérience de survie dans l'ensemble du groupe de patients.

Nous pouvons rapidement vérifier le temps de suivi min et max sur la courbe.  

Une manière simple d'interpréter est de dire qu'au temps zéro, tous les participants sont encore en vie et que la probabilité de survie est alors de 100%. Cette probabilité diminue au fil du temps, à mesure que les patients meurent. La proportion de participants survivant après 60 jours de suivi est d'environ 40 %.

```{r }

plot(linelistsurv_fit, 
     xlab = "Days of follow-up", # étiquette de l'axe des x
     ylab="Probabilité de survie", # étiquette de l'axe des y
     main= "Courbe de survie globale" # titre de la figure
     )

```

L'intervalle de confiance des estimations de survie KM est également tracé par défaut et peut être écarté en ajoutant l'option `conf.int = FALSE` à la commande `plot()`.

Puisque l'événement d'intérêt est la "mort", dessiner une courbe décrivant les compléments des proportions de survie conduira à dessiner les proportions de mortalité cumulées. Ceci peut être fait avec `lines()`, qui ajoute des informations à un graphique existant.  


```{r}

# tracé original
plot(
  linelistsurv_fit,
  xlab = "Jours de suivi",       
  ylab = "Probabilité de survie",       
  mark.time = TRUE, # marque les événements sur la courbe : un "+" est imprimé à chaque événement
  conf.int = FALSE, # ne pas tracer l'intervalle de confiance
  main = "Courbe de survie globale et mortalité cumulée"
  )

# Dessinez une courbe supplémentaire au tracé précédent
lines(
  linelistsurv_fit,
  lty = 3, # utiliser un type de ligne différent pour plus de clarté
  fun = "event", # dessine les événements cumulés au lieu de la survie 
  mark.time = FALSE,
  conf.int = FALSE
  )

# Ajoutez une légende au graphique
legend(
  "topright", # position de la légende
  legend = c("Survival", "Cum. Mortality"), # texte de la légende 
  lty = c(1, 3), # types de lignes à utiliser dans la légende
  cex = .85, # paramètres qui définissent la taille du texte de la légende
  bty = "n", # aucun type de boîte à dessiner pour la légende
  )

```

<!-- ======================================================= -->
## Comparaison des courbes de survie 

Pour comparer la survie au sein de différents groupes de nos participants ou patients observés, nous pourrions avoir besoin de regarder d'abord leurs courbes de survie respectives, puis d'effectuer des tests pour évaluer la différence entre les groupes indépendants. Cette comparaison peut concerner des groupes basés sur le sexe, l'âge, le traitement, la comorbidité...

### Test du log rank {.unnumbered}

Le test du log rank est un test populaire qui compare l'ensemble de l'expérience de survie entre deux ou plusieurs groupes *indépendants* et peut être considéré comme un test permettant de savoir si les courbes de survie sont identiques (se chevauchent) ou non (hypothèse nulle d'aucune différence de survie entre les groupes). La fonction `survdiff()` du **paquet survie** permet d'exécuter le test log-rank lorsque l'on spécifie `rho = 0` (ce qui est le cas par défaut). Le résultat du test donne une statistique de chi-deux ainsi qu'une valeur p puisque la statistique de log-rang est approximativement distribuée comme une statistique de test de chi-deux.

Nous essayons d'abord de comparer les courbes de survie par groupe de sexe. Pour cela, nous essayons d'abord de les visualiser (vérifier si les deux courbes de survie se chevauchent). Un nouvel objet *survfit* sera créé avec une formule légèrement différente. Ensuite, l'objet *survdiff* sera créé.

En fournissant ` ~ gender` comme partie droite de la formule, nous ne traçons plus la survie globale mais plutôt par sexe.  


```{r comp_surv, warning=FALSE}

# créez le nouvel objet survfit basé sur le sexe
linelistsurv_fit_sex <- survfit(Surv(futime, event) ~ gender, data = linelist_surv)
```

Maintenant, nous pouvons tracer les courbes de survie par sexe. Jetez un oeil à l'*ordre* des niveaux de strates dans la colonne sexe avant de définir vos couleurs et votre légende.  

```{r}
# définissez les couleurs
col_sex <- c("light green", "dark green")

# Créez le graphique
plot(
  linelistsurv_fit_sex,
  col = col_sex,
  xlab = "Jours de suivi",
  ylab = "Probabilité de survie")

# ajouter une légende
legend(
  "topright",
  legend = c("Female", "Male"),
  col = col_sex,
  lty = 1,
  cex = .9,
  bty = "n")
```

Et maintenant nous pouvons calculer le test de la différence entre les courbes de survie en utilisant `survdiff()``

```{r}
#Test de la différence entre les courbes de survie
survival::survdiff(
  Surv(futime, event) ~ gender, 
  data = linelist_surv
  )

```

Nous constatons que la courbe de survie des femmes et celle des hommes se chevauchent et que le test log-rank ne met pas en évidence de différence de survie entre les femmes et les hommes.

Certains autres packages R permettent d'illustrer les courbes de survie de différents groupes et de tester la différence en une seule fois. En utilisant la fonction `ggsurvplot()` du paquet **survminer**, nous pouvons également inclure dans notre courbe les tableaux de risque imprimés pour chaque groupe, ainsi que la p-value du test log-rank. 

<span style="color : orange ;">**_CAUTION:_** Les fonctions **survminer** exigent que vous spécifiiez l'objet de survie *et* que vous spécifiiez à nouveau les données utilisées pour ajuster l'objet de survie. N'oubliez pas de le faire pour éviter les messages d'erreur non spécifiques. </span>

```{r, warning=F, message=F}

survminer::ggsurvplot(
    linelistsurv_fit_sex, 
    data = linelist_surv, # spécifiez à nouveau les données utilisées pour ajuster linelistsurv_fit_sex 
    conf.int = FALSE, # ne pas montrer l'intervalle de confiance des estimations KM
    surv.scale = "percent", # présente les probabilités sur l'axe des ordonnées en %.
    break.time.by = 10, # présente l'axe du temps avec un incrément de 10 jours
    xlab = "Jours de suivi",
    ylab = "Probabilité de survie",
    pval = T, # imprimer la valeur p du test de Log-rank 
    pval.coord = c(40,.91), # imprimer la valeur p à ces coordonnées de tracé
    risk.table = T, # imprime le tableau des risques en bas de page 
    legend.title = "Gender", # légende des caractéristiques
    legend.labs = c("Female", "Male"),
    font.legend = 10, 
    palette = "Dark2", # spécifier la palette de couleurs 
    surv.median.line = "hv", # dessine des lignes horizontales et verticales sur les médianes de survie
    ggtheme = theme_light() # simplifie le fond du graphique
)

```


Nous pouvons également vouloir tester les différences de survie en fonction de la source d'infection (source de contamination).  

Dans ce cas, le test Log rank donne suffisamment de preuves d'une différence dans les probabilités de survie à `alpha= 0.005`. Les probabilités de survie des patients qui ont été infectés lors de funérailles sont plus élevées que les probabilités de survie des patients qui ont été infectés dans d'autres lieux, ce qui suggère un bénéfice de survie.

```{r}

linelistsurv_fit_source <- survfit(
  Surv(futime, event) ~ source,
  data = linelist_surv
  )

# plot
ggsurvplot( 
  linelistsurv_fit_source,
  data = linelist_surv,
  size = 1, linetype = "strata", # types de lignes
  conf.int = T,
  surv.scale = "percent",  
  break.time.by = 10, 
  xlab = "Jours de suivi",
  ylab= "Probabilité de survie",
  pval = T,
  pval.coord = c(40, .91),
  risk.table = T,
  legend.title = "Source d'infection",
  legend.labs = c("Funéraire", "Autre"),
  font.legend = 10,
  palette = c("#E7B800", "#3E606F"),
  surv.median.line = "hv", 
  ggtheme = theme_light()
)

```

<!-- ======================================================= -->
## Analyse de régression de Cox {}

La régression des risques proportionnels de Cox est l'une des techniques de régression les plus populaires pour l'analyse de survie. D'autres modèles peuvent également être utilisés puisque le modèle de Cox requiert des *hypothèses importantes* qui doivent être vérifiées pour une utilisation appropriée, comme l'hypothèse des risques proportionnels : voir les références. 

Dans un modèle de régression à risques proportionnels de Cox, la mesure de l'effet est le **_taux de risque_** (HR), qui est le risque d'échec (ou le risque de décès dans notre exemple), étant donné que le participant a survécu jusqu'à un moment spécifique.  Habituellement, nous sommes intéressés par la comparaison de groupes *indépendants* en ce qui concerne leurs risques, et nous utilisons un rapport de risque, qui est analogue à un rapport de cotes dans le cadre d'une analyse de régression logistique multiple. La fonction `cox.ph()` du paquet **survival** est utilisée pour ajuster le modèle. La fonction `cox.zph()` du paquet **survival** peut être utilisée pour tester l'hypothèse de risques proportionnels pour un ajustement du modèle de régression de Cox. 

<span style="color : black ;">**_NOTE:_** Une probabilité doit être comprise entre 0 et 1. Cependant, le hasard représente le nombre attendu d'événements par unité de temps. 

* Si le rapport de risque d'un prédicteur est proche de 1, alors ce prédicteur n'affecte pas la survie,
* si le HR est inférieur à 1, alors le prédicteur est protecteur (c'est-à-dire associé à une meilleure survie),
* et si le HR est supérieur à 1, alors le prédicteur est associé à un risque accru (ou à une diminution de la survie).</span> 

### Ajustement d'un modèle de Cox {.unnumbered}

Nous pouvons d'abord ajuster un modèle pour évaluer l'effet de l'âge et du sexe sur la survie. En imprimant simplement le modèle, nous avons les informations sur :

  + les coefficients de régression estimés `coef` qui quantifient l'association entre les prédicteurs et le résultat,
  + leur exponentielle (pour faciliter l'interprétation, `exp(coef)`) qui produit le *rapport de risque*,
  + leur erreur standard `se(coef)`,
  + le z-score : combien d'erreurs standard le coefficient estimé est-il éloigné de 0,
  + et la valeur p : la probabilité que le coefficient estimé puisse être 0.
  
La fonction `summary()` appliquée à l'objet modèle de cox donne plus d'informations, comme l'intervalle de confiance du HR estimé et les différents résultats du test.

L'effet de la première covariable `gender` est présenté dans la première ligne. `genderm` (masculin) est imprimé, ce qui implique que le premier niveau de strate ("f"), c'est-à-dire le groupe féminin, est le groupe de référence pour le sexe. Ainsi, l'interprétation du paramètre de test est celle des hommes par rapport aux femmes. La valeur p indique qu'il n'y a pas suffisamment de preuves d'un effet du sexe sur le risque attendu ou d'une association entre le sexe et la mortalité toutes causes confondues.

Le même manque de preuves est noté concernant le groupe d'âge.

```{r coxmodel_agesex}

#ajustement du modèle de cox
linelistsurv_cox_sexage <- survival::coxph(
              Surv(futime, event) ~ gender + age_cat_small, 
              data = linelist_surv
              )


#imprimer le modèle ajusté
linelistsurv_cox_sexage


#sommaire du modèle
summary(linelistsurv_cox_sexage)

```


Il était intéressant d'exécuter le modèle et de regarder les résultats, mais un premier coup d'oeil pour vérifier si les hypothèses de risques proportionnels sont respectées pourrait aider à gagner du temps.

```{r test_assumption}

test_ph_sexage <- survival::cox.zph(linelistsurv_cox_sexage)
test_ph_sexage

```


<span style="color : black ;">**_NOTE:_** Un deuxième argument appelé *méthode* peut être spécifié lors du calcul du modèle de cox, qui détermine comment les liens sont traités. Le *défaut* est "efron", et les autres options sont "breslow" et "exact".</span>

Dans un autre modèle, nous ajoutons d'autres facteurs de risque tels que la source de l'infection et le nombre de jours entre la date d'apparition et l'admission. Cette fois, nous vérifions d'abord l'hypothèse des risques proportionnels avant de poursuivre.

Dans ce modèle, nous avons inclus un prédicteur continu (`days_onset_hosp`). Dans ce cas, nous interprétons les estimations des paramètres comme l'augmentation du logarithme attendu du risque relatif pour chaque augmentation d'une unité du prédicteur, les autres prédicteurs restant constants. Nous vérifions d'abord l'hypothèse de risques proportionnels.  

```{r coxmodel_fit_ph, message=FALSE}

#fit le modèle
linelistsurv_cox <- coxph(
                        Surv(futime, event) ~ gender + age_years+ source + days_onset_hosp,
                        data = linelist_surv
                        )


#Tester le modèle de risque proportionnel
linelistsurv_ph_test <- cox.zph(linelistsurv_cox)
linelistsurv_ph_test
```

La vérification graphique de cette hypothèse peut être effectuée avec la fonction `ggcoxzph()` du paquet **survminer**. 

```{r}
survminer::ggcoxzph(linelistsurv_ph_test)

```


Les résultats du modèle indiquent qu'il existe une association négative entre la durée entre le début de la maladie et l'admission et la mortalité toutes causes confondues. Le risque attendu est 0,9 fois plus faible chez une personne qui est admise un jour plus tard qu'une autre, le sexe restant constant. Ou, de manière plus directe, une augmentation d'une unité de la durée entre le début de la maladie et l'admission est associée à une diminution de 10,7 % (`coef *100`) du risque de décès.

Les résultats montrent également une association positive entre la source d'infection et la mortalité toutes causes confondues. C'est-à-dire qu'il y a un risque accru de décès (1,21x) pour les patients qui ont eu une source d'infection autre que les funérailles.


```{r coxmodel_summary, message=FALSE}

#imprimez le résumé du modèle
summary(linelistsurv_cox)

```


Nous pouvons vérifier cette relation avec une table :  


```{r}
linelist_case_data %>% 
  tabyl(days_onset_hosp, outcome) %>% 
  adorn_percentages() %>%  
  adorn_pct_formatting()

```


Nous devrions examiner et étudier pourquoi cette association existe dans les données. Une explication possible serait que les patients qui vivent assez longtemps pour être admis plus tard avaient une maladie moins grave au départ. Une autre explication peut-être plus probable est que, puisque nous avons utilisé un faux ensemble de données simulées, ce schéma ne reflète pas la réalité !  


<!-- ======================================================= -->

### Forest plots {.unnumbered}

Nous pouvons ensuite visualiser les résultats du modèle de cox en utilisant les parcelles forestières pratiques avec la fonction `ggforest()` du paquet **survminer**.

```{r forestp}

ggforest(linelistsurv_cox, data = linelist_surv)

```

<!-- ======================================================= -->
## Covariables dépendantes du temps dans les modèles de survie {}

Certaines des sections suivantes ont été adaptées avec la permission d'une excellente [introduction à l'analyse de survie dans R](https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html) par [le Dr Emily Zabor](https://www.emilyzabor.com/). 

Dans la dernière section, nous avons abordé l'utilisation de la régression de Cox pour examiner les associations entre les covariables d'intérêt et les résultats de survie, mais ces analyses reposent sur la mesure de la covariable au départ, c'est-à-dire avant le début du suivi de l'événement.

Que se passe-t-il si vous vous intéressez à une covariable qui est mesurée **après** le début du suivi ? Ou, que se passe-t-il si vous avez une covariable qui peut changer dans le temps ?

Par exemple, vous travaillez peut-être avec des données cliniques où vous avez répété les mesures des valeurs de laboratoire de l'hôpital qui peuvent changer dans le temps. C'est un exemple de **Covariable Dépendante du Temps**. Pour résoudre ce problème, vous avez besoin d'une configuration spéciale, mais heureusement, le modèle cox est très flexible et ce type de données peut également être modélisé avec les outils du paquet **survival**. 

### Configuration des covariables dépendantes du temps {.unnumbered} 

L'analyse des covariables dépendantes du temps dans R nécessite la configuration d'un ensemble de données spécial. Si cela vous intéresse, consultez l'article plus détaillé de l'auteur du paquet **survival** [Utilisation de covariables dépendantes du temps et de coefficients dépendants du temps dans le modèle de Cox](https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf).

Pour cela, nous allons utiliser un nouvel ensemble de données du package `SemiCompRisks` nommé `BMT`, qui comprend des données sur 137 patients ayant subi une greffe de moelle osseuse. Les variables sur lesquelles nous allons nous concentrer sont :  

* `T1` - temps (en jours) jusqu'au décès ou au dernier suivi.  
* `delta1` - indicateur de décès ; 1-Dead, 0-Alive  
* `TA` - temps (en jours) jusqu'à la maladie aiguë du greffon contre l'hôte.  
* `deltaA` - indicateur de la maladie aiguë du greffon contre l'hôte ;  
  * 1 - Développement d'une réaction aiguë du greffon contre l'hôte.  
  * 0 - N'a jamais développé de maladie aiguë du greffon contre l'hôte.

Nous allons charger cet ensemble de données à partir du paquet **survival** en utilisant la commande **base** R `data()`, qui peut être utilisée pour charger des données qui sont déjà incluses dans un paquet R qui est chargé. Le cadre de données `BMT` apparaîtra dans votre environnement R.  

```{r}
data(BMT, package = "SemiCompRisks")
```

#### Ajouter l'identifiant unique du patient {.unnumbered}  

Il n'y a pas de colonne d'identifiant unique dans les données `BMT`, ce qui est nécessaire pour créer le type de jeu de données que nous voulons. Nous utilisons donc la fonction `rowid_to_column()` du paquet **tidyverse** **tibble** pour créer une nouvelle colonne d'identification appelée `my_id` (ajoute une colonne au début du cadre de données avec des identifiants de ligne séquentiels, en commençant par 1). Nous nommons le cadre de données `bmt`.  

```{r}
bmt <- rowid_to_column(BMT, "my_id")
```

L'ensemble de données ressemble maintenant à ceci :  

```{r message=FALSE, echo=F}
DT::datatable(bmt, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Développer les lignes de patients {.unnumbered} 

Ensuite, nous allons utiliser la fonction `tmerge()` avec les fonctions d'aide `event()` et `tdc()` pour créer le jeu de données restructuré. Notre but est de restructurer l'ensemble de données pour créer une ligne séparée pour chaque patient pour chaque intervalle de temps où ils ont une valeur différente pour `deltaA`. Dans ce cas, chaque patient peut avoir au maximum deux lignes selon qu'il a développé ou non une maladie aiguë du greffon contre l'hôte pendant la période de collecte des données. Nous appellerons notre nouvel indicateur de développement de la maladie aiguë du greffon contre l'hôte `agvhd`.

- `tmerge()` crée un long jeu de données avec plusieurs intervalles de temps pour les différentes valeurs de covariables pour chaque patient.
- `event()` crée le nouvel indicateur d'événement pour aller avec les intervalles de temps nouvellement créés.
- `tdc()` crée la colonne de covariable dépendante du temps, `agvhd`, pour aller avec les intervalles de temps nouvellement créés.

```{r}
td_dat <- 
  tmerge(
    data1 = bmt %>% select(my_id, T1, delta1), 
    data2 = bmt %>% select(my_id, T1, delta1, TA, deltaA), 
    id = my_id, 
    death = event(T1, delta1),
    agvhd = tdc(TA)
    )
```

Pour voir ce que cela donne, examinons les données des 5 premiers patients individuels.

Les variables d'intérêt dans les données originales ressemblaient à ceci :

```{r}
bmt %>% 
  select(my_id, T1, delta1, TA, deltaA) %>% 
  filter(my_id %in% seq(1, 5))
```

Le nouvel ensemble de données pour ces mêmes patients ressemble à ceci :

```{r}
td_dat %>% 
  filter(my_id %in% seq(1, 5))
```

Maintenant, certains de nos patients ont deux lignes dans l'ensemble de données correspondant aux intervalles où ils ont une valeur différente de notre nouvelle variable, `agvhd`. Par exemple, le patient 1 a maintenant deux lignes avec une valeur `agvhd` de zéro du temps 0 au temps 67, et une valeur de 1 du temps 67 au temps 2081. 

### Régression de Cox avec covariables dépendantes du temps {.unnumbered} 

Maintenant que nous avons remodelé nos données et ajouté la nouvelle variable `aghvd` dépendante du temps, ajustons un simple modèle de régression de Cox à variable unique. Nous pouvons utiliser la même fonction `coxph()` que précédemment, nous devons juste changer notre fonction `Surv()` pour spécifier à la fois le temps de début et de fin pour chaque intervalle en utilisant les arguments `time1 = ` et `time2 = `. 


```{r}
bmt_td_model = coxph(
  Surv(time = tstart, time2 = tstop, event = death) ~ agvhd, 
  data = td_dat
  )

summary(bmt_td_model)
```

Encore une fois, nous allons visualiser les résultats de notre modèle cox en utilisant la fonction `ggforest()` du paquet **survminer** :

```{r}

ggforest(bmt_td_model, data = td_dat)

```

Comme vous pouvez le constater à partir du diagramme forestier, de l'intervalle de confiance et de la valeur p, il ne semble pas y avoir de forte association entre le décès et la maladie aiguë du greffon contre l'hôte dans le contexte de notre modèle simple. 

<!-- ======================================================= -->
## Ressources { }

[Analyse de survie partie I : concepts de base et premières analyses](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2394262/)

[Analyse de survie en R](https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html)

[Analyse de survie dans la recherche sur les maladies infectieuses : décrire les événements dans le temps](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2954271/)

[Chapitre sur les modèles de survie avancés Princeton](https://data.princeton.edu/wws509/notes/c7.pdf)

[Utilisation de covariables et de coefficients dépendant du temps dans le modèle de Cox](https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf)

[Aide-mémoire pour l'analyse de survie R](https://publicifsv.sund.ku.dk/~ts/survival/survival-cheat.pdf)

[Feuille de calcul Survminer](https://paulvanderlaken.files.wordpress.com/2017/08/survminer_cheatsheet.pdf)

[Article sur les différentes mesures de survie pour les données des registres du cancer avec le code R fourni comme matériel supplémentaire](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6322561/)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/survival_analysis.Rmd-->

# GIS basics {#gis}  

```{r, out.width=c('100%', '100%'), echo=F}
knitr::include_graphics(here::here("images", "gis_head_image.png"))
```

<!-- ======================================================= -->

## Présentation {  }

Les caracteristiques geospatiales de vos données peuvent fournir  des informations capitales en situation de pandemie.En effet, ils permettent de repondent à des questions tels que:

* Ou se trouve les zones à risques de la maladie

* Comment les zones à risques evoluent dans le temps

* L’accessibilité du plateau medical et la nécessité d’apporter des améliorations

L’accent mis sur le SIG à travers ces pages rend accessible toutes les compétences  pertinentes pouvant être utilisées en  cas de reponse pandemique.On va voir les methodes simples pour visualiser les donnees spatiales grace aux packages **tmap** et **ggplot**.Nous allons aussi passer en revue le package  **sf**  qui permet une manipulation des donnees de types vecteurs. Enfin on va aborder brievement les statistiques spatiales à travers les notions  de dependance spatiale, l’autocorrelation spatiale et la regression spatiale en utilisant le package **spdep**

## Mots clés {}

Quelques mots clés sont definis en bas.Pour une bonne initiation au SIG nous suggérons de suivre l’un des tutoriels proposés dans les références.

**Systeme d'Information Geographique(SIG)** - Un SIG est un outil informatique ou un  environnement  qui permet de regrouper, de manipuler, d'analyser et de visualiser les données spatiales

## Logiciels SIG {.unnumbered}

Les logiciels populaires de SIG  qui existent, fonctionnent sur la base d'une interface graphique  qui interagit apres un clic de l’utlisateur.Ces outils peuvent etre utlisés sans aucun prerequis en programmation et ont comme avantage de pouvoir selectionner et placer des icones et des elements sur une carte manuellement.Les logiciels de SIG les plus connus sont :

**ARCGIS** -  un logiciel commercial developpé par la compagnie ESRI.A ce jour il reste le logiciel de SIG le plus populaire.

**QGIS** -   un logiciel de SIG libre d’utilisation  qui fait pratiquement la meme choses que ARCGIS. On peut [Telecharger QGIS par ici](https://qgis.org/en/site/forusers/download.html) 

Faire du SIG en utilisant R peut  sembler frustant  pour  les premiers pas  parce qu'à la place de pointer et cliquer , il faut maintenant ecrire des lignes de commande dans une interface (on a besoin de coder pour obtenir un resultat ).Mais  ce changement de paradigme présente des avantages tels que  l'automatisation et  la reproductibilité dans la conception des carte et les analyses realisées. 

### Données spatiales {.unnumbered}

Les types de données utilisées en SIG sont de deux natures: les rasters et les vecteurs

**Données vecteurs** - Les données vecteurs dont l’utilisation est la plus courante dans l’environnement  SIG sont dotés de proprietes géometriques par  les sommets et les lignes reliant les sommets. Les données  vecteurs  largement utilisées peuvent être divisées en trois :

  * *Points* - le point  est constitué d’un pair de coordonnnées (x,y) qui marque un lieu précis dans l’espace à travers le systeme de coordonnées. Il est la forme simple de représentation des donnees spatiales  et peuvent etre utilisé pour situer le lieu d'apparition d'une maladie lors d'une pandémie (la maison du patient) ou un endroit (exemple :  hôpital) sur une carte .
  
  * *Lignes* - 	Une ligne est composée de deux points reliés.Elle a  une longueur et peut etre utilisée pour representer les routes ou les cours d’eaux
  
  * *Polygones* - Un polygones est  une association minimum de 3 points reliés les uns aux autres .Les caractéristiques d’un polygone sont : le périmètre et l'aire.En pratique ils sont utilisés pour delmiter les contours d'une zone (village) ou une infractructures(hopital)

**Données Rasters** - A coté des données vecteurs on a des rasters qui sont des matrix constitués par des cellules contenant chacunes des informations par exemple l'altitude, la temperature, la pente, la couverture forestière etc. les rasters peuvent servir de fond de carte pour les donnees vecteurs

### Visualisation des données spatiales {.unnumbered}

Pour afficher  sur une carte des donnnées spatiales à partir d'un logiciel de SIG il est  nécessaire de la part de l’utilisateur de connaitre l'emplacement des donnees .Lors qu’il nous arrive de manipuler des données vecteurs ce qui est d’ailleurs le plus frequents, les données sont stocker dans des fichiers shapefiles

**Shapefiles** - Un shapefile est un format de données vecteurs tres repandus dans le monde du SIG et il permet de stocker des donnees pouvant etre des points des lignes et des polygones.C'est en realite une collection ou un ensemnble de fichiers   se terminant avec les extensions .shp,.shx et .dbf.Tout ces fichiers doivent etre presents dans un et unique dossiers pour obtenir une donnée shapefile fiable.
L’ensemble de ces fichiers shapefile peuvent être comprimer en format zip pour un envoi à travers un mail ou un telechargement à partir d’un site web

Le shapefile contient des information sur un objet de la surface terreste modelisé et sa localisation géographique .Cela est important parce que bien que la terre est une geoide le système de coordonnées  bidimentionnel au niveau des shapefiles permet de situer un objet sur la surface de la terre

**Systéme de réference de coordonnées ** - Un CRS est un système de coordonnées utilisé pour localiser géographiquement un objet sur la surface de la terre .Il a quelques composantes clés:

  * *Systeme de Coordonnées* - Plusieurs systèmes de coordonnées existent mais il est important de savoir les coodonnées géographiques  utilisées sont adossées  à quel système.Les degres pour le longitude/latitude sont fréquents  mais on note aussi l'usage des coordonnées [UTM](https://www.maptools.com/tutorials/utm/quick_guide) 
  
  * *Unités* - Connaitre l'unité du système de coordonnées(degres ou decimal) est primordiale pour faire certaines analyses spatiales
  
  * *Datum* - C'est une modélisation de la terre revue au fil des annees, il est important de vérifier  que les cartes utilisées ont le meme datum ou systeme geodesique
  
  * *Projection* - C'est l'utilisation d'équation mathematique pour projeter la forme geoide de la terre sur une surface plane
  
Rappellons que  les données spatiales peuvent être manipulées  sans avoir recours aux  outils  cartographiques ci-dessous

## Débuter avec le SIG  

Il y a quelques éléments clés que vous devrez avoir et auxquels vous devrez penser pour faire une carte. Ces éléments sont les suivants :

  * Un **jeu de données** - il peut s'agir d'un format de données spatiales (comme les shapefiles, comme indiqué ci-dessus) ou d'un format non spatial (par exemple, un simple csv).
  
  * Si votre ensemble de données n'est pas dans un format spatial, vous aurez également besoin d'un **jeu données de référence**. Les données de référence sont constituées de la représentation spatiale des données et des **attributs** associés, qui peuvent inclure des éléments contenant les informations relatives à l'emplacement et à l'adresse d'entités spécifiques.
  
    + Si vous travaillez avec des limites géographiques prédéfinies (par exemple, des régions administratives), les fichiers de forme de référence peuvent souvent être téléchargés gratuitement depuis une agence gouvernementale ou une organisation de partage de données. En cas de doute, un bon point de départ est de rechercher sur Google " [régions] shapefile ".
    
    + Si vous avez des informations d'adresse, mais pas de latitude et de longitude, vous devrez peut-être utiliser un **moteur de géocodage** pour obtenir les données de référence spatiale pour vos enregistrements. 
    
  * Une idée de **la manière dont vous voulez présenter** les informations de vos ensembles de données à votre public cible. Il existe de nombreux types de cartes, et il est important de réfléchir au type de carte qui correspond le mieux à vos besoins.

### Types of maps for visualizing your data {.unnumbered}

### Les types de cartes pour visualiser vos données {.unnumbered}

**Carte chloroplethe** -   ce sont des cartes thematiques ou les couleurs l’ombrage et les motifs sont utilisés pour representer les valeurs des variables présentes dans les attributs  en fonction des  unités geograpphiques.Par exemple, une valeur plus grande peut être indiquée par une couleur plus foncée qu'une valeur plus petite. Ce type de carte est particulièrement utile pour visualiser une variable et son évolution dans des régions ou des zones géopolitiques définies.

```{r, out.width = '50%', fig.align = "center", fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "gis_choropleth.png"))
```


**carte de fréquentation de la densite des cas** - C'est Un type de carte thematique ou l'intensité des couleurs est proportionnelle à la valeur de l’observation cela ne fait pas intervenir des zones ou des limites geographiques et adminitratives. Ce genre de carte est utilisé pour montrerles zones à risques ou les lieux à forte concentration de variables observées 

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "gis_heatmap.png"))
```

**Carte de densité de points** - Une carte thematique qui utilise des points pour représenter la valeur du variable dans les données.Ce type de carte  permet de visualiser les données avec un nuage de point afin d’identifier la presence de clusters ou foyers

```{r, fig.align = "center", echo=F}
# dot density img here
```

**Carte à symboles proportionnels (carte à symboles gradués)** - une carte thématique similaire à une carte choroplèthe, mais au lieu d'utiliser une couleur pour indiquer la valeur d'un attribut, elle utilise un symbole (généralement un cercle) en relation avec la valeur. Par exemple, une valeur plus grande peut être indiquée par un symbole plus grand qu'une valeur plus petite. Ce type de carte est idéal lorsque vous souhaitez visualiser la taille ou la quantité de vos données dans des régions géographiques. 

```{r, fig.align = "center", echo=F}
# proportional symbols img here
```


Vous pouvez également combiner plusieurs types de visualisations différentes pour montrer des schémas géographiques complexes. Par exemple, les cas (points) de la carte ci-dessous sont colorés en fonction de l'établissement de santé le plus proche (voir la légende). Les grands cercles rouges montrent les *zones de couverture des établissements de santé* d'un certain rayon, et les points rouges brillants les cas qui étaient en dehors de toute zone de desserte :
```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "gis_hf_catchment.png"))
```



Remarque : L'objectif principal de cette page SIG est basé sur le contexte de la réponse aux épidémies sur le terrain. Par conséquent, le contenu de la page couvrira les manipulations, visualisations et analyses de données spatiales de base.

<!-- ======================================================= -->
## Preparation {  }

### Chargement packages {.unnumbered}  

Ce morceau de code montre le chargement des paquets nécessaires aux analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* le charge pour l'utiliser. Vous pouvez également charger les paquets installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les paquets R.  

```{r}
pacman::p_load(
  rio,           # Pour importer les données
  here,          # Pour situer l'emplacement des donnes
  tidyverse,     # nettoyer manipuler et visualiser les données( inclure le package ggplot2)
  sf,            # manipuler les donnees spatiales avec le package sf
  tmap,          # pour produire des cartes simples, fonctionne pour les cartes interactives et statiques
  janitor,       # pour nettoyer les noms de colonnes
  OpenStreetMap, # pour ajouter la  carte de base OSM sur la carte ggplot
  spdep          # statistiques spatiales
  ) 
                  
```

Vous pouvez consulter une vue d'ensemble de tous les paquets R qui traitent des données spatiales sur le site [CRAN "Spatial Task View"](https://cran.r-project.org/web/views/Spatial.html).  


### Exemple de données de cas {.unnumbered}

À des fins de démonstration, nous travaillerons avec un échantillon aléatoire de 1000 cas provenant du dataframe `linelist` de l'épidémie d'Ebola simulée (d'un point de vue computationnel, travailler avec moins de cas est plus facile à afficher dans ce manuel). Si vous souhaitez suivre le processus, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la linelist "propre"</a> (en tant que fichier .rds).  

Comme nous prenons un échantillon aléatoire de cas, vos résultats peuvent être légèrement différents de ce qui est démontré ici lorsque vous exécutez les codes par vous-même.

Importez les données avec la fonction `import()` du paquet **rio** (elle gère de nombreux types de fichiers comme .xlsx, .csv, .rds - voir la page [Importation et exportation](#import_export) pour plus de détails).  

```{r, echo=F}
# importer  le jeu de données linelist nettoyé
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))  
```

```{r, eval=F}
# importer le jeu de données linelist nettoyé
linelist <- import("linelist_cleaned.rds")  
```

Ensuite, nous sélectionnons un échantillon aléatoire de 1000 lignes en utilisant `sample()` de **base** R.   

```{r}
# générer 1000 numéros de ligne aléatoires, à partir du nombre de lignes de la linelist
sample_rows <- sample(nrow(linelist), 1000)

#  creons un sous-ensembles de linelist   pour ne garder que les lignes de l'échantillon, et toutes les colonnes
linelist <- linelist[sample_rows,]
```
Maintenant nous voulons convertir cette `linelist` qui est de classe dataframe, en un objet de classe "sf" (caractéristiques spatiales). Étant donné que la linelist a deux colonnes "lon" et "lat" représentant la longitude et la latitude de la résidence de chaque cas, cela sera facile.  

Nous utilisons le package **sf** (caractéristiques spatiales) et sa fonction `st_as_sf()` pour créer le nouvel objet que nous appelons `linelist_sf`. Ce nouvel objet ressemble essentiellement à la linelist, mais les colonnes `lon` et `lat` ont été désignées comme des colonnes de coordonnées, et un système de référence de coordonnées (CRS) a été attribué pour l'affichage des points. 4326 identifie nos coordonnées comme étant basées sur le [Système géodésique mondial 1984 (WGS84)](https://gisgeography.com/wgs84-world-geodetic-system/) - qui est la norme pour les coordonnées GPS.  


```{r}
# Creons un objet  sf 
linelist_sf <- linelist %>%
     sf::st_as_sf(coords = c("lon", "lat"), crs = 4326)
```

Voici à quoi ressemble le dataframe original `linelist`. Dans cette démonstration, nous n'utiliserons que la colonne `date_onset` et `geometry` (qui a été construite à partir des champs de longitude et de latitude ci-dessus et qui est la dernière colonne du dataframe). 

```{r}
DT::datatable(head(linelist_sf, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

###  shapefiles des limites administratives {.unnumbered}  

**Sierra Leone: shapefiles des limites administratives**  


Par avance, nous avons téléchargé toutes les limites administratives de la Sierra Leone à partir du Humanitarian Data Exchange (HDX) [site Web ici](https://data.humdata.org/dataset/sierra-leone-all-ad-min-level-boundaries). Alternativement, vous pouvez télécharger ces données et toutes les autres données d'exemple pour ce manuel via notre package R, comme expliqué dans la page [Télécharger le manuel et les données](#download_book_data).  

Nous allons maintenant procéder comme suit pour enregistrer le shapefile Admin Level 3 dans R :  

1) Importer les shapefiles
2) Nettoyer les noms des colonnes  
3) Filtrer les lignes pour ne garder que les zones d'intérêt.  

Pour importer un fichier shapefile, nous utilisons la fonction `read_sf()` de **sf**. Le chemin du fichier est fourni via `here()`. - Dans notre cas, le fichier se trouve dans notre projet R dans les sous-dossiers "data", "gis" et "shp", avec le nom de fichier "sle_adm3.shp" (voir les pages sur [Importation et exportation](#import_export) et [Projets R](#r_projects) pour plus d'informations). Vous devrez fournir votre propre chemin de fichier.  
```{r, echo=F}
sle_adm3_raw <- sf::read_sf(here("data", "gis", "shp", "sle_adm3.shp"))
```

Ensuite, nous utilisons `clean_names()` du package **janitor** pour normaliser les noms de colonnes du fichier shapefile. Nous utilisons également `filter()` pour ne conserver que les lignes dont le nom d'administrateur est "Western Area Urban" ou "Western Area Rural".    


```{r}
# ADM3 level clean
sle_adm3 <- sle_adm3_raw %>%
  janitor::clean_names() %>% # normaliser les noms de colonnes
  filter(admin2name %in% c("Western Area Urban", "Western Area Rural")) # filtre pour garder certaines zones
```

Vous pouvez voir ci-dessous à quoi ressemble le fichier shapefile après importation et nettoyage. *Faites défiler vers la droite* pour voir s'il y a des colonnes avec le niveau d'administration 0 (pays), le niveau d'administration 1, le niveau d'administration 2, et enfin le niveau d'administration 3. Chaque niveau a un nom de caractère et un identifiant unique "pcode". Le pcode se développe avec chaque niveau d'administration croissant, par exemple SL (Sierra Leone) -> SL04 (Western) -> SL0410 (Western Area Rural) -> SL040101 (Koya Rural).  

```{r message=FALSE, echo=F}
# Afficher le shapefile comme un tableau
DT::datatable(head(sle_adm3, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



### Population data {.unnumbered}  

**Sierra Leone : Population par ADM3**  

Ces données peuvent être téléchargées sur HDX (lien [ici](https://data.humdata.org/dataset/sierra-leone-population)) ou via notre package R **epirhandbook** comme expliqué [dans la page Télécharger le manuel et les données](#download_book_data). Nous utilisons `import()` pour charger le fichier .csv. Nous passons également le fichier importé à `clean_names()` pour standardiser la syntaxe des noms de colonnes.   



```{r}
# Population par ADM3
sle_adm3_pop <- import(here("data", "gis", "population", "sle_admpop_adm3_2020.csv")) %>%
  janitor::clean_names()
```

Voici à quoi ressemble le fichier de la population. Faites défiler vers la droite pour voir comment chaque juridiction a des colonnes avec la population "masculine", la population "féminine", la population "totale", et la répartition de la population en colonnes par groupe d'âge.  

```{r message=FALSE, echo=F}
# afficher la population sous forme de tableau
DT::datatable(head(sle_adm3_pop, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Infractructures sanitaires {.unnumbered}

**Sierra Leone : Données sur les établissements de santé provenant d'OpenStreetMap**.  

Encore une fois, nous avons téléchargé les emplacements des établissements de santé à partir de HDX [ici](https://data.humdata.org/dataset/hotosm_sierra_leone_health_facilities) ou via les instructions dans la page [Télécharger le manuel et les données](#download_book_data.   

Nous importons le shapefile des points des établissements avec `read_sf()`, nettoyons à nouveau les noms des colonnes, et filtrons ensuite pour ne garder que les points étiquetés comme "hôpital", "clinique", ou "médecins".  


```{r}
#  Les donnees shapefiles de OSM sur  les infractuctures sanitaires 
sle_hf <- sf::read_sf(here("data", "gis", "shp", "sle_hf.shp")) %>% 
  janitor::clean_names() %>%
  filter(amenity %in% c("hospital", "clinic", "doctors"))
```

Voici le dataframe obtenu defile jusqu'a en bas pour voir le nom de l'infrastructures sanitaires et les coordonnées geographiques à travers la colonne `geometry`.  


```{r message=FALSE, echo=F}
# afficher la population sous forme de tableau
DT::datatable(head(sle_hf, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





<!-- ======================================================= -->
## Visualiser les coordonnées { }

La manière la plus simple de visualiser des coordonnées X-Y (longitude/latitude, points), dans ce cas de figure, est de les dessiner sous forme de points directement à partir de l'objet `linelist_sf` que nous avons créé dans la section de préparation.

Le package **tmap** offre des capacités de cartographie simples à la fois pour le mode statique (mode "plot") et interactif (mode "view") avec seulement quelques lignes de code. La syntaxe de **tmap** est similaire à celle de **ggplot2**, de sorte que les commandes sont ajoutées les unes aux autres avec `+`. Vous trouverez plus de détails dans cette [vignette](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html). 


1) Définir le mode **tmap**. Dans ce cas, nous utiliserons le mode "plot", qui produit des sorties statiques.  


```{r, warning = F, message=F}
tmap_mode("plot") # choisir soit  "view" ou "plot"
```

Ci-dessous, seulement les points sont affichés. `tm_shape()` prend comme argument : l'objet  `linelist_sf`. Ensuite , Nous ajoutons  des points via `tm_dots()`, en spécifiant la taille et la couleur. Comme `linelist_sf` est un objet sf, nous avons déjà désigné les deux colonnes qui contiennent les coordonnées lat/long et le système de référence des coordonnées (CRS) : 


```{r, warning = F, message=F}
# Juste les cas (points)
tm_shape(linelist_sf) + tm_dots(size=0.08, col='blue')
```

les points seulement  ne sont pas assez informatifs. Nous devons donc également cartographier les limites administratives :  

Apres cela, nous utilisons `tm_shape()` (voir [documentation](https://www.rdocumentation.org/packages/tmap/versions/3.3/topics/tm_shape)) mais au lieu de fournir le shapefile des points ou les cas sont localisés, nous fournissons le shapefile des limites administratives (polygones).  

Avec l'argument `bbox = ` (bbox signifie "bounding box") nous pouvons spécifier l'emprise des coordonnées. Nous allons  d'abord visualiser la carte  sans l'emprise `bbox`, et ensuite avec l'emprise de l'objet.  

```{r, out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F}
# Uniquement les frontieres administratives (polygones)
tm_shape(sle_adm3) +               # Shapefiles des limites administratives
  tm_polygons(col = "#F7F7F7")+    # afficher les polygones en gris clairs
  tm_borders(col = "#000000",      # parametrer la couleur des bordures   et l'epaisseur des lignes
             lwd = 2) +
  tm_text("admin3name")            # le texte de la colonne à afficher pour chaque polygone


#Comme ci-dessus, mais avec un zoom à partir de l'emprise
tm_shape(sle_adm3,
         bbox = c(-13.3, 8.43,    # sommet
                  -13.2, 8.5)) +  # sommet
  tm_polygons(col = "#F7F7F7") +
  tm_borders(col = "#000000", lwd = 2) +
  tm_text("admin3name")

```


Et maintenant les points et les polygones ensemble :  

```{r, warning=F, message=FALSE}
# tous ensemble
tm_shape(sle_adm3, bbox = c(-13.3, 8.43, -13.2, 8.5)) +     #
  tm_polygons(col = "#F7F7F7") +
  tm_borders(col = "#000000", lwd = 2) +
  tm_text("admin3name")+
tm_shape(linelist_sf) +
  tm_dots(size=0.08, col='blue', alpha = 0.5) +
  tm_layout(title = "Distribution of Ebola cases")   # donner un titre à la carte

```


Pour lire une bonne comparaison des options de mappage dans R, consultez cet [article de blog](https://rstudio-pubs-static.s3.amazonaws.com/324400_69a673183ba449e9af4011b1eeb456b9.html).  




<!-- ======================================================= -->
## Jointures spatiales {}

Vous êtes peut-être familier avec la *jonction* de données d'un jeu de données à un autre. Plusieurs méthodes sont présentées à la page [Joindre des données](#joining_matching) de ce manuel. Une jointure spatiale a un objectif similaire mais exploite les relations spatiales. Au lieu de compter sur des valeurs communes dans les colonnes pour faire correspondre correctement les observations, vous pouvez utiliser leurs relations spatiales, comme le fait qu'une observation soit *dans* une autre, ou *le plus proche voisin* d'une autre, ou dans un *buffer* d'un certain rayon d'une autre, etc.  


Le package **sf** offre diverses méthodes de jointures spatiales. Vous trouverez plus de documentation sur la méthode st_join et les types de jointures spatiales dans cette [référence](https://r-spatial.github.io/sf/reference/geos_binary_pred.html).  


### Points dans le polygone {.unnumbered}
**Attribuer spatialement des unités administratives aux cas**

Voici une question intéressante : la liste de cas ne contient aucune information sur les unités administratives des cas. Bien qu'il soit idéal de collecter ces informations au cours de la phase initiale de collecte des données, nous pouvons également attribuer des unités administratives aux cas individuels sur la base de leurs relations spatiales (c'est-à-dire l'intersection d'un point avec un polygone).  

Ci-dessous, nous allons croiser spatialement les emplacements de nos cas (points) avec les limites d'ADM3 (polygones) :  

1) Commencer par l'objet linelist (points)  
2) Jointure spatiale aux limites, en définissant le type de jointure comme etant "st_intersects".  
3) Utilisez `select()` pour ne garder que certaines des colonnes de la nouvelle limite administrative.  

```{r, warning=F, message=F}
linelist_adm <- linelist_sf %>%
  
  # joindre le fichier des limites administratives à  linelist sur la base de l'intersection spatiale.
  sf::st_join(sle_adm3, join = st_intersects)
```

Toutes les colonnes de `sle_adms` ont été ajoutées à la linelist ! Chaque cas a maintenant des colonnes détaillant les niveaux administratifs dont il fait partie. Dans cet exemple, nous voulons seulement garder deux des nouvelles colonnes (niveau administratif 3), donc nous `sélectionnons()` les anciens noms de colonnes et seulement les deux supplémentaires d'intérêt :  

```{r, warning=F, message=F}
linelist_adm <- linelist_sf %>%
  
  # joindre le fichier des limites administratives au fichier linelist, sur la base de l'intersection spatiale.
  sf::st_join(sle_adm3, join = st_intersects) %>% 
  
  # Conservez les anciens noms de colonnes et deux nouveaux noms admin d'intérêt
  select(names(linelist_sf), admin3name, admin3pcod)
```

Ci-dessous, à des fins d'affichage, vous pouvez voir les dix premiers cas et les limites administratives de niveau 3(ADM3) qui y ont été rattachées, en fonction de l'endroit où le point a croisé les formes polygonales.    

```{r, warning=F, message=F}
# Vous verrez maintenant les noms ADM3 attachés à chaque cas.
linelist_adm %>% select(case_id, admin3name, admin3pcod)
```

Nous pouvons maintenant décrire nos cas par unité administrative - ce que nous ne pouvions pas faire avant la jointure spatiale !  

```{r, warning=F, message=F}
# Créer un nouveau dataframe contenant le nombre de cas par unité administrative.
case_adm3 <- linelist_adm %>%          # commencer avec linelist avec de nouveaux admin colonnes
  as_tibble() %>%                      # convert en format  tibble pour un meilleur affichage
  group_by(admin3pcod, admin3name) %>% # regrouper par   unite admin, à la fois  par le nom   et le  pcode 
  summarise(cases = n()) %>%           # utilisons la fonction summarize et  comptons les lignes
  arrange(desc(cases))                     # arrangement par ordre decroissant

case_adm3
```

Nous pouvons également créer un diagramme en barres du nombre de cas par unité administrative.  

Dans cet exemple, nous commençons le `ggplot()` avec la `linelist_adm`, afin de pouvoir appliquer des fonctions de facteurs comme `fct_infreq()` qui ordonne les barres par fréquence (voir la page sur les [Facteurs](#factors) pour des conseils).  

```{r, warning=F, message=F}
ggplot(
    data = linelist_adm,                       # Debuter  avec linelist qui contient les infos sur les unites admin 
    mapping = aes(
      x = fct_rev(fct_infreq(admin3name))))+ # L'axe des x est constitué d'unités administratives, classées par fréquence (inversée).
  geom_bar()+                                # créer des barres, la hauteur est le nombre de lignes
  coord_flip()+                              # retournement des axes X et Y pour une lecture plus aisée des unités d'admin
  theme_classic()+                           # simplifier le fond
  labs(                                      # titres et les  labels
    x = "Admin Niveau 3",
    y = "Number of cases",
    title = "Nombre de cas par unité adminstrative",
    caption = "Tel que déterminé par une jointure spatiale, à partir de 1000 cas échantillonnés de façon aléatoire dans linelist."
  )
```


<!-- ======================================================= -->
### Le voisin le plus proche {.unnumbered}

**Trouver l'établissement de santé le plus proche / la zone de captage**  

Il peut être utile de savoir où sont situés les établissements de santé par rapport aux foyers de maladies.

Nous pouvons utiliser la méthode de jointure *st_nearest_feature* de la fonction `st_join()` (package **sf**) pour visualiser l'établissement de santé le plus proche des cas individuels.  

1) Nous commençons avec le fichier de forme linelist `linelist_sf`.  
2) Nous joignons spatialement avec `sle_hf`, qui est les emplacements des établissements de santé et des cliniques (points)  

```{r, warning=F, message=F}
# Établissement de santé le plus proche de chaque cas
linelist_sf_hf <- linelist_sf %>%                  # commencons avec linelist shapefile  
  st_join(sle_hf, join = st_nearest_feature) %>%   # données de la clinique la plus proche jointes aux données du cas 
  select(case_id, osm_id, name, amenity) %>%       # conserver les colonnes d'intérêt, notamment l'identifiant, le nom, le type et la géométrie de l'établissement de santé
  rename("nearest_clinic" = "name")                # renommer pour plus de clarté
```

Nous pouvons voir ci-dessous (les 50 premières lignes) que chaque cas a maintenant des données sur la clinique/hôpital le plus proche.  

```{r message=FALSE, echo=F}
DT::datatable(head(linelist_sf_hf, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


Nous pouvons voir que "Den Clinic" est l'établissement de santé le plus proche pour environ 30 % des cas.

```{r}
# Nombre de cas par établissement de santé
hf_catchment <- linelist_sf_hf %>%   # commencer par linelist comprenant les données de la clinique la plus proche
  as.data.frame() %>%                # convertir de shapefile à dataframe
  count(nearest_clinic,              # compter les lignes par "nom" (de la clinique)
        name = "case_n") %>%         # assign new counts column as "case_n"
  arrange(desc(case_n))              # classer par ordre décroissant

hf_catchment                         # Afficher sur  la console
```

Pour visualiser les résultats, nous pouvons utiliser **tmap** - cette fois-ci en mode interactif pour une visualisation plus facile.  

```{r, warning=F, message=F}
tmap_mode("view")   # Utiliser tmap en  mode  interactive  

# Visualiser les points ou sont localisés les cas et cliniques
tm_shape(linelist_sf_hf) +            # visualiser les cas
  tm_dots(size=0.08,                  # cas colorés par la clinique la plus proche
          col='nearest_clinic') +    
tm_shape(sle_hf) +                    # tracer les  cliniques en gros points noirs
  tm_dots(size=0.3, col='black', alpha = 0.4) +      
  tm_text("name") +                   # superposition du nom de l'installation
tm_view(set.view = c(-13.2284, 8.4699, 13), # ajuster le zoom (coordonnées du centre, zoom)
        set.zoom.limits = c(13,14))+
tm_layout(title = "Cas, colorés par la clinique la plus proche")
```


### Buffer {.unnumbered} 

Nous pouvons également explorer combien de cas sont situés à moins de 2,5 km (~30 minutes) de distance de marche de l'établissement de santé le plus proche.

*Remarque : pour des calculs de distance plus précis, il est préférable de reprojeter votre objet sf dans le système de projection de la carte locale, tel que l'UTM (Terre projetée sur une surface plane). Dans cet exemple, pour des raisons de simplicité, nous nous en tiendrons au système de coordonnées géograhpiques World Geodetic System (WGS84) (la Terre est représentée par une surface sphérique / ronde, les unités sont donc en degrés décimaux). Nous utiliserons une conversion générale de : 1 degré décimal = ~111km.*  

Pour plus d'informations sur les projections cartographiques et les systèmes de coordonnées, consultez cet [article esri](https://www.esri.com/arcgis-blog/products/arcgis-pro/mapping/gcs_vs_pcs/). Ce [blog](http://www.geo.hunter.cuny.edu/~jochen/gtech201/lectures/lec6concepts/map%20coordinate%20systems/how%20to%20choose%20a%20projection.htm) traite des différents types de projection cartographique et de la manière de choisir une projection appropriée en fonction de la zone d'intérêt et du contexte de votre carte / analyse.


**Tout d'abord**, créez un tampon circulaire d'un rayon de ~2,5 km autour de chaque établissement de santé. Ceci est fait avec la fonction `st_buffer()` de **tmap**. Parce que l'unité de la carte est en degrés décimaux lat/long, c'est ainsi que "0,02" est interprété. Si le système de coordonnées de votre carte est en mètres, le nombre doit être fourni en mètres.  

```{r, warning=F, message=F}
sle_hf_2k <- sle_hf %>%
  st_buffer(dist=0.02)       # degrés décimaux se traduisant par environ 2,5 km 
```

Ci-dessous, nous traçons les zones tampons elles-mêmes, avec les valeurs de :  

```{r, warning=F, message=F}
tmap_mode("plot")
# Creons une zone tampon circulaire
tm_shape(sle_hf_2k) +
  tm_borders(col = "black", lwd = 2)+
tm_shape(sle_hf) +                    # materialiser les installations cliniques avec  de gros points rouges
  tm_dots(size=0.3, col='black')      
```


**Ensuite**, nous intersectons ces tampons avec les cas (points) en utilisant `st_join()` et le type de jointure de *st_intersects*. C'est-à-dire que les données de la zone tampon sont jointes aux points qu'ils croisent. 

```{r, warning=F, message=F}
# Intersecter les cas observés  avec la zone tampon
linelist_sf_hf_2k <- linelist_sf_hf %>%
  st_join(sle_hf_2k, join = st_intersects, left = TRUE) %>%
  filter(osm_id.x==osm_id.y | is.na(osm_id.y)) %>%
  select(case_id, osm_id.x, nearest_clinic, amenity.x, osm_id.y)
```

Maintenant, nous pouvons compter les résultats : ` nrow(linelist_sf_hf_2k[is.na(linelist_sf_hf_2k$osm_id.y),])` sur 1000 cas ne recoupent aucun tampon (cette valeur est manquante), et vivent donc à plus de 30 minutes de marche de l'établissement de santé le plus proche.

```{r}
# Cas qui n'ont pas été croisés avec l'un des tampons de l'établissement de santé
linelist_sf_hf_2k %>% 
  filter(is.na(osm_id.y)) %>%
  nrow()
```

Nous pouvons visualiser les résultats de telle sorte que les cas qui n'ont intersecté aucun tampon apparaissent en rouge.  

```{r, out.width = '100%', warning=F, message=F}
tmap_mode("view")

# Affichez  d'abord les cas en points
tm_shape(linelist_sf_hf) +
  tm_dots(size=0.08, col='nearest_clinic') +

# tracer les installations cliniques en gros points noirs
tm_shape(sle_hf) +                    
  tm_dots(size=0.3, col='black')+   

# Superposez ensuite les zones tampons des établissements de santé sous forme de polylignes.
tm_shape(sle_hf_2k) +
  tm_borders(col = "black", lwd = 2) +

# Highlight cases that are not part of any health facility buffers
# en points noirs  
tm_shape(linelist_sf_hf_2k %>%  filter(is.na(osm_id.y))) +
  tm_dots(size=0.1, col='red') +
tm_view(set.view = c(-13.2284,8.4699, 13), set.zoom.limits = c(13,14))+

# ajouter le titre
tm_layout(title = "Nombre de Cas par zone couverture des cliniques ")

```


### les autres jointures  spatiales  {.unnumbered}  

Les valeurs alternatives pour l'argument `join` comprennent (de la [documentation](https://r-spatial.github.io/sf/reference/st_join.html))

* st_contains_properly  
* st_contains  
* st_covered_by  
* st_covers  
* st_crosses  
* st_disjoint  
* st_equals_exact  
* st_equals  
* st_is_within_distance  
* st_nearest_feature  
* st_overlaps  
* st_touches  
* st_within  





## Choropleth maps {}  


Les cartes choroplèthes peuvent être utiles pour visualiser vos données par zone prédéfinie, généralement une unité administrative ou une zone de santé. Dans le cadre de la réponse aux épidémies, cela peut aider à cibler l'allocation des ressources pour des zones spécifiques présentant des taux d'incidence élevés, par exemple.

Maintenant que nous avons les noms des unités administratives attribués à tous les cas (voir la section sur les jointures spatiales, ci-dessus), nous pouvons commencer à cartographier le nombre de cas par zone (cartes choroplèthes).

Puisque nous disposons également des données de population par ADM3, nous pouvons ajouter ces informations à la table *case_adm3* créée précédemment.

Nous commençons avec le cadre de données créé à l'étape précédente `case_adm3`, qui est un tableau récapitulatif de chaque unité administrative et de son nombre de cas.  

1) Les données de population `sle_adm3_pop` sont jointes à l'aide d'un `left_join()` de **dplyr** sur la base des valeurs communes à la colonne `admin3pcod` dans le dataframe `case_adm3`, et à la colonne `adm_pcode` dans le dataframe `sle_adm3_pop`. Voir la page [Joindre des données](#joining_matching)).  
2) `select()` est appliqué au nouveau dataframe, pour ne garder que les colonnes utiles - `total` est la population totale.  
3) Les cas pour 10.000 habitants sont calculés comme une nouvelle colonne avec `mutate()`.  


```{r}
# Ajouter les données de la population et calculer les cas pour 10K de population
case_adm3 <- case_adm3 %>% 
     left_join(sle_adm3_pop,                             #ajouter des colonnes à partir du jeu données pop
               by = c("admin3pcod" = "adm3_pcode")) %>%  # jointure basee sur les valeurs communes à ces deux colonnes
     select(names(case_adm3), total) %>%                 # ne conserver que les colonnes importantes, notamment la population totale
     mutate(case_10kpop = round(cases/total * 10000, 3)) # créer une nouvelle colonne avec le taux de cas pour 10000, arrondi à 3 décimales

case_adm3                                                # imprimer sur la console pour l'affichage
```

faire une Jointure  cette table au shapefile ADM3 polygones pour la cartographie.

```{r, warning=F, message=F}
case_adm3_sf <- case_adm3 %>%                 # Commencer par les cas et classer par unité administrative
  left_join(sle_adm3, by="admin3pcod") %>%    # jointure aux données shapefile par colonne commune
  select(objectid, admin3pcod,                # ne conserver que certaines colonnes d'intérêt
         admin3name = admin3name.x,           # nettoyer le nom de   ce  column
         admin2name, admin1name,
         cases, total, case_10kpop,
         geometry) %>%                        # conserver la géométrie pour que les polygones puissent être cartographier
  drop_na(objectid) %>%                       # supprimer les lignes vides
  st_as_sf()                                  # convertir en  shapefile

```


Cartographier le resultat

```{r, message=F, warning=F}
# tmap mode
tmap_mode("plot")               # carte statique

# visualiser les polygones
tm_shape(case_adm3_sf) + 
        tm_polygons("cases") +  # colorier en fonction  du nombre de cas 
        tm_text("admin3name")   # afficher les noms
```

Nous pouvons cartographier le taux d'incidence


```{r, warning=F, message=F}
# Cases per 10K population
tmap_mode("plot")             # mode affiche statique

# plot
tm_shape(case_adm3_sf) +                # visualiser lespolygons
  tm_polygons("case_10kpop",            # colorier en fonction du colonnes contenant le pourcentages des cas
              breaks=c(0, 10, 50, 100), # définir des points de rupture pour les couleurs
              palette = "Purples"       # utiliser une palette de couleurs violettes
              ) +
  tm_text("admin3name")                 # afficher le texte

```

## Cartographie avec ggplot2
Si vous êtes déjà familiarisé avec l'utilisation de **ggplot2**, vous pouvez utiliser ce package pour créer des cartes statiques de vos données. La fonction `geom_sf()` dessinera différents objets en fonction des caractéristiques (points, lignes ou polygones) présentes dans vos données. Par exemple, vous pouvez utiliser `geom_sf()` dans un `ggplot()` utilisant des données `sf` avec une géométrie polygonale pour créer une carte choroplèthe.

Pour illustrer comment cela fonctionne, nous pouvons commencer avec le fichier de forme ADM3 polygones que nous avons utilisé plus tôt. Rappelez-vous qu'il s'agit des régions de niveau administratif 3 de la Sierra Leone :

```{r}
sle_adm3
```
Nous pouvons utiliser la fonction `left_join()` de **dplyr** pour ajouter les données que nous souhaitons mapper à l'objet shapefile. Dans ce cas, nous allons utiliser le cadre de données `case_adm3` que nous avons créé plus tôt pour résumer le nombre de cas par région administrative ; cependant, nous pouvons utiliser cette même approche pour mapper n'importe quelle donnée stockée dans un cadre de données.
```{r}
sle_adm3_dat <- sle_adm3 %>% 
  inner_join(case_adm3, by = "admin3pcod") # inner join =  retenir seulement si dans les deux objets de données

select(sle_adm3_dat, admin3name.x, cases) # affiche les variables sélectionnées sur la console
```

Pour réaliser un graphique en colonnes du nombre de cas par région, en utilisant **ggplot2**, nous pourrions alors appeler `geom_col()` comme suit :

```{r, fig.align = "center"}
ggplot(data=sle_adm3_dat) +
  geom_col(aes(x=fct_reorder(admin3name.x, cases, .desc=T),   # réorganiser l'axe des x par ordre décroissant 'cases'
               y=cases)) +                                  # l'axe des y est le nombre de cas par région
  theme_bw() +
  labs(                                                     # définir le texte de la figure
    title="Number of cases, by administrative unit",
    x="Admin level 3",
    y="Number of cases"
  ) + 
  guides(x=guide_axis(angle=45))                            # angle des étiquettes de l'axe des x de 45 degrés pour un meilleur ajustement

```

Si nous voulons utiliser **ggplot2** pour réaliser une carte choroplèthe du nombre de cas, nous pouvons utiliser une syntaxe similaire pour appeler la fonction `geom_sf()` :

```{r, fig.align = "center"}
ggplot(data=sle_adm3_dat) + 
  geom_sf(aes(fill=cases))    # donnons une variable d'entrée  à fill pour varier selon le nombre de cas

```

Nous pouvons ensuite personnaliser l'apparence de notre carte en utilisant une grammaire cohérente dans **ggplot2**, par exemple :
```{r, fig.align = "center"}
ggplot(data=sle_adm3_dat) +                           
  geom_sf(aes(fill=cases)) +						
  scale_fill_continuous(high="#54278f", low="#f2f0f7") +    # changeons le gradient de couleur
  theme_bw() +
  labs(title = "Number of cases, by administrative unit",   # mettre du texte dans le graphe
       subtitle = "Admin level 3"
  )
```

Pour les utilisateurs de R qui sont à l'aise avec **ggplot2**, `geom_sf()` offre une implémentation simple et directe qui convient aux visualisations cartographiques de base. Pour en savoir plus, lisez la vignette [geom_sf()](https://ggplot2.tidyverse.org/reference/ggsf.html) ou le [livre ggplot2](https://ggplot2-book.org/maps.html). 





<!-- ======================================================= -->
### Basemaps { }

### OpenStreetMap {.unnumbered} 

Nous décrivons ci-dessous comment obtenir un plan de base pour une carte **ggplot2** en utilisant les fonctionnalités d'OpenStreetMap. Les méthodes alternatives incluent l'utilisation de **ggmap** qui nécessite un enregistrement gratuit auprès de Google ([détails](https://www.earthdatascience.org/courses/earth-analytics/lidar-raster-data-r/ggmap-basemap/)).  

[**OpenStreetMap**](https://en.wikipedia.org/wiki/OpenStreetMap) est un projet collaboratif visant à créer une carte du monde librement modifiable. Les données de géolocalisation sous-jacentes (par exemple, l'emplacement des villes, des routes, des caractéristiques naturelles, des aéroports, des écoles, des hôpitaux, des routes, etc.) sont considérées comme le principal résultat du projet.

Tout d'abord, nous chargeons le paquet **OpenStreetMap**, à partir duquel nous allons obtenir notre carte de base.  

Ensuite, nous créons l'objet `map`, que nous définissons en utilisant la fonction `openmap()` du paquet **OpenStreetMap** ([documentation](https://www.rdocumentation.org/packages/OpenStreetMap/versions/0.3.4/topics/openmap)). Nous fournissons les éléments suivants :  

* `upperLeft` et `lowerRight` Deux paires de coordonnées spécifiant les limites du carreau de la carte de base.  
* Dans ce cas, nous avons mis les valeurs max et min des lignes de la linelist, afin que la carte réponde dynamiquement aux données.  
* `zoom = ` (si null il est déterminé automatiquement)  
* `type = ` quel type de carte de base - nous avons listé plusieurs possibilités ici et le code utilise actuellement la première (`[1]`) "osm".  
* `mergeTiles = ` nous avons choisi TRUE pour que les tuiles de base soient toutes fusionnées en une seule.


```{r, message=FALSE, warning=FALSE}
# charger  package
pacman::p_load(OpenStreetMap)

# Ajustez la carte de base par le couple de coordonnées lat/long. Choisir le type de tuile
map <- openmap(
  upperLeft = c(max(linelist$lat, na.rm=T), max(linelist$lon, na.rm=T)),   # limits of basemap tile
  lowerRight = c(min(linelist$lat, na.rm=T), min(linelist$lon, na.rm=T)),
  zoom = NULL,
  type = c("osm", "stamen-toner", "stamen-terrain", "stamen-watercolor", "esri","esri-topo")[1])
```

Si nous traçons cette carte de base maintenant, en utilisant `autoplot.OpenStreetMap()` du paquet **OpenStreetMap**, vous voyez que les unités sur les axes ne sont pas des coordonnées de latitude/longitude. Il utilise un système de coordonnées différent. Pour afficher correctement les résidences du cas (qui sont stockées en lat/long), cela doit être changé.  

```{r, warning=F, message=F}
autoplot.OpenStreetMap(map)
```
Ainsi, nous voulons convertir la carte en latitude/longitude avec la fonction `openproj()` du paquet **OpenStreetMap**. Nous fournissons la carte de base `map` et nous fournissons également le système de référence de coordonnées (CRS) que nous voulons. Nous le faisons en fournissant la chaîne de caractères "proj.4" pour la projection WGS 1984, mais vous pouvez également fournir le CRS d'autres manières. (voir [cette page](https://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/understand-epsg-wkt-and-other-crs-definition-file-types/) pour mieux comprendre ce qu'est une chaîne proj.4)  

```{r, warning=F, message=F}
# Projection WGS84
map_latlon <- openproj(map, projection = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
```

Maintenant, lorsque nous créons le tracé, nous voyons que les axes contiennent des coordonnées de latitude et de longitude. Le système de coordonnées a été converti. Maintenant, nos cas seront tracés correctement s'ils sont superposés !  

```{r, warning=F, message=F}
# la representation graphique d'une carte doit se faire en  utilisant "autoplot" afin de pouvoir travailler avec ggplot.
autoplot.OpenStreetMap(map_latlon)
```

Consultez les tutoriels [ici](http://data-analytics.net/cep/Schedule_files/geospatial.html) et [ici](https://www.rdocumentation.org/packages/OpenStreetMap/versions/0.3.4/topics/autoplot.OpenStreetMap) pour plus d'informations.  




## Cartes thermiques de densité avec contours {}

Nous décrivons ci-dessous comment réaliser une carte thermique de densité des cas, sur une carte de base, en commençant par une liste de lignes (une ligne par cas).  

1) Créer une tuile basemap à partir d'OpenStreetMap, comme décrit ci-dessus.  
2) Tracez les cas de `linelist` en utilisant les colonnes de latitude et de longitude.  
3) Convertir les points en une carte thermique de densité avec `stat_density_2d()` de **ggplot2**, 


Lorsque nous avons une carte de base avec des coordonnées de latitude et de longitude, nous pouvons tracer nos cas par-dessus en utilisant les coordonnées de latitude et de longitude de leur résidence. 

En s'appuyant sur la fonction `autoplot.OpenStreetMap()` pour créer la carte de base, les fonctions de **ggplot2** s'ajouteront facilement par-dessus, comme le montre `geom_point()` ci-dessous :  

```{r, warning=F, message=F}
## la representation graphique d'une carte doit se faire en  utilisant "autoplot" afin de pouvoir travailler avec ggplot
autoplot.OpenStreetMap(map_latlon)+                 # commercer avec basemap
  geom_point(                                       # ajouter des points xy à partir des colonnes lon et lat de la linelist 
    data = linelist,                                
    aes(x = lon, y = lat),
    size = 1, 
    alpha = 0.5,
    show.legend = FALSE) +                          # abandonner entièrement la légende
  labs(x = "Longitude",                             # titres et etiquettes
       y = "Latitude",
       title = "Cumulative cases")

```
La carte ci-dessus peut être difficile à interpréter, surtout si les points se chevauchent. Vous pouvez donc tracer une carte de densité en 2d en utilisant la fonction **ggplot2** `stat_density_2d()`. Vous utilisez toujours les coordonnées lat/lon de la linelist, mais une estimation de densité à noyau 2D est effectuée et les résultats sont affichés avec des lignes de contour - comme une carte topographique. Lisez la [documentation complète ici](https://ggplot2.tidyverse.org/reference/geom_density_2d.html).  


```{r, warning=F, message=F}
# commencer par la carte de base
autoplot.OpenStreetMap(map_latlon)+
  
  # ajouter le graphique de densité
  ggplot2::stat_density_2d(
        data = linelist,
        aes(
          x = lon,
          y = lat,
          fill = ..level..,
          alpha = ..level..),
        bins = 10,
        geom = "polygon",
        contour_var = "count",
        show.legend = F) +                          
  
  # spécifier l'échelle de couleurs
  scale_fill_gradient(low = "black", high = "red")+
  
  # etiquettes
  labs(x = "Longitude",
       y = "Latitude",
       title = "Distribution of cumulative cases")

```





<!-- ======================================================= -->
### Carte thermique des séries chronologiques {.unnumbered}

La carte thermique de densité ci-dessus montre les *cas cumulés*. Nous pouvons examiner l'épidémie dans le temps et l'espace en facettant la carte de chaleur en fonction du *mois d'apparition des symptômes*, tel qu'il est dérivé de la liste des lignes.  

Nous commençons dans la `linelist`, en créant une nouvelle colonne avec l'année et le mois d'apparition des symptômes. La fonction `format()` de **base** R change la façon dont une date est affichée. Dans ce cas, nous voulons "YYYY-MM".  

```{r, warning=F, message=F}
# Extrait mois de l'apparition
linelist <- linelist %>% 
  mutate(date_onset_ym = format(date_onset, "%Y-%m"))

# Examiner les valeurs 
table(linelist$date_onset_ym, useNA = "always")
```

Maintenant, nous introduisons simplement le facettage via **ggplot2** à la carte thermique de densité. `facet_wrap()` est appliqué, en utilisant les nouvelles colonnes comme lignes. Nous avons fixé le nombre de colonnes de facettes à 3 pour plus de clarté.  


```{r, warning=F, message=F}
# packages
pacman::p_load(OpenStreetMap, tidyverse)

# commencer avec  basemap
autoplot.OpenStreetMap(map_latlon)+
  
  # ajouter le graphique de densité
  ggplot2::stat_density_2d(
        data = linelist,
        aes(
          x = lon,
          y = lat,
          fill = ..level..,
          alpha = ..level..),
        bins = 10,
        geom = "polygon",
        contour_var = "count",
        show.legend = F) +                          
  
  # spécifier l'échelle de couleurs
  scale_fill_gradient(low = "black", high = "red")+
  
  # etiquettes
  labs(x = "Longitude",
       y = "Latitude",
       title = "Distribution of cumulative cases over time")+
  
  # facetter le graphique par mois-année de début d'activité
  facet_wrap(~ date_onset_ym, ncol = 4)               

```



<!-- LA SECTION SUR LES STATISTIQUES SPATIALES EST EN COURS DE DÉVELOPPEMENT --> 
## Statistiques spatiales
Jusqu'à présent, la plupart de nos discussions ont porté sur la visualisation des données spatiales. Dans certains cas, vous pouvez également être intéressé par l'utilisation des *statistiques spatiales* pour mesurer la force des relations spatiales des attributs dans vos données. Cette section donne un bref aperçu de certains concepts clés des statistiques spatiales et suggère quelques ressources utiles à explorer si vous souhaitez effectuer des analyses spatiales plus poussées. 

### Les relations spatiales {.unnumbered}  

Avant de pouvoir calculer toute statistique spatiale, nous devons spécifier les relations entre les objets de nos données. Il existe de nombreuses façons de conceptualiser les relations spatiales, mais un modèle simple et couramment appliqué est celui de la *adjacence* - plus précisément, nous nous attendons à une relation géographique entre les zones qui partagent une frontière ou qui sont "voisines" les unes des autres. 

Nous pouvons quantifier les relations d'adjencence entre les polygones des régions administratives dans les données `sle_adm3` que nous avons utilisées avec le package **spdep**. Nous allons spécifier une contiguïté *queen*, ce qui signifie que les voisins possèdent au moins un segment de frontière commune.les voisins possèdent au moins un segment de frontière commune Dans notre cas, avec des polygones irréguliers, la distinction est triviale, mais dans certains cas, le choix entre la reine et la tour peut ne pas etre evident


 
```{r}
sle_nb <- spdep::poly2nb(sle_adm3_dat, queen=T) #Extraction de la liste des voisins 
sle_adjmat <- spdep::nb2mat(sle_nb)    # créer une matrice résumant les relations entre voisins
sle_listw <- spdep::nb2listw(sle_nb)   # créer l'objet listw (liste de poids) -- nous en aurons besoin plus tard

sle_nb
round(sle_adjmat, digits = 2)
```

La matrice affiché ci-dessus montre les relations entre les 9 régions de nos données `sle_adm3`. Un score de 0 indique que deux régions ne sont pas voisines, tandis que toute valeur différente de 0 indique une relation de voisinage. Les valeurs de la matrice sont normalisées afin que chaque région ait un poids total  par ligne egale à 1.

Une meilleure façon de faire ressortir ces relations de voisinage est de les reprensenter graphiquement :

```{r, fig.align='center', results='hide'}
plot(sle_adm3_dat$geometry) +                                           # visualiser les limites de la region
  spdep::plot.nb(sle_nb,as(sle_adm3_dat, 'Spatial'), col='grey', add=T) # ajout des relation de voisinage
```

Nous avons utilisé une approche d'adjacence pour identifier les polygones voisins ; les voisins que nous avons identifiés sont aussi parfois appelés **voisins basés sur la contiguïté**. Mais ce n'est qu'une façon de choisir les régions qui sont censées avoir une relation géographique. Les approches alternatives les plus courantes pour identifier les relations géographiques génèrent des **voisins basés sur la distance** ; brièvement, il s'agit de :
  
  * **K-plus proches voisins** - Sur la base de la distance entre les centroïdes (le centre géographiquement pondéré de chaque région polygonale), sélectionnez les *n* régions les plus proches comme voisines. Un seuil de proximité de distance maximale peut également être fixé. Dans **spdep**, vous pouvez utiliser `knearneigh()` (voir [documentation](https://r-spatial.github.io/spdep/reference/knearneigh.html)).
  
  * **Distance threshold neighbors** - Sélectionne  les voisins sur la base  d'une un seuil de distance definie. Dans **spdep**, ces relations de voisinage peuvent être identifiées en utilisant `dnearneigh()` (voir [documentation](https://www.rdocumentation.org/packages/spdep/versions/1.1-7/topics/dnearneigh)).

### Autocorrélation spatiale {.unnumbered}  

La première loi de la géographie de Tobler, souvent citée, stipule que "Tout interagit avec tout, mais deux objets proches ont plus de
chance de le faire que deux objets éloignés". En épidémiologie, cela signifie souvent que la probabilité d'obtenir un résultat sanitaire particulier dans une région donnée est plus liée  au resultats des régions voisines qu'à celui des régions éloignées. Ce concept a été formalisé sous le nom d'**autocorrélation spatiale** - la propriété statistique selon laquelle les objets spatiaux ayant des valeurs similaires sont localisées dans l'espace. Les mesures statistiques de l'autocorrélation spatiale peuvent être utilisées pour *quantifier la dépendance spatiale* dans vos données, *localiser l'endroit où ont trouve des patterns*, et *identifier les patterns d'autocorrélation spatiale* entre des variables distinctes dans vos données. Cette section donne un aperçu de certaines mesures courantes d'autocorrélation spatiale et de la façon de les calculer dans R.

**I de Moran** - Il s'agit d'une statistique globale de synthèse de la corrélation entre la valeur d'une variable dans une région et les valeurs de cette même variable dans les régions voisines. La statistique I de Moran est généralement comprise entre -1 et 1. Une valeur de 0 n'indique aucun modèle de corrélation spatiale, tandis que des valeurs plus proches de 1 ou de -1 indiquent une autocorrélation spatiale plus forte (valeurs similaires proches) ou une dispersion spatiale (valeurs dissemblables proches), respectivement.

Par exemple, nous allons calculer une statistique I de Moran pour quantifier l'autocorrélation spatiale dans les cas d'Ebola que nous avons cartographiés plus tôt (rappelez-vous, il s'agit d'un sous-ensemble de cas provenant du cadre de données de la `linelist` épidémique simulée). Le paquet **spdep** possède une fonction, `moran.test`, qui peut faire ce calcul pour nous :



```{r}
moran_i <-spdep::moran.test(sle_adm3_dat$cases,    # vecteur numérique avec la variable d'intérêt
                            listw=sle_listw)       # listw object résumant les relations de voisinage

moran_i                                            # print les résultats du test I de Moran
```
La sortie de la fonction `moran.test()` nous montre une statistique de Moran I de ` round(moran_i$estimate[1],2)`. Cela indique la présence d'une autocorrélation spatiale dans nos données - plus précisément, que les régions présentant un nombre similaire de cas d'Ebola sont susceptibles d'être proches les unes des autres. La valeur p fournie par `moran.test()` est générée par comparaison avec l'espérance sous l'hypothèse nulle d'absence d'autocorrélation spatiale, et peut être utilisée si vous avez besoin de rapporter les résultats d'un test d'hypothèse formel.

**Local Moran's I** - Nous pouvons décomposer la statistique (globale) de Moran I calculée ci-dessus pour identifier l'autocorrélation spatiale *localisée*, c'est-à-dire pour identifier des groupes spécifiques dans nos données. Cette statistique, qui est parfois appelée **Indicateur local d'association spatiale (LISA)**, résume l'étendue de l'autocorrélation spatiale autour de chaque région individuelle. Elle peut être utile pour trouver les points "chauds" et "froids" sur la carte.

Pour montrer un exemple, nous pouvons calculer et cartographier le I de Moran local pour les comptes de cas d'Ebola utilisés ci-dessus, avec la fonction `local_moran()` de **spdep** :
```{r, fig.align='center'}
# calculer le I de Moran local
local_moran <- spdep::localmoran(                  
  sle_adm3_dat$cases,                              # variable d'interet 
  listw=sle_listw                                  # listw object avec la ponderation de voisinage
)

# joindre les results  à un jeu de données sf 
sle_adm3_dat<- cbind(sle_adm3_dat, local_moran)    

# cartographier
ggplot(data=sle_adm3_dat) +
  geom_sf(aes(fill=Ii)) +
  theme_bw() +
  scale_fill_gradient2(low="#2c7bb6", mid="#ffffbf", high="#d7191c",
                       name="Local Moran's I") +
  labs(title="Local Moran's I statistic for Ebola cases",
       subtitle="Admin level 3 regions, Sierra Leone")

```

**Getis-Ord Gi\*** - Il s'agit d'une autre statistique couramment utilisée pour l'analyse des points chauds ; en grande partie, la popularité de cette statistique est liée à son utilisation dans l'outil d'analyse des points chauds d'ArcGIS. Elle est basée sur l'hypothèse que, généralement, la différence de valeur d'une variable entre des régions voisines devrait suivre une distribution normale. Elle utilise une approche de type z-score pour identifier les régions qui ont des valeurs significativement plus élevées (point chaud) ou significativement plus basses (point froid) d'une variable spécifiée, par rapport à leurs voisins. 

Nous pouvons calculer et cartographier la statistique Gi* en utilisant la fonction `localG()` de **spdep** :  

```{r}
# Effectuer une analyse G locale
getis_ord <- spdep::localG(
  sle_adm3_dat$cases,
  sle_listw
)

# joindre les résultats aux données SF
sle_adm3_dat$getis_ord <- as.numeric(getis_ord)

# afficher la carte
ggplot(data=sle_adm3_dat) +
  geom_sf(aes(fill=getis_ord)) +
  theme_bw() +
  scale_fill_gradient2(low="#2c7bb6", mid="#ffffbf", high="#d7191c",
                       name="Gi*") +
  labs(title="Getis-Ord Gi* statistic for Ebola cases",
       subtitle="Admin level 3 regions, Sierra Leone")

```


Comme vous pouvez le constater, la carte des Getis-Ord Gi* est légèrement différente de la carte des Moran locaux que j'ai produite précédemment. Cela reflète le fait que la méthode utilisée pour calculer ces deux statistiques est légèrement différente ; celle que vous devez utiliser dépend de votre cas d'utilisation spécifique et de la question de recherche qui vous intéresse.

**Test L de Lee** - Il s'agit d'un test statistique de corrélation spatiale bivariée. Il vous permet de vérifier si la configuration spatiale d'une variable donnée *x* est similaire à la configuration spatiale d'une autre variable, *y*, dont on suppose qu'elle est liée spatialement à *x*. 

Pour donner un exemple, testons si la configuration spatiale des cas d'Ebola de l'épidémie simulée est corrélée à la configuration spatiale de la population. Pour commencer, nous devons avoir une variable `population` dans nos données `sle_adm3`. Nous pouvons utiliser la variable `total` du dataframe `sle_adm3_pop` que nous avons chargé précédemment.

```{r}
sle_adm3_dat <- sle_adm3_dat %>% 
  rename(population = total)                          # renommer 'total' en  'population'
```

Nous pouvons rapidement visualiser les configurations spatiales des deux variables côte à côte, pour voir si elles se ressemblent :
```{r, fig.align='center', warning=F, message=F}
tmap_mode("plot")

cases_map <- tm_shape(sle_adm3_dat) + tm_polygons("cases") + tm_layout(main.title="Cases")
pop_map <- tm_shape(sle_adm3_dat) + tm_polygons("population") + tm_layout(main.title="Population")

tmap_arrange(cases_map, pop_map, ncol=2)   # arranger en facettes 2x1
```

Visuellement, les modèles semblent dissemblables. Nous pouvons utiliser la fonction `lee.test()` de **spdep** pour tester statistiquement si le modèle d'autocorrélation spatiale des deux variables est lié. La statistique L sera proche de 0 s'il n'y a pas de corrélation entre les modèles, proche de 1 s'il y a une forte corrélation positive (c'est-à-dire que les modèles sont similaires), et proche de -1 s'il y a une forte corrélation négative (c'est-à-dire que les modèles sont inverses). 

```{r, warning=F, message=F}
lee_test <- spdep::lee.test(
  x=sle_adm3_dat$cases,          # variable 1 à comparer
  y=sle_adm3_dat$population,     # variable 2 à comparer
  listw=sle_listw                # listw objet avec les poids des voisins
)

lee_test
```

Le résultat ci-dessus montre que la statistique L de Lee pour nos deux variables était ` round(lee_test$estimate[1],2)`, ce qui indique une faible corrélation négative. Cela confirme notre évaluation visuelle selon laquelle le schéma des cas et la population ne sont pas liés l'un à l'autre, et fournit la preuve que le schéma spatial des cas n'est pas strictement le résultat de la densité de population dans les zones à haut risque.

La statistique de Lee L peut être utile pour faire ce genre d'inférences sur la relation entre des variables distribuées dans l'espace ; cependant, pour décrire la nature de la relation entre deux variables de manière plus détaillée, ou pour ajuster les facteurs de confusion, des techniques de régression *spatiale* seront nécessaires. Celles-ci sont décrites brièvement dans la section suivante.

### Régression spatiale {.unnumbered}  

Vous pouvez souhaiter faire des inférences statistiques sur les relations entre les variables de vos données spatiales. Dans ce cas, il est utile d'envisager des techniques de régression *spatiale*, c'est-à-dire des approches de la régression qui prennent explicitement en compte l'organisation spatiale des unités dans vos données. Voici quelques raisons pour lesquelles vous pouvez avoir besoin d'envisager des modèles de régression spatiale, plutôt que des modèles de régression standard tels que les GLM :

  * Les modèles de régression standard supposent que les résidus sont indépendants les uns des autres. En présence d'une forte *autocorrélation spatiale*, les résidus d'un modèle de régression standard sont susceptibles d'être également autocorrélés dans l'espace, violant ainsi cette hypothèse. Cela peut entraîner des problèmes d'interprétation des résultats du modèle, auquel cas un modèle spatial serait préférable.
  
  * Les modèles de régression supposent aussi généralement que l'effet d'une variable *x* est constant sur toutes les observations. Dans le cas d'une *hétérogénéité spatiale*, les effets que nous souhaitons estimer peuvent varier dans l'espace, et nous pouvons être intéressés par la quantification de ces différences. Dans ce cas, les modèles de régression spatiale offrent plus de flexibilité pour estimer et interpréter les effets.
  
Les détails des approches de régression spatiale dépassent le cadre de ce manuel. Cette section donnera plutôt un aperçu des modèles de régression spatiale les plus courants et de leurs utilisations, et vous renverra à des références qui pourront vous être utiles si vous souhaitez approfondir ce domaine.

**Modèles d'erreurs spatiales** - Ces modèles supposent que les termes d'erreur entre les unités spatiales sont corrélés, auquel cas les données violeraient les hypothèses d'un modèle MCO standard. Les modèles d'erreur spatiaux sont aussi parfois appelés **modèles autorégressifs simultanés (SAR)**. Ils peuvent être ajustés en utilisant la fonction `errorsarlm()` du paquet **spatialreg** (fonctions de régression spatiale qui faisaient autrefois partie de **spdep**). 

**Modèles de décalage spatial** - Ces modèles supposent que la variable dépendante pour une région *i* est influencée non seulement par la valeur des variables indépendantes dans *i*, mais aussi par les valeurs de ces variables dans les régions voisines de *i*. Comme les modèles à erreurs spatiales, les modèles à décalage spatial sont aussi parfois décrits comme des **modèles autorégressifs simultanés (SAR)**.  Ils peuvent être ajustés en utilisant la fonction `lagsarlm()` du paquet **spatialreg**.

Le paquet **spdep** contient plusieurs tests de diagnostic utiles pour décider entre les modèles MCO standard, les modèles à décalage spatial et les modèles à erreur spatiale. Ces tests, appelés *Diagnostics du multiplicateur de Lagrange*, peuvent être utilisés pour identifier le type de dépendance spatiale de vos données et choisir le modèle le plus approprié. La fonction `lm.LMtests()` peut être utilisée pour calculer tous les tests du multiplicateur de Lagrange. Anselin (1988) fournit également un diagramme de flux utile pour décider du modèle de régression spatiale à utiliser en fonction des résultats des tests du multiplicateur de Lagrange :

```{r, fig.align='center', echo=F}
knitr::include_graphics(here::here("images", "gis_lmflowchart.jpg"))
```

**Modèles hiérarchiques bayésiens** - Les approches bayésiennes sont couramment utilisées pour certaines applications de l'analyse spatiale, le plus souvent pour la [cartographie des maladies](https://pubmed.ncbi.nlm.nih.gov/15690999/). Elles sont préférables dans les cas où les données de cas sont peu distribuées (par exemple, dans le cas d'un résultat rare) ou statistiquement "bruyantes", car elles peuvent être utilisées pour générer des estimations "lissées" du risque de maladie en tenant compte du processus spatial latent sous-jacent. Cela peut améliorer la qualité des estimations. Elles permettent également à l'enquêteur de préspécifier (via le choix de l'antériorité) les modèles de corrélation spatiale complexes qui peuvent exister dans les données, ce qui peut rendre compte de la variation spatialement dépendante et indépendante des variables indépendantes et dépendantes. Dans R, les modèles hiérarchiques bayésiens peuvent être ajustés à l'aide du paquet **CARbayes** (voir [vignette](https://cran.r-project.org/web/packages/CARBayes/vignettes/CARBayes.pdf)) ou de R-INLA (voir [site web](https://www.r-inla.org/home) et [manuel](https://becarioprecario.bitbucket.io/inla-gitbook/)). R peut également être utilisé pour appeler un logiciel externe qui effectue des estimations bayésiennes, comme JAGS ou WinBUGS.

<!-- ======================================================= -->
## Resources {  }

* R Simple Features and sf package [vignette](https://cran.r-project.org/web/packages/sf/vignettes/sf1.html)


* R tmap package [vignette](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html)


* ggmap: [Spatial Visualization with ggplot2](https://journal.r-project.org/archive/2013-1/kahle-wickham.pdf)


* [Intro to making maps with R, overview of different packages](https://bookdown.org/nicohahn/making_maps_with_r5/docs/introduction.html)  

* Spatial Data in R [(EarthLab course)](https://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/)

* Applied Spatial Data Analysis in R [textbook](https://link.springer.com/book/10.1007/978-1-4614-7618-4)

* **SpatialEpiApp** - a [Shiny app that is downloadable as an R package](https://github.com/Paula-Moraga/SpatialEpiApp), allowing you to provide your own data and conduct mapping, cluster analysis, and spatial statistics.  


* An Introduction to Spatial Econometrics in R [workshop](http://www.econ.uiuc.edu/~lab/workshop/Spatial_in_R.html)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/gis.Rmd-->

# (PART) Data Visualization {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_data_viz.Rmd-->

# Présenter avec des tables {#tables_presentation}

```{r echo=FALSE, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}

linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds")) 

border_style = officer::fp_border(color="black", width=1)

pacman::p_load(
  rio,            # import/export
  here,           # file pathways
  flextable,      # make HTML tables 
  officer,        # helper functions for tables
  tidyverse)      # data management, summary, and visualization

table <- linelist %>% 
  # filter
  ########
  #filter(!is.na(outcome) & hospital != "Missing") %>%  # Remove cases with missing outcome or hospital
  
  # Get summary values per hospital-outcome group
  ###############################################
  group_by(hospital, outcome) %>%                      # Group data
  summarise(                                           # Create new summary columns of indicators of interest
    N = n(),                                            # Number of rows per hospital-outcome group     
    ct_value = median(ct_blood, na.rm=T)) %>%           # median CT value per group
  
  # add totals
  ############
  bind_rows(                                           # Bind the previous table with this mini-table of totals
    linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouped only by outcome, not by hospital    
      summarise(
        N = n(),                                       # Number of rows for whole dataset     
        ct_value = median(ct_blood, na.rm=T))) %>%     # Median CT for whole dataset
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivot from long to wide
    values_from = c(ct_value, N),                       # new values are from ct and count columns
    names_from = outcome) %>%                           # new column names are from outcomes
  mutate(                                              # Add new columns
    N_Known = N_Death + N_Recover,                               # number with known outcome
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # percent who recovered (to 1 decimal)
  select(                                              # Re-order columns
    hospital, N_Known,                                   # Intro columns
    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns
    N_Death, Pct_Death, ct_value_Death)  %>%             # Death columns
  arrange(N_Known) %>%                                 # Arrange rows from lowest to highest (Total row at bottom)

  # formatting
  ############
  flextable() %>% 
  add_header_row(
    top = TRUE,                # New header goes on top of existing header row
    values = c("Hospital",     # Header values for each column below
               "Total cases with known outcome", 
               "Recovered",    # This will be the top-level header for this and two next columns
               "",
               "",
               "Died",         # This will be the top-level header for this and two next columns
               "",             # Leave blank, as it will be merged with "Died"
               "")) %>% 
    set_header_labels(         # Rename the columns in original header row
      hospital = "", 
      N_Known = "",                  
      N_Recover = "Total",
      Pct_Recover = "% of cases",
      ct_value_Recover = "Median CT values",
      N_Death = "Total",
      Pct_Death = "% of cases",
      ct_value_Death = "Median CT values")  %>% 
  merge_at(i = 1, j = 3:5, part = "header") %>% # Horizontally merge columns 3 to 5 in new header row
  merge_at(i = 1, j = 6:8, part = "header") %>%  
  border_remove() %>%  
  theme_booktabs() %>% 
  vline(part = "all", j = 2, border = border_style) %>%   # at column 2 
  vline(part = "all", j = 5, border = border_style) %>%   # at column 5
  merge_at(i = 1:2, j = 1, part = "header") %>% 
  merge_at(i = 1:2, j = 2, part = "header") %>% 
  width(j=1, width = 2.7) %>% 
  width(j=2, width = 1.5) %>% 
  width(j=c(4,5,7,8), width = 1) %>% 
  flextable::align(., align = "center", j = c(2:8), part = "all") %>% 
  bg(., part = "body", bg = "gray95")  %>% 
  #bg(., j=c(1:8), i= ~ hospital == "Military Hospital", part = "body", bg = "#91c293") %>% 
  bg(j = 7, i = ~ Pct_Death >= 55, part = "body", bg = "red") %>% 
  colformat_num(., j = c(4,7), digits = 1) %>%
  bold(i = 1, bold = TRUE, part = "header") %>% 
  bold(i = 7, bold = TRUE, part = "body")

table
```

Cette page montre comment synthétiser les données d'un tableau ou d'un fichier dans des tables prêtes à être présentées en utilisant le package **flextable** (on appelera ces tables les *tables finales* à présenter). Ces tables peuvent être insérées dans des diapositives Powerpoint, des pages HTML, des documents PDF ou Word, etc.

Comprenez qu'*avant* d'utiliser **flextable**, vous devez créer la *table finale* sous forme de tableau de données. Utilisez les méthodes décrites dans les pages [Tables descriptives](#descriptive_tables) et [Restructurer des données](#pivoting_data) telles que les tabulations, les tableaux croisés, les pivots et le calcul de statistiques descriptives pour cela. Le tableau de données résultant peut ensuite être fourni à **flextable** pour le formatage de l'aspect de la table finale à présenter.

Il existe de nombreuses autres extensions ("packages") R qui peuvent être utilisées pour créer des tables pour les présentations- nous avons choisi de mettre en avant **flextable** dans cette page. Un exemple d'utilisation du package **knitr** et de sa fonction `kable()` se trouve à la page [Suivi de contacts](#contact_tracing). De même, le package **DT** est mis en avant dans la page [Tableaux de bord avec Shiny](#shiny). D'autres, comme **GT** et **huxtable**, sont mentionnés dans la page [Packages suggérés](#suggested_packages).

<!-- ======================================================= -->

## Préparation

### Charger les packages {.unnumbered}

Installer et charger **flextable**. Dans ce manuel, nous mettons l'accent sur la fonction `p_load()` du "package" **pacman**, qui installe le (ou une liste de) "package (s)" que si nécessaire (uniquement si le package n'est pas déjà installé) et le charge pour l'utiliser . Vous pouvez également charger des "packages" avec `library()` à partir de `R` **base**. Voir la page sur [Bases de R](#rbasics) pour plus d'informations sur les "packages" `R`.

```{r}
pacman::p_load(
  rio,            # importer/exporter
  here,           # chemin vers les fichiers
  flextable,      # creer des tables HTML  
  officer,        # fonctions d'aide pour les tables
  tidyverse)      # data management, resume, et visualisation

```

### Importer des données {.unnumbered}

Pour commencer, nous importons la liste linéaire ("linelist") nettoyée des cas d'une épidémie d'Ebola qui a été simulée. Si vous voulez suivre en travaillant sur la base, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la version "clean" </a> (en fichier .rds). Importez les données avec la fonction `import()` du "package" **rio** (elle gère de nombreux types de fichiers comme .xlsx, .csv, .rds - voir la page [Importation et exportation](#import_export) pour plus de détails).

```{r, echo=F}
# importer la liste lineaire dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# importer la liste lineaire
linelist <- import("linelist_cleaned.rds")
```

Les 50 premières lignes de la liste linéaire sont affichées ci-dessous.

```{r, message=FALSE, echo=F}
# afficher la liste lineaire sous forme de table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Préparer la table {.unnumbered}

Comme expliqué plus haut, *avant* de commencer à utiliser **flextable**, vous devez d'abord *créer* la table que vous voulez présenter sous forme de tableau de données. Consultez la page sur les [Tables descriptives](#descriptive_tables) et les [Données pivotantes](#pivoting_data) pour apprendre à créer un tableau de données à l'aide de "packages" tels que **janitor** et **dplyr**. Vous devez disposer le contenu en lignes et en colonnes comme vous voulez qu'il soit affiché. C'est à dire on part de notre base de données principales, on lui applique les modifications et opérations nécessaires pour synthétiser l'information que l'on veut présenter dans notre table finale et ce résultat sera enregistré dans un tableu de données. Ensuite, ce tableau de données sera soumis à **flextable** pour l'afficher avec la mise en forme voulue ajoutant des couleurs, des en-têtes, des polices, etc.

Voici un exemple tiré de la page [Tables descriptives](#descriptive_tables) sur la conversion de la "liste linéaire des cas" en un tableau de données qui résume/synthétise l'issue finale des patients et les valeurs CT (seuil de cycle dans un test de détection du virus par PCR) par hôpital, avec une ligne de totaux en bas. Le résultat est enregistré sous le nom de `table`.

```{r message=FALSE, warning=FALSE}
table <- linelist %>% 
  
  # Obtenez les valeurs résumees par hôpital et issue finale
  ###############################################
  group_by(hospital, outcome) %>%                      # Grouper les donnees selon ces deux variables
  summarise(                                           # Creer un nouveau résumé des variables d'intérêt 
    N = n(),                                            # Nombre de lignes par  groupe "hospital-outcome"     
    ct_value = median(ct_blood, na.rm=T)) %>%           # Valeur CT mediane  par groupe
  
  # ajouter le total
  ############
  bind_rows(                                           # Liez le tableau précédent avec ce mini-tableau de totaux
    linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouper avec var "outcome" uniquement et non par  "hospital"    
      summarise(
        N = n(),                                       # Nombre de lignes pour l'ensemble des données    
        ct_value = median(ct_blood, na.rm=T))) %>%     # Valeur CT mediane pour l'ensemble des données    
  
  # Modifier en format long-large
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivoter du format long au format large
    values_from = c(ct_value, N),                       # nouvelles valeurs sont crees depuis les vars "ct" et "count"
    names_from = outcome) %>%                           # nouveaux noms de colonne crees depuis var "outcomes"
  mutate(                                              # Creer de nouvelles colonnes
    N_Known = N_Death + N_Recover,                               # nombre de cas avec l'issue finale connue
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # pourcentage de cas decedes (1 decimale)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # pourcentage de cas gueris (1 decimale)
  select(                                              # Re-ordonner l'apparition des colonnes
    hospital, N_Known,                                   # Colonnes d'Intro
    N_Recover, Pct_Recover, ct_value_Recover,            # Colonnes concernant les gueris
    N_Death, Pct_Death, ct_value_Death)  %>%             # Colonnes concernant les deces
  arrange(N_Known)                                    # Trier les lignes de façon croissante (Total de la ligne en dernier)

table  # afficher la table

```

<!-- ======================================================= -->

## Premiers pas avec flextable

### Créer un objet flextable {.unnumbered}

Pour créer et gérer les objets **flextable**, nous passons d'abord le tableau de données avec les informqtions que nous voulons présenter par la fonction `flextable()` et nous enregistrons le résultat sous le nom de `my_table`.

```{r}

my_table <- flextable(table) 
my_table

```

Après avoir fait cela, nous pouvons progressivement faire passer l'objet `my_table` par plus de fonctions de formatage **flextable**.

Dans cette page, pour des raisons de clarté, nous sauvegarderons la table à des étapes intermédiaires sous le nom de `my_table`, en ajoutant des fonctions **flextable** étape par étape. Si vous voulez voir *tout* le code du début à la fin écrit en un seul bloc, visitez la section [Tout le code ensemble](#tbl_pres_all) ci-dessous.

La syntaxe générale de chaque ligne de code **flextable** est la suivante :

-   `function(table, i = X, j = X, part = "X")`, where:

    -   La "fonction" peut être l'une des nombreuses fonctions différentes, telles que `width()` pour déterminer la largeur des colonnes, `bg()` pour définir les couleurs d'arrière-plan, `align()` pour définir si le texte est aligné au centre/à droite/à gauche, et ainsi de suite.
    -   `table =` est le nom du tableau de données, mais il n'est pas nécessaire de l'indiquer si le tableau de données est intégré à la fonction.
    -   `part =` indique la partie de la table à laquelle la fonction appelée sera appliqué. Par ex: "header" pour l'entête de la table, "body" pour le corps de la table ou "all" pour toutes les parties de la table.
    -   `i =` spécifie la *ligne* à laquelle appliquer la fonction, où 'X' est le numéro de la ligne. S'il s'agit de plusieurs lignes, par exemple de la première à la troisième ligne, on peut spécifier : `i = c(1:3)`. Notez que si 'body' est sélectionné, la première ligne commence sous la section d'en-tête.
    -   `j =` spécifie la *colonne* à laquelle appliquer la fonction, où 'x' est le numéro ou le nom de la colonne. Si plusieurs colonnes, par exemple la cinquième et la sixième, on peut spécifier : \`j = c(5,6).

Vous pouvez trouver la liste complète des fonctions de formatage **flextable** [ici](https://davidgohel.github.io/flextable/reference/index.html) ou consulter la documentation en tapant `?flextable`.

### Largeur de la colonne {.unnumbered}

Nous pouvons utiliser la fonction `autofit()`, qui permet d'étirer et de réajuster la table de façon esthétique de sorte que chaque cellule ne comporte qu'une seule ligne de texte. La fonction `qflextable()` est un raccourci pratique pour `flextable()` et `autofit()`.

```{r}

my_table %>% autofit()

```

Cependant, cela n'est pas toujours approprié, surtout si les cellules contiennent des valeurs très longues, ce qui signifie que le tableau risque de ne pas tenir sur la page.

À la place, nous pouvons spécifier des largeurs avec la fonction `width()`. Il faut parfois essayer plusieurs valeurs pour savoir laquelle correspond le mieux. Dans l'exemple ci-dessous, nous spécifions des largeurs différentes pour la colonne 1, la colonne 2 et les colonnes 4 à 8.

```{r}

my_table <- my_table %>% 
  width(j=1, width = 2.7) %>% 
  width(j=2, width = 1.5) %>% 
  width(j=c(4,5,7,8), width = 1)

my_table
  
```

### En-têtes de colonnes {.unnumbered}

Nous voulons des en-têtes plus clairs pour faciliter l'interprétation du contenu du tableau.

Pour cette table, nous voudrons ajouter une deuxième couche d'en-tête afin que les colonnes couvrant les mêmes sous-groupes puissent être regroupées. Nous faisons cela avec la fonction `add_header_row()` avec `top = TRUE`. Nous précisons le nouveau nom de chaque colonne avec l'option `values =`, en laissant des valeurs vides `""` pour les colonnes que nous savons que nous fusionnerons plus tard.

Nous renommons également les noms des en-têtes dans le désormais deuxième en-tête dans une commande séparée en utilisant `set_header_labels()`.

Enfin, pour "regrouper" certains en-têtes de colonnes dans l'en-tête supérieur, nous utilisons `merge_at()` pour fusionner les en-têtes de colonnes dans la ligne d'en-tête supérieure.

```{r}
my_table <- my_table %>% 
  
  add_header_row(
    top = TRUE,                # Nouvel en-tête placé au-dessus de la rangée d'en-tête existante
    values = c("Hospital",     # Valeurs d'en-tête pour chaque colonne ci-dessous
               "Total cases with known outcome", 
               "Recovered",    # Celui ci servira d'en-tête de niveau supérieur pour cette colonne et les deux suivantes
               "",
               "",
               "Died",         # Celui ci servira d'en-tête de niveau supérieur pour cette colonne et les deux suivantes
               "",             # Laisser vide comme ce sera fusionné avec "Died".
               "")) %>% 
    
  set_header_labels(         # Renommer les colonnes de la ligne d'en-tête originale
      hospital = "", 
      N_Known = "",                  
      N_Recover = "Total",
      Pct_Recover = "% of cases",
      ct_value_Recover = "Median CT values",
      N_Death = "Total",
      Pct_Death = "% of cases",
      ct_value_Death = "Median CT values")  %>% 
  
  merge_at(i = 1, j = 3:5, part = "header") %>% # Fusionner horizontalement les colonnes 3 à 5 dans une nouvelle ligne d'en-tête
  merge_at(i = 1, j = 6:8, part = "header")     # Fusionnez horizontalement les colonnes 6 à 8 dans une nouvelle ligne d'en-tête.

my_table  # afficher la table résultante

```

### Bordures et arrière-plan {.unnumbered}

Vous pouvez ajuster les bordures, les lignes internes, etc. avec diverses fonctions **flextable**. Il est souvent plus facile de commencer par supprimer toutes les bordures existantes avec `border_remove()`.

Ensuite, vous pouvez appliquer des thèmes de bordure par défaut en passant la table à `theme_box()`, `theme_booktabs()`, ou `theme_alafoli()`.

Vous pouvez ajouter des lignes verticales et horizontales avec une variété de fonctions. `hline()` et `vline()` ajoutent des lignes à une ligne ou une colonne spécifiée, respectivement. Dans chacune d'elles, vous devez spécifier dans quelle partie de la table vous voulez le rajouter en précisant `part =` comme étant soit "all","body" ou "header". Pour les lignes verticales, spécifiez la colonne à l'argument `j =`, et pour les lignes horizontales la ligne à `i =`. D'autres fonctions comme `vline_right()`, `vline_left()`, `hline_top()`, et `hline_bottom()` ajoutent des lignes aux bords externes de la table seulement.

Dans toutes ces fonctions, le style de ligne lui-même doit être spécifié par `border =` et doit être le résultat d'une commande séparée utilisant la fonction `fp_border()` du "package" **officer**. Cette fonction vous aide à définir la largeur et la couleur de la ligne. Vous pouvez la définir avant d'appeler les commandes de table, comme indiqué ci-dessous.

```{r}
# définir le style de la ligne de bordure
border_style = officer::fp_border(color="black", width=1)

# ajouter des lignes de bordure au tableau
my_table <- my_table %>% 

  # Enlever toutes les bordures existantes
  border_remove() %>%  
  
  # ajouter des lignes horizontales via un thème prédéterminé
  theme_booktabs() %>% 
  
  # ajouter des lignes verticales pour séparer les sections "Recovered" et "Died"
  vline(part = "all", j = 2, border = border_style) %>%   # a la colonne 2 
  vline(part = "all", j = 5, border = border_style)       # a la colonne 5

my_table
```

### Police et alignement {.unnumbered}

Nous alignons au centre toutes les colonnes, sauf la colonne la plus à gauche, avec les noms des hôpitaux, en utilisant la fonction `align()` de **flextable**.

```{r}
my_table <- my_table %>% 
   flextable::align(align = "center", j = c(2:8), part = "all") 
my_table
```

De plus, nous pouvons augmenter la taille de la police de l'en-tête et la mettre en gras. Nous pouvons également mettre en gras la ligne du total.

```{r}

my_table <-  my_table %>%  
  fontsize(i = 1, size = 12, part = "header") %>%   # ajuster la taille de la police de l'en-tête
  bold(i = 1, bold = TRUE, part = "header") %>%     # ajuster le caractère en gras de l'en-tête
  bold(i = 7, bold = TRUE, part = "body")           # ajuster les caractères en gras de la ligne totale (ligne 7 du corps de la table)

my_table

```

Pour aérer la table, nous pouvons nous assurer que les colonnes de proportion n'affichent qu'une seule décimale en utilisant la fonction `colformat_num()`. Notez que cela aurait également pu être fait au stade de la gestion des données dans le tableau de données créé et fourni à `flextable()` avec la fonction `round()`.

```{r}
my_table <- colformat_num(my_table, j = c(4,7), digits = 1)
my_table
```

### Fusionner des cellules {.unnumbered}

Tout comme nous fusionnons les cellules horizontalement dans la ligne d'en-tête, nous pouvons également fusionner les cellules verticalement en utilisant `merge_at()` et en spécifiant les lignes (`i`) et les colonnes (`j`). Ici, nous fusionnons les valeurs "Hospital" et "Total cases with known outcome" verticalement pour leur donner plus d'espace.

```{r}
my_table <- my_table %>% 
  merge_at(i = 1:2, j = 1, part = "header") %>% 
  merge_at(i = 1:2, j = 2, part = "header")

my_table
```

### Couleur d'arrière-plan {.unnumbered}

Pour distinguer le contenu du corps de la table des en-têtes, nous pouvons ajouter une mise en forme supplémentaire, par exemple en modifiant la couleur de l'arrière-plan. Dans cet exemple, nous changeons le corps du tableau en gris.

```{r}
my_table <- my_table %>% 
    bg(part = "body", bg = "gray95")  

my_table 
```

<!-- ======================================================= -->

## Mise en forme conditionnelle

L'un des points forts de **flextable** est qu'il nous permet de faire des mises en forme de notre table finale selon des conditions que nous aurons fixées selon l'information que nous voulons souligner. Nous pouvons ainsi donc mettre en évidence toutes les valeurs d'une colonne qui répondent à une certaine condition. Par exemple nous voulons mettre l'accent sur les cas où plus de 55 % des cas sont décédés. Il suffit de mettre les critères dans l'argument `i =` ou `j =`, précédé d'un tilde `~`. **Attention**: la condition doit être précisée en utilisant le nom de la colonne (variable) dans le tableau de donnée fourni à `flextable()` non en utilisant le nom de la colonne qui s'affiche dans l'en-tête de la table finale.

```{r}

my_table %>% 
  bg(j = 7, i = ~ Pct_Death >= 55, part = "body", bg = "red") 

```

Ou bien, nous pouvons mettre en évidence la ligne entière répondant à un certain critère, tel qu'un hôpital d'intérêt. Pour ce faire, il suffit de supprimer la spécification de la colonne (`j`) afin que les critères s'appliquent à toutes les colonnes.

```{r}

my_table %>% 
  bg(., i= ~ hospital == "Military Hospital", part = "body", bg = "#91c293") 

```

## L'ensemble du code {#tbl_pres_all}

Ci-dessous, nous regroupons tout le code des sections précédentes en un seul bloc comme vous serez amené à le faire.

```{r}

border_style = officer::fp_border(color="black", width=1)

pacman::p_load(
  rio,            # importer/exporter
  here,           # chemin vers les fichiers
  flextable,      # creer des tables HTML 
  officer,        # fonctions d'aide pour les tables
  tidyverse)      # data management, resume, et visualisation

table <- linelist %>% 

  # Obtenez les valeurs résumees par hôpital et issue finale
  ###############################################
  group_by(hospital, outcome) %>%                      # Grouper les donnees selon ces deux variables
  summarise(                                           # Creer un nouveau résumé des variables d'intérêt
    N = n(),                                            # Nombre de lignes par  groupe "hospital-outcome"
    ct_value = median(ct_blood, na.rm=T)) %>%           # Valeur CT mediane  par groupe
  
  # add totals
  ############
  bind_rows(                                           # Liez le tableau précédent avec ce mini-tableau de totaux
    linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # # Grouper avec var "outcome" uniquement et non par  "hospital"    
      summarise(
        N = n(),                                       # Nombre de lignes pour l'ensemble des données      
        ct_value = median(ct_blood, na.rm=T))) %>%     # Valeur CT mediane pour l'ensemble des données
  
  # Passer du format long de la table au format large
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivoter du format long au format large
    values_from = c(ct_value, N),                       # nouvelles valeurs sont crees depuis les vars "ct" et "count"
    names_from = outcome) %>%                           # nouveaux noms de colonne crees depuis var "outcomes"
  mutate(                                              # Creer de nouvelles colonnes
    N_Known = N_Death + N_Recover,                               # nombre de cas avec l'issue finale connue
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # pourcentage de cas decedes (1 decimale)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # pourcentage de cas gueris (1 decimale)
  select(                                              # Re-ordonner l'apparition des colonnes
    hospital, N_Known,                                   # Colonnes d'Intro
    N_Recover, Pct_Recover, ct_value_Recover,            # Colonnes concernant les gueris
    N_Death, Pct_Death, ct_value_Death)  %>%             # Colonnes concernant les deces
  arrange(N_Known) %>%                                 # Trier les lignes de façon croissante (Total de la ligne en dernier)

  # formatting
  ############
  flextable() %>%              # la table est pippee depuis les codes ci-dessus
  add_header_row(
    top = TRUE,                # Nouvel en-tête placé au-dessus de la rangée d'en-tête existante
    values = c("Hospital",     # Valeurs d'en-tête pour chaque colonne ci-dessous
               "Total cases with known outcome", 
               "Recovered",    # Celui ci servira d'en-tête de niveau supérieur pour cette colonne
               "",
               "",
               "Died",         # Celui ci servira d'en-tête de niveau supérieur pour cette colonne
               "",             # Laisser vide comme ce sera fusionné avec "Died"
               "")) %>% 
    set_header_labels(         # Renommer les colonnes de la ligne d'en-tête originale
      hospital = "", 
      N_Known = "",                  
      N_Recover = "Total",
      Pct_Recover = "% of cases",
      ct_value_Recover = "Median CT values",
      N_Death = "Total",
      Pct_Death = "% of cases",
      ct_value_Death = "Median CT values")  %>% 
  merge_at(i = 1, j = 3:5, part = "header") %>% # Fusionner horizontalement les colonnes 3 à 5 dans une nouvelle ligne d'en-tête
  merge_at(i = 1, j = 6:8, part = "header") %>%  
  border_remove() %>%  
  theme_booktabs() %>% 
  vline(part = "all", j = 2, border = border_style) %>%   # at column 2 
  vline(part = "all", j = 5, border = border_style) %>%   # at column 5
  merge_at(i = 1:2, j = 1, part = "header") %>% 
  merge_at(i = 1:2, j = 2, part = "header") %>% 
  width(j=1, width = 2.7) %>% 
  width(j=2, width = 1.5) %>% 
  width(j=c(4,5,7,8), width = 1) %>% 
  flextable::align(., align = "center", j = c(2:8), part = "all") %>% 
  bg(., part = "body", bg = "gray95")  %>% 
  bg(., j=c(1:8), i= ~ hospital == "Military Hospital", part = "body", bg = "#91c293") %>% 
  colformat_num(., j = c(4,7), digits = 1) %>%
  bold(i = 1, bold = TRUE, part = "header") %>% 
  bold(i = 7, bold = TRUE, part = "body")

table
```

<!-- ======================================================= -->

## Sauvegarder votre table

Il existe différentes façons d'intégrer la table finale dans votre production.

### Sauvegarder une seule table {.unnumbered}

Vous pouvez exporter les tableaux vers des fichiers Word, PowerPoint ou HTML ou sous format image (PNG). Pour ce faire, utilisez l'une des fonctions suivantes :

-   `save_as_docx()`\
-   `save_as_pptx()`\
-   `save_as_image()`\
-   `save_as_html()`

Par exemple, ci-dessous, nous enregistrons notre table comme un document Word. Notez la syntaxe du premier argument - vous pouvez simplement fournir le nom de votre objet flextable, par exemple `my_table`, ou vous pouvez lui donner un "nom" comme indiqué ci-dessous (le nom est "my table"). Si vous lui donnez un nom, celui-ci apparaîtra comme le titre de la table dans le document Word. Nous fournissons également le code pour sauvegarder la table sous format image PNG.

```{r message=FALSE, warning=FALSE, eval=F}
# Modifiez le tableau "my table" en fonction des besoins pour le titre du tableau.  
save_as_docx("my table" = my_table, path = "file.docx")

save_as_image(my_table, path = "file.png")
```

Notez que les "packages" `webshot` ou `webshot2` sont nécessaires pour sauvegarder un flextable comme une image. Les images peuvent sortir avec des arrière-plans transparents.

Si vous voulez voir une version "en direct" de la sortie du **flextable** dans le format de document prévu, utilisez `print()` et spécifiez un des éléments ci-dessous pour `preview =`. Le document s'ouvrira "en direct" sur votre ordinateur dans le logiciel spécifié, mais ne sera pas sauvegardé. Cela peut être utile pour vérifier si le tableau tient dans une page/diapositive ou pour le copier rapidement dans un autre document. Vous pouvez utiliser la méthode d'impression avec l'argument preview défini à "pptx" ou "docx".

```{r, eval=F}
print(my_table, preview = "docx") # Exemple de document Word
print(my_table, preview = "pptx") # Exemple de document Powerpoint
```

### Intégrer la table dans R markdown {.unnumbered}

Cette table peut être intégrée dans un document automatisé, une sortie R markdown, si l'objet table est appelé dans le chunk R markdown. Cela signifie que la table peut être mise à jour dans le cadre d'un rapport où les données sont susceptibles de changer, de sorte que les chiffres peuvent être actualisés.

Voir les détails dans la page [Rapports avec R Markdown](#rmarkdown) de ce manuel.

<!-- ======================================================= -->

## Ressources

La documentation compléte sur **flextable**  est ici: <https://ardata-fr.github.io/flextable-book/> Le lien Github est [ici](https://davidgohel.github.io/flextable/)\
Un guide sur toutes les fonctions **flextable** peût être trouvée [ici](https://davidgohel.github.io/flextable/reference/index.html)

Vous pouvez accéder à une galerie de beaux exemples de tables **flextable** avec code [ici](https://ardata-fr.github.io/flextable-gallery/gallery/)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/tables_presentation.Rmd-->


# Les bases de ggplot {#ggplot_basics}

```{r, out.width=c('100%', '100%'), fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "ggplot_basics_top.png"))
```

**ggplot2** est le "package" R de visualisation de données le plus populaire. Sa fonction `ggplot()` est au cœur de ce "package", et toute cette approche est familièrement connue sous le nom de *"ggplot "* avec les figures qui en résultent parfois affectueusement appelées "ggplots". Le préfixe "gg" dans ce jargon reflète la "**g**ramar of **g**raphics" (la grammaire des graphiques) utilisée pour construire les figures. **ggplot2** bénéficie d'une grande variété de "packages" R supplémentaires qui améliorent encore ses fonctionnalités.  

La syntaxe est très différente de celle de la visualiation avec `R` **base**, et une courbe d'apprentissage y est associée. L'utilisation de **ggplot2** exige généralement de l'utilisateur qu'il formate ses données d'une manière hautement compatible avec **tidyverse**, ce qui rend finalement l'utilisation conjointe de ces packages très efficace. 

Dans cette page, nous allons couvrir les principes fondamentaux de la visualisation avec **ggplot2**. Voir la page [Astuces de ggplot](#ggplot_tips) pour des suggestions et des techniques avancées pour que vos graphiques soient vraiment esthétiques.  

Plusieurs tutoriels **ggplot2** détaillés sont disponibles dans la section des ressources. Vous pouvez également télécharger cette [fiche d'aide à la visualisation de données avec ggplot](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf) sur le site Web de RStudio. Si vous souhaitez trouver de l'inspiration pour visualiser vos données de manière créative, nous vous suggérons de consulter des sites Web tels que [R graph gallery](https://www.r-graph-gallery.com/) et [Data-to-viz](https://www.data-to-viz.com/caveats.html).



<!-- ======================================================= -->
## Préparation {}

### Charger les extensions ("packages") {.unnumbered}

Ce chunk de code montre le chargement des "packages" nécessaires aux analyses. Dans ce manuel, nous souligons la fonction `p_load()` du "package" **pacman**, qui installe le (ou une liste de) "package (s)" que si nécessaire (uniquement si le package n'est pas déjà installé) *et* le charge pour l'utiliser. On peut également charger des "packages" avec `library()` à partir de `R` **base**. Voir la page sur [Bases de R](#rbasics) pour plus d'informations sur les "packages" `R`.

```{r}
pacman::p_load(
  rio,            # importer/exporter 
  here,           # localiser des fichiers
  stringr,         # travailler avec des caractères 
  janitor,
  tidyverse,      # inclut ggplot2 et d'autres extensions de data management
  ggforce
)
```

### Importer des données {.unnumbered}  

Pour commencer, nous importons le jeu de données des cas d'une épidémie d'Ebola simulée. Si vous voulez suivre en travaillant sur le jeu de données, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la version "clean" </a> (en fichier .rds). Importez les données avec la fonction `import()` du "package" **rio** (elle gère de nombreux types de fichiers comme .xlsx, .csv, .rds - voir la page [Importation et exportation](#import_export) pour plus de détails).


```{r,  echo=F}
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
linelist <- rio::import("linelist_cleaned.rds")
```

Les 50 premières lignes de la liste linéaire sont affichées ci-dessous.
Nous allons nous concentrer sur les variables continues `age`, `wt_kg` (le poids en kilos), `ct_blood` (valeurs CT), et `days_onset_hosp` (différence entre la date de début de symptômes et l'hospitalisation).  

```{r, message=FALSE, echo=F}
# afficher le jeu de donnees de la liste lineaire comme une table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



### Nettoyage général {.unnumbered}

Lorsque nous préparons des données pour les visualiser, il est préférable de faire en sorte qu'elles respectent autant que possible les [normes pour des données bien rangées](https://r4ds.had.co.nz/tidy-data.html). Les pages de ce manuel consacrées à la gestion des données, telles que [Nettoyage des données et fonctions de base](#cleaning_data), expliquent comment y parvenir. 

En préparant les données pour la visualisation, nous pouvons avoir recours à certaines pratiques simples qui pourraient améliorer le contenu des données pour faciliter et rendre pratique leur représentation. Toutefois cela n'équivaut pas nécessairement à une meilleure manipulation des données. Par exemple :  

* Remplacer les valeurs manquantes `NA` dans une colonne de caractères par la chaîne de caractères "Inconnu".  
* Envisager de convertir une colonne en classe *facteur* pour que leurs valeurs aient des niveaux ordinaux prescrits.  
* Nettoyer certaines colonnes de manière à ce que leurs valeurs (qui étaient codées de façon à être maniables) avec des caractères spéciaux comme des "underscores" (tirets bas), etc. soient transformées en texte normal ou en majuscules (voir [Caractères et chaînes de caractères](#character_strings)).  

Voici quelques exemples concrets de ce genre de pratiques :

```{r }
#creer une version d'affichage des colonnes avec des noms plus pratiques/maniables
linelist <- linelist %>%
  mutate(
    gender_disp = case_when(gender == "m" ~ "Male",        # m à Male 
                            gender == "f" ~ "Female",      # f à Female,
                            is.na(gender) ~ "Unknown"),    # NA à Unknown
    
    outcome_disp = replace_na(outcome, "Unknown")          # remplacer les valeurs NA de la variable "outcome" par "Unknown" ("Inconnu").
  )
```

### Transformation large-long {.unnumbered}

En ce qui concerne la structure des données, pour **ggplot2**, nous voulons souvent faire pivoter nos données dans des formats *longs*. Pour en savoir plus, consultez la page [Pivoter les données](#pivoting_data).  

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "pivoting", "pivot_longer_new.png"))
```

Par exemple, supposons que nous voulons visualiser des données qui sont dans un format "large", comme pour chaque cas dans la `linelist` et leurs symptômes. Ci-dessous, nous créons une mini-linelist appelée `symptoms_data` qui ne contient que les colonnes `case_id` et les différentes variables des symptômes. 

```{r}
symptoms_data <- linelist %>% 
  select(c(case_id, fever, chills, cough, aches, vomit))
```

Voici à quoi ressemblent les 50 premières lignes de cette mini-linelist - voyez comment elles sont présentées en format "large" avec chaque symptôme en tant que colonne : 

```{r, message=FALSE, echo=F}
#afficher les données de la linelist sous forme de tableau

DT::datatable(head(symptoms_data, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Si nous voulions représenter graphiquement le nombre de cas présentant des symptômes spécifiques, nous sommes limités par le fait que chaque symptôme est une colonne spécifique. Cependant, nous pouvons *restructurer* ("pivoter") les colonnes de symptômes dans un format plus long comme ceci :

```{r, }
symptoms_data_long <- symptoms_data %>%    # commencer avec la mini-linelist appelée symptoms_data
  
  pivot_longer(
    cols = -case_id,                       # pivoter toutes les colonnes à l'exception de case_id (on veut regrouper les colonnes avec les symptômes)
    names_to = "symptom_name",             # assigner un nom à la nouvelle colonne qui va contenir les différents symptômes regroupés 
    values_to = "symptom_is_present") %>%  # assigner un nom à la nouvelle colonne qui va contenir les valeurs des différents symptômes regroupés (yes/no)
  
  mutate(symptom_is_present = replace_na(symptom_is_present, "unknown")) # convertir les valeurs NA en "unknown" (inconnu)

```

Voici les 50 premières lignes. Notez que chaque cas a désormais 5 lignes - une pour chaque symptôme possible. Les nouvelles colonnes `symptom_name` et `symptom_is_present` sont le résultat de la restructuration (ou "pivot"). Il faut cependant retenir que ce format peut ne pas être très utile pour d'autres opérations, mais qu'il est utile pour la représentation des données.

```{r, message=FALSE, echo=F}
DT::datatable(head(symptoms_data_long, 50), rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```






<!-- ======================================================= -->
## Bases de ggplot {}

**"Grammaire des graphiques" - ggplot2** 

Constuire/générer des graphiques avec **ggplot2** est basé sur "l'ajout" de couches de graphique et d'éléments de conception/représentation les uns sur les autres, chaque commande étant ajoutée aux précédentes avec un symbole "plus" (`+`). Le résultat est un ensemble d'objets graphiques multicouche qui peut être enregistré, modifié, imprimé, exporté, etc.  

Les objets ggplot peuvent être très complexes, mais l'ordre de base des couches ressemble généralement à ceci :  

1. Commencez par la commande de base `ggplot()` - cela "ouvre" le ggplot et permet d'ajouter les fonctions suivantes avec `+`. Généralement, le jeu de données à partir duquel on veut générer des graphiques est également spécifié comme argument dans cette commande.  
2. Ajouter des couches "geom" - ces fonctions sont des éléments de représentation graphique qui permettent de visualiser les données comme des formes géométriques (*geoms*), par exemple comme un graphique à barres, un graphique linéaire, un nuage de points, un histogramme (ou une combinaison des différentes formes!). Ces fonctions commencent toutes par le préfixe `geom_`.  
3. Ajoutez des éléments de conception au graphique tels que les noms des axes, le titre, les polices, les tailles, les schémas de couleurs, les légendes ou la rotation des axes.  

Un exemple simple de code fictif permettant de dessiner un graphique avec **ggplot2** est le suivant. Nous allons expliquer chaque composante dans les sections ci-dessous. 

```{r, eval=F}
# représenter les données dans my_data comme des points coloriés en rouge
ggplot(data = my_data)+                   # utiliser le jeu de données "my_data"
  geom_point(                             # ajouter une couche de points (dots)
    mapping = aes(x = col1, y = col2),    # préciser quelles données de my_data nous voulons représenter sous forme de points en donnant les coordonnées précises des points pour chaque axe
    color = "red")+                       # autres spécifications pour le geom
  labs()+                                 # ici on ajoute les titres, noms des axes, etc.
  theme()                                 # ici on ajuste les couleurs, les polices, les tailles, etc. pour les éléments du graphique qui ne dépende pas des données (axes, titres, etc.) 
```

 


## `ggplot()`  

La commande initiale de tout graphique ggplot2 est `ggplot()`. Cette commande crée simplement un cadre blanc qui représente la base de l'objet graphique et sur lequel on peut ajouter des couches. Elle "ouvre" la voie à l'ajout de couches supplémentaires avec le symbole `+`.

Généralement, la commande `ggplot()` inclut l'argument `data = ` pour le graphique. Ceci permet de définir le jeu de données qui sera utilisé par défaut pour les couches suivantes du graphique.  

Cette commande se terminera par un `+` après la fermeture des parenthèses. Cela laisse la commande "ouverte". Les fonctions ne s'exécuteront et le graphique n'apparaîtra que si la commande complète inclut une couche finale *sans* un `+` à la fin. Cela indique qu'on ne veut plus rajouter d'éléments de représentation graphique et que le graphique final peut être affiché.

```{r, eval=F}
# Ceci dessine juste un cadre blanc qui est la base de l'objet graphique
ggplot(data = linelist)
```


## Geoms  

Un cadre blanc (la base de l'objet graphique) n'est certainement pas suffisant - nous devons créer des premiers éléments du graphique qui définissent les formes géométriques du graphique à partir de nos données (par exemple, des diagrammes en barres, des histogrammes, des nuages de points, des diagrammes en boîte).  

Ceci est fait en ajoutant des couches "geoms" à la commande initiale `ggplot()`. Il existe de nombreuses fonctions **ggplot2** qui créent des "geoms". Chacune de ces fonctions commence par "geom_", nous les appellerons donc génériquement `geom_XXXX()`. Il y a plus de 40 "geoms" dans **ggplot2** et beaucoup d'autres créés par des utilisateurs. Vous pouvez les voir sur la [galerie ggplot2](https://exts.ggplot2.tidyverse.org/gallery/). Certains parmi les "geoms" les plus utilisés sont listés ci-dessous :  

* Histogrammes - `geom_histogram()`  
* Diagrammes en barres - `geom_bar()` ou `geom_col()` (voir la section ["Diagrammes en barres"](#ggplot_basics_bars))  
* Les diagrammes en boîte - `geom_boxplot()`.  
* Les nuages de points (par exemple les diagrammes de dispersion) - `geom_point()`.  
* Graphiques linéaires - `geom_line()` ou `geom_path()`.  
* Lignes de tendance - `geom_smooth()`.  

Dans un graphique, on peut afficher un ou plusieurs "geom". Chacun d'entre eux est ajouté aux commandes **ggplot2** précédentes avec un `+`, et ils sont représentés séquentiellement de sorte que les "geom" les plus récents soient tracés au-dessus des précédents.

## "Mappage" ou comment faire correspondre les données au graphique{#ggplot_basics_mapping}  

La plupart des fonctions "geom" doivent savoir *les variables précises du jeu de données qu'elles doivent utiliser* pour créer leurs formes. Nous devons donc leur indiquer comment *mapper (assigner) ces variables* aux attributs du graphique tels que les axes (quelle variable sera représentée sur quel axe), les couleurs des formes (quelle modalité de quelle variable représenter en telle ou telle couleur), les tailles des formes, etc. Pour la plupart des "geom", les composantes *essentielles* qui doivent être mises en correspondance avec les colonnes des données sont l'axe des x et (si nécessaire) celui des y.  

On parle ainsi de "mappage" qui n'est dans ce cadre rien d'autre qu'une mise en relation entre un attribut graphique du "geom" et une variable du jeu de données utilisée pour faire la représentation graphique.

Ce "mappage" (correspondance/assignation) se fait avec l'argument `mapping = `. Les "mappages" que nous fournissons à l'argument `mapping` doivent être enveloppés dans la fonction `aes()`, donc nous écririons quelque chose comme `mapping = aes(x = col1, y = col2)`, comme montré ci-dessous.

Ci-dessous, dans la commande `ggplot()`, les données sont définies comme la liste  des cas `linelist`. Dans l'argument `mapping = aes()`, la colonne `age` est mise en correspondance avec l'axe des x, et la colonne `wt_kg` est mise en correspondance avec l'axe des y.  

Après un `+`, les commandes de représentation graphique continuent. Une forme est créée avec la fonction "geom" `geom_point()`. Ce "geom" *hérite* des "mappages" de la commande `ggplot()` ci-dessus - il connaît les affectations axe-colonne et procède à la visualisation de ces relations sous forme de *points* sur la base du graphique (le cadre blanc dessiné avec la première fonction).

```{r, warning=F, message=F}
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+
  geom_point()
```

Comme autre exemple, les commandes suivantes utilisent les mêmes données, un "mappage" légèrement différent, et un "geom" différent. La fonction `geom_histogram()` ne nécessite qu'une colonne mappée sur l'axe des x, puisque l'axe des y est généré automatiquement (représente le comptage de chaque modalité fait automatiquement par la fonction). 

```{r, warning=F, message=F}
ggplot(data = linelist, mapping = aes(x = age))+
  geom_histogram()
```


### Attributs (esthétiques) du graphique {.unnumbered}  

Dans le jargon de ggplot, "l'esthétique" d'un graphique a une signification assez spécifique. Il s'agit d'une propriété visuelle des *données représentées*. Notons que le terme "esthétique" fait ici référence aux *données de variables "mappées" dans les "geoms"/formes* - et non à l'affichage des éléments environnants du graphique qui ne dépendent pas des données tel que les titres, les noms des axes, la couleur de fond (qu'on pourrait associer au mot "esthétique" en français courant). Dans ggplot, ces éléments d'affichage non reliés aux données sont appelés "thèmes" et sont déterminés par une commande `theme()` (voir [cette section](#ggplot_basics_themes)).  

Par conséquent, les caractéristiques esthétiques/attributs du graphique peuvent concerner les couleurs, les tailles, la transparence, le placement, etc. *des données représentées*. Tous les "geoms" n'auront pas les mêmes options esthétiques, mais beaucoup parmi ces options peuvent être utilisées par la plupart des "geoms". Voici quelques exemples :  

* `shape =` Afficher un point avec `geom_point()` comme un point, une étoile, un triangle, ou un carré...  
* `fill = ` La couleur intérieure (par exemple d'une barre ou d'un diagramme en boîte)  
* `color = ` La ligne extérieure d'une barre, d'un diagramme en boîte, etc., ou la couleur du point si on utilise `geom_point()`  
* `size = ` Taille (par exemple, l'épaisseur de la ligne, la taille du point)  
* `alpha = ` La transparence (1 = opaque, 0 = invisible)  
* `binwidth = ` La largeur des cases de l'histogramme  
* `width = ` La largeur des colonnes du diagramme en barre
* `linetype =` Le type de ligne (par exemple, solide, en pointillés ...) 

Il est possible d'affecter des valeurs à ces attributs de deux manières :  

1) Affecter une valeur fixe/statique (par exemple, `color = "blue"`) qui sera donc appliquée à toutes les observations représentées  
2) Relier l'attribut à une variable de données (par exemple, `color = hospital`) de telle sorte que l'affichage de chaque observation dépende de sa valeur dans cette variable.
<!-- *These non-axis aesthetics can be assigned static values (e.g. `size = 1`) or can be mapped to a column (e.g. `size = age`).* If you want the aesthetic to be assigned a static value, the assignment is placed *outside* the `mapping = aes()`. If you want the aesthetic to be scaled/depend on the value in each row of data, the assignment is made *inside* the `mapping = aes()`.   -->

### Affecter un attribut à une valeur fixe {.unnumbered}  

Si nous souhaitions que l'attribut de l'objet graphique soit statique, c'est-à-dire qu'elle soit la même pour chaque observation des données, nous écrivons son affectation dans le "geom" mais *en dehors* de toute instruction `mapping = aes()`. Ces affectations peuvent ressembler à `size = 1` ou `color = "blue"`. Voici deux exemples :  

* Dans le premier exemple, l'instruction `mapping = aes()` se trouve dans la commande `ggplot()` et les axes sont associés aux variables d'âge et de poids dans notre jeu de données. Les attributs du graphique, `color = `, `size = `, et `alpha = ` (pour déterminer la transparence) sont assignées à des valeurs statiques. Pour plus de clarté, ceci est fait dans la fonction `geom_point()`, car vous pouvez ajouter d'autres "geoms" par la suite qui prendraient des valeurs différentes pour leur esthétique d'affichage.  
* Dans le deuxième exemple, l'histogramme ne nécessite que la mise en relation de la variable d'intérêt à l'axe des x . Les valeurs statiques de l'histogramme `binwidth = `, `color = `, `fill = ` (couleur de  l'intérieur des barres) et `alpha = ` sont à nouveau définies dans le "geom". 

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
# Diagramme de dispersion
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  # définir les données et le mappage des axes
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)         # définir l'esthétique du point statique

# histogramme
ggplot(data = linelist, mapping = aes(x = age))+       # définir les données et les axes
  geom_histogram(              # afficher l'histogramme
    binwidth = 7,                # largeur des barres
    color = "red",               # couleur de la ligne de la barre
    fill = "blue",               # couleur intérieure de la barre
    alpha = 0.1)                 # transparence de la barre
```


### Relier un attribut aux valeurs d'une variable {.unnumbered}  

L'alternative consiste à relier l'attribut de l'objet graphique aux valeurs d'une variable. Dans cette approche, l'affichage de cet attribut dépendra des valeurs prises dans cette variable. Si les valeurs de la variable sont continues, l'échelle d'affichage (légende) de cet attribut sera continue. Si les valeurs de la variable sont discrètes, la légende affichera chaque valeur et les données représentées apparaîtront comme "groupées" de manière distincte (pour en savoir plus, consultez la section [groupage](#ggplotgroups) de cette page).  

Pour ce faire, nous devons associer l'attribut du graphique à un *nom de variable de notre jeu de données* (sans guillemets) ie le "mapper". Ceci doit donc être fait dans une fonction `mapping = aes()` (note : il y a plusieurs endroits dans le code où nous pouvons faire ces assignations (de mappage), comme discuté [ci-dessous](#ggplot_basics_map_loc)).  

Deux exemples sont présentés ci-dessous.  

* Dans le premier exemple, l'attribut `color = ` (de chaque point) est mappé à la variable `age` - et une échelle est apparue dans la légende ! Pour l'instant, notons simplement que l'échelle existe - nous montrerons comment la modifier dans les sections suivantes.  
* Dans le deuxième exemple, deux nouveaux attributs sont également associés à des variables (`color = ` et `size = `), tandis que les attributs `shape = ` et `alpha = ` sont associés à des valeurs fixes en dehors de toute fonction `mapping = aes()`. 

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
# Diagramme de dispersion
ggplot(data = linelist,   # définir les données
       mapping = aes(     # mapper l'attribut aux valeurs de la colonne
         x = age,         # mapper l'axe des x à la variable des âges           
         y = wt_kg,       # mapper l'axe des y à la variable des poids
         color = age)     # mapper l'attribut color à la variable des âges
       )+     
  geom_point()         # afficher les données comme des points 

#  Diagramme de dispersion
ggplot(data = linelist,   # définir les données
       mapping = aes(     # mapper les attributs aux variables
         x = age,           # mapper l'axe des x à la variable des âges           
         y = wt_kg,         # mapper l'axe des y à la variable des poids
         color = age,       #mapper l'attribut color à la variable des âges
         size = age))+      # mapper l'attribut size (taille des points) à la variable des âges
  geom_point(             # afficher les données comme des points
    shape = "diamond",      # preciser la forme des points comme des diamants
    alpha = 0.3)            # transparence des points à 30%.


```


Note : Les axes sont toujours assignées à des variables dans les données (pas à des valeurs statiques), et sont donc toujours mappées dans `mapping = aes()`.  


Il devient important de garder la trace des différentes couches du graphique et les attributs lorsque vous créez des graphiques plus complexes - par exemple des graphiques avec plusieurs "geoms". Dans l'exemple ci-dessous, l'attribut `size = ` est assigné deux fois - une fois pour `geom_point()` et une fois pour `geom_smooth()` - et les deux fois comme une valeur statique.  

```{r, warning=F, message=F}
ggplot(data = linelist,
       mapping = aes(           # mapper les attributs qux variables
         x = age,
         y = wt_kg,
         color = age_years)
       ) + 
  geom_point(                   # ajouter des points pour chaque ligne de données
    size = 1,
    alpha = 0.5) +  
  geom_smooth(                  # ajouter une courbe de tendance 
    method = "lm",              # avec une méthode linéaire
    size = 2)                   # taille (largeur de la ligne) de 2
```


### Comment et quand faire le mappage {#ggplot_basics_map_loc .unnumbered}

Le mappage dans `mapping = aes()` peut être écrit à plusieurs endroits dans les commandes du ggplot et peut même être écrit plus d'une fois. Cela peut être écrit dans la commande supérieure `ggplot()`, et/ou pour chaque "geom" individuel en dessous. Les possibilités comprennent :  

* Les affectations de mappage effectuées dans la commande supérieure `ggplot()` et qui seront héritées par défaut dans tous les "geom" inférieurs, comme c'est le cas pour `x = ` et `y = ` ci-dessous. 
* Les affectations de mappage effectuées dans un "geom" et qui ne s'appliquent qu'à ce "geom".  

De même, l'argument `data = ` spécifié dans la commande supérieure `ggplot()` s'appliquera par défaut à tous les "geom" inférieurs. Toutefois on peut aussi spécifier des jeux de données différents pour chaque "geom" (mais c'est plus complexe).  

Ainsi, chacune des 3 commandes suivantes (avec des mappages faits à différents niveaux du code) créera le même graphique :  

```{r, eval=F, warning=F, message=F}
#  Ces commandes produiront exactement le même graphique.
ggplot(data = linelist, mapping = aes(x = age))+
  geom_histogram()

ggplot(data = linelist)+
  geom_histogram(mapping = aes(x = age))

ggplot()+
  geom_histogram(data = linelist, mapping = aes(x = age))
```




### Groupage {#ggplotgroups .unnumbered}  

On peut facilement regrouper les données et les "représenter par groupe". En fait, nous l'avons déjà fait !  

Pour cela nous allons assigner la colonne de "regroupement" à l'attribut approprié du graphique, dans un `mapping = aes()`. Ci-dessus, nous avons fait une démonstration en utilisant des valeurs continues lorsque nous avons assigné l'attribut`size = ` à la variable `age`. Cependant, cela fonctionne de la même manière pour les colonnes discrètes/catégorielles.  

Par exemple, si nous voulons que les points soient affichés par sexe, nous pouvons définir `mapping = aes(color = gender)`. Une légende apparaît automatiquement. Cette affectation peut être faite dans le `mapping = aes()` de la commande supérieure `ggplot()` (et elle va être héritée par le "geom"), ou elle peut être définie dans un `mapping = aes()` séparé dans le "geom". Les deux approches sont présentées ci-dessous :  

```{r, warning=F, message=F}
ggplot(data = linelist,
       mapping = aes(x = age, y = wt_kg, color = gender))+
  geom_point(alpha = 0.5)
```


```{r, eval=F}
# Cette autre version de code produit le même graphique
ggplot(data = linelist,
       mapping = aes(x = age, y = wt_kg))+
  geom_point(
    mapping = aes(color = gender),
    alpha = 0.5)

```


Notez que selon le "geom", vous devrez utiliser différents arguments pour regrouper les données. Pour `geom_point()`, vous utiliserez probablement `color =`, `shape = ` ou `size = `. Alors que pour `geom_bar()`, vous utiliseriez plus probablement `fill = `. Cela dépend simplement du "geom" et de l'attribut du graphique que vous voulez utiliser pour refléter les groupages.  

Pour votre information - la manière la plus basique d'regrouper les données est d'utiliser seulement l'argument `group = ` dans `mapping = aes()`. Cependant, cela ne changera pas les couleurs, le remplissage ou les formes. Elle ne créera pas non plus de légende. Pourtant, les données sont groupées, donc les affichages statistiques des données peuvent être affectés.  

Pour ajuster l'ordre des groupes dans un graphique, consultez la page [Trucs et Astuces avec ggplot ](#ggplot_tips) ou la page sur les [Facteurs](#factors). Vous trouverez de nombreux exemples de graphiques groupés dans les sections ci-dessous sur la représentation des données continues et catégorielles.   


## Facets / Petits-multiples {#ggplot_basics_facet}  

Les "facets", ou "petits-multiples", sont utilisés pour séparer un graphique en une figure à plusieurs sections selon les valeurs d’une ou plusieurs variables qualitatives. Le même type de graphique est ainsi créé plusieurs fois, chaque (sous-)graphique utilisant un sous-groupe du même ensemble de données.  

Le "faceting" est une fonctionnalité fournie avec **ggplot2**, de sorte que les légendes et les axes de chaque graphe d'un sous-groupe du jeu de données ("facet") sont automatiquement alignés. Il existe d'autres paquets ("packages") discutés dans la page [Trucs et Astuces avec ggplot](*ggplot_tips) qui sont utilisés pour combiner des graphiques complètement différents (i.e. qui cette fois ne sont pas les mêmes graphiques répétés pour chaque sous-groupe d'un même jeu de données) en une seule figure. On peut citer **cowplot** et **patchwork**.  

Le "faceting" est effectué avec l'une des fonctions **ggplot2** suivantes :

  1. `facet_wrap()` Pour montrer un graphique différent pour chaque niveau d'une *seule* variable. Un exemple de ceci pourrait être de montrer une courbe d'épidémie différente pour chaque hôpital dans une région. Les "facets" sont ordonnées par ordre alphabétique, sauf si la variable est un facteur avec un autre ordre défini.  
  + On peut utiliser certaines options pour déterminer la disposition des "facets", par exemple `nrow = 1` ou `ncol = 1` pour contrôler le nombre de lignes ou de colonnes dans lesquelles les "facets" sont disposés.  
  
  2. `facet_grid()` Cette fonction est utilisée lorsque nous souhaitons introduire une seconde variable dans l'arrangement des "facets". Ici, chaque graphe d'une grille montre l'intersection entre les valeurs de *deux variables*. Par exemple, des courbes épidémiques pour chaque combinaison hôpital-groupe d'âge avec les hôpitaux en haut (colonnes) et les groupes d'âge sur les côtés (lignes).  
  + Dans ce cas-ci `nrow` et `ncol` ne sont pas pertinents, car les sous-groupes sont présentés dans une grille.  

Chacune de ces fonctions accepte une syntaxe de formule pour spécifier la ou les variables à utiliser pour le "faceting" Les deux acceptent jusqu'à deux variables, une de chaque côté d'un tilde `~`.  

* Pour `facet_wrap()`, on écrira le plus souvent le nom d'une seule variable précédée d'un tilde `~` comme `facet_wrap(~hospital)`. Cependant on peut préciser deux noms de variables si c'est que l'on veut représenter `facet_wrap(outcome ~hospital)` - chaque combinaison unique s'affichera dans un graphique séparé, mais ils ne seront pas disposés dans une grille.  Si on décide de ne fournir qu'une seule variable à la fonction, un point `.` est utilisé comme bouche-trou de l'autre côté de la formule - voir les exemples de code.  

* Pour `facet_grid()` nous pouvons également spécifier une ou deux variables à la formule (`facet_grid( rows ~ columns)`). Si on ne veut en spécifier qu'une, on peut placer un point `.` de l'un ou l'autre côté du tilde comme `facet_grid(. ~ hospital)` ou `facet_grid(hospital ~ .)`.  

Les "facets" peuvent rapidement contenir une quantité écrasante d'informations - il est important de s'assurer que nous n'avons pas trop de modalités pour chaque variable qualitative que nous choisissons de "facetter". Voici quelques exemples rapides avec le jeu de données sur le paludisme (voir [Télécharger le manuel et les données](#download_book_data)) qui contient les données sur le nombre de cas  de paludisme quotidiens pour différentes structures de santé par groupe d'âge.  

Ci-dessous, nous importons ces données et effectuons quelques modifications rapides pour plus de simplicité :  

```{r, , warning=F, message=F}
# Ces donnees sont le nombre de cas de palu par jour par structure
malaria_data <- import(here("data", "malaria_facility_count_data.rds")) %>%  # importer
  select(-submitted_date, -Province, -newid)                                 # enlever les colonnes (variables) dont on n'a pas besoin pour les prochaines étapes

```

Les 50 premières lignes des données sur le paludisme sont affichées ci-dessous. Notez qu'il y a une colonne `malaria_tot`, mais aussi des colonnes pour le nombre de cas par groupe d'âge (celles-ci seront utilisées dans le second exemple `facet_grid()`).  


```{r, message=FALSE, echo=F}
DT::datatable(head(malaria_data, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



### `facet_wrap()` {.unnumbered}

Pour le moment, concentrons-nous sur les variables `malaria_tot` et `District`. Ignorons pour l'instant les colonnes du nombre de cas par âge. Nous allons tracer des courbes épidémiques avec `geom_col()`, qui produit une colonne pour chaque jour à la hauteur spécifiée sur l'axe des y fournie par la variable `malaria_tot` (les données sont déjà des nombres de cas quotidiens, donc nous utilisons `geom_col()` - voir [la section "Diagramme en barres" ci-dessous](#ggplot_basics_bars)).  

Lorsque nous ajoutons la commande `facet_wrap()`, nous spécifions un tilde (~) et ensuite la variable à utiliser pour le "facet" (`District` dans ce cas). On peut placer une autre variable sur le côté gauche du tilde, - cela créera un "facet" pour chaque combinaison - mais nous recommandons de le faire avec `facet_grid()` à la place. Dans ce cas d'utilisation, un "facet" est créé pour chaque valeur unique de `District`.  

```{r, warning=F, message=F}
# Un graphique avec des facets par district
ggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +
  geom_col(width = 1, fill = "darkred") +       # tracer le nombre de cas sous forme de colonnes
  theme_minimal()+                              # simplifier les  arrière-plans
  labs(                                         # ajouter  les noms d'axes, titres ... 
    x = "Date of report",
    y = "Malaria cases",
    title = "Malaria cases by district") +
  facet_wrap(~District)                       # les facets sont créés
```

### `facet_grid()` {.unnumbered}  

Nous pouvons utiliser une approche `facet_grid()` pour croiser deux variables. Disons que nous voulons croiser `District` et la variable âge. Eh bien, nous devons faire quelques transformations de données sur les colonnes d'âge pour obtenir ces données dans le format "long" préféré de ggplot. Les groupes d'âge ont tous leurs propres colonnes - nous les voulons dans une seule colonne appelée `age_group` et une autre appelée `num_cases`. Voir la page sur [Pivoter les données](#pivoting_data) pour plus d'informations sur ce processus.  


```{r, message=F, warning=F}
malaria_age <- malaria_data %>%
  select(-malaria_tot) %>% 
  pivot_longer(
    cols = c(starts_with("malaria_rdt_")),  # choisir la variable à mettre en format "long"
    names_to = "age_group",      # la nouvelle variable avec tous les groupes d'âge est nommée age_group
    values_to = "num_cases"      # les valeurs dans les anciennes colonnes séparées sont regroupées dans une nouvelle unique colonne appelée num_cases
  ) %>%
  mutate(
    age_group = str_replace(age_group, "malaria_rdt_", ""),
    age_group = forcats::fct_relevel(age_group, "5-14", after = 1))
```

Les 50 premières lignes des données transformées ressemblent désormais comme suit :

```{r, message=FALSE, echo=F}
DT::datatable(head(malaria_age, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Lorsque vous passez les deux variables à `facet_grid()`, le plus simple est d'utiliser la notation de formule (par exemple `x ~ y`) où x représente les lignes et y les colonnes. Voici le graphique, utilisant `facet_grid()` pour montrer les graphiques pour chaque combinaison des colonnes `age_group` et `District`.


```{r, message=F, warning=F}
ggplot(malaria_age, aes(x = data_date, y = num_cases)) +
  geom_col(fill = "darkred", width = 1) +
  theme_minimal()+
  labs(
    x = "Date of report",
    y = "Malaria cases",
    title = "Malaria cases by district and age group"
  ) +
  facet_grid(District ~ age_group)
```

### Axes libres ou fixes {.unnumbered}  

Les échelles des axes affichées lors du "faceting" sont par défaut les mêmes (fixes) pour tous les "facets". C'est utile pour les comparaisons croisées, mais pas toujours approprié.  

Lorsque l'on utilise `facet_wrap()` ou `facet_grid()`, on peut ajouter `scales = "free_y"` pour "libérer" ou rendre indépendant les axes y des "facets" afin qu'ils soient représentés à l'échelle de leur sous-ensemble de données spécifique. Ceci est particulièrement utile si les nombres sont faibles pour une des sous-catégories et que les tendances sont difficiles à voir en laissant l'échelle pareille pour tous les "facets". Au lieu de "free_y", on peut aussi écrire "free_x" pour faire la même chose pour l'axe des x (par exemple pour les dates) ou pour faire court "free" pour les deux axes. Notez que dans `facet_grid`, les échelles y seront les mêmes pour les "facets" de la même ligne, et les échelles x seront les mêmes pour les "facets" de la même colonne.

En utilisant uniquement `facet_grid`, on peut ajouter `space = "free_y"` ou `space = "free_x"` pour que la hauteur ou la largeur réelle de la "facets" soit pondérée par les valeurs de la figure à l'intérieur. Cela ne fonctionne que si `scales = "free"` (y ou x) est déjà appliqué. 

```{r, message=FALSE, warning=FALSE}

# Axe des y libre
ggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +
  geom_col(width = 1, fill = "darkred") +       # tracer le nombre de cas sous forme de colonnes
  theme_minimal()+                              # simplifier les  arrière-plans
  labs(                                         # ajouter  les noms d'axes, titres ... 
    x = "Date of report",
    y = "Malaria cases",
    title = "Malaria cases by district - 'free' x and y axes") +
  facet_wrap(~District, scales = "free")        # les facets sont créés
```


<!-- ```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')} -->
<!-- # A) Facet hospitalsation date by hospital, free y axis -->
<!-- ggplot(data = linelist %>% filter(hospital != "Missing"), # filter removes unknown hospital -->
<!--        aes(x = date_hospitalisation ))+ -->
<!--   geom_histogram(binwidth=7) + # Bindwidth = 7 days -->
<!--   labs(title = "A) Histogram with free y axis scales")+ -->
<!--   facet_grid(hospital~., # Facet with hospital as the row  -->
<!--              scales = "free_y") # Free the y scale of each facet -->

<!-- # B) Facet hospitalisation date by hospital, free y axis and vertical spacing -->
<!-- ggplot(data = linelist %>% filter(hospital != "Missing"), # filter removes unknown hospital -->
<!--        aes(x = date_hospitalisation ))+ -->
<!--   geom_histogram(binwidth=7) + # Bindwidth = 7 days -->
<!--   labs(title = "B) Histogram with free y axis scales and spacing")+ -->
<!--   facet_grid(hospital~., # Facet with hospital as the row  -->
<!--              scales = "free_y", # Free the y scale of each facet -->
<!--              space = "free_y") # Free the vertical spacing of each facet to optimise space -->

<!-- ``` -->

### Réorganiser l'affichage des "facets" {.unnumbered}  

Voir ce [post](https://juliasilge.com/blog/reorder-within/) sur la façon de réorganiser les modalités/niveaux des variables facteurs *dans* les "facets".  

## Stocker les graphiques produits  

### Sauvegarder les  graphiques dans l'environnement {.unnumbered}

Par défaut, lorsque nous exécutons une commande `ggplot()`, le graphique sera affiché dans l'onglet "Plots" de RStudio. Cependant, nous pouvons également enregistrer le celui-ci en tant qu'objet en utilisant l'opérateur d'affectation `<-` et en lui donnant un nom. Il ne s'affichera alors que si le nom de l'objet lui-même est exécuté. On peut également l'afficher en faisant appel à la fonction **R base** `print()`, mais cela n'est nécessaire que dans certaines circonstances, par exemple si le graphique est créé à l'intérieur d'une *boucle for* utilisée pour afficher plusieurs graphiques à la fois (voir la page [Itération, boucles et listes](iteration)).  

```{r, warning=F, message=F}
# definir le graphique
age_by_wt <- ggplot(data = linelist, mapping = aes(x = age_years, y = wt_kg, color = age_years))+
  geom_point(alpha = 0.1)

# l'afficher
age_by_wt    
```


### Modifier des graphiques de l'environnement {.unnumbered}  

Une des particularités de **ggplot2** est que nous pouvons définir un graphiquee (comme ci-dessus), puis lui ajouter des couches en commençant par son nom. Nous n'avons pas besoin de répéter toutes les commandes qui ont créé le graphique original ! 

Par exemple, pour modifier le graphe `age_by_wt` qui a été défini ci-dessus, pour inclure une ligne verticale à l'âge de 50 ans, il suffit d'ajouter un `+` et de commencer à ajouter des couches supplémentaires au graphe. 

```{r, warning=F, message=F}
age_by_wt+
  geom_vline(xintercept = 50)
```


### Exporter les graphiques {.unnumbered}   

L'exportation de ggplots est facilitée par la fonction `ggsave()` de **ggplot2**. Elle peut fonctionner de deux façons :  

* Spécifier le nom de l'objet graphique, puis le chemin d'accès au fichier et le nom du fichier avec l'extension.  
  * Par exemple : `ggsave(my_plot, here("plots", "my_plot.png"))`  
* Exécutez la commande avec seulement un chemin d'accès au fichier, pour sauvegarder le dernier graphique qui a été imprimé.  
  * Par exemple : `ggsave(here("plots", "my_plot.png"))`.  
  
Vous pouvez exporter en png, pdf, jpeg, tiff, bmp, svg, ou plusieurs autres types de fichiers, en spécifiant l'extension du fichier dans le chemin d'accès au fichier.  

Vous pouvez également spécifier les arguments `width = ` (largeur), `height = ` (hauteur), et `units = ` (unités) (soit "in", "cm", ou "mm"). Vous pouvez également spécifier `dpi = ` avec un nombre pour la résolution du graphe (par exemple 300). Consultez les détails de la fonction en entrant `?ggsave` ou en lisant la [documentation en ligne](https://ggplot2.tidyverse.org/reference/ggsave.html). 

Rappelez-vous que vous pouvez utiliser la syntaxe `here()` pour fournir le chemin d'accès au fichier souhaité. Voir la page [Importation et exportation](#import_export) pour plus d'informations. 


## Etiquettes du graphe

Vous voudrez certainement ajouter ou ajuster les étiquettes du graphique. Ceci est le plus facile à faire avec la fonction `labs()` qui est ajoutée au graphe avec `+` tout comme les "geoms" l'ont été.  

Dans `labs()`, vous pouvez fournir des chaînes de caractères à ces arguments :  

* `x = ` et `y = ` Le titre de l'axe des x et de l'axe des y (étiquettes des axes)  
* `title = ` Le titre du graphique principal  
* `subtitle = ` Le sous-titre du graphique, en plus petit texte sous le titre  
* `caption = ` La note de bas de graphe du graphique, en bas à droite par défaut.  

Voici un graphique que nous avons fait plus tôt, mais avec des étiquettes plus jolies :  

```{r, warning=F, message=F}
age_by_wt <- ggplot(
  data = linelist,   # preciser le jeu de donnees
  mapping = aes(     # mapper les attributs aux valeurs des variables
         x = age,           # mapper l'axe des x à l'âge            
         y = wt_kg,         # mapper l'axe des y au poids (weight)
         color = age))+     # mapper la couleur à l'âge
  geom_point()+           # afficher les données comme des points
  labs(
    title = "Age and weight distribution",
    subtitle = "Fictional Ebola outbreak, 2014",
    x = "Age in years",
    y = "Weight in kilos",
    color = "Age",
    caption = stringr::str_glue("Data as of {max(linelist$date_hospitalisation, na.rm=T)}"))

age_by_wt
```


<span style="color: darkgreen;">**_ASTUCE:_** Remarquez comment, dans l'affectation de la note de bas de graphe, nous avons utilisé `str_glue()` du package **stringr** pour implanter du code R dynamique dans le texte de la chaîne de caractères. La légende affichera la date "Data as of :" qui reflète la date d'hospitalisation maximale dans la liste linéaire utilisée pour dessiner le graphe. Pour en savoir plus, consultez la page [Caractères et chaînes de caractères](#character_strings).</span>  

<span style="color: black;">**_NOTE:_** Une remarque sur la spécification du titre de la *légende* : Il n'y a pas un unique argument "titre de légende", car on peut avoir plusieurs échelles dans votre légende. Dans `labs()`, on peut écrire l'argument pour l'attribut graphique utilisé pour créer la légende, et fournir le titre de cette façon. Par exemple, ci-dessus, nous avons assigné `color = age` pour créer la légende. Par conséquent, nous fournissons `color = ` à `labs()` et nous attribuons le titre de légende souhaité ("Age" avec un A majuscule). Si on crée la légende avec `aes(fill = COLUMN)`, alors dans `labs()` on écrira `fill = ` pour ajuster le titre de cette légende. La section sur les échelles de couleurs dans la page [ggplot tips](#ggplot_tips) fournit plus de détails sur l'édition des légendes, et une approche alternative utilisant les fonctions `scales_()`.</span>


## Thèmes {#ggplot_basics_themes} 

Une des meilleures parties de **ggplot2** est le large contrôle que nous pouvons avoir sur le graphique - nous pouvons définir n'importe quoi ! Comme mentionné plus haut, les éléments du graphique qui ne sont *pas* reliés aux données sont ajustés par la fonction `theme()`. Par exemple, la couleur de fond du graphique, la présence/absence de lignes de grille, et la police/taille/couleur/alignement du texte (titres, sous-titres, légendes, texte des axes...). Ces ajustements peuvent être effectués de deux manières :  

* Utiliser une fonction [*thème toute faite*](https://ggplot2.tidyverse.org/reference/ggtheme.html) `theme_()` pour faire des ajustements généraux - ceux-ci incluent `theme_classic()`, `theme_minimal()`, `theme_dark()`, `theme_light()`, `theme_grey()`, `theme_bw()` entre autres.  
* Ajustez chaque petit aspect du graphique individuellement dans `theme()`.  

### Fonctions thème toute faites {.unnumbered}  

Comme elles sont assez simples, nous allons démontrer l'utilisation des fonctions thème prêtes à l'utilisation ci-dessous et ne les décrirons pas davantage ici. 

<span style="color: black;">**_NOTE:_** Notez que tout micro-ajustement *supplémentaire* avec `theme()` doit être fait *après* l'utilisation d'une fonction thème toute faite (sinon les ajustements ne seront pas pris en compte).</span>  

Ecrivez-les avec des parenthèses vides. 

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}

ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)+
  labs(title = "Theme classic")+
  theme_classic()

ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)+
  labs(title = "Theme bw")+
  theme_bw()

ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)+
  labs(title = "Theme minimal")+
  theme_minimal()

ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)+
  labs(title = "Theme gray")+
  theme_gray()
  


```

### Modifier le thème {.unnumbered}  

La fonction `theme()` peut prendre un grand nombre d'arguments, dont chacun modifie un aspect très spécifique du graphique. Il n'est pas possible de couvrir tous les arguments, mais nous allons décrire le modèle général pour leur utilisation et vous montrer comment trouver le nom de l'argument dont vous avez besoin. La syntaxe de base est la suivante :

1. Dans `theme()` écrivez le nom de l'argument pour l'élément du graphique que vous voulez modifier, comme `plot.title = `  
2. Fournissez une fonction `element_()` à l'argument  
  + Le plus souvent, utilisez `element_text()`, mais d'autres incluent `element_rect()` pour les couleurs d'arrière-plan du canevas, ou `element_blank()` pour supprimer les éléments du graphique  
3. A l'intérieur de la fonction `element_()`, écrivez des affectations d'arguments pour faire les ajustements fins que vous désirez.  

Cette description était assez abstraite, voici donc quelques exemples.  

Le graphique ci-dessous a l'air assez stupide, mais il sert à vous montrer une variété de façons dont vous pouvez ajuster vos graphes.  

* Nous commençons avec le graphique `age_by_wt` défini juste au-dessus et ajoutons `theme_classic()`.  
* Pour des ajustements plus fins, on ajoute `theme()` et on inclut un argument pour chaque élément du graphe à ajuster.  

Il peut être intéressant d'organiser les arguments en sections logiques. Pour décrire quelques-uns de ceux utilisés ci-dessous :  

* `legend.position = ` est unique en ce qu'il accepte des valeurs simples comme "bottom", "top", "left" et "right". Mais en général, les arguments liés au texte nécessitent que vous placiez les détails *dans* `element_text()`.  
* La taille du titre avec `element_text(size = 30)`  
* L'alignement horizontal de la note de bas de graphe avec `element_text(hjust = 0)` (de droite à gauche)  
* Le sous-titre est en italique avec `element_text(face = "italic")`  

```{r, , warning=F, message=F}
age_by_wt + 
  theme_classic()+                                 # pre-definir les ajustements avec un theme tout-prêt
  theme(
    legend.position = "bottom",                    # déplacer la legende en dessous
    
    plot.title = element_text(size = 30),          # taille du titre à 30
    plot.caption = element_text(hjust = 0),        # note de bas de graphe alignée à gauche
    plot.subtitle = element_text(face = "italic"), # sous-titre en italique
    
    axis.text.x = element_text(color = "red", size = 15, angle = 90), # ajustement pour le texte sur l'axe des x uniquement 
    axis.text.y = element_text(size = 15),         # ajustement pour le texte sur l'axe des y uniquement
    
    axis.title = element_text(size = 20)           # ajustement pour les titres des deux axes
    )     
```


Voici quelques arguments `theme()` particulièrement courants. Vous reconnaîtrez certains motifs, comme l'ajout de `.x` ou `.y` pour appliquer le changement seulement à un axe.


`theme()` argument                 |Ce qu'il ajuste
-----------------------------------|----------------------------------
`plot.title = element_text()`      |Le titre
`plot.subtitle = element_text()`   |Le sous-titre
`plot.caption = element_text()`    |La note de bas de graphe (family, face, color, size, angle, vjust, hjust...) 
`axis.title = element_text()`      |Titre des axes (both x and y) (size, face, angle, color...)
`axis.title.x = element_text()`    |Titre des axes: axe des x uniquement (utiliser `.y` pour axe des y uniquement)
`axis.text = element_text()`       |Texte sur les axes (pour les deux axes)
`axis.text.x = element_text()`     |Texte sur les axes: axe des x uniquement (utiliser `.y` pour axe des y uniquement)  
`axis.ticks = element_blank()`     |Supprimer les coches des axes
`axis.line = element_line()`       |Lignes des axes (color, size, linetype: solid dashed dotted etc)
`strip.text = element_text()`      |Texte de bande des "facet" (colour, face, size, angle...)
`strip.background = element_rect()`|Bande des "facet" (fill, colour, size...)  


Vous vous dîtes surement "Mais il y a tellement d'arguments pour les  thémes! Comment pourrais-je me les rappeler tous ?". Ne vous inquiétez pas - il est impossible de se souvenir de tous. Heureusement, il existe quelques outils pour vous aider :  

La documentation **tidyverse** sur [modifier le thème](https://ggplot2.tidyverse.org/reference/theme.html), qui contient une liste complète.  

<span style="color: darkgreen;">**_TIP:_** Exécutez la commande `theme_get()` de **ggplot2** pour imprimer une liste de tous les >90 arguments de `theme()` sur la console.</span>  

<span style="color: darkgreen;">**_TIP:_** Si jamais vous voulez supprimer un élément d'un graphe, vous pouvez aussi le faire via `theme()`. Passez simplement `element_blank()` en argument pour le faire disparaître complètement. Pour les légendes, préciser `legend.position = "none"`.</span>  




## Couleurs  


Veuillez consulter cette [section sur les échelles de couleurs de la page "Trucs et Astuces dans ggplot"](#ggplot_tips_colors).


## Utiliser le "pipe" avec **ggplot2**   

Lorsque vous utilisez des "pipes" pour nettoyer et transformer vos données, il est facile de passer les données transformées dans `ggplot()`.  

Les "pipes" (qui passent le jeu de données d'une fonction à l'autre) laisseront place aux `+` une fois que la fonction `ggplot()` sera appelée. Notez que dans ce cas, il n'est pas nécessaire de spécifier l'argument `data = `, car il est automatiquement défini comme le jeu de données passé dans le pipe.  

Voici à quoi cela peut ressembler :  

```{r, warning=F, message=F}
linelist %>%                                                     # commencer avec la liste lineaire
  select(c(case_id, fever, chills, cough, aches, vomit)) %>%     # selectionner les  variables qui nous interessent
  pivot_longer(                                                  # pivoter en format long
    cols = -case_id,                                  
    names_to = "symptom_name",
    values_to = "symptom_is_present") %>%
  mutate(                                                        # remplacer les valeurs  manquantes
    symptom_is_present = replace_na(symptom_is_present, "unknown")) %>% 
  
  ggplot(                                                        # commencer le ggplot!
    mapping = aes(x = symptom_name, fill = symptom_is_present))+ # remarquez qu'ici on passe aux +
  geom_bar(position = "fill", col = "black") +                    
  theme_classic() +
  labs(
    x = "Symptom",
    y = "Symptom status (proportion)"
  )
```



## Représenter des données continues

Tout au long de cette page, vous avez déjà vu de nombreux exemples de représentation de données continues. Nous les consolidons ici brièvement et présentons quelques variantes.  
Les visualisations couvertes ici incluent :

* Les graphiques pour une variable continue :  
  * **Histogramme**, un graphique classique pour présenter la distribution d'une variable continue. 
  **Diagramme en boîte** (également appelé boîte à moustaches), pour montrer les 25ème, 50ème et 75ème percentiles, les extrémités de la distribution et les valeurs aberrantes ([limitations importantes](https://www.data-to-viz.com/caveat/boxplot.html)).  
  **Graphique de gigue**, pour montrer toutes les valeurs sous forme de points qui sont "gigueux" afin qu'ils puissent (presque) tous être vus, même si deux d'entre eux ont la même valeur.  
  **Graphiques en violon**, montre la distribution d'une variable continue basée sur la largeur symétrique du "violon". 
  **Sina plot**, est une combinaison du graphique de gigue et du graphique de violin, où les points individuels sont montrés mais dans la forme symétrique de la distribution (via le "package" **ggforce**).  
**Nuage de points** pour deux variables continues.  
**Heatmaps** pour trois variables continues (lien vers la page [Heat plots](#heatmaps)).

### Histogrammes {.unnumbered}

Les histogrammes peuvent ressembler à des diagrammes en barres, mais ils sont distincts car ils mesurent la distribution d'une variable *continue*. Il n'y a pas d'espace entre les "barres", et une seule colonne est fournie à `geom_histogram()`.

Le code ci-dessous permet de générer des **histogrammes**, qui regroupent les données continues en gammes et les affichent dans des barres adjacentes de hauteur variable. Ceci est fait en utilisant `geom_histogram()`. Voir la section ["Diagrammes en barres"](#ggplot_basics_bars) de la page ggplot basics pour comprendre la différence entre `geom_histogram()`, `geom_bar()`, et `geom_col()`.  

Nous allons montrer la distribution des âges des cas. Dans `mapping = aes()`, nous spécifierons la colonne dont nous voulons voir la distribution. On peut affecter cette colonne à l'axe des x ou des y. 

Les lignes seront assignées à des "bins" basés sur leur âge numérique, et ces "bins" seront représentés graphiquement par des barres. Si vous spécifiez un nombre de "bins" avec l'attribut graphique `bins = `, les points de rupture sont espacés de manière égale entre les valeurs minimum et maximum de l'histogramme. Si `bins = ` n'est pas spécifié, un nombre approprié de bins sera deviné et ce message sera affiché après le tracé :  

```
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
``` 

Si vous ne voulez pas spécifier un nombre de "bins" à `bins = `, vous pouvez alternativement spécifier `binwidth = ` dans les unités de l'axe. Nous donnons quelques exemples montrant différents bins et largeurs de bins :  

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# A) Histogramme tracé par défaut
ggplot(data = linelist, aes(x = age))+  # provide x variable
  geom_histogram()+
  labs(title = "A) Default histogram (30 bins)")

# B) Plus de bins
ggplot(data = linelist, aes(x = age))+  # mapper la variable age à l'axe des x
  geom_histogram(bins = 50)+
  labs(title = "B) Set to 50 bins")

# C) Moins de bins
ggplot(data = linelist, aes(x = age))+  # mapper la variable age à l'axe des x
  geom_histogram(bins = 5)+
  labs(title = "C) Set to 5 bins")


# D) Plus de bins
ggplot(data = linelist, aes(x = age))+  # mapper la variable age à l'axe des x
  geom_histogram(binwidth = 1)+
  labs(title = "D) binwidth of 1")

```


Pour obtenir des proportions lissées, on peut utiliser `geom_density()` : 

```{r, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# Fréquence avec axe de proportion, lissée
ggplot(data = linelist, mapping = aes(x = age)) +
  geom_density(size = 2, alpha = 0.2)+
  labs(title = "Proportional density")

# Fréquence empilée avec axe de proportion, lissée
ggplot(data = linelist, mapping = aes(x = age, fill = gender)) +
  geom_density(size = 2, alpha = 0.2, position = "stack")+
  labs(title = "'Stacked' proportional densities")
```


Pour obtenir un histogramme "empilé" (d'une colonne continue de données), nous pouvons faire l'une des choses suivantes :  

1) Utilisez `geom_histogram()` avec l'argument `fill = ` dans `aes()` et affecté à la colonne de regroupement, ou   
2) Utilisez `geom_freqpoly()`, qui est probablement plus facile à lire (vous pouvez toujours définir `binwidth = `).  
3) Pour voir les proportions de toutes les valeurs, définissez le paramètre `y = after_stat(density)` (utilisez exactement cette syntaxe non modifiée pour vos données). Note : ces proportions seront affichées *par groupe*.  

Chacun d'entre eux est présenté ci-dessous (*notez* l'utilisation de `color = ` ou  `fill = ` dans chacun d'entre eux) :  


```{r, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# Histogramme "empilé"
ggplot(data = linelist, mapping = aes(x = age, fill = gender)) +
  geom_histogram(binwidth = 2)+
  labs(title = "'Stacked' histogram")

# Frequence
ggplot(data = linelist, mapping = aes(x = age, color = gender)) +
  geom_freqpoly(binwidth = 2, size = 2)+
  labs(title = "Freqpoly")

# Frequence avec axe en proportion 
ggplot(data = linelist, mapping = aes(x = age, y = after_stat(density), color = gender)) +
  geom_freqpoly(binwidth = 5, size = 2)+
  labs(title = "Proportional freqpoly")

# Frequence avec axe en proportion, lissé
ggplot(data = linelist, mapping = aes(x = age, y = after_stat(density), fill = gender)) +
  geom_density(size = 2, alpha = 0.2)+
  labs(title = "Proportional, smoothed with geom_density()")
```

Si vous voulez vous amuser un peu, essayez `geom_density_ridges` du "package" **ggridges** ([vignette ici](https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html).  

Pour plus de détails sur les histogrammes, consultez la page **tidyverse** [page sur geom_histogram()](https://ggplot2.tidyverse.org/reference/geom_histogram.html).  


### Diagrammes en boîtes {.unnumbered}

Les diagrammes en boîte sont très utilisés, mais ils ont des limites importantes. Ils peuvent masquer la distribution réelle - par exemple, une distribution bimodale. Voir cette [galerie de graphiques R](https://www.r-graph-gallery.com/boxplot.html) et cet [article data-to-viz](https://www.data-to-viz.com/caveat/boxplot.html) pour plus de détails. Cependant, ils affichent joliment l'écart interquartile et les valeurs aberrantes - ils peuvent donc être superposés à d'autres types de graphiques qui montrent la distribution de manière plus détaillée.  

Nous vous rappelons ci-dessous les différentes composantes d'un diagramme en boîte :  
```{r, out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "boxplot.png"))
```

Lorsque vous utilisez `geom_boxplot()` pour créer un box plot, vous mappez généralement un seul axe (x ou y) dans `aes()`. L'axe spécifié détermine si les tracés sont horizontaux ou verticaux. 

Dans la plupart des "geoms", nous créons un graphique par groupe en faisant correspondre un attribut comme `color = ` ou `fill = ` à une variable dans `aes()`. Cependant, pour les diagrammes en boîte, nous pouvons le faire en assignant la variable de regroupement à l'axe non assigné (x ou y). Ci-dessous se trouve le code pour un diagramme de boîte de *toutes* les valeurs d'âge dans l'ensemble de données, et ensuite le code pour afficher un box plot pour chaque sexe (non manquant) dans l'ensemble du jeu de données. Notez que les valeurs `NA` (manquantes) apparaîtront comme un diagramme de boîte séparé, sauf si elles sont supprimées. Dans cet exemple, nous avons également défini le "remplissage" de la colonne "gender" (issue finale de chaque cas) afin que chaque diagramme de boîtes soit d'une couleur différente - mais ce n'est pas nécessaire. 

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# A) Diagramme de boîte d'ensemble
ggplot(data = linelist)+  
  geom_boxplot(mapping = aes(y = age))+   # uniquement axe y mappé (non axe des x)
  labs(title = "A) Overall boxplot")

# B) Diagramme de boîte par groupe
ggplot(data = linelist, mapping = aes(y = age, x = gender, fill = gender)) + 
  geom_boxplot()+                     
  theme(legend.position = "none")+   # supprimer la légende (redondant)
  labs(title = "B) Boxplot by gender")      
```

 
Pour le code permettant d'ajouter un diagramme en boîte aux bords d'un nuage de points (diagrammes "marginaux"), voir la page [Trucs et Astuces avec ggplot](#ggplot_tips).  


### Graphes en violon, gigue, et "sina" {.unnumbered}

Ci-dessous, vous trouverez le code pour créer des diagrammes en **violon** (`geom_violin`) et **jitter** (gigue) (`geom_jitter`) pour montrer les distributions. Vous pouvez spécifier que le remplissage ou la couleur est également déterminé par les données, en insérant ces options dans `aes()`. 

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}


# A) Jitter par groupe
ggplot(data = linelist %>% drop_na(outcome),      # supprimer les valeurs manquantes
       mapping = aes(y = age,                     # mapper la variable continue
           x = outcome,                           # mapper la variable de regroupement
           color = outcome))+                     # mapper la couleur avec la variable outcome 
  geom_jitter()+                                  # Creer le graphique
  labs(title = "A) jitter plot by gender")     



# B) Violin par groupe
ggplot(data = linelist %>% drop_na(outcome),       # supprimer les valeurs manquantes
       mapping = aes(y = age,                      # mapper la variable continue
           x = outcome,                            # mapper la variable de regroupement
           fill = outcome))+                       # mapper la ouleur avec la variable outcome
  geom_violin()+                                   # Creer le graphique
  labs(title = "B) violin plot by gender")    
```


Vous pouvez combiner les deux en utilisant la fonction `geom_sina()` du "package" **ggforce**. La fonction trace les points de gigue dans la forme du tracé de violon. Lorsqu'il est superposé au tracé du violon (en ajustant les transparences), il peut être plus facile à interpréter visuellement.  


```{r, warning=F, message=F}

# A) Sina  par group
ggplot(
  data = linelist %>% drop_na(outcome), 
  aes(y = age,           # mapper la variable numérique
      x = outcome)) +    # mapper la variable de regroupement
  geom_violin(
    aes(fill = outcome), # remplissage (couleur de fond du violon)
    color = "white",     # contour blanc
    alpha = 0.2)+        # transparence
  geom_sina(
    size=1,                # Changer la taille des gigues
    aes(color = outcome))+ # mapper la couleur des points avec la variable outcome
  scale_fill_manual(       # Definir des couleurs de remplissage (de fond) des violons en precisant quelle couleur prend chaque modalite de la variable outcome
    values = c("Death" = "#bf5300", 
              "Recover" = "#11118c")) + 
  scale_color_manual(      # Definir des couleurs des points en precisant quelle couleur prend chaque modalite de la variable outcome
    values = c("Death" = "#bf5300", 
              "Recover" = "#11118c")) + 
  theme_minimal() +                                # Supprimer l'arriere-plan gris
  theme(legend.position = "none") +                # Supprimer la legende non-necessaire
  labs(title = "B) violin and sina plot by gender, with extra formatting")      


```



### Deux variables continues  {.unnumbered}

En suivant une syntaxe similaire, `geom_point()` vous permettra de tracer deux variables continues l'une en fonction de l'autre dans un **scatter plot** (un nuage de points/un diagramme de dispersion). Ceci est utile pour montrer les valeurs réelles plutôt que leurs distributions. Un diagramme de dispersion basique de l'âge par rapport au poids est montré dans (A). Dans (B), nous utilisons à nouveau `facet_grid()` pour montrer la relation entre deux variables continues dans la liste lineaire. 

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# Diagramme de dispersion du poids et de l'âge
ggplot(data = linelist, 
       mapping = aes(y = wt_kg, x = age))+
  geom_point() +
  labs(title = "A) Scatter plot of weight and age")

# Diagramme de dispersion du poids et de l'âge par sexe et issue finale du cas
ggplot(data = linelist %>% drop_na(gender, outcome), #garder que le sexe/issue finale non manquant
       mapping = aes(y = wt_kg, x = age))+
  geom_point() +
  labs(title = "B) Scatter plot of weight and age faceted by gender and outcome")+
  facet_grid(gender ~ outcome) 

```


### Trois variables continues {.unnumbered}  

Vous pouvez afficher trois variables continues en utilisant l'argument `fill = ` pour créer un *graphique thermique* (heat plot). La couleur de chaque "cellule" reflétera la valeur de la troisième colonne de données continues. Voir la page [Astuces en ggplot](#ggplot_tips) et la page sur les [Graphiques thermiques](#heatmaps) pour plus de détails et plusieurs exemples. 

Il existe des moyens de créer des graphiques 3D dans R, mais pour l'épidémiologie appliquée, ils sont souvent difficiles à interpréter et donc moins utiles pour la prise de décision.  



## Représenter des données catégorielles

Les données catégoriques peuvent être des valeurs de caractères, des valeurs logiques (VRAI/FAUX) ou des facteurs (voir la page [Facteurs](#factors)). 

### Préparation  {.unnumbered}

#### Structure des données {.unnumbered}  

La première chose à comprendre au sujet de vos données catégorielles est de savoir si elles existent sous forme d'observations brutes, comme une liste linéaire de cas, ou sous forme de résumé ou de tableau de données agrégées contenant des comptages ou des proportions. L'état de vos données aura un impact sur la fonction de traçage que vous utiliserez :  

* Si vos données sont des observations brutes avec une ligne par observation, vous utiliserez probablement `geom_bar()`.  
* Si vos données sont déjà agrégées en nombres ou en proportions, vous utiliserez probablement `geom_col()`.  


#### Classe des variables et ordre des valeurs{.unnumbered}  

Ensuite, examinez la classe des colonnes que vous voulez tracer. Nous examinons `hospital`, d'abord avec `class()` de **base** R, et avec `tabyl()` de **janitor**.

```{r}
# Voir la classe de la variable hospital - on peut voir que c'est une variable de type caractère
class(linelist$hospital)

# Regardez les valeurs et les proportions dans la variable hospital
linelist %>% 
  tabyl(hospital)
```


Nous pouvons voir que les valeurs à l'intérieur sont des caractères, car il s'agit de noms d'hôpitaux, et par défaut elles sont classées par ordre alphabétique. Il existe des valeurs "autres" et "manquantes", que nous préférerions voir figurer dans les dernières sous-catégories lors de la présentation des répartitions. Nous transformons donc cette variable en facteur et la réorganisons. Ce point est traité plus en détail dans la page [Facteurs](#factors).


```{r}
# Convertir en facteur et définir l'ordre des niveaux pour que "Other" et "Missing" soient les derniers.
linelist <- linelist %>% 
  mutate(
    hospital = fct_relevel(hospital, 
      "St. Mark's Maternity Hospital (SMMH)",
      "Port Hospital", 
      "Central Hospital",
      "Military Hospital",
      "Other",
      "Missing"))

```


```{r}
levels(linelist$hospital)
```

### `geom_bar()` {#ggplot_basics_bars .unnumbered}  

Utilisez `geom_bar()` si vous voulez que la hauteur des barres (ou la hauteur des composants des barres empilées) reflète *le nombre de lignes pertinentes dans les données*. Ces barres auront des espaces entre elles, à moins que l'attribut graphique `width = ` soit ajusté.  

* Ne fournissez qu'une seule affectation de colonne d'axe (généralement l'axe des x). Si vous fournissez x et y, vous obtiendrez `Error: stat_count() can only have an x or y aesthetic.`  
* Vous pouvez créer des barres empilées en ajoutant une affectation de colonne `fill = ` dans `mapping = aes()`.  
* L'axe opposé sera intitulé "count" par défaut, car il représente le nombre de lignes.  

Ci-dessous, nous avons affecté la variable "outcome" (issue finale) à l'axe des y, mais il pourrait tout aussi bien être sur l'axe des x. Si vous avez des valeurs de caractères plus longues, il est parfois préférable de retourner les barres sur le côté et de placer la légende en bas. Cela peut avoir un impact sur la façon dont vos niveaux de facteurs sont ordonnés - dans ce cas, nous les inversons avec `fct_rev()` pour mettre les manquants et les autres en bas.    

```{r, out.width=c('50%', '50%'), fig.show='hold'}
# A) Issues finales pour l'ensemble des cas
ggplot(linelist %>% drop_na(outcome)) + 
  geom_bar(aes(y = fct_rev(hospital)), width = 0.7) +
  theme_minimal()+
  labs(title = "A) Number of cases by hospital",
       y = "Hospital")


# B) Issues finales pour les cas par hôpital
ggplot(linelist %>% drop_na(outcome)) + 
  geom_bar(aes(y = fct_rev(hospital), fill = outcome), width = 0.7) +
  theme_minimal()+
  theme(legend.position = "bottom") +
  labs(title = "B) Number of recovered and dead Ebola cases, by hospital",
       y = "Hospital")

```





### `geom_col()` {.unnumbered}  

Utilisez `geom_col()` si vous voulez que la hauteur des barres (ou la hauteur des composants des barres empilées) reflète des *valeurs* pré-calculées qui existent dans les données. Il s'agit souvent de chiffres ou de proportions résumés ou "agrégés".  

Fournissez des affectations de variables pour *les deux* axes à `geom_col()`. Généralement, la colonne de l'axe des x est discrète et celle de l'axe des y est numérique. 

Disons que nous avons cet ensemble de données `outcomes` : 

```{r, echo = F}
outcomes <- linelist %>% 
  drop_na() %>% 
  group_by(outcome) %>% 
  count %>% 
  ungroup() %>% # désagréger pour que les proportions soient sur le total
  mutate(proportion = n/sum(n)*100) # Calculer le pourcentage
  
outcomes #Voir la table
```


Le code ci-dessous utilise `geom_col` pour créer des diagrammes en barres simples afin de montrer la distribution de l'issue finale des cas Ebola. Avec `geom_col`, x et y doivent être spécifiés. Ici, x est la variable catégorielle sur l'axe des x, et y est la colonne de proportions générée `proportion`. 


```{r, fig.height = 3, fig.width=4.5}
# Issues finales pour l'ensemble des cas
ggplot(outcomes) + 
  geom_col(aes(x=outcome, y = proportion)) +
  labs(subtitle = "Number of recovered and dead Ebola cases")

```


Pour montrer les répartitions par hôpital, il faudrait que notre tableau contienne plus d'informations et qu'il soit au format "long". Nous créons ce tableau avec les fréquences des catégories combinées `outcome` et `hospital` (voir la page [Travailler sur des données groupées](#grouping_data) pour des conseils sur le regroupement). 

```{r, fig.height = 4, fig.width=6}
outcomes2 <- linelist %>% 
  drop_na(outcome) %>% 
  count(hospital, outcome) %>%  # compter les lignes par hôpital et issue finale
  group_by(hospital) %>%        # regrouper pour que les proportions soient sur le total de "hospital"
  mutate(proportion = n/sum(n)*100) # calculer les proportions de décès et de guerison au sein de chaque "hospital" 

head(outcomes2) # Voir les premières lignes de la table 
```
  
Nous créons ensuite le ggplot avec quelques mises en forme supplémentaires :

  * **Inverser les axes** : Nous avons inversé les axes avec `coord_flip()` pour pouvoir lire les noms des hôpitaux.
  **Barres côte-à-côte** : Nous avons ajouté d'un argument `position = "dodge"` pour que les barres pour les décès et la guérison soient présentées côte à côte plutôt qu'empilées. Notez que les barres empilées sont la valeur par défaut.
  * **Largeur de colonne** : Nous avons spécifié 'width', donc les colonnes sont deux fois moins larges que la largeur maximale possible.
  **Ordre des variable** : Nous avons inversé l'ordre des catégories sur l'axe des y de sorte que 'Autre' et 'Manquant' soient en bas, avec `scale_x_discrete(limits=rev)`. Notez que nous avons utilisé cette méthode plutôt que `scale_y_discrete` parce que l'hôpital est indiqué dans l'argument `x` de `aes()`, même si visuellement il est sur l'axe des ordonnées. Nous faisons cela parce que ggplot semble présenter les catégories à l'envers, sauf si nous lui disons de ne pas le faire.  
  * **Autres détails** : Les étiquettes/titres et couleurs ont été ajoutés dans `labs` et `scale_fill_color` respectivement.
  
```{r, fig.height = 4, fig.width=8}

# Issue finale pour l'ensemble des cas par hopital
ggplot(outcomes2) +  
  geom_col(
    mapping = aes(
      x = proportion,                 # mapper axe des x avec les proportions pre-calculées
      y = fct_rev(hospital),          # inverser les catégories de la variable 'hospital' pour que missing/other sont en dernier
      fill = outcome),                # remplissage par issue finale
    width = 0.5)+                    # barres moins larges (sur 1)
  theme_minimal() +                  # theme minimal 
  theme(legend.position = "bottom")+
  labs(subtitle = "Number of recovered and dead Ebola cases, by hospital",
       fill = "Outcome",             # titre légende 
       y = "Count",                  # titre axe des y 
       x = "Hospital of admission")+ # titre axe des x
  scale_fill_manual(                 # préciser des couleurs manuellement
    values = c("Death"= "#3B1c8C",
               "Recover" = "#21908D" )) 

```


Notez que les proportions sont binaires, et que l'on peut donc préférer ne pas utiliser le terme "guérison" et ne montrer que la proportion de décès. Ceci n'est qu'une illustration.  


Si vous utilisez `geom_col()` avec des données de dates (par exemple une courbe épidémique à partir de données regroupées) - vous voudrez ajuster l'argument `width = ` pour supprimer les lignes de "gap" entre les barres. Si vous utilisez des données quotidiennes, réglez `width = 1`. Si vous utilisez des données hebdomadaires, `width = 7`. 
<span style="color: orange;">**_CAUTION:_** Les mois ne sont pas possibles car chaque mois a un nombre de jours différent..</span>
 

### `geom_histogram()` {.unnumbered}  

Les histogrammes peuvent ressembler à des diagrammes en barres, mais ils sont distincts car ils mesurent la distribution d'une variable *continue*. Il n'y a pas d'espace entre les "barres", et une seule colonne est fournie à `geom_histogram()`. Il existe des arguments spécifiques aux histogrammes tels que `bin_width = ` et `breaks = ` pour spécifier comment les données doivent être classées. La section ci-dessus sur les données continues et la page sur les [Courbes épidémiques](#epicurves) fournissent des détails supplémentaires.  

## Ressources  


Il existe une grande quantité de ressources et d'aide en ligne, en particulier avec ggplot. Voir :

* [Antisèche ggplot2](http://r-statistics.co/ggplot2-cheatsheet.html)
* [Un autre antisèche](https://biostats.w.uib.no/the-ggplot2-cheat-sheet-by-rstudio/)
* [Page tidyverse sur les bases de ggplot](https://ggplot2.tidyverse.org/reference/)  
* [Representation des variables continues](http://www.sthda.com/english/articles/32-r-graphics-essentials/131-plot-two-continuous-variables-scatter-graph-and-alternatives/)  
* Page R for Data Science sur la [visualisation des donnees](https://r4ds.had.co.nz/data-visualisation.html)
* Page R for Data Science sur les [graphiques pour mieux communiquer](https://r4ds.had.co.nz/graphics-for-communication.html)  

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/ggplot_basics.Rmd-->

# Traçage de données continues {#plot_continuous}  

```{r echo=F, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
pacman::p_load(ggplot2,
               dplyr)

linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds")) %>% #Charger les données
  mutate(age = as.numeric(age),
         wt_kg = as.numeric(wt_kg)) # Conversion de l'âge et du poids en valeur numérique si nécessaire

# avec la fonction boxplot()
# graphics::boxplot(age ~ outcome,
#                   data = linelist,
#                   col = c("gold", "darkgreen"),
#                   main = "A) PLOT EN BOÎTE réalisé à l'aide du graphique boxplot intégré à R")



# avec ggplot2 historgram
ggplot(data = linelist,
       mapping = aes(x = age))+
  geom_histogram(binwidth=2)+
  labs(title = "A) histogram of age")


# avec les box plots de ggplot2
ggplot(data = linelist %>% drop_na(gender), mapping = aes(y = age, x = gender, fill = gender))+
  geom_violin()+
  #ggforce::geom_sina(aes(color = gender), alpha = 0.5)+
  geom_boxplot( fill = NA)+
  labs(title = "B) Box and Violin plots of age by gender")


# avec ggplot2 - graphique en nuage de points
ggplot(data = linelist,
       mapping = aes(y = wt_kg, x = age))+
  geom_point()+
  labs(title = "C) Scatter plot of weight and age")



```

Cette page traite du traçage approprié de données continues, telles que l'âge, les mesures cliniques et la distance. Nous nous concentrons sur l'utilisation de **ggplot2** (qui fait partie de la famille de paquets **tidyverse**), mais nous décrivons aussi brièvement les fonctions de traçage de **base** de R.  

Les visualisations couvertes ici incluent :

* Tracés pour une variable continue :  
  * **Histogramme**, un graphique classique pour présenter la distribution d'une variable continue. 
  * **Graphique en boîte** (également appelé boîte et moustaches), pour montrer les 25ème, 50ème et 75ème percentiles, les extrémités de la distribution et les valeurs aberrantes ([limitations importantes](https://www.data-to-viz.com/caveat/boxplot.html)).  
  * **Graphique de gigue**, pour montrer toutes les valeurs sous forme de points qui sont "gigueux" de sorte qu'ils peuvent (presque) tous être vus, même si deux ont la même valeur.  
  * **Violin plot**, montre la distribution d'une variable continue basée sur la largeur symétrique du "violon". 
  * **Sina plot**, est une combinaison de jitter et de violin plot, où les points individuels sont montrés mais dans la forme symétrique de la distribution (via le paquet **ggforce**).  
* **Scatter plot** pour deux variables continues.  
* **Graphes de chaleur** pour trois variables continues (lien vers la page [Graphiques thermiques](#heatmaps) ).  







<!-- ======================================================= -->
## Préparation { }

La préparation comprend le chargement des paquets pertinents, ici **ggplot2** et **dplyr** (tous deux faisant partie de **tidyverse**), et l'assurance que vos colonnes de données sont de la bonne classe. 

### Chargement des paquets {.unnumbered}

Ce morceau de code montre le chargement des paquets requis pour les analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* le charge pour l'utiliser. Vous pouvez aussi charger les paquets installés avec `library()` de **base** R. Voir la page sur [R - les bases](#rbasics) pour plus d'informations sur les paquets R.  

Note : Le méga-paquet **tidyverse** comprend les paquets **ggplot2** et **dplyr** parmi beaucoup d'autres (par exemple **stringr**, **tidyr**, et **forcats**).  

```{r}
pacman::p_load(
  rio, # importation et exportation de données
  here, # chemins de fichiers relatifs
  tidyverse # inclut ggplot2 et dplyr
  )
```



### Importer des données {.unnumbered}

Pour les exemples de cette page, nous importons la linelist nettoyée des cas d'une épidémie d'Ebola simulée. Si vous voulez suivre, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la linelist "propre"</a> (en tant que fichier .rds). Importez des données avec la fonction `import()` du paquet **rio** (elle gère de nombreux types de fichiers comme .xlsx, .csv, .rds - voir la page [Importer et exporter des données](#import_export) pour plus de détails).  

```{r, echo=F}
# Importez la liste de diffusion dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# Importez la liste de cas
linelist <- import("linelist_cleaned.rds")
```

Les 50 premières lignes de la linelist sont affichées ci-dessous. Nous allons nous concentrer sur les variables continues `age`, `wt_kg` (poids en kilos), `ct_blood` (CT, valeurs de seuil de cycle), et `days_onset_hosp` (différence entre la date d'apparition des symptômes et la date d'hospitalisation).  

```{r, message=FALSE, echo=F}
# affichez les données de la liste des lignes sous forme de tableau
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





### Classe de colonnes {.unnumbered}  

Assurez-vous que les colonnes continues sont correctement classées dans R comme classe numérique. Nous utilisons ci-dessous `mutate()` et `as.numeric()` pour assurer la classe des colonnes qui sont importantes pour notre analyse. Voir la page [Nettoyage de données et fonctions essentielles](#cleaning_data) pour plus de détails.  

```{r}
linelist <- linelist %>%  
  mutate(
    across( # mutate plusieurs colonnes à la fois
      .cols = c(age, ct_blood, days_onset_hosp, wt_kg), # colonnes à transformer
      .fns = as.numeric # fonction à utiliser (sans parenthèses)
      )
    )                               
 
```

Remarque : Vous devez avoir effectué plusieurs vérifications des données avant cette étape, y compris la vérification de l'absence de données. Consultez la page [Données manquantes](#missing_data) pour obtenir des conseils sur ces analyses.  





### Une variable continue  



### Histogrammes {.unnumbered}

Vous trouverez ci-dessous le code pour générer des **histogrammes**, qui regroupent les données continues en plages et les affichent dans des barres adjacentes de hauteur variable. Ceci est fait en utilisant `geom_histogram()`. Voir la section ["Tracé des barres"](#ggplot_basics_bars) de la page ggplot basics pour comprendre la différence entre `geom_histogram()`, `geom_bar()`, et `geom_col()`.  

Nous allons montrer la distribution des âges des cas. Dans `mapping = aes()`, spécifiez la colonne dont vous voulez voir la distribution. Vous pouvez affecter cette colonne à l'axe des x ou des y. 

Les lignes seront assignées à des "bacs" basés sur leur âge numérique, et ces bacs seront représentés graphiquement par des barres. Si vous spécifiez un nombre de bins avec l'esthétique de tracé `bins = `, les points de rupture sont espacés de manière égale entre les valeurs minimum et maximum de l'histogramme. Si `bins = ` n'est pas spécifié, un nombre approprié de bins sera deviné et ce message sera affiché après le tracé :  

```
## `stat_bin()`` utilisant `bins = 30`. Choisissez une meilleure valeur avec `binwidth`.
``` 

Si vous ne voulez pas spécifier un nombre de bins à `bins = `, vous pouvez alternativement spécifier `binwidth = ` dans les unités de l'axe. Nous donnons quelques exemples montrant différents bins et largeurs de bins :  

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# A) Histogramme régulier
ggplot(data = linelist, aes(x = age))+ # fournir la variable x
  geom_histogram()+
  labs(title = "A) Histogramme par défaut (30 bins)")

# B) Plus de bins
ggplot(data = linelist, aes(x = age))+ # fournit la variable x
  geom_histogram(bins = 50)+
  labs(title = "B) Définir à 50 bins")

# C) Moins de bins
ggplot(data = linelist, aes(x = age))+ # fournir la variable x
  geom_histogram(bins = 5)+
  labs(title = "C) Définir à 5 bins")


# D) Plus de bacs
ggplot(data = linelist, aes(x = age))+ # fournir la variable x
  geom_histogram(binwidth = 1)+
  labs(title = "D) largeur de bande de 1")

# E) Histogrammes en couches avec différentes largeurs de bande
ggplot(data = linelist, aes(x = age))+ # fournir la variable x 
  geom_histogram(
    binwidth = 2) + # La couche sous-jacente a une largeur de bin de 2
  geom_histogram(
    binwidth = 1, # La couche supérieure a une largeur de bin de 1
    alpha = 0.4, # Définir la couche supérieure pour qu'elle soit légèrement transparente
    fill = "blue")+ 
  labs(title = "E) Histogrammes en couches avec différentes largeurs de bande")

```



Pour obtenir des proportions lissées, vous pouvez utiliser `geom_density()`` :  

```{r, warning=F, message=F}
# Fréquence avec axe de proportion, lissée
ggplot(data = linelist, mapping = aes(x = age), fill = gender) +
  geom_density(binwidth = 2, size = 2, alpha = 0.2)+
  labs(title = "Proportionnelle, lissée avec geom_density()")
```


Pour obtenir un histogramme "empilé" (d'une colonne continue de données), vous pouvez effectuer l'une des opérations suivantes :  

1) Utilisez `geom_histogram()` avec l'argument `fill = ` dans `aes()` et affecté à la colonne de regroupement, ou bien  
2) Utilisez `geom_freqpoly()`, qui est probablement plus facile à lire (vous pouvez toujours définir `binwidth = `).  
3) Pour voir les proportions de toutes les valeurs, définissez le paramètre `y = after_stat(density)` (utilisez exactement cette syntaxe - elle n'a pas été modifiée pour vos données). Note : ces proportions seront affichées *par groupe*.  

Chacune d'entre elles est présentée ci-dessous (*notez l'utilisation de `color = ` par rapport à `fill = ` dans chacune) :  

```{r, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# Histogramme "empilé"
ggplot(data = linelist, mapping = aes(x = age, fill = gender)) +
  geom_histogram(binwidth = 2)+
  labs(title = "Histogramme 'empilé'")

# Fréquence 
ggplot(data = linelist, mapping = aes(x = age, color = gender)) +
  geom_freqpoly(binwidth = 2, size = 2)+
  labs(title = "Freqpoly")

# Fréquence avec axe de proportion
ggplot(data = linelist, mapping = aes(x = age, y = after_stat(density), color = gender)) +
  geom_freqpoly(binwidth = 5, size = 2)+
  labs(title = "Freqpoly proportionnelle")

# Fréquence avec axe de proportion, lissée
ggplot(data = linelist, mapping = aes(x = age, y = after_stat(density), fill = gender)) +
  geom_density(binwidth = 2, size = 2, alpha = 0.2)+
  labs(title = "Proportionnelle, lissée avec geom_density()")
```

Si vous voulez vous amuser un peu, essayez `geom_density_ridges` du paquet **ggridges** ( [vignette ici](https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html).  

Pour en savoir plus sur les histogrammes, consultez la page **tidyverse** [page sur geom_histogram()](https://ggplot2.tidyverse.org/reference/geom_histogram.html).  



### Diagrammes en boîte {.unnumbered}

Les diagrammes en boîte sont courants, mais ils ont des limites importantes. Ils peuvent masquer la distribution réelle - par exemple, une distribution bimodale. Voir cette [galerie de graphiques R](https://www.r-graph-gallery.com/boxplot.html) et cet [article data-to-viz](https://www.data-to-viz.com/caveat/boxplot.html) pour plus de détails. Cependant, ils affichent joliment l'écart interquartile et les valeurs aberrantes, et peuvent donc être superposés à d'autres types de graphiques qui montrent la distribution de manière plus détaillée.  

Voici l'anatomie d'un box plot ( [source de l'image](https://www.leansigmacorporation.com/box-plot-with-minitab/)).  

```{r, out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "boxplot.png"))
```


Lorsque vous utilisez `geom_boxplot()` pour créer un box plot, vous n'indiquez généralement qu'un seul axe (x ou y) dans `aes()`. L'axe spécifié détermine si les tracés sont horizontaux ou verticaux. 

Dans la plupart des géométries, vous créez un graphique par groupe en faisant correspondre une esthétique comme `color = ` ou `fill = ` à une colonne dans `aes()`. Cependant, pour les boxplot, vous pouvez le faire en assignant la colonne de regroupement à l'axe non assigné (x ou y). Ci-dessous se trouve le code pour un boxplot de *toutes* les valeurs d'âge dans l'ensemble de données, et ensuite le code pour afficher un box plot pour chaque sexe (non manquant) dans l'ensemble de données. Notez que les valeurs `NA` (manquantes) apparaîtront comme un box plot séparé, sauf si elles sont supprimées. Dans cet exemple, nous avons également défini le `fill` sur la colonne `outcome` pour que chaque diagramme soit d'une couleur différente - mais ce n'est pas nécessaire.  

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# A) Diagramme numérique global
ggplot(data = linelist)+  
  geom_boxplot(mapping = aes(y = age))+ # seulement l'axe des y cartographié (pas les x)
  labs(title = "A) Boxplot global")

# B) Box plot par groupe
ggplot(data = linelist, mapping = aes(y = age, x = gender, fill = gender)) + 
  geom_boxplot()+                     
  theme(legend.position = "none")+ # supprimer la légende (redondante)
  labs(title = "B) Boxplot par sexe")      
```

Pour le code permettant d'ajouter un box plot sur les bords d'un nuage de points (tracés "marginaux"), voir la page [Trucs et Astuces avec ggplot](#ggplot_tips).  





### Diagrammes de violon, de gigue et de sina {.unnumbered}

Ci-dessous se trouve le code pour créer des **graphiques de violon** (`geom_violin`) et des **graphiques de gigues** (`geom_jitter`) pour montrer des distributions. Vous pouvez spécifier que le remplissage ou la couleur est également déterminé par les données, en insérant ces options dans `aes()`. 

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}


# A) Graphe de gigue par groupe
ggplot(data = linelist %>% drop_na(outcome), # supprimer les valeurs manquantes
       mapping = aes(y = age, # Variable continue
           x = outcome, # variable de groupement
           color = outcome))+ # Variable de couleur
  geom_jitter()+ # Créez le graphique en forme de violon
  labs(title = "A) graphique de gigue par sexe")     



# B) Tracé de violon par groupe
ggplot(data = linelist %>% drop_na(outcome), # Suppression des valeurs manquantes
       mapping = aes(y = age, # Variable continue
           x = outcome, # variable de regroupement
           fill = outcome))+ # remplir la variable (couleur)
  geom_violin()+ # crée le graphique en forme de violon
  labs(title = "B) tracé du violon par sexe")    


```

On peut combiner les deux en utilisant la fonction `geom_sina()` du paquet **ggforce**. Le sina plot les points de gigue dans la forme du violin plot. Lorsqu'ils sont superposés sur le tracé du violon (en ajustant les transparences), ils peuvent être plus faciles à interpréter visuellement.  

Le tracé A) à gauche montre une superposition basique de `geom_violin()` et `geom_sina()`. Le tracé B montre un peu plus d'efforts dans l'apparence du ggplot (voir les commentaires en ligne).  

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}

pacman::p_load(ggforce)

# A) Tracé Sina par groupe
ggplot(data = linelist %>% drop_na(outcome), 
       aes(y = age, # variable numérique
           x = outcome)) + # variable de groupe
  geom_violin()+ # création de la base du tracé violine
  geom_sina()+ # ajout de points de gigue en forme de violon
  labs(title = "A) tracé violon et sina par sexe")      


# A) Tracé Sina par groupe
ggplot(
  data = linelist %>% drop_na(outcome), 
  aes(y = age, # variable numérique
      x = outcome)) + # variable de groupe
  geom_violin(
    aes(fill = outcome), # fill (couleur du fond du violon)
    color = "white", # contour blanc
    alpha = 0.2)+ # transparence
  geom_sina(
    size=1, # modification de la taille de la gigue
    aes(color = outcome))+ # couleur (couleur des points)
  scale_fill_manual( # Définir le remplissage pour le fond du violon par death/recover
    values = c( "Death" = "#bf5300", 
              "Recover" = "#11118c")) + 
  scale_color_manual( # Définir les couleurs pour les points par death/recover
    values = c( "Death" = "#bf5300", 
              "Recover" = "#11118c")) + 
  theme_minimal() + # Supprimer le fond gris
  theme(legend.position = "none") + # Suppression de la légende inutile
  labs(title = "B) tracé du violon et du sina par sexe, avec formatage supplémentaire")      



```



<!-- ### Une variable continue dans les facettes {.unnumbered} -->

<!-- **Facettes de base** -->

<!-- Pour examiner d'autres sous-groupes, on peut "facetter" le graphique. Cela signifie que le graphique sera recréé dans les sous-groupes spécifiés. On peut utiliser : -->

<!-- * `facet_wrap()` - ceci recréera les sous-graphes et les présentera par ordre alphabétique (typiquement, sauf indication contraire). Vous pouvez invoquer certaines options pour déterminer l'aspect des facettes, par exemple `nrow=1` ou `ncol=1` pour contrôler le nombre de lignes ou de colonnes dans lesquelles les graphiques à facettes sont disposés. Voir le graphique A ci-dessous.  -->
<!-- * `facet_grid()` - ceci est adapté pour voir des sous-groupes pour des combinaisons particulières de variables discrètes. Voir le graphique B ci-dessous. `nrow` et `ncol` ne sont pas pertinents, car les sous-groupes sont présentés dans une grille, avec les sous-groupes toujours dans l'axe x ou y (voir les notes dans le code ci-dessous) -->.

<!-- Vous pouvez stipuler jusqu'à deux variables de facettes, avec un '~' entre elles. Si une seule variable à facettes, un '.' est utilisé comme placeholder pour une deuxième variable à facettes non utilisée - voir les exemples de code. -->

<!-- ``{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')} -->
<!-- # A) Histogramme des dates d'hospitalisation facettes par hôpital -->
<!-- ggplot(data = linelist %>% -->
<!-- filter(hospital != "Missing"), # le filtre supprime les hôpitaux inconnus -->
<!-- aes(x = date_hospitalisation ))+ -->
<!-- geom_histogram(binwidth=7) + # Bindwidth = 7 jours -->
<!-- labs(title = "A) 2 histogramme des dates d'hospitalisation par hôpital")+ -->
<!-- facet_wrap(hospital~., # Facet by just hospital -->
<!-- ncol = 2) # Facet en deux colonnes -->

<!-- # B) Boxplot de l'âge facetté dans une grille avec deux variables, le sexe et le résultat -->
<!-- ggplot(data = linelist %>% -->
<!-- filter(!is.na(gender) & !is.na(outcome)), # filter retains non-missing gender/outcome -->
<!-- aes(y = âge))+ -->
<!-- geom_boxplot()+ -->
<!-- labs(title = "A) A boxplot by gender and outcome")+ -->
<!-- facet_grid(outcome~gender) # Le résultat est la ligne, le sexe est la colonne -->

<!-- ``` -->


## Deux variables continues  

En suivant une syntaxe similaire, `geom_point()` vous permettra de tracer deux variables continues l'une contre l'autre dans un **graphique en nuage de points**. Ceci est utile pour montrer les valeurs réelles plutôt que leurs distributions.

Un diagramme de dispersion basique de l'âge par rapport au poids est montré dans (A). Dans (B), nous utilisons à nouveau `facet_grid()` pour montrer la relation entre deux variables continues dans la linelist. 

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# Diagramme de dispersion de base du poids et de l'âge
ggplot(data = linelist, 
       mapping = aes(y = wt_kg, x = age))+
  geom_point() +
  labs(title = "A) Diagramme de dispersion du poids et de l'âge")

# Diagramme de dispersion du poids et de l'âge par sexe et résultat Ebola
ggplot(data = linelist %>% drop_na(gender, outcome), # le filtre retient le genre/résultat non manquant
       mapping = aes(y = wt_kg, x = age))+
  geom_point() +
  labs(title = "B) Diagramme de dispersion du poids et de l'âge en fonction du sexe et de l'issue")+
  facet_grid(gender ~ outcome) 

```


### Trois variables continues {.unnumbered}  

Vous pouvez afficher trois variables continues en utilisant l'argument `fill = ` pour créer un *graphique thermique*. La couleur de chaque "cellule" reflétera la valeur de la troisième colonne de données continues. Voir la page sur les [Heat plots] pour plus de détails et plusieurs exemples. 

Il existe des moyens de faire des tracés 3D dans R, mais pour l'épidémiologie appliquée, ceux-ci sont souvent difficiles à interpréter et donc moins utiles pour la prise de décision.  



<!-- ======================================================= -->
## Tracer avec les graphiques de base { }

Nous présentons des méthodes pour tracer en utilisant les graphiques **base** de R au cas où l'utilisateur n'est pas encore familier avec **ggplot2** ou ne l'a pas installé. La syntaxe peut être plus simple, mais les tracés ne sont pas aussi beaux et sont plus difficiles à personnaliser.  

### Une variable continue {.unnumbered}

#### Graphiques en boîte et histogrammes {.unnumbered}

Le paquet graphique intégré **base** de R est livré avec les fonctions `boxplot()` et `hist()`, permettant une visualisation directe d'une variable continue. Ces commandes peuvent être tapées rapidement et sont donc utiles pour un "coup d'œil rapide". Rappelez-vous que vous devez écrire *chaque* cadre de données et colonne dans la syntaxe du "signe du dollar" - `dataframe$column`. L'argument `main = ` est utilisé pour créer un titre.  

```{r, fig.show='hold', out.width=c('50%', '50%')}

# Boxplot
boxplot(linelist$wt_kg, main = "A) Base boxplot") 


# Histogramme
hist(linelist$wt_kg, main = "B) Base histogram") 

```

#### Sous-groupes {.unnumbered}  

Les sous-groupes peuvent être créés avec la syntaxe des formules. Notez que dans le graphique B ci-dessous, ```outcome``` et ```gender``` sont écrits comme ```outcome*gender```, de sorte que les boxplots existent pour les quatre combinaisons de ces deux colonnes. Ils ne sont pas facettés sur différentes lignes et colonnes comme dans **ggplot2**. Nous spécifions `linelist` à `data = ` pour ne pas avoir à utiliser la syntaxe `$` tout le temps.  

```{r, fig.show='hold', out.width=c('50%', '50%')}

# Box plot par sous-groupe
boxplot(age ~ outcome,
                  data = linelist, 
                  main = "A) Base boxplot by subgroup")

# Box plot par sous-groupes croisés
boxplot(age ~ outcome*gender,
                  data = linelist, 
                  main = "B) Base boxplot) by crossed groups")

```

Voici quelques options supplémentaires avec `boxplot()` présentées ci-dessous :  

* Largeur du boxplot proportionnelle à la taille de l'échantillon (A)
* Graphiques en forme de violon, avec une encoche représentant la médiane et un x autour de celle-ci (B)
* Horizontal (C)  


```{r, out.width=c('33%', '33%', '33%'), fig.show='hold'}

# Varier la largeur en fonction de la taille de l'échantillon 
boxplot(linelist$age ~ linelist$outcome,
                  varwidth = TRUE, # Largeur variable selon la taille de l'échantillon
                  main="A) Proportional boxplot() widths")

                  
# Entaille (graphique en forme de violon), et largeur variable
boxplot(age ~ outcome,
        data=linelist,
        notch=TRUE, # encoche à la médiane
        main="B) Encoche boxplot()",
        col=(c("gold", "darkgreen")),
        xlab="Supplément et dose")

# Horizontal
boxplot(age ~ outcome,
        data=linelist,
        horizontal=TRUE, # bascule à l'horizontale
        col=(c("gold", "darkgreen")),
        main="C) Boxplot horizontal()",
        xlab="Supplément et dose")
```

### Deux variables continues {.unnumbered}

En utilisant la base R, nous pouvons rapidement visualiser la relation entre deux variables continues avec la fonction `plot`.


```{r}
plot(linelist$age, linelist$wt_kg)
```


<!-- ======================================================= -->
## Ressources { }

Il existe une énorme quantité d'aide en ligne, notamment avec ggplot. Voir :

*[ggplot2 cheat sheet](http://r-statistics.co/ggplot2-cheatsheet.html)
*[un autre aide-mémoire](https://biostats.w.uib.no/the-ggplot2-cheat-sheet-by-rstudio/)
*[tracer des variables continues](http://www.sthda.com/english/articles/32-r-graphics-essentials/131-plot-two-continuous-variables-scatter-graph-and-alternatives/)  
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/plot_continuous.Rmd-->

# Tracer des données catégorielles { }  


Dans cette page, nous couvrons l'utilisation de base des fonctions **ggplot2** pour visualiser des données discrètes/catégorielles. La fonctionnalité supplémentaire de **ggplot2** par rapport à **base** R signifie que nous la recommandons pour des visualisations prêtes à être présentées. 

Nous abordons également la visualisation des distributions de valeurs catégorielles, comme les comptages et les proportions.


```{r echo=F, fig.height=3, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('100%')}

pacman::p_load(tidyverse)

linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

linelist <- linelist %>% 
  mutate(hospital = fct_relevel(hospital, 
                                c("St. Mark's Maternity Hospital (SMMH)", 
                                  "Port Hospital", 
                                  "Central Hospital",
                                  "Military Hospital",
                                  "Other",
                                  "Missing")))


ggplot(linelist %>% drop_na(outcome)) + 
  geom_bar(aes(x=hospital, fill = outcome)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90)) +
  coord_flip() +
  scale_x_discrete(limits=rev) + 
  scale_fill_manual(values = c("Death"= "#3B1c8C",
                               "Recover" = "#21908D" )) +
  labs(fill = "Résultat", y = "Comptes",x = "Hôpital d'admission") +
  labs(subtitle = "Nombre de cas d'Ebola par hôpital, par résultat")

```


<!-- ======================================================= -->
## Préparation { }

La préparation comprend le chargement des paquets pertinents, à savoir **ggplot2** pour les exemples couverts ici. Nous chargeons également les données.

### Chargement des paquets {.unnumbered}

Ce morceau de code montre le chargement des paquets requis pour les analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* le charge pour l'utiliser. Vous pouvez également charger les paquets installés avec `library()` de **base** R. Voir la page sur [R basics] pour plus d'informations sur les paquets R.  

```{r}
# Chargement des paquets que nous utiliserons à plusieurs reprises
pacman::p_load(
  rio, # importation de fichiers
  here, # fichiers de localisation
  janitor, # tables
  tidyverse) # ggplot2, dplyr, et forcats entre autres 
```


### Importer les données {.unnumbered}

Pour commencer, nous importons la linelist nettoyée des cas d'une épidémie d'Ebola simulée. Si vous voulez suivre, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la linelist "propre"</a> (en tant que fichier .rds). Importez des données avec la fonction `import()` du paquet **rio** (elle gère de nombreux types de fichiers comme .xlsx, .csv, .rds - voir la page [Importer et exporter des données](#import_export) pour plus de détails).  

```{r, echo=F}
# Importez la liste de diffusion dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# Importez la liste de cas
linelist <- import("linelist_cleaned.rds")
```

Les 50 premières lignes de la linelist sont affichées ci-dessous.

```{r, message=FALSE, echo=F}
# affiche les données de la liste de diffusion sous forme de tableau
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Nettoyer les colonnes {.unnumbered}

Pour les exemples de cette section, nous utilisons la linelist Ebola simulée, en nous concentrant sur les variables catégorielles `hospital`, et `outcome`. Celles-ci doivent être de la bonne classe et du bon format. 

Regardons la colonne `hospital`, d'abord avec `class()` de **base** R, et avec `tabyl()` de **janitor**.  

```{r}
# Affiche la classe de la colonne hospital - on peut voir que c'est un caractère
class(linelist$hospital)

# Regardez les valeurs contenues dans la colonne hospital
linelist %>% 
  tabyl(hospital)

```

Nous pouvons voir que les valeurs contenues dans la colonne sont des caractères, car ce sont des noms d'hôpitaux, et par défaut ils sont classés par ordre alphabétique. Il existe des valeurs "autres" et "manquantes", que nous préférerions voir figurer dans les dernières sous-catégories lors de la présentation des ventilations. Nous transformons donc cette colonne en facteur et la réorganisons. Ce point est traité plus en détail dans la section sur la gestion des données [Facteurs].


```{r}
# Convertir en facteur et définir l'ordre des niveaux pour que "Other" et "Missing" soient les derniers.
linelist <- linelist %>% 
  mutate(
    hospital = fct_relevel(hospital, 
      "St. Mark's Maternity Hospital (SMMH)",
      "Port Hospital", 
      "Central Hospital",
      "Military Hospital",
      "Other",
      "Missing"))

```

### Assurer une structure de données correcte {.unnumbered}

Comme nous l'avons vu dans le ["Graphiques en barres"](#ggplot_basics_bars)Pour afficher les fréquences et les distributions des variables catégorielles, vous pouvez créer des diagrammes basés sur : 

* Les données de la linelist, avec une ligne par observation, ou bien. 
* Un tableau récapitulatif basé sur la liste de lignes, avec une ligne par catégorie. Un exemple est donné ci-dessous pour montrer l'utilisation de `dplyr` pour créer un tableau de nombre de cas par hôpital. 

Les tableaux peuvent être créés en utilisant la méthode 'table' pour les graphiques intégrés. L'argument `useNA = "ifany"` permet de s'assurer que les valeurs manquantes sont incluses, car sinon la méthode 'table' les exclut automatiquement. 

```{r}
#Méthode table
  outcome_nbar <- table(linelist$outcome, 
                        useNA = "ifany")

  outcome_nbar # Afficher le tableau complet
```

Ou en utilisant d'autres paquets de gestion de données tels que dplyr. Dans cet exemple, nous ajoutons une colonne de pourcentage.

```{r}
#Méthode Dplyr
  outcome_n <- linelist %>% 
    group_by(outcome) %>% 
    count %>% 
    ungroup() %>% # Dégroupement pour que la proportion soit sur le total
    mutate(proportion = n/sum(n)*100) # Calculer le pourcentage
  
  
   outcome_n #Voir le tableau complet
```

### Filtrer les données {.unnumbered}

Vous pouvez envisager de supprimer les lignes qui ne sont pas nécessaires pour cette analyse. Par exemple, dans les exemples suivants, nous voulons comprendre les tendances parmi les personnes dont l'issue est connue, nous supprimons donc les lignes dont les valeurs de la colonne `outcome` sont manquantes.

```{r}
#Drop missing from full linelist
linelist <- linelist %>% 
  drop_na(outcome)

#Drop missing à partir du tableau dplyr
outcome_n <- outcome_n %>% 
  drop_na(outcome)

```

<!-- ======================================================= -->
## Tracer avec ggplot2 { }

Voir la page [Les bases de ggplot](#ggplot_basics) pour les principes fondamentaux, et la page [Trucs et Astuces avec ggplot](#ggplot_tips) pour les techniques plus avancées.  



### Graphiques à barres utilisant des données brutes {.unnumbered}

Le code ci-dessous utilise `geom_bar` pour créer des diagrammes à barres simples afin de montrer les fréquences des résultats des patients Ebola : A) pour tous les cas, et B) par hôpital.

Dans le crochet `aes`, seul `x` doit être spécifié - ou `y` si vous voulez que les barres soient présentées horizontalement. Ggplot sait que le y (ou x) non spécifié sera le nombre d'observations qui entrent dans ces catégories. 

```{r, out.width=c('50%', '50%'), fig.show='hold'}
# A) Résultats dans tous les cas
ggplot(linelist) + 
  geom_bar(aes(x=outcome)) +
  labs(title = "A) Nombre de cas d'Ebola récupérés et morts")


# B) Résultats de tous les cas par hôpital
ggplot(linelist) + 
  geom_bar(aes(x=outcome, fill = hospital)) +
  theme(axis.text.x = element_text(angle = 90)) + # Ajouter une préférence pour faire pivoter le texte de l'axe x
  labs(title = "B) Nombre de cas d'Ebola récupérés et morts, par hôpital")

```


### Diagrammes à barres utilisant des données traitées {.unnumbered}

Vous trouverez ci-dessous le code utilisant `geom_col` pour créer des diagrammes à barres simples afin de montrer la distribution des résultats des patients Ebola. Avec geom_col, x et y doivent être spécifiés. Ici, x est la variable catégorielle sur l'axe des x, et y est la colonne des proportions générées `proportion`. 

```{r, fig.height = 3, fig.width=4.5}
# Résultats dans tous les cas
ggplot(outcome_n) + 
  geom_col(aes(x=outcome, y = proportion)) +
  labs(subtitle = "Nombre de cas d'Ebola guéris et morts")

```

Pour montrer les ventilations par hôpital, un tableau supplémentaire doit être créé pour les fréquences des catégories combinées `outcome` et `hospital`. 

```{r, fig.height = 4, fig.width=6}
outcome_n2 <- linelist %>% 
  group_by(hospital, outcome) %>% 
  count() %>% 
  group_by(hospital) %>% # Grouper pour que les proportions soient hors du total de l'hôpital
  mutate(proportion = n/sum(n)*100)

head(outcome_n2) #Examen des données
```

Nous créons ensuite le ggplot avec quelques mises en forme supplémentaires :

  * **Retournement de l'axe** : Nous avons inversé les axes avec `coord_flip()` pour pouvoir lire les noms des hôpitaux.
  * **Colonnes côte à côte** : Ajout d'un argument `position = "dodge"` pour que les barres pour la mort et la récupération soient présentées côte à côte plutôt qu'empilées. Notez que les barres empilées sont la valeur par défaut.
  * **Largeur de colonne** : Spécifié 'width', donc les colonnes sont deux fois moins larges que la largeur maximale possible.
  * **Ordre des colonnes** : Inversez l'ordre des catégories sur l'axe des ordonnées pour que 'Autre' et 'Manquant' soient en bas, avec `scale_x_discrete(limits=rev)`. Notez que nous avons utilisé cette méthode plutôt que `scale_y_discrete` parce que l'hôpital est indiqué dans l'argument `x` de `aes()`, même si visuellement il est sur l'axe des ordonnées. Nous faisons cela parce que Ggplot semble présenter les catégories à l'envers, sauf si nous lui disons de ne pas le faire.  
  * **Autres détails** : Étiquettes/titres et couleurs ajoutés dans `labs` et `scale_fill_color` respectivement.
  
```{r, fig.height = 4, fig.width=8}

# Résultats dans tous les cas par hôpital
ggplot(outcome_n2) +  
  geom_col(aes(x=hospital, 
               y = proportion, 
               fill = outcome),
           width = 0.5, # Rendre les barres un peu plus fines (sur 1)
           position = "dodge") + # Les barres sont affichées côte à côte, pas empilées
  scale_x_discrete(limits=rev) + # Inverser l'ordre des catégories
  theme_minimal() + # Thème minimal 
  coord_flip() +
  labs(subtitle = "Nombre de cas Ebola récupérés et morts, par hôpital",
       fill = "Résultat", # Titre de la légende
       x = "Comptes", # Titre de l'axe X
       y = "Hôpital d'admission") + # Titre de l'axe Y
  scale_fill_manual(values = c("Death"= "#3B1c8C",
                               "Recover" = "#21908D" )) 

```



Notez que les proportions sont binaires, nous préférerions donc laisser tomber le terme "récupérer" et ne montrer que la proportion de personnes décédées. Ceci n'est qu'une illustration. 

### Facettes {.unnumbered}  

Nous pouvons également utiliser le facettage pour créer d'autres mini-graphes, ce qui est détaillé avec des exemples dans la section sur la visualisation des données continues. Plus précisément, on peut utiliser :

* `facet_wrap()` - ceci recréera les sous-graphes et les présentera par ordre alphabétique (typiquement, sauf indication contraire). Vous pouvez invoquer certaines options pour déterminer l'aspect des facettes, par exemple `nrow=1` ou `ncol=1` pour contrôler le nombre de lignes ou de colonnes dans lesquelles les graphiques à facettes sont disposés. 
* `facet_grid()` - ceci est adapté pour voir des sous-groupes pour des combinaisons particulières de variables catégorielles. 


<!-- ======================================================= -->
## Tracer avec les graphiques de base { }

**Graphes à barres**

Pour créer des diagrammes en barres dans R, nous créons un tableau de fréquence en utilisant la fonction `table`. Cela crée un objet de la classe table, que R peut reconnaître pour le traçage. Nous pouvons créer un graphique de fréquence simple montrant les résultats des cas Ebola (A), ou ajouter des couleurs pour présenter les résultats par sexe (B).

Notez que les valeurs NA sont exclues de ces graphiques par défaut.

```{r, out.width=c('50%', '50%'), fig.show='hold'}
# A) Résultats dans tous les cas
outcome_nbar <- table(linelist$outcome)
barplot(outcome_nbar, main= "A) Outcomes")

# B) Résultats dans tous les cas par sexe du cas
outcome_nbar2 <- table(linelist$outcome, linelist$gender) # La première colonne est pour les groupements dans une barre, la seconde est pour les barres séparées.
barplot(outcome_nbar2, legend.text=TRUE, main = "B) Outcomes by gender") # Spécifiez l'inclusion de la légende

```




<!-- ======================================================= -->
## Ressources { }

Il existe une énorme quantité d'aide en ligne, en particulier avec ggplot. voir :

* http://r-statistics.co/ggplot2-cheatsheet.html
* https://biostats.w.uib.no/the-ggplot2-cheat-sheet-by-rstudio/
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/plot_discrete.Rmd-->

# Trucs et Astuces avec ggplot {#ggplot_tips}

Dans cette page, nous couvrirons les trucs et astuces pour rendre vos ggplots nets et esthétiques. Voir la page sur les [bases de ggplot](#ggplot_basics) pour les principes de base.

Il existe plusieurs [tutoriels **ggplot2**](https://ggplot2.tidyverse.org/) dont certains listés à la section Ressources. Vous pouvez également télécharger cette [fiche technique sur la visualisation de données avec ggplot](https://rstudio.com/resources/cheatsheets/) sur le site Web de RStudio. Nous vous recommandons vivement aussi de vous inspirer de la [R graph gallery](https://www.r-graph-gallery.com/) et de [Data-to-viz](https://www.data-to-viz.com/caveats.html).

<!-- ======================================================= -->

## Préparation

### Charger les extensions ("packages")  {.unnumbered}

Ce bout de code montre le chargement des "packages" nécessaires aux analyses. Dans ce manuel, nous mettons l'accent sur la fonction `p_load()` du "package" **pacman**, qui installe le (ou une liste de) "package (s)" que si nécessaire (uniquement si le package n'est pas déjà installé) *et* le charge pour l'utiliser . On peut également charger des "packages" avec `library()` à partir de `R` **base**. Voir la page sur [Bases de R](#rbasics) pour plus d'informations sur les "packages" `R`.

```{r}
pacman::p_load(
  tidyverse,      # inclut ggplot2 et d'autres extensions de  data management
  rio,            # importer/exporter
  here,           # localiser des fichiers
  stringr,        #  travailler avec des caracteres     
  scales,         # transformer des valeurs numeriques
  ggrepel,        # bien placer des  étiquettes 
  gghighlight,    # mettre en évidence une partie de l'intrigue
  RColorBrewer    # gammes de couleurs
)
```

### Importer des données {.unnumbered}  

Pour commencer, nous importons le jeu de données des cas d'une épidémie d'Ebola simulée. Si vous voulez suivre en travaillant sur la base, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la version "clean" </a> (en fichier .rds). Importez les données avec la fonction `import()` du "package" **rio** (elle gère de nombreux types de fichiers comme .xlsx, .csv, .rds - voir la page [Importation et exportation](#import_export) pour plus de détails).

```{r,  echo=F}
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

```

```{r, eval=F}
linelist <- rio::import("linelist_cleaned.rds")
```

Les 50 premières lignes de la liste linéaire sont affichées ci-dessous.

```{r, message=FALSE, echo=F}
# afficher le jeu de donnees de la liste lineaire comme une table

DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

<!-- ======================================================= -->

## Palettes de couleur, le remplissage, les axes, etc. {#ggplot_tips_colors}

Nous avons déjà parlé du "mappage" dans la page sur les [Bases de ggplot](#ggplot_basics). Ainsi lorsque le mappage est établi, ie les attributs du graphique (par exemple, la taille, la couleur, la forme, le remplissage, l'axe du tracé) associés à des variables dans les données, on pourrait amener à vouloir modifier  ce mappage pour que notre graphique reflète mieux le message que nous voulons délivrer. Ceci est possible avec les commandes `scales` qui vont permettre des ajustements de l'affichage exact du résultat de nos mappages . Par exemple la couleur reliée à une variable précise sera modifiée ou précisée selon nos besoins avec `scale_color`, la taille (`size`) pourrait être ajustée selon les valeurs minimales et maximales avec `scale_size` etc. Dans cette section, nous développerons l'utilisation de certains `scales` courants.

### Palettes de couleurs

Une chose qui peut être initialement difficile à comprendre avec **ggplot2** est le contrôle des palettes de couleurs. Notez que cette section traite de la couleur des *objets du graphe* ("geoms"/formes) tels que les points, les barres, les lignes, les tuiles, etc. Pour ajuster la couleur des textes accessoires (non reliés aux données) , des titres ou de la couleur de fond, consultez la section [Themes](#ggplot_basics_themes) de la page [ggplot basics](#ggplot_basics).

Pour contrôler la "couleur" des *objets du graphe*, vous devrez ajuster soit l'attribut `color =` (la couleur *extérieure*), soit l'attribut `fill =` (la couleur *intérieure*). Une exception à cette configuration est `geom_point()`, où vous ne pouvez contrôler que `color =`, qui contrôle la couleur du point (intérieur et extérieur).

Lorsque vous définissez la couleur ou le remplissage, vous pouvez soit utiliser des noms de couleurs reconnus par R comme "red" (voir [complete list](http://sape.inf.usi.ch/quick-reference/ggplot2/colour) ou entrer dans `?colors`), ou une couleur hexadécimale spécifique comme `"#ff0505"`.


```{r, warning=F, message=F}
# histogramme - 
ggplot(data = linelist, mapping = aes(x = age))+       # definir donnees et axes
  geom_histogram(              # afficher l'histogramme
    binwidth = 7,                # taille des bins
    color = "red",               # couleur de ligne des bins
    fill = "lightblue")          # couleur interieure des bins (fill) 
```

Comme expliqué dans la page [ggplot basics](#ggplot_basics) sur [comment relier les données aux éléments graphiques](#ggplot_basics_mapping), les attributs graphiques tels que `fill =` et `color =` peuvent être définis soit *à l'extérieur* d'une instruction `mapping = aes()` soit *à l'intérieur* d'une telle instruction. Si vous êtes *en dehors* de `aes()`, la valeur assignée doit être statique (par exemple, `color = "blue"`) et s'appliquera à *toutes* les données tracées par le "geom". Si elle est *à l'intérieur*, l'attribut doit être mise en correspondance avec une variable, comme `color = hospital`, et l'expression variera en fonction de la valeur de cette ligne dans les données. Quelques exemples :


```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
# Couleur statique pour les points et pour la ligne
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     
  geom_point(color = "purple")+
  geom_vline(xintercept = 50, color = "orange")+
  labs(title = "Static color for points and line")

# Couleur mappée sur une variable continue
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     
  geom_point(mapping = aes(color = temp))+         
  labs(title = "Color mapped to continuous column")

# Couleur mappée sur une variable discrète
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     
  geom_point(mapping = aes(color = gender))+         
  labs(title = "Color mapped to discrete column")

# diagramme en barres, remplissage pour variable discrète, couleur pour la valeur statique
ggplot(data = linelist, mapping = aes(x = hospital))+     
  geom_bar(mapping = aes(fill = gender), color = "yellow")+         
  labs(title = "Fill mapped to discrete column, static color")

```

### Scales {#ggplot_tips_scales .unnumbered}

Une fois que vous avez associé une variable à un attribut graphique (par exemple `x =`, `y =`, `fill =`, `color =`...) via un mappage, la manière de l'affichage de celui-ci pourra être modifié par les "scales" qui vont aussi définir  la façon dont la légende correspondante va être affichée. On peut voir ci-dessus comment les "scales" peuvent être continues, discrètes, en format date, etc. en fonction du type/classe de variable assignée. Si vous avez plusieurs attributs affectés à des variables, votre graphique aura plusieurs "scales".

Vous pouvez contrôler les "scqles" avec la fonction `scales_()` appropriée. Les fonctions "scales" de **ggplot()** ont 3 parties qui s'écrivent comme ceci : `scale_AESTHETIC_METHOD()`.

1) La première partie, `scale_()`, est fixe.
2) La deuxième partie ("AESTHETIC"), doit être l'attribut pour lequel vous voulez ajuster l'échelle (`_fill_`, `_shape_`, `_color_`, `_size_`, `_alpha_`...) - les options ici incluent également `_x_` et `_y_`.\i
3) La troisième partie ("METHOD"), sera soit `_discrete()`, `continuous()`, `_date()`, `_gradient()`, ou `_manual()` selon la classe de la variable et *comment* on veut la contrôler. Il en existe d'autres, mais ce sont les plus utilisées.

Assurez-vous que vous utilisez la bonne fonction pour la "scale" ! Sinon, votre commande "scale" n'aura pas l'air de changer quoi que ce soit. Si vous avez plusieurs "scales", vous pouvez utiliser plusieurs fonctions "scale" pour les ajuster ! Par exemple :


### Arguments des "Scales" {.unnumbered}

Chaque type de "scale" a ses propres arguments, bien qu'il y ait quelques chevauchements. Interrogez la fonction comme `?scale_color_discrete` dans la console R pour voir la documentation des arguments de la fonction.

Pour les "scales" continues, utilisez `breaks =` pour fournir une séquence de valeurs avec `seq()` (prenez `to =`, `from =`, et `by =` comme indiqué dans l'exemple ci-dessous. Définissez `expand = c(0,0)` pour éliminer l'espace de remplissage autour des axes (ceci peut être utilisé sur toute "scale" `_x_` ou `_y_`.

Pour les "scales" discrètes, vous pouvez ajuster l'ordre d'apparition des modalités de la variable avec `breaks =`, et la façon dont les valeurs s'affichent avec l'argument `labels =`. Fournissez un vecteur caractère à chacun d'eux (voir l'exemple ci-dessous). Vous pouvez également écarter les valeurs manquantes `NA` facilement en définissant `na.translate = FALSE`.

Les nuances des "scales" au format date sont traitées plus en détail dans la page [Courbes épidémiques](#epicurves).

### Réglages manuels {.unnumbered}

L'une des astuces les plus utiles consiste à utiliser des fonctions "scale" de façon "manuelle" pour assigner explicitement les couleurs comme vous le souhaitez. Ce sont des fonctions avec la syntaxe `scale_xxx_manual()` (par exemple `scale_colour_manual()` ou `scale_fill_manual()`). Chacun des arguments ci-dessous est démontré dans l'exemple de code ci-dessous.

- Attribuer des couleurs aux valeurs de données avec l'argument `values =` argument.\
- Spécifier une couleur pour les données manquantes `NA` avec l'argument `na.value =`\
- Modifier la façon dont les valeurs sont *écrites* dans la légende avec l'argument `labels =`\
- Modifier le titre de la légende avec l'argument `name =`

Ci-dessous, nous créons un graphique à barres et montrons comment il apparaît par défaut, puis avec trois "scales" ajustées - la "scale" continue de l'axe des y, la "scale" discrète de l'axe des x, et l'ajustement manuel du remplissage (couleur intérieure de la barre).


```{r, warning=F, message=F}
# BASELINE - pas d'ajustement de la scale
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = outcome, fill = gender))+
  labs(title = "Baseline - no scale adjustments")

# SCALES AJUSTEES
ggplot(data = linelist)+
  
  geom_bar(mapping = aes(x = outcome, fill = gender), color = "black")+
  
  theme_minimal()+                   # simplifier le background
  
  scale_y_continuous(                # scale continue pour l'axe des y (comptage)
    expand = c(0,0),                 # eviter un graphe trop rembourré
    breaks = seq(from = 0,
                 to = 3000,
                 by = 500))+
  
  scale_x_discrete(                   # scale discrete pour l'axe des x (gender)
    expand = c(0,0),                  # eviter un graphe trop rembourré
    drop = FALSE,                     # afficher toutes les modalités de la variable facteur (même si non utilisée dans la représentation)
    na.translate = FALSE,             # retirer les valeurs NA 
    labels = c("Died", "Recovered"))+ # Changer l'affichage des valeurs
    
  
  scale_fill_manual(                  # Spécifier Manuellement la couleur intérieure des barres
    values = c("m" = "violetred",     # préciser les couleurs que prennent chaque modalité
               "f" = "aquamarine"),
    labels = c("m" = "Male",          #  ré-étiqueter la légende (utiliser l'affectation "=" pour éviter les erreurs)
              "f" = "Female",
              "Missing"),
    name = "Gender",                  # titre de la legende
    na.value = "grey"                 # assigner une couleur aux valeurs manquantes
  )+
  labs(title = "Adjustment of scales") # Préciser le titre

```

### "Scales" d'axes continus  {.unnumbered}

Lorsque les données sont mappées sur les axes du graphique, ceux-ci peuvent également être ajustés à l'aide de commandes "scales'. Un exemple courant est l'ajustement de l'affichage d'un axe (par exemple l'axe des y) qui est mappé à une variable avec des données continues.

Nous pouvons vouloir ajuster la continuité ou l'affichage des valeurs dans le ggplot en utilisant `scale_y_continuous()`. Comme indiqué ci-dessus, utilisez l'argument `breaks =` pour fournir une séquence de valeurs de graduation qui serviront de "ruptures" le long de la "scale". A cet argument, vous pouvez fournir un vecteur `c()` contenant le nombre de graduation souhaitées, ou vous pouvez fournir une séquence régulière de nombres en utilisant la fonction R **base** `seq()`. Cette fonction `seq()` accepte `to =`, `from =`, et `by =`.


```{r, warning=F, message=F, out.width=c('50%', '50%'), fig.show='hold'}
# BASELINE - pas d'ajustement de la scale
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = outcome, fill = gender))+
  labs(title = "Baseline - no scale adjustments")

# 
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = outcome, fill = gender))+
  scale_y_continuous(
    breaks = seq(
      from = 0,
      to = 3000,
      by = 100)
  )+
  labs(title = "Adjusted y-axis breaks")

```

#### Afficher les pourcentages {.unnumbered}

Si les valeurs de vos données originales sont des proportions, vous pouvez facilement les afficher sous forme de pourcentages avec "%" en fournissant `labels = scales::percent` dans votre commande scales, comme montré ci-dessous.

Une alternative serait de convertir les valeurs en caractères et d'ajouter un caractère "%" à la fin, mais cette approche entraînera des complications car vos données ne seront plus des valeurs numériques continues.

```{r, warning=F, message=F, out.width=c('50%', '50%'), fig.show='hold'}
# Axe des y originellement en proportions
#############################
linelist %>%                                   # commencer avec les données d'intérêt: linelist
  group_by(hospital) %>%                       # agréger les données selon les modalités de la variable hopital
  summarise(                                   # créer un résumé par colonnes
    n = n(),                                     # compter le nombre total de lignes 
    deaths = sum(outcome == "Death", na.rm=T),   # compter le nombre de décès par groupe
    prop_death = deaths/n) %>%                   # calculer la proportion de décès par groupe
  ggplot(                                      # tracer le graphique
    mapping = aes(
      x = hospital,
      y = prop_death))+ 
  geom_col()+
  theme_minimal()+
  labs(title = "Display y-axis original proportions")



# Afficher les proportions de l'axe des y avec des pourcentages
########################################
linelist %>%         
  group_by(hospital) %>% 
  summarise(
    n = n(),
    deaths = sum(outcome == "Death", na.rm=T),
    prop_death = deaths/n) %>% 
  ggplot(
    mapping = aes(
      x = hospital,
      y = prop_death))+
  geom_col()+
  theme_minimal()+
  labs(title = "Display y-axis as percents (%)")+
  scale_y_continuous(
    labels = scales::percent                    # afficher les proportions comme des pourcentages
  )

```

#### Échelle logarithmique {.unnumbered}

Pour transformer un axe continu en échelle logarithmique, ajouter `trans = "log2"` à la commande "scale". Pour les besoins de l'exemple, nous créons un jeu de données des régions avec leurs valeurs respectives de `preparedness_index` et de cas cumulés.

```{r}
plot_data <- data.frame(
  region = c("A", "B", "C", "D", "E", "F", "G", "H", "I"),
  preparedness_index = c(8.8, 7.5, 3.4, 3.6, 2.1, 7.9, 7.0, 5.6, 1.0),
  cases_cumulative = c(15, 45, 80, 20, 21, 7, 51, 30, 1442)
)

plot_data
```

Les cas cumulés pour la région "I" sont nettement supérieurs à ceux de toutes les autres régions. Dans de telles circonstances, vous pouvez choisir d'afficher l'axe des y en utilisant une échelle logarithmique afin que le lecteur puisse voir les différences entre les régions ayant moins de cas cumulés.

```{r, warning=F, message=F, out.width=c('50%', '50%'), fig.show='hold'}
# Axe y original
preparedness_plot <- ggplot(data = plot_data,  
       mapping = aes(
         x = preparedness_index,
         y = cases_cumulative))+
  geom_point(size = 2)+            # points pour chaque region 
  geom_text(
    mapping = aes(label = region),
    vjust = 1.5)+                  # ajouter les etiquette
  theme_minimal()

preparedness_plot                  # affichier le graphe originel


# print with y-axis transformed
preparedness_plot+                   # appeler le graphe créé ci-dessus
  scale_y_continuous(trans = "log2") # ajouter la transformation pour l'axe des y
```

### Gradient de couleur {.unnumbered}

Les fonctions "scales" pour un remplissage en gradient de couleur (dégradé) peuvent impliquer des nuances supplémentaires. Les valeurs par défaut sont généralement très agréables, mais vous pouvez souhaiter ajuster les valeurs, les coupures, etc.

Pour illustrer comment ajuster une "scale" de couleur continue, nous utiliserons un ensemble de données de la page [Suivi des contacts](#contact_tracing) qui contient les âges des cas et de leurs cas sources.

```{r, warning=F, message=F}
case_source_relationships <- rio::import(here::here("data", "godata", "relationships_clean.rds")) %>% 
  select(source_age, target_age) 
```

Ci-dessous, nous produisons un diagramme "raster" de la densité des tuiles thermiques. Nous ne détaillerons pas comment (voir le lien dans le paragraphe ci-dessus) mais nous nous concentrerons sur la façon dont nous pouvons ajuster la "scale" de couleurs. Pour en savoir plus sur la fonction `stat_density2d()` **ggplot2** [ici](https://ggplot2.tidyverse.org/reference/geom_density_2d.html). Notez que la "scale" de remplissage est *continue*.

```{r, warn=F, message=F}
trans_matrix <- ggplot(
    data = case_source_relationships,
    mapping = aes(x = source_age, y = target_age))+
  stat_density2d(
    geom = "raster",
    mapping = aes(fill = after_stat(density)),
    contour = FALSE)+
  theme_minimal()
```

Nous allons maintenant montrer quelques variations de la "scale" de remplissage :

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
trans_matrix
trans_matrix + scale_fill_viridis_c(option = "plasma")
```

Maintenant, nous allons montrer quelques exemples d'ajustement du nombre de graduations de l'échelle :

- `scale_fill_gradient()` accepte deux couleurs (haut/bas)\.
- `scale_fill_gradientn()` accepte un vecteur de n'importe quelle longueur de couleurs à `values =` (les valeurs intermédiaires seront interpolées)\
- Utiliser [`scales::rescale()`](https://www.rdocumentation.org/packages/scales/versions/0.4.1/topics/rescale) pour ajuster la façon dont les couleurs sont positionnées le long du gradient ; il redonne à votre vecteur de positions une valeur comprise entre 0 et 1.

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
trans_matrix + 
  scale_fill_gradient(     # gradient à deux côtés
    low = "aquamarine",    # petites valeurs
    high = "purple",       # grandes valeurs
    na.value = "grey",     # valeur des NA
    name = "Density")+     # Titre de la Legende
  labs(title = "Manually specify high/low colors")

# 3+ couleurs à mapper
trans_matrix + 
  scale_fill_gradientn(    # gradient de 3 couleurs (low/mid/high)
    colors = c("blue", "yellow","red") # fournir les couleurs dans un vecteur
  )+
  labs(title = "3-color scale")

# Utiliser rescale() pour ajuster le positionnement des couleurs
trans_matrix + 
  scale_fill_gradientn(    # fournir autant de coleurs que l'on veut
    colors = c("blue", "yellow","red", "black"),
    values = scales::rescale(c(0, 0.05, 0.07, 0.10, 0.15, 0.20, 0.3, 0.5)) # positions des couleurs sont redimensionnées entre 0 and 1
    )+
  labs(title = "Colors not evenly positioned")

# utilisation de limites pour découper les valeurs qui prennent une couleur de remplissage
trans_matrix + 
  scale_fill_gradientn(    
    colors = c("blue", "yellow","red"),
    limits = c(0, 0.0002))+
  labs(title = "Restrict value limits, resulting in grey space")

```

### Palettes {.unnumbered}

#### Colorbrewer et Viridis {.unnumbered}

Plus généralement, si vous voulez des palettes prédéfinies, vous pouvez utiliser les fonctions `scale_xxx_brewer` ou `scale_xxx_viridis_y`.

Les fonctions 'brewer' peuvent fonctionner à partir des palettes [colorbrewer.org](colorbrewer.org).

Les fonctions 'viridis' s'inspirent des palettes viridis (adaptées aux daltoniens !), qui "fournissent des cartes de couleurs qui sont perceptiblement uniformes en couleur et en noir et blanc. Elles sont également conçues pour être perçues par des spectateurs souffrant de formes courantes de daltonisme." (Pour en savoir plus, voir [ici](https://ggplot2.tidyverse.org/reference/scale_viridis.html) et [ici](https://bids.github.io/colormap/)). Préciser si la palette est discrète, continue ou binaire en le spécifiant à la fin de la fonction (par exemple, discrète est `scale_xxx_viridis_d`).

Il est conseillé de tester votre graphe dans ce [simulateur de daltonisme](https://www.color-blindness.com/coblis-color-blindness-simulator/). Si vous avez un schéma de couleurs rouge/vert, essayez plutôt un schéma "chaud-froid" (rouge-bleu) comme décrit [ici](https://www.visualisingdata.com/2019/08/five-ways-to-design-for-red-green-colour-blindness/#:~:text=The%20pink%2Dred%20through%20to,green%20hues%20used%20by%20default.)

Voici un exemple tiré de la page [ggplot basics](#ggplot_basics), utilisant différents schémas de couleurs.

```{r, out.width=c('50%'), fig.show='hold', warning=F, message=F}
symp_plot <- linelist %>%                                         # commencer avec la linelist
  select(c(case_id, fever, chills, cough, aches, vomit)) %>%     # selectionner les colonnes
  pivot_longer(                                                  # pivoter en format long
    cols = -case_id,                                  
    names_to = "symptom_name",
    values_to = "symptom_is_present") %>%
  mutate(                                                        # remplacer les valeurs manquantes
    symptom_is_present = replace_na(symptom_is_present, "unknown")) %>% 
  ggplot(                                                        # commencer le ggplot!
    mapping = aes(x = symptom_name, fill = symptom_is_present))+
  geom_bar(position = "fill", col = "black") +                    
  theme_classic() +
  theme(legend.position = "bottom")+
  labs(
    x = "Symptom",
    y = "Symptom status (proportion)"
  )

symp_plot  #afficher le graphe avec les couleurs par defaut

#################################
# afficher le graphe avec les couleurs specifiées manuellement
symp_plot +
  scale_fill_manual(
    values = c("yes" = "black",         # definir explicitement les couleurs
               "no" = "white",
               "unknown" = "grey"),
    breaks = c("yes", "no", "unknown"), # ordonner les facteurs correctement
    name = ""                           # ne pas afficher de titre pour la légende

  ) 

#################################
# afficher avec les couleurs discretes viridis
symp_plot +
  scale_fill_viridis_d(
    breaks = c("yes", "no", "unknown"),
    name = ""
  )


```

<!-- ======================================================= -->

## Changement de l'ordre des variables discrètes 

Changer l'ordre dans lequel les variables discrètes apparaissent est souvent difficile à comprendre pour les personnes qui ne connaissent pas les graphiques `ggplot2`. Il est cependant facile de comprendre comment faire cela une fois que vous avez compris comment `ggplot2` gère les variables discrètes en intrinsèque. En général, si une variable discrète est utilisée, elle est automatiquement convertie en un type `factor` - qui ordonne les facteurs par ordre alphabétique par défaut. Pour gérer cela, vous devez simplement réorganiser les niveaux de facteurs (modalités) pour refléter l'ordre dans lequel vous souhaitez qu'ils apparaissent dans le graphique. Pour des informations plus détaillées sur la façon de réorganiser les objets `facteur`, voir la section [Variables de type facteur](#factors) du guide.

Nous pouvons voir un exemple commun en utilisant les groupes d'âge - par défaut le groupe d'âge 5-9 sera placé au milieu des groupes d'âge (vu l'ordre alphanumérique), mais nous pouvons le déplacer derrière le groupe d'âge 0-4 du graphique en réordonnant les facteurs.

```{r, , warning=F, message=F}
ggplot(
  data = linelist %>% drop_na(age_cat5),                         # supprimer les lignes où age_cat5 est manquant
  mapping = aes(x = fct_relevel(age_cat5, "5-9", after = 1))) +  # reordonner la var facteur

  geom_bar() +
  
  labs(x = "Age group", y = "Number of hospitalisations",
       title = "Total hospitalisations by age group") +
  
  theme_minimal()


```

#### **ggthemr** {.unnnumbered}

Pensez également à utiliser le "package" (extension) **ggthemr**. Vous pouvez télécharger ce "package" sur Github en suivant les instructions [ici](https://github.com/Mikata-Project/ggthemr). Il propose des palettes très agréables d'un point de vue esthétique, mais sachez que celles-ci ont généralement un nombre maximal de valeurs qui peut être limitatif si vous voulez plus de 7 ou 8 couleurs.

## Courbes de niveau

Les courbes de niveau sont utiles lorsque vous avez de nombreux points qui risquent de se superposer les uns les autres lors de la représentation ("surtraçage"). Elles permettent une différente  visualisation de la répartition des points dans l’espace. Les données de cas utilisées ci-dessus sont à nouveau représentées, mais plus simplement en utilisant `stat_density2d()` et `stat_density2d_filled()` pour produire des niveaux de contour discrets - comme une carte topographique. Pour en savoir plus sur les statistiques, cliquez [ici](https://ggplot2.tidyverse.org/reference/geom_density_2d.html).


```{r, out.width=c('50%'), fig.show='hold', warning=F, message=F}
case_source_relationships %>% 
  ggplot(aes(x = source_age, y = target_age))+
  stat_density2d()+
  geom_point()+
  theme_minimal()+
  labs(title = "stat_density2d() + geom_point()")


case_source_relationships %>% 
  ggplot(aes(x = source_age, y = target_age))+
  stat_density2d_filled()+
  theme_minimal()+
  labs(title = "stat_density2d_filled()")

```

## Distributions marginales

Pour montrer les distributions sur les rebords d'un nuage de points `geom_point()`, vous pouvez utiliser le "package" **ggExtra** et sa fonction `ggMarginal()`. Sauvegardez votre ggplot original comme un objet, puis passez-le à `ggMarginal()` comme indiqué ci-dessous. Voici les arguments clés :

- Vous devez spécifier le `type =` comme étant soit un histogramme ("histogram"), une densité ("density"), un "boxplot", un graphe en violon ("violin"), ou un "densigram").\
- Par défaut, les graphiques marginaux apparaissent pour les deux axes. Vous pouvez définir `margins =` sur "x" ou "y" si vous n'en voulez qu'un seul.\
- Parmi les autres arguments facultatifs, citons `fill =` (couleur de la barre), `color =` (couleur de la ligne), `size =` (taille du tracé par rapport à la taille de la marge, donc plus le nombre est grand, plus le tracé marginal est petit).\
- Vous pouvez fournir d'autres arguments spécifiques aux axes à `xparams =` et `yparams =`. Par exemple, pour avoir des tailles de cases d'histogramme différentes, comme indiqué ci-dessous.

Vous pouvez faire en sorte que les tracés marginaux reflètent les groupes (les variables qui ont été assignées à `color =` dans l'attribut de votre `ggplot()`). Si c'est le cas, définissez l'argument `ggMarginal()` `groupColour =` ou `groupFill =` à `TRUE`, comme indiqué ci-dessous.

Pour en savoir plus, consultez [cette vignette](https://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.html), la [R Graph Gallery](https://www.r-graph-gallery.com/277-marginal-histogram-for-ggplot2.html) ou la documentation de la fonction R `?ggMarginal`.


```{r, message=FALSE, warning=FALSE}
# Installer/charger ggExtra
pacman::p_load(ggExtra)

# Diagramme de dispersion basique du poids et de l'âge
scatter_plot <- ggplot(data = linelist)+
  geom_point(mapping = aes(y = wt_kg, x = age)) +
  labs(title = "Scatter plot of weight and age")
```

Pour ajouter des histogrammes marginaux, utilisez `type = "histogram"`. Vous pouvez éventuellement définir `groupFill = TRUE` pour obtenir des histogrammes empilés.

```{r, message=FALSE, warning=FALSE}
# le graphe d'avant avec les histogrammes de chaque variable présentés sur les rebords
ggMarginal(
  scatter_plot,                     # ajouter les histogrammes marginaux
  type = "histogram",               # specifier qu'on veut un  histogramme
  fill = "lightblue",               # couleur intérieure des barres de l'histogramme
  xparams = list(binwidth = 10),    # autres parametres pour l'axe des x
  yparams = list(binwidth = 5))     # autres parametres pour l'axe des y
```

Graphique de densité marginale avec valeurs groupées/colorées :

```{r, message=FALSE, warning=FALSE}

# Scatter plot, coloriée selon la variable d'interet (le sexe)
# la variable d'interet est assignee à l'argument "color" dans ggplot. groupFill dans ggMarginal est fixée à TRUE
scatter_plot_color <- ggplot(data = linelist %>% drop_na(gender))+
  geom_point(mapping = aes(y = wt_kg, x = age, color = gender)) +
  labs(title = "Scatter plot of weight and age")+
  theme(legend.position = "bottom")

ggMarginal(scatter_plot_color, type = "density", groupFill = TRUE)
```

Définissez l'argument `size =` pour ajuster la taille relative du graphe marginal. Plus le nombre est petit, plus le graphe marginal est grand. Vous pouvez également définir `color =` au besoin. 

Ci-dessous se trouve un boxplot marginal, avec une démonstration de l'utilisation de l'argument `margins =` pour avoir le graphique marginal que sur un seul axe :

```{r, message=FALSE, warning=FALSE}
# avec boxplot 
ggMarginal(
  scatter_plot,
  margins = "x",      # afficher un graphe marginal uniquement sur l'axe x
  type = "boxplot")   
```

<!-- ======================================================= -->

## Étiquetage pratique/intelligent

Dans **ggplot2**, il est également possible d'ajouter du texte aux graphiques. Cependant, cela s'accompagne d'une limitation notable : les étiquettes de texte entrent souvent en conflit avec les points de données dans un graphique, ce qui les rend désordonnées ou difficiles à lire. Il n'y a pas de moyen idéal de gérer ce problème dans le "package" de base, mais il existe un module complémentaire **ggplot2**, connu sous le nom de **ggrepel**, qui rend la gestion de ce problème très simple !

Le "package" **ggrepel** fournit deux nouvelles fonctions, `geom_label_repel()` et `geom_text_repel()`, qui remplacent `geom_label()` et `geom_text()`. Utilisez simplement ces fonctions à la place des fonctions de base pour produire des étiquettes soignées. Dans la fonction, mappez l'attribut graphique `aes()` comme toujours, mais incluez l'argument `label =` auquel vous fournissez un nom de variable contenant les valeurs que vous voulez afficher (par exemple l'id du patient, ou le nom, etc.). Vous pouvez créer des étiquettes plus complexes en combinant des variables et des retours à la ligne (`\n`) dans `str_glue()` comme indiqué ci-dessous.

<span style="color: darkgreen;">**_TIP:_** Quelques conseils</span>

- Utilisez `min.segment.length = 0` pour toujours dessiner des segments de ligne, ou `min.segment.length = Inf` pour ne jamais les dessiner.\

- Utilisez `size =` en dehors de `aes()` pour définir la taille du texte.\

- Utilisez `force =` pour modifier le degré de répulsion entre les étiquettes et leurs points respectifs (la valeur par défaut est 1).\

- Incluez `fill =` dans `aes()` pour que l'étiquette soit colorée par la valeur.

    - Une lettre "a" peut apparaître dans la légende - ajoutez `guides(fill = guide_legend(override.aes = aes(color = NA)))` pour la supprimer

Pour en savoir plus, consultez ce [tutoriel très détaillé](https://ggrepel.slowkow.com/articles/examples.html).

```{r, , warning=F, message=F}
pacman::p_load(ggrepel)

linelist %>%                                               # commencer avec les données d'intérêt linelist
  group_by(hospital) %>%                                   # agréger les données par les différentes modalités de la variable hopital
  summarise(                                               # créer une nouvelles base avec les données résumées par hopital
    n_cases = n(),                                           # nombre de cas pq hospital
    delay_mean = round(mean(days_onset_hosp, na.rm=T),1),    # délai moyen par hopital
  ) %>% 
  ggplot(mapping = aes(x = n_cases, y = delay_mean))+      # envoyer la base modifiée dans la fonction ggplot
  geom_point(size = 2)+                                    # ajouter les points
  geom_label_repel(                                        # ajouter les étiquettes des points 
    mapping = aes(
      label = stringr::str_glue(
        "{hospital}\n{n_cases} cases, {delay_mean} days")  # comment les étiquettes vont apparaître
      ), 
    size = 3,                                              # taille du texte pour les étiquettes
    min.segment.length = 0)+                               # afficher  tous les segments de ligne              
  labs(                                                    # ajouter des étiquettes aux axes 
    title = "Mean delay to admission, by hospital",
    x = "Number of cases",
    y = "Mean delay (days)")
```

Vous pouvez étiqueter seulement un sous-ensemble de points de données - en utilisant la syntaxe standard `ggplot()` pour fournir différentes `data =` pour chaque couche `geom` du graphique. Ci-dessous, tous les cas sont représentés, mais seulement quelques-uns sont étiquetés.

```{r, warning=F, message=FALSE}

ggplot()+
  # Tous les points en gris
  geom_point(
    data = linelist,                                   # la base complète fournie à ggplot
    mapping = aes(x = ht_cm, y = wt_kg),
    color = "grey",
    alpha = 0.5)+                                              # gris et semi-transparent
  
  # Quelques points en noir
  geom_point(
    data = linelist %>% filter(days_onset_hosp > 15),  # filtrer les données à représenter
    mapping = aes(x = ht_cm, y = wt_kg),
    alpha = 1)+                                                # couleur par défaut (noir) et non transparente
  
  # point labels for few points
  geom_label_repel(
    data = linelist %>% filter(days_onset_hosp > 15),  # filtrer les données pour les étiquettes à afficher
    mapping = aes(
      x = ht_cm,
      y = wt_kg,
      fill = outcome,                                          # couleurs des boites d'étiquettes selon outcome
      label = stringr::str_glue("Delay: {days_onset_hosp}d")), # étiquette créée avec str_glue()
    min.segment.length = 0) +                                  # afficher  tous les segments de ligne 
  
  # supprimer lettre "a" de l'intérieur des boites
  guides(fill = guide_legend(override.aes = aes(color = NA)))+
  
  # étiquettes des axes
  labs(
    title = "Cases with long delay to admission",
    y = "weight (kg)",
    x = "height(cm)")
```

<!-- ======================================================= -->

## Axes de temps

Travailler avec des axes temporels dans ggplot peut sembler intimidant, mais est très facile grâce à quelques fonctions clés. Rappelez-vous que lorsque vous travaillez avec le temps ou la date, vous devez vous assurer que les variables correctes sont formatées en tant que classe de date ou de "datetime" - voir la page [Travailler avec les dates](#dates) pour plus d'informations à ce sujet, ou la page [Courbes épidémiques](#epicurves) (section ggplot) pour des exemples.

L'ensemble de fonctions les plus utiles pour travailler avec des dates dans `ggplot2` sont les fonctions d'échelle (`scale_x_date()`, `scale_x_datetime()`), et leurs fonctions d'axe des ordonnées. Ces fonctions vous permettent de définir la fréquence des étiquettes d'axe et le format des étiquettes d'axe. Pour savoir comment formater les dates, consultez à nouveau la section [*travailler avec les dates*](#dates) ! Vous pouvez utiliser les arguments `date_breaks` et `date_labels` pour spécifier l'apparence des dates :

1.  `date_breaks` vous permet de spécifier la fréquence des ruptures d'axe (le nombre de graduations) - vous pouvez passer une chaîne ici (par exemple `"3 months"`, ou "`2 days"`)

2.  `date_labels` vous permet de définir le format dans lequel les dates sont affichées. Vous pouvez passer une chaîne de format de date à ces arguments (par exemple, `"%b-%d-%Y"`) :


```{r, , warning=F, message=F}
# faire une courbe épi en fonction de la date d'apparition des symptômes, lorsque cela est disponible
ggplot(linelist, aes(x = date_onset)) +
  geom_histogram(binwidth = 7) +
  scale_x_date(
    # graduation par mois
    date_breaks = "1 months",
    # les étiquettes vont afficher le mois puis le jour
    date_labels = "%b %d"
  ) +
  theme_classic()

```

Une solution facile pour obtenir des étiquettes de date efficaces sur l'axe des x est d'assigner l'argument `labels =` dans `scale_x_date()` à la fonction `label_date_short()` du "package" **scales**. Cette fonction construira automatiquement des étiquettes de date efficaces (pour en savoir plus, cliquez [ici](https://scales.r-lib.org/reference/label_date.html)). Un avantage supplémentaire de cette fonction est que les étiquettes s'adapteront automatiquement à l'évolution de vos données dans le temps, des jours aux semaines, aux mois et aux années.

Vous trouverez un exemple complet dans la section de la page [Courbes épidémiques](#epicurves) sur les étiquettes de date à plusieurs niveaux, mais un exemple rapide est présenté ci-dessous pour référence :

```{r, warning=F, message=F}

ggplot(linelist, aes(x = date_onset)) +
  geom_histogram(binwidth = 7) +
  scale_x_date(
    labels = scales::label_date_short()  # étiquettes de date plus pratiques d'un coup
  )+
  theme_classic()

```


<!-- ======================================================= -->

## Mise en évidence

La mise en évidence d'éléments spécifiques dans un graphique est un moyen utile d'attirer l'attention sur une instance spécifique d'une variable tout en fournissant des informations sur la dispersion de l'ensemble des données. Bien que cela ne soit pas facile à faire dans la base **ggplot2**, il existe un "package" externe qui peut aider à le faire, connu sous le nom de **gghighlight**. Il est facile à utiliser dans la syntaxe ggplot.

Le "package" **gghighlight** utilise la fonction `gghighlight()` pour obtenir cet effet. Pour utiliser cette fonction, fournissez une déclaration logique à la fonction - cela peut avoir des résultats assez flexibles, mais ici nous allons montrer un exemple de la distribution de l'âge des cas dans notre liste linéaire, en les mettant en évidence par résultat.


```{r, , warning=F, message=F}
# charger gghighlight
library(gghighlight)

# remplacer les valeurs NA avec "unknown" dans la variable "outcome"
linelist <- linelist %>%
  mutate(outcome = replace_na(outcome, "Unknown"))

# produire un histogramme de tous les cas par age
ggplot(
  data = linelist,
  mapping = aes(x = age_years, fill = outcome)) +
  geom_histogram() + 
  gghighlight::gghighlight(outcome == "Death")     # mettre en évidence les cas ou le patient est décédé

```

Cela fonctionne aussi très bien avec les fonctions de "facet" - cela permet à l'utilisateur de produire des graphiques répétitifs (selon les modalités la variable d'intérêt) mais cette fois au lieu que chaque sous-graphe concerne une modalité particulière de la variable sur laquelle le "faceting" est fait, tout sera représenté dans chaque sous-graphe mais avec les données de la modalité d'intérêt qui seront mises en évidence avec une couleur spécifique! Ci-dessous, nous comptons les cas par semaine et traçons les courbes épidémiques par hôpital (`color =` et `facet_wrap()` réglé sur la colonne `hospital`).

```{r, , warning=F, message=F}

# produire un histogramme de tous les cas par age
linelist %>% 
  count(week = lubridate::floor_date(date_hospitalisation, "week"),
        hospital) %>% 
  ggplot()+
  geom_line(aes(x = week, y = n, color = hospital))+
  theme_minimal()+
  gghighlight::gghighlight() +                      # mettre en évidence les cas ou le patient est décédé
  facet_wrap(~hospital)                              # créer les "facets" par outcome

```

## Représenter différents/multiples jeux de données 

Notez qu'il peut être difficile d'aligner correctement les axes pour tracer les données de plusieurs ensembles de données différents dans le même graphique. Considérez l'une des stratégies suivantes :

- Fusionnez les données avant de les représenter, et convertissez-les au format "long" avec une colonne reflétant l'ensemble de données.
- Utilisez **cowplot** ou un logiciel similaire pour combiner deux graphiques (voir ci-dessous).

<!-- ======================================================= -->

## Combiner des graphiques

Deux "packages" très utiles pour combiner des graphiques sont **cowplot** et **patchwork**. Dans cette page, nous nous concentrerons principalement sur **cowplot**, avec une utilisation occasionnelle de **patchwork**.

Voici l'[introduction au cowplot](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html) en ligne. Vous pouvez lire la documentation plus complète de chaque fonction  [ici](https://www.rdocumentation.org/packages/cowplot/versions/1.1.1). Nous couvrirons ci-dessous quelques-uns des cas d'utilisation et des fonctions les plus courantes.

Le "package" **cowplot** fonctionne en tandem avec **ggplot2** - essentiellement, vous l'utilisez pour arranger et combiner les ggplots et leurs légendes en figures composées. Il peut également accepter les graphiques R **base**.

```{r}
pacman::p_load( #charger les packages dont on aura besoin
  tidyverse,      # pour manipuler et visualiser des données
  cowplot,        # pour combiner des graphes
  patchwork       # pour combiner des graphes
)
```

Bien que le "faceting" (décrit dans la page [bases de ggplot](#ggplot_basics)) soit une approche pratique de la représentation graphique, il est parfois impossible d'obtenir les résultats souhaités avec son approche relativement restrictive. Dans ce cas, vous pouvez choisir de combiner des graphiques en les collant ensemble dans un graphique plus grand. Il y a trois packages bien connus qui sont parfaits pour cela - **cowplot**, **gridExtra**, et **patchwork**. Cependant, ces package font largement les mêmes choses, donc nous nous concentrerons sur **cowplot** pour cette section.

### `plot_grid()` {.unnumbered}

Le package **cowplot** a une gamme assez large de fonctions, mais l'utilisation la plus simple peut être réalisée par l'utilisation de `plot_grid()`. Il s'agit en fait d'un moyen d'arranger des graphiques prédéfinis dans une formation en grille. Nous pouvons travailler sur un autre exemple avec le jeu de données sur le paludisme - ici, nous pouvons représenter le nombre total de cas par district, et également montrer la courbe épidémique dans le temps.

```{r, , warning=F, message=F}
malaria_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) 

# diagramme en barres du nombre total de cas par district
p1 <- ggplot(malaria_data, aes(x = District, y = malaria_tot)) +
  geom_bar(stat = "identity") +
  labs(
    x = "District",
    y = "Total number of cases",
    title = "Total malaria cases by district"
  ) +
  theme_minimal()

# courbe epidemique en fonction du temps
p2 <- ggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +
  geom_col(width = 1) +
  labs(
    x = "Date of data submission",
    y =  "number of cases"
  ) +
  theme_minimal()

cowplot::plot_grid(p1, p2,
                  # 1 colonne et deux lignes - empilées l'une sur l'autre
                   ncol = 1,
                   nrow = 2,
                   # le graphe du haut fait 2/3 de la hauteur du second
                   rel_heights = c(2, 3))


```

### Combiner les légendes {.unnumbered}

Si vos graphiques ont la même légende, il est relativement simple de les combiner. Utilisez simplement l'approche **cowplot** ci-dessus pour combiner les graphiques, mais supprimez la légende de l'une d'entre elles (pour éviter la dé-duplication).

Si vos graphiques ont des légendes différentes, vous devez utiliser une autre approche :

1) Créez et enregistrez vos graphiques *sans légendes* en utilisant `theme(legend.position = "none")`.
2) Extrayez les légendes de chaque graphe en utilisant `get_legend()` comme indiqué ci-dessous - *mais extrayez les légendes des graphes modifiés pour afficher réellement la légende*\.
3) Combinez les légendes dans un panneau de légendes.
4) Combinez les graphes et le panneau de légendes.

Pour la démonstration, nous montrons les deux graphiques séparément, puis disposés dans une grille avec leurs propres légendes (utilisation laide et inefficace de l'espace) :

```{r, out.width=c('50%'), fig.show='hold', warning=F, message=F}
p1 <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, outcome) %>% 
  ggplot()+
  geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+
  scale_fill_brewer(type = "qual", palette = 4, na.value = "grey")+
  coord_flip()+
  theme_minimal()+
  labs(title = "Cases by outcome")


p2 <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, age_cat) %>% 
  ggplot()+
  geom_col(mapping = aes(x = hospital, y = n, fill = age_cat))+
  scale_fill_brewer(type = "qual", palette = 1, na.value = "grey")+
  coord_flip()+
  theme_minimal()+
  theme(axis.text.y = element_blank())+
  labs(title = "Cases by age")

```

Voici à quoi ressemblent les deux graphiques lorsqu'ils sont combinés en utilisant `plot_grid()` sans combiner leurs légendes :

```{r, warning=F, message=F}
cowplot::plot_grid(p1, p2, rel_widths = c(0.3))
```

Et maintenant, nous montrons comment combiner les légendes. Essentiellement, ce que nous faisons est de définir chaque graphique *sans* sa légende (`theme(legend.position = "none"`), et ensuite nous définissons la légende de chaque graphe *séparément*, en utilisant la fonction `get_legend()` de **cowplot**. Lorsque nous extrayons la légende du graphe sauvegardé, nous devons ajouter `+` la légende à nouveau, y compris en spécifiant le placement (`"right"`) et des ajustements plus petits pour l'alignement des légendes et de leurs titres. Ensuite, nous combinons les légendes ensemble verticalement, puis nous combinons les deux graphes avec les légendes nouvellement combinées. Voilà !

```{r, warning=F, message=F}

# Définir graphe 1 sans legende
p1 <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, outcome) %>% 
  ggplot()+
  geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+
  scale_fill_brewer(type = "qual", palette = 4, na.value = "grey")+
  coord_flip()+
  theme_minimal()+
  theme(legend.position = "none")+
  labs(title = "Cases by outcome")


# Définir graphe 2 sans legende
p2 <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, age_cat) %>% 
  ggplot()+
  geom_col(mapping = aes(x = hospital, y = n, fill = age_cat))+
  scale_fill_brewer(type = "qual", palette = 1, na.value = "grey")+
  coord_flip()+
  theme_minimal()+
  theme(
    legend.position = "none",
    axis.text.y = element_blank(),
    axis.title.y = element_blank()
  )+
  labs(title = "Cases by age")


# extraire légende de p1 (de p1 + legend)
leg_p1 <- cowplot::get_legend(p1 +
                                theme(legend.position = "right",        # extraire légende verticale
                                      legend.justification = c(0,0.5))+ # pour bien  aligner la légende
                                labs(fill = "Outcome"))                 # titre de la légende
# extraire légende de p2 (de p2 + legend)
leg_p2 <- cowplot::get_legend(p2 + 
                                theme(legend.position = "right",         # extraire légende verticale   
                                      legend.justification = c(0,0.5))+  # pour bien  aligner la légende
                                labs(fill = "Age Category"))             # titre de la légende

# créer un tracé vierge pour l'alignement de la légende
#blank_p <- patchwork::plot_spacer() + theme_void()

# créer un panneau de légendes, qui peut être superposé (ou utiliser l'espaceur commenté ci-dessus)
legends <- cowplot::plot_grid(leg_p1, leg_p2, nrow = 2, rel_heights = c(.3, .7))

# combiner les deux graphiques et le panneau de légendes combiné
combined <- cowplot::plot_grid(p1, p2, legends, ncol = 3, rel_widths = c(.4, .4, .2))

combined  # afficher ce qui est enregistré sous "combined"


```

Cette solution a été tirée de [cette pubication](https://stackoverflow.com/questions/52060601/ggplot-multiple-legends-arrangement) avec une correction mineure pour aligner les légendes inspiré de [cette pubication](https://github.com/wilkelab/cowplot/issues/33).

[***_NOTE:_*** Note amusante - le "cow" dans **cowplot** vient du nom du créateur - Claus O. Wilke.]{style="color : black;"}

### Encastrer des graphiques {.unnumbered}

Vous pouvez encastrer un graphique dans un autre en utilisant **cowplot**. Voici les points à prendre en compte :

- Définissez le graphique principal avec `theme_half_open()` de **cowplot** ; il peut être préférable d'avoir la légende en haut ou en bas.
- Définissez le graphique à encastrer. Le mieux est d'avoir un graphe où vous n'avez pas besoin de légende. Vous pouvez supprimer les éléments du thème du graphe avec `element_blank()` comme indiqué ci-dessous.
- Combinez-les en appliquant `ggdraw()` au graphe principal, puis en ajoutant `draw_plot()` au graphe à encastrer et en spécifiant les coordonnées (x et y du coin inférieur gauche), la hauteur et la largeur en tant que proportion par rapport au graphe principal.

```{r, out.width=c('100%'), fig.show='hold', warning=F, message=F}

# Définir graphe principal
main_plot <- ggplot(data = linelist)+
  geom_histogram(aes(x = date_onset, fill = hospital))+
  scale_fill_brewer(type = "qual", palette = 1, na.value = "grey")+ 
  theme_half_open()+
  theme(legend.position = "bottom")+
  labs(title = "Epidemic curve and outcomes by hospital")


# Définir graphe à encastrer
inset_plot <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, outcome) %>% 
  ggplot()+
    geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+
    scale_fill_brewer(type = "qual", palette = 4, na.value = "grey")+
    coord_flip()+
    theme_minimal()+
    theme(legend.position = "none",
          axis.title.y = element_blank())+
    labs(title = "Cases by outcome") 


# Combiner graphe principal avec celui à encastrer
cowplot::ggdraw(main_plot)+
     draw_plot(inset_plot,
               x = .6, y = .55,    #x = .07, y = .65,
               width = .4, height = .4)

```

Cette technique est expliquée plus en détail dans ces deux vignettes:

[Wilke lab](https://wilkelab.org/cowplot/articles/drawing_with_on_plots.html)\
[Documentation draw_plot()](https://www.rdocumentation.org/packages/cowplot/versions/1.1.1/topics/draw_plot)

<!-- ======================================================= -->

## Axes doubles

Un axe y secondaire est souvent un ajout demandé à un graphique `ggplot2`. Bien que la validité de tels graphiques fasse l'objet d'un débat animé au sein de la communauté de la visualisation de données, et qu'ils ne sont souvent pas recommandés, il se peut que vous devriez y avoir recours. Nous présentons ci-dessous une méthode pour y parvenir : l'utilisation du package **cowplot** pour combiner deux graphiques séparés.

Cette approche implique la création de deux graphiques distincts - l'un avec un axe y sur la gauche, et l'autre avec un axe y sur la droite. Tous deux utiliseront un `theme_cowplot()` spécifique et doivent avoir le même axe des x. Ensuite, dans une troisième commande, les deux graphiques sont alignés et superposés l'un sur l'autre. Les fonctionnalités de **cowplot**, dont celle-ci n'est qu'une partie, sont décrites en profondeur sur ce [site](https://wilkelab.org/cowplot/articles/aligning_plots.html).

Pour démontrer cette technique, nous allons superposer la courbe épidémique avec une ligne représentant le pourcentage hebdomadaire de patients décédés. Nous utilisons cet exemple parce que l'alignement des dates sur l'axe des x est plus complexe que, par exemple, l'alignement d'un graphique à barres avec un autre graphique. Quelques points à noter :

- L'épicurve et la ligne sont agrégées en semaines avant d'être tracées *et* les `date_breaks` et les `date_labels` sont identiques - nous faisons cela pour que les axes x des deux graphiques soient les mêmes lorsqu'ils sont superposés.
- L'axe des y est déplacé vers la droite pour le graphique 2 avec l'argument `position =` de `scale_y_continuous()`.
- Les deux graphiques utilisent `theme_cowplot()`.

Notez qu'il existe un autre exemple de cette technique sur la page [Courbes épidémiques](#epicurves) - superposition de l'incidence cumulée sur l'épicurve.

**Tracer le graphique 1**\
Ceci reste essentiellement une épicurve. Nous utilisons `geom_area()` juste pour démontrer son utilisation (aire sous une ligne, par défaut).

```{r, warning=F, message=F}
pacman::p_load(cowplot)            # charger/installer cowplot au besoin

p1 <- linelist %>%                 # sauvegarder le graphe comme un objet nommé p1
     count(
       epiweek = lubridate::floor_date(date_onset, "week")) %>% 
     ggplot()+
          geom_area(aes(x = epiweek, y = n), fill = "grey")+
          scale_x_date(
               date_breaks = "month",
               date_labels = "%b")+
     theme_cowplot()+
     labs(
       y = "Weekly cases"
     )

p1                                      # afficher p1
```

**Tracer le graphique 2**\
Créez le deuxième graphique qui montre une ligne du pourcentage hebdomadaire de décès.

```{r, warning=F, message=F}

p2 <- linelist %>%         # sauvegarder le graphe comme un objet nommé p2
     group_by(
       epiweek = lubridate::floor_date(date_onset, "week")) %>% 
     summarise(
       n = n(),
       pct_death = 100*sum(outcome == "Death", na.rm=T) / n) %>% 
     ggplot(aes(x = epiweek, y = pct_death))+
          geom_line()+
          scale_x_date(
               date_breaks = "month",
               date_labels = "%b")+
          scale_y_continuous(
               position = "right")+
          theme_cowplot()+
          labs(
            x = "Epiweek of symptom onset",
            y = "Weekly percent of deaths",
            title = "Weekly case incidence and percent deaths"
          )

p2     # afficher p2
```

Maintenant, nous alignons le graphique en utilisant la fonction `align_plots()`, en spécifiant l'alignement horizontal et vertical ("hv", qui peut aussi être "h", "v", "none"). Nous spécifions également l'alignement de tous les axes (haut, bas, gauche et droite) avec "tblr". Il en résulte un objet de classe liste (avec 2 éléments).

Ensuite, nous dessinons les deux graphiques ensemble en utilisant `ggdraw()` (de **cowplot**) et en référençant les deux parties de l'objet `aligned_plots`.

```{r, warning=F, message=F}
aligned_plots <- cowplot::align_plots(p1, p2, align="hv", axis="tblr")         # aligner les deux graphes et les sauvegarder en tant que liste
aligned_plotted <- ggdraw(aligned_plots[[1]]) + draw_plot(aligned_plots[[2]])  # les superposer et sauvegarder le  visuel résultant
aligned_plotted                                                                # afficher les graphiques superposés

```

<!-- ### Statistical transformation {.unnumbered}   -->

<!-- Another way to do this to have the second axis be a direct transformation of the secondary axis.  -->

<!-- Differences in axis values will be purely cosmetic - if you want to show two different variables on one graph, with different y-axis scales for each variable, this will not work without some work behind the scenes. To obtain this effect, you will have to transform one of your variables in the data, and apply the same transformation *in reverse* when specifying the axis labels. Based on this, you can either specify the transformation explicitly (e.g. variable a is around 10x as large as variable b) or calculate it in the code (e.g. what is the ratio between the maximum values of each dataset). -->

<!-- The syntax for adding a secondary axis is very straightforward! When calling a `scale_xxx_xxx()` function (e.g. `scale_y_continuous()`), use the `sec.axis` argument to call the `sec_axis()` function. The `trans` argument in this function allows you to specify the label transformation for the axis - provide this in standard tidyverse syntax.  -->

<!-- For example, if we want to show the number of positive RDTs in the malaria dataset for facility 1, showing 0-4 year olds and all cases on chart: -->

<!-- ```{r, , warning=F, message=F} -->

<!-- # take malaria data from facility 1 -->

<!-- malaria_facility_1 <- malaria_data %>% -->

<!--   filter(location_name == "Facility 1") -->

<!-- # calculate the ratio between malaria_rdt_0-4 and malaria_tot  -->

<!-- tf_ratio <- max(malaria_facility_1$malaria_tot, na.rm = T) / max(malaria_facility_1$`malaria_rdt_0-4`, na.rm = T) -->

<!-- # transform the values in the dataset -->

<!-- malaria_facility_1 <- malaria_facility_1 %>% -->

<!--   mutate(malaria_rdt_0_4_tf = `malaria_rdt_0-4` * tf_ratio) -->

<!-- # plot the graph with dual axes -->

<!-- ggplot(malaria_facility_1, aes(x = data_date)) + -->

<!--   geom_line(aes(y = malaria_tot, col = "Total cases")) + -->

<!--   geom_line(aes(y = malaria_rdt_0_4_tf, col = "Cases: 0-4 years old")) + -->

<!--   scale_y_continuous( -->

<!--     name = "Total cases", -->

<!--     sec.axis = sec_axis(trans = ~ . / tf_ratio, name = "Cases: 0-4 years old") -->

<!--   ) + -->

<!--   labs(x = "date of data collection") + -->

<!--   theme_minimal() + -->

<!--   theme(legend.title = element_blank()) -->

<!-- ``` -->

<!-- ## Sparklines   -->

<!-- UNDER CONSTRUCTION   -->

<!-- (perhaps move to Tables for presentation page) -->

## Packages pour vous aider

Il existe quelques packages R très intéressants, spécialement conçus pour vous aider à naviguer dans **ggplot2** :

### Faire du **ggplot2** via clique-boutton avec **equisse** {.unnumbered}

**equisse** fournit une interface graphique pour la construction de graphiques avec **ggplot2**. "Cet addin vous permet d'explorer interactivement vos données en les visualisant avec le package ggplot2. Il vous permet de dessiner des diagrammes en barres, des courbes, des diagrammes de dispersion, des histogrammes, des boxplot et des objets sf, puis d'exporter le graphique ou de récupérer le code pour reproduire le graphique."

Installez puis lancez l'addin via le menu RStudio ou avec `esquisse::esquisser()`.

Voir la [page Github](https://github.com/dreamRs/esquisse)

[Documentation](https://dreamrs.github.io/esquisse/index.html)


## Divers

### Affichage numérique {.unnumbered}

Vous pouvez désactiver la notation scientifique en exécutant cette commande avant le graphique.

```{r, eval=F}
options(scipen=999)
```

Ou appliquez `number_format()` du package **scales** à une valeur ou une colonne spécifique, comme indiqué ci-dessous.

Utilisez les fonctions du package **scales** pour ajuster facilement l'affichage des nombres. Ces fonctions peuvent être appliquées aux variables de votre jeu de données, mais sont présentées sur des nombres individuels pour les besoins de l'exemple.

```{r}
scales::number(6.2e5)
scales::number(1506800.62,  accuracy = 0.1,)
scales::comma(1506800.62, accuracy = 0.01)
scales::comma(1506800.62, accuracy = 0.01,  big.mark = "." , decimal.mark = ",")
scales::percent(0.1)
scales::dollar(56)
scales::scientific(100000)
```

## Ressources

Inspiration [ggplot graph gallery](https://www.tidyverse.org/blog/2018/07/ggplot2-3-0-0/)

[Guide pour la présentation des données de surveillance](https://ecdc.europa.eu/sites/portal/files/documents/Guidelines%20for%20presentation%20of%20surveillance%20data-final-with-cover-for-we....pdf)

[Facets et étiquettes](http://www.cookbook-r.com/Graphs/Facets_(ggplot2)/#modifying-facet-label-text) [Fonction d'étiquetage](https://ggplot2.tidyverse.org/reference/labellers.html)

Ajuster l'ordre des modalités des variables de type facteur [fct_reorder](https://forcats.tidyverse.org/reference/fct_reorder.html)\
[fct_inorder](https://forcats.tidyverse.org/reference/fct_inorder.html)\
[Ré-arranger un boxplot](https://cmdlinetips.com/2019/02/how-to-reorder-a-boxplot-in-r/)\
[Ré-ordonner une variable dans ggplot2](https://www.r-graph-gallery.com/267-reorder-a-variable-in-ggplot2.html)\
[R for Data Science - Factors](https://r4ds.had.co.nz/factors.html)

Légendes\
[Ajuster l'ordre d'une légende](https://stackoverflow.com/questions/38425908/reverse-stacking-order-without-affecting-legend-order-in-ggplot2-bar-charts)

Notes de bas de graphique: [Alignement des notes du graphique](https://stackoverflow.com/questions/64701500/left-align-ggplot-caption)

Etiquettes\
[ggrepel](https://ggrepel.slowkow.com/articles/examples.html)

Anti-sèches\
[Beautiful plotting with ggplot2](http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/)

<!-- TO DO - Under construction -->

<!-- * Straight horizontal, vertical, or other line -->

<!-- You can also add straight lines to your plot with `geom_hline()` (horizontal), `geom_vline()` (vertical) or `geom_abline()` (with a specified y intercept and slope) -->

<!-- Using option `label_wrap_gen` in facet_wrap to have multiple strip lines -->

<!-- labels and colors of strips -->

<!-- Axis text vertical adjustment -->

<!-- rotation -->

<!-- Labellers -->

<!-- limit range with limit() and coord_cartesian(), ylim(), or scale_x_continuous() -->

<!-- theme_classic() -->

<!-- expand = c(0,0) -->

<!-- coord_flip() -->

<!-- tick marks -->

<!-- ggrepel -->

<!-- animations -->

<!-- remove -->

<!-- remove title -->

<!-- using fill = or color = in labs() -->

<!-- flip order / don't flip order -->

<!-- move location -->

<!-- color?    theme(legend.title = element_text(colour="chocolate", size=16, face="bold"))+ scale_color_discrete(name="This color is\ncalled chocolate!?") -->

<!-- Color of boxes behind points in legend  -->

<!--      theme(legend.key=element_rect(fill='pink'))   or use fill = NA to remove them. http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/  -->

<!-- Change size of symbols in legend only guides(colour = guide_legend(override.aes = list(size=4))) -->

<!-- Turn off a layer in the legend -->

<!-- geom_text(data=nmmaps, aes(date, temp, label=round(temp)), size=4) -->

<!-- geom_text(data=nmmaps, aes(date, temp, label=round(temp), size=4), show_guide=FALSE) -->

<!-- Force a legend even if there is no aes().  -->

<!-- ggplot(nmmaps, aes(x=date, y=o3))+ -->

<!--      geom_line(aes(color="Important line"))+ -->

<!--      geom_point(aes(color="My points")) -->

<!-- Control the shape in the legend with guides - a list with linetype and shape -->

<!-- ggplot(nmmaps, aes(x=date, y=o3))+geom_line(aes(color="Important line"))+ -->

<!--    geom_point(aes(color="Point values"))+ -->

<!--   scale_colour_manual(name='', values=c('Important line'='grey', 'Point values'='red'), guide='legend') + -->

<!--   guides(colour = guide_legend(override.aes = list(linetype=c(1,0) -->

<!--                                                       , shape=c(NA, 16)))) -->
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/ggplot_tips.Rmd-->


# Courbes épidémiques {#epicurves}  

```{r, out.width=c('75%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "epicurve_top.png"))
```    


Une courbe épidémique (également connue sous le nom de "courbe épi") est un graphique épidémiologique de base généralement utilisé pour visualiser le schéma temporel d'apparition de la maladie parmi un groupe de cas ou une épidémie.  

L'analyse de la courbe épi peut révéler des tendances temporelles, des valeurs aberrantes, l'ampleur de l'épidémie, la période d'exposition la plus probable, les intervalles de temps entre les générations de cas, et peut même aider à identifier le mode de transmission d'une maladie non identifie (par exemple, source ponctuelle, source commune continue, propagation de personne à personne). Une leçon en ligne sur l'interprétation des courbes épi est disponible sur le site Web du [US CDC](https://www.cdc.gov/training/quicklearns/epimode/index.html).    

Dans cette page, nous démontrons deux approches pour produire des épicurves dans R :  

* Le paquet **incidence2**, qui peut produire une courbe épi avec des commandes simples.  
* Le paquet **ggplot2**, qui permet une personnalisation avancé via des commandes plus complexes.  

Nous abordons également des cas d'utilisation spécifiques tels que : 

* Les tracés de données de comptages agrégées  
* Le facettage ou la production de petits multiples  
* Application de moyennes mobiles  
* Montrer quelles données sont "provisoires" ou sujettes à des retards de rapport  
* Superposer l'incidence cumulative des cas à l'aide d'un deuxiéme axe  

<!-- ======================================================= -->
## Préparation


### Paquets {.unnumbered}  

Ce chunk de code montre le chargement des paquets nécessaires aux analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* le charge pour l'utiliser. Vous pouvez également charger les paquets installs avec `library()` de **base** R. Voir la page sur [R basics](#rbasics) pour plus d'informations sur les paquets R.  

```{r message=F, warning=F}
pacman::p_load(
  rio,         # import/export de fichiers
  here,        # chemins de fichiers relatifs 
  lubridate,   # travailler avec des dates/epiweeks
  aweek,       # paquet alternatif pour travailler avec les dates/semaines
  incidence2,  # épicurves de données linelist
  i2extras,    # supplément à incidence2
  stringr,      # recherche et manipulation de chaînes de caractères
  forcats,      # travail avec des facteurs
  RColorBrewer, # palettes de couleurs de colorbrewer2.org
  tidyverse     # gestion des données + graphiques ggplot2
) 
```


### Importer des données {.unnumbered}

Deux exemples de jeux de données sont utilisés dans cette section :  

* Liste de cas individuels d'une épidémie simulée.  
* Comptage agrégé par hôpital à partir de la même épidémie simulée.  

Les jeux de données sont importés à l'aide de la fonction `import()` du paquetage **rio**. Voir la page [Importation et exportation](#import_export) pour les différentes maniéres d'importer des données.  


```{r, echo=F, message=F}
# Importez la linelist dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# importer les données de comptage dans R
count_data <- linelist %>% 
  group_by(hospital, date_hospitalisation) %>% 
  summarize(n_cases = dplyr::n()) %>% 
  filter(date_hospitalisation > as.Date("2013-06-01")) %>% 
  ungroup()
```


**Liste de cas**

Nous importons le jeu de données des cas d'une épidémie d'Ebola simulée. Si vous souhaitez télécharger les données pour les suivre pas à pas, consultez les instructions de la page [Télécharger le manuel et les données](#date_used). Nous supposons que le fichier est dans le répertoire de travail, donc aucun sous-dossier n'est spécifié dans ce chemin de fichier.  

```{r, eval=F}
linelist <- import("linelist_cleaned.xlsx")
```

Les 50 premiéres lignes sont affichées ci-dessous.

```{r, message=FALSE, echo=F}
# affiche les données de la liste de lignes sous forme de tableau
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



**Comptes de cas agrégés par hôpital**  

Pour les besoins du manuel, le jeu de données des comptages hebdomadaires agrégés par hôpital est créé à partir de la `linelist` avec le code suivant. 

```{r, eval=F}
# Importez les données de comptage dans R
count_data <- linelist %>% 
  group_by(hospital, date_hospitalisation) %>% 
  summarize(n_cases = dplyr::n()) %>% 
  filter(date_hospitalisation > as.Date("2013-06-01")) %>% 
  ungroup()
```

Les 50 premiéres lignes sont affichées ci-dessous :  

```{r message=FALSE, echo=F}
# afficher les données de la liste de lignes sous forme de tableau
DT::datatable(head(count_data, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space : nowrap' )
```




### Définir les paramétres {.unnumbered} 

Pour la production d'un rapport, vous pouvez souhaiter définir des paramétres modifiables tels que la date à laquelle les données sont actuelles (la "date des données"). Vous pouvez ensuite faire référence à l'objet `data_date` dans votre code lorsque vous appliquez des filtres ou dans des légendes dynamiques.

```{r set_parameters}
## définit la date du rapport pour le rapport
## note : peut étre défini à Sys.Date() pour la date actuelle
data_date <- as.Date("2015-05-15")
```



### Vérifier les dates {.unnumbered}

Vérifiez que chaque colonne de date pertinente est de la classe Date et posséde une plage de valeurs appropriée. Vous pouvez le faire simplement en utilisant `hist()` pour les histogrammes, ou `range()` avec `na.rm=TRUE`, ou avec `ggplot()` comme ci-dessous.  

```{r, out.width = c('50%', '50%', '50%'), fig.show='hold', warning=F, message=F}
# vérifier la plage de dates d'apparition
ggplot(data = linelist)+
  geom_histogram(aes(x = date_onset))
```



<!-- ======================================================= -->
## Epicurves avec le paquet **incidence2** { }

Nous démontrons ci-dessous comment faire des épicurves en utilisant le paquet **incidence2**. Les auteurs de ce paquet ont essayé de permettre à l'utilisateur de créer et de modifier des épicurves sans avoir besoin de connaître la syntaxe de **ggplot2**. Une grande partie de cette page est adaptée des vignettes du paquet, qui peuvent étre trouvées sur la page **incidence2** [github](https://github.com/reconhub/incidence2).   


<!-- ======================================================= -->
### Exemple simple {.unnumbered}

**2 étapes sont nécessaires pour tracer une courbe épidémique avec le paquet *incidence2*:** 

1) **Créer** un objet *incidence* (en utilisant la fonction `incidence()`)  
    + Fournir les données  
    + Spécifiez la colonne de date à `date_index = `  
    + Spécifiez l'"intervalle" dans lequel les cas doivent étre agrégés (quotidien, hebdomadaire, mensuel..)
    + Spécifiez toutes les colonnes de regroupement (par exemple, sexe, hôpital, résultat).  
2) **Graphique** de l'objet d'incidence  
    + Spécifier les étiquettes, les couleurs, les titres, etc.  


Ci-dessous, nous chargeons le paquet **incidence2**, créons l'objet incidence à partir de la `linelist` sur la colonne `date_onset` et agrégeons les cas par semaine. Nous imprimons ensuite un résumé de l'objet d'incidence. 

```{r, warning=F, message=F}
# Chargement du paquet incidence2
pacman::p_load(incidence2)


# crée l'objet incidence, en agrégeant les cas par jour
epi_day <- incidence( # create incidence object
  x = linelist, # dataset
  date_index = "date_onset", # colonne de date
  interval = "week" # intervalle de regroupement par semaine
  )
```

L'objet **incidence2** lui-même ressemble à un tibble (comme un cadre de données) et peut étre imprimé ou manipulé comme un cadre de données.  

```{r}
class(epi_day)
```

Voici à quoi il ressemble une fois imprimé. Il posséde une colonne `date_index` et une colonne `count`.  

```{r}
epi_day
```

Vous pouvez également imprimer un résumé de l'objet :  

```{r}
# imprimer le résumé de l'objet incidence
summary(epi_day)
```

Pour *tracer* l'objet *incidence*, utilisez `plot()` sur le *nom de l'objet incidence*. En arriére-plan, la fonction `plot.incidence2()` est appelée, donc pour lire la documentation spécifique à **incidence2**, vous devez exécuter `éplot.incidence2`.  

```{r}
# Trace l'objet incidence
plot(epi_day)
```

Si vous remarquez beaucoup de petites lignes verticales blanches, essayez d'ajuster la taille de votre image. Par exemple, si vous exportez votre tracé avec `ggsave()`, vous pouvez fournir des nombres à `width = ` et `height = `. Si vous élargissez le tracé, ces lignes peuvent disparaétre.    



### Changer l'intervalle de temps de l'agrégation des cas {.unnumbered}  
L'argument `interval = ` de `incidence()` définit comment les observations sont regroupées en barres verticales. 

**Spécifier l'intervalle**  

**incidence2** offre une flexibilité et une syntaxe compréhensible pour spécifier comment vous voulez regrouper vos cas en barres épicurves. Fournissez une valeur comme celles ci-dessous à l'argument `interval = `. Vous pouvez écrire n'importe laquelle des valeurs ci-dessous au pluriel (par exemple "semaine**s**"), et vous pouvez ajouter des chiffres avant (par exemple "3 mois").  

Argument option | Explication complémentaire 
------------------- | ------------------------------------ |
Nombre (1, 7, 13, 14, etc.) | Nombre de jours par intervalle  
"semaine" | Note : le jour de départ du lundi est la valeur par défaut
"2 semaines" | ou 3, 4, 5...
"semaine du dimanche" | semaines commençant le dimanche (pourrait aussi utiliser le jeudi, etc.)
"2 semaines de dimanche" | ou 3, 4, 5...
"MMWRweek" | semaine commençant le dimanche - voir US CDC
"mois" | 1er du mois
"trimestre" | 1er du mois du trimestre
"2 mois" | ou 3, 4, 5...
"année" | 1er jour de l'année civile


Vous trouverez ci-dessous des exemples de l'aspect des différents intervalles lorsqu'ils sont appliqués à la liste de lignes. Notez comment le format et la fréquence par défaut des *étiquettes* de date sur l'axe des x changent lorsque l'intervalle de date change.  

```{r incidence, out.width=c('50%', '50%', '50%', '50%'), fig.show='hold', warning=F, message=F}
# Créez les objets d'incidence (avec différents intervalles)
##############################
# Hebdomadaire (semaine du lundi par défaut)
epi_wk <- incidence(linelist, "date_onset", interval = 1)

# Semaine du dimanche
epi_Sun_wk <- incidence(linelist, "date_onset", interval = "epiweeks")

# Trois semaines (semaines de lundi par défaut)
epi_2wk <- incidence(linelist, "date_onset", interval = "weekly")

# Mensuel
epi_month <- incidence(linelist, "date_onset", interval = "month")

# Trimestrielle
epi_quarter <- incidence(linelist, "date_onset", interval = "quarter")

# Années
epi_year <- incidence(linelist, "date_onset", interval = "year")


# Tracez les objets d'incidence (+ titres pour plus de clarté)
############################
plot(epi_wk)+ labs(title = "Daily")
plot(epi_Sun_wk)+ labs(title = "Sunday weeks")
plot(epi_2wk)+ labs(title = "Monday Weeks")
plot(epi_month)+ labs(title = "Months")
plot(epi_quarter)+ labs(title = "Quarters")
plot(epi_year)+ labs(title = "Years")

```


<!-- **Débuter au premier cas** -->

<!-- Si vous voulez que les intervalles commencent au premier cas, vous pouvez ajouter l'argument `standard = TRUE` à la commande `incidence()`. Cela ne fonctionne que si l'intervalle est soit "semaine", "mois", "trimestre" ou "année".   -->

**Premiére date**

Vous pouvez optionnellement spécifier une valeur de la classe Date (par exemple `as.Date("2016-05-01")`) à `firstdate = ` dans la commande `incidence()`. Si elle est donnée, les données seront rognées à cette plage et les intervalles commenceront à cette date. 



### Groupes {.unnumbered}

Les groupes sont spécifiés dans la commande `incidence()`, et peuvent étre utilisés pour colorer les barres ou pour facetter les données. Pour spécifier des groupes dans vos données, fournissez le(s) nom(s) de colonne(s) à l'argument `groups =` de la commande `incidence()` (sans guillemets autour du nom de colonne). Si vous spécifiez plusieurs colonnes, mettez leurs noms dans `c()`.

Vous pouvez spécifier que les cas avec des valeurs manquantes dans les colonnes de regroupement soient listés comme un groupe distinct `NA` en mettant `na_as_group = TRUE`. Sinon, ils seront exclus du graphique.   

* Pour *colorer les barres par une colonne de regroupement*, vous devez à nouveau fournir le nom de la colonne à `fill = ` dans la commande `plot()`.  

* Pour *faire des facettes basées sur une colonne de regroupement*, voir la section ci-dessous sur les facettes avec **incidence2**.  

Dans l'exemple ci-dessous, les cas de l'ensemble de l'épidémie sont groupés par leur catégorie d'âge. Les valeurs manquantes sont incluses en tant que groupe. L'intervalle d'épicurve est des semaines.  


```{r, message=F, warning=F}
# Créez un objet incidence, avec des données groupées par catégorie d'âge
age_outbreak <- incidence(
  linelist, # jeu de données
  date_index = "date_onset", # colonne de date
  interval = "week", # agrégation hebdomadaire des cas du lundi
  groups = "age_cat") #, # age_cat est défini comme un groupe
  #na_as_group = TRUE) # les valeurs manquantes se voient attribuer leur propre groupe


# Tracez l'objet d'incidence groupée

ggplot(age_outbreak) +                                    # objet d'incidence avec age_cat comme groupe
     geom_col(aes(date_index, count, fill = age_cat)) + # age_cat est utilisé pour la couleur de remplissage des barres (doit avoir été défini comme une colonne de groupes ci-dessus)
     labs(fill = "Age Category",                        # change le titre de la légende de "age_cat" par défaut (c'est une modification de ggplot2)
          y = "Date",
          x = "Count") +
     theme_bw()

# Tracez l'objet d'incidence groupée
#plot(
#  age_outbreak # objet d'incidence avec age_cat comme groupe
#  )+  age_cat est utilisé pour la couleur de remplissage des barres (doit avoir été défini comme une colonne de groupes ci-dessus)
#labs(fill = "Age Category") # change le titre de la légende de "age_cat" par défaut (c'est une modification de ggplot2)

```
<span style="color : darkgreen ;">**_ATTENTION:_** Changez le titre de la légende en ajoutant `+` la commande **ggplot2** `labs(fill = "your title")` à votre graphe **incidence2**.</span>  

Vous pouvez également afficher les barres groupées côte à côte en définissant `stack = FALSE` dans `plot()`, comme indiqué ci-dessous :  

```{r, warning=F, message=F}
# Créez un objet d'incidence pour les comptes mensuels. 
monthly_gender <- incidence(
 linelist,
 date_index = "date_onset",
 interval = "month",
 groups = "gender" # définit le genre comme colonne de regroupement
)

ggplot(monthly_gender) + # objet d'incidence
     geom_col(aes(date_index, count, fill = gender), # affichage de barres colorées par sexe
              position = "dodge") +  # côte à côte (non empilées)
     labs(fill = "Gender", 
          x = "Date", 
          y = "Count")
                 
#plot(
#  monthly_gender, # objet d'incidence
#  fill = "gender", # affichage de barres colorées par sexe
#  stack = FALSE) # côte à côte (non empilées)

``` 

Vous pouvez définir l'argument `na_as_group = ` à FALSE dans la commande `incidence()` pour supprimer les lignes avec des valeurs manquantes du graphique.  




### Données filtrées {.unnumbered}

Pour tracer l'épicurve d'un sous-ensemble de données :  

1) Filtrer les données de la liste de lignes  
2) Fournissez les données filtrées à la commande `incidence()`.  
3) Tracez l'objet d'incidence

L'exemple ci-dessous utilise des données filtrées pour ne montrer que les cas de l'hôpital central.  

```{r, warning=F, message=F}
# filtrez la liste de lignes
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")

# créez un objet d'incidence en utilisant les données filtrées
central_outbreak <- incidence(central_data, date_index = "date_onset", interval = "week")

# tracer l'objet d'incidence
plot(central_outbreak, title = "Incidence hebdomadaire des cas à l'hôpital central")
```




### Comptages agrégés {.unnumbered}

Si vos données d'origine sont agrégées (comptes), fournissez le nom de la colonne qui contient les comptes de cas à l'argument `count = ` lorsque vous créez l'objet d'incidence avec `incidence()`.  

Par exemple, ce cadre de données `count_data` est la liste de lignes agrégée en comptes quotidiens par hôpital. Les 50 premiéres lignes ressemblent à ceci :  

```{r message=FALSE, echo=F}
DT::datatable(head(count_data,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Si vous commencez votre analyse avec des données de comptage quotidiennes comme l'ensemble de données ci-dessus, votre commande `incidence()` pour convertir ces données en une épicurve hebdomadaire par hôpital ressemblerait à ceci :  

```{r}
epi_counts <- incidence( # créer un objet d'incidence hebdomadaire
  count_data, # ensemble de données avec les comptes agrégés par jour
  date_index = "date_hospitalisation", # colonne avec les dates
  counts = "n_cases", # colonne avec les comptages
  interval = "week", # agréger les comptages quotidiens jusqu'aux semaines
  groups = "hospital" # groupe par hôpital
  )


ggplot(epi_counts) + # objet d'incidence
     geom_col(aes(date_index, count, fill = hospital) # affichage de barres colorées par hôpital
             ) +  
     labs(fill = "Hospital", 
          x = "Date", 
          y = "Count") +
     theme_bw()
                 


# Tracez la courbe d'incidence hebdomadaire de l'épi, avec des barres empilées par hôpital.
#plot(epi_counts, # objet incidence
#     fill = hospital) # colorer les barres par hôpital
```




### Facettes/petits multiples {.unnumbered}  

Pour facetter les données par groupe (i.e. produire des "petits multiples") :  

1) Spécifiez la colonne de facettes à `groups = ` lorsque vous créez l'objet d'incidence.  
2) Utilisez la commande `facet_plot()` au lieu de `plot()`.  
3) spécifiez les colonnes de regroupement à utiliser comme `fill = ` et celles à utiliser comme `facets = `.  

Ci-dessous, nous avons défini les deux colonnes `hospital` et `outcome` comme colonnes de regroupement dans la commande `incidence()`. Ensuite, dans `facet_plot()` nous traçons l'épicurve, en spécifiant que nous voulons une épicurve différente pour chaque hôpital et que dans chaque épicurve les barres doivent étre empilées et colorées par résultat.  
 

```{r, warning=F, message=F}
epi_wks_hosp_out <- incidence(
  linelist, # ensemble de données
  date_index = "date_onset", # colonne de date
  interval = "month", # barres mensuelles  
  groups = c("outcome", "hospital") # le résultat et l'hôpital sont données comme colonnes de regroupement
  )



ggplot(epi_wks_hosp_out) + 
     geom_col(aes(date_index, count, fill = outcome )) +
     facet_wrap(~hospital) + 
     labs(fill = "Outcome", 
          y = "Count", 
          x = "Date") +
     theme_bw()
     

```

Notez que le package **ggtree** (utilisé pour afficher les arbres phylogénétiques) posséde également une fonction `facet_plot()` - c'est pourquoi nous avons spécifié `incidence2::facet_plot()`` ci-dessus.  



### Modifications avec `plot()` {.unnumbered} 

Une épicurve produite par **incidence2** peut étre modifiée via ces arguments *dans la fonction `plot()`*.  

**Voici les arguments de `plot()` qui modifient l'apparence des barres:**  

Argument | Description | Exemples
------------------|---------------------------------------|-------------------
`fill = `|Couleur des barres. Soit un nom de couleur ou un nom de colonne probablement spécifié à `groups = ` dans la commande `incidence()`|fill = "red"`, ou `fill = gender`  
`color = ` |Couleur autour de chaque barre, ou autour de chaque groupement dans une barre|`border = "white"` 
`legend = `|Localisation de la légende|Une des options suivantes : "bottom", "top", "left", "right" ou "none".  
`alpha = `|Transparence des barres/boîtes|1 est totalement opaque, 0 est totalement transparent
`width = `Valeur entre 0 et 1 indiquant la taille relative des barres par rapport à leur intervalle de temps|`width = .7`  
`show_cases = `|Logique ; si VRAI, chaque cas s'affiche sous forme de boîte. L'affichage est meilleur pour les petites épidémies.  

**Voici les arguments de `plot()` qui modifient l'axe des dates:**  

Argument(s)|Description
----------------------|----------------------------------------------------
`centre_dates = `|TRUE/FALSE pour savoir si les dates sont affichôes au centre des barres ou au début des barres.  
`date_format = `|Régler le format d'affichage de la date en utilisant la syntaxe strptime ("%"). Ne fonctionne que si `centre_dates = FALSE` (détails ci-dessous).  
`n.breaks = `|Nombre approximatif de ruptures d'étiquettes sur l'axe des x souhaité.  
`angle = `|Angle des étiquettes de date sur l'axe des x (nombre de degrés)  
`size = `|Taille du texte en points  

Notez que l'argument `date_breaks = ` ne fonctionne que si `centre_dates = FALSE`. Fournissez une valeur entre guillemets en utilisant la syntaxe strptime ci-dessous, comme détaillé dans la page [travailler avec des dates](#dates). Vous pouvez utiliser `\n` pour une "nouvelle ligne".  

%d = Numéro du jour du mois (5, 17, 28, etc.)  
%j = Numéro du jour de l'année (jour julien 001-366)  
%a = Jour de semaine abrégé (Lun, Mar, Mer, etc.)  
%A = Jour de la semaine complet (lundi, mardi, etc.)  
%w = Numéro du jour de la semaine (0-6, le dimanche est 0)  
%u = Numéro du jour de la semaine (1-7, le lundi est 1)  
%W = Numéro de la semaine (00-53, le lundi est le début de la semaine)  
%U = Numéro de la semaine (01-53, le dimanche est le début de la semaine)  
%m = Numéro du mois (par exemple 01, 02, 03, 04)  
%b = Mois abrégé (Jan, Feb, etc.)  
%B = Mois complet (janvier, février, etc.)  
%y = Année à 2 chiffres (ex. 89)  
%Y = Année à 4 chiffres (ex. 1989)  
%h = heures (horloge de 24 heures)  
%m = minutes  
%s = secondes  
%z = décalage par rapport à GMT  
%Z = fuseau horaire (caractère)


<!-- <span style="color : darkgreen ;">**_TIP:_** Pour des pauses a tous les "n" intervalles (par exemple tous les 4), utilisez `n.breaks = nrow(i)/n` (où "i" est le nom de votre objet d'incidence et "n" est un nombre). Si vos données sont groupées, vous devrez multiplier "n" par le nombre de groupes uniques.</span> -->



**Voici les arguments de `plot()` qui modifient les étiquettes des graphiques:**

Argument(s)|Description
----------------------|----------------------------------------------------
`title = `|Titre du graphique|`title = "Epidemic curve of Acute Jaundice Syndrome (AJS)"`
`xlab = `|Titre de l'axe des x|`xlab = "Date of onset"`  
`ylab = `|Titre de l'axe des y|`ylab = "Daily case"`  
`size = `|Taille du texte de l'axe des x en pts (utilisez theme() de ggplot pour ajuster les autres tailles)  


Un exemple utilisant plusieurs des arguments ci-dessus :  

```{r, warning=F, message=F}
# filtrez la liste des lignes
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")

# créez un objet d'incidence en utilisant les données filtrées
central_outbreak <- incidence(
  central_data,
  date_index = "date_onset",
  interval = "week",
  groups = "outcome")


ggplot(central_outbreak) + 
     geom_col(aes(x = date_index, y = count, fill =  outcome), # couleur de la boéte/barre
              alpha = 0.7, # transparence 
              color = "grey") +  #bordure de la boîte
     
     labs(title = "Cases at Central Hospital", #titre
          x = "Week of onset",  # étiquette de l'axe des x
          y = "Count", # étiquette de l'axe des y
          fill = "Outcome") + 
     theme_bw() +
     theme(legend.position="top") # légende en haut de page
     


# tracer l'objet incidence
#plot(
#  central_outbreak,
#  fill = outcome, # couleur de la boéte/barre
#  legend = "top", # légende en haut de page
#  title = "Cas à l'hôpital central", # titre
#  xlab = "Semaine d'apparition", # étiquette de l'axe des x
#  ylab = "Incidence", # étiquette de l'axe des y
#  show_cases = TRUE, # afficher chaque cas comme une boîte individuelle
#  alpha = 0.7, # transparence 
#  border = "grey", # bordure de la boîte
#  angle = 30, # angle des étiquettes de date
#  centre_dates = FALSE, # étiquettes de date au bord de la barre
#  date_format = "%a %d %b %Y\n(Week %W)" # ajuste la façon dont les dates sont affichôes
 # )

```

Pour ajuster davantage l'apparence du graphique, consultez la section ci-dessous sur les modifications avec `ggplot()`.  






### Modifications avec ggplot2 {.unnumbered}

Vous pouvez modifier davantage un graphique **incidence2** en ajoutant des modifications **ggplot2** avec un `+` aprés la fin de la fonction `plot()` d'incidence, comme démontré ci-dessous.  

Ci-dessous, le tracé **incidence2** se termine, puis les commandes **ggplot2** sont utilisées pour modifier les axes, ajouter une légende et ajuster la police en gras et la taille du texte.  

Notez que si vous ajoutez `scale_x_date()`, la plupart des formatages de date de `plot()` seront écrasés. Voir la section sur les épicurves de `ggplot()` et la page du manuel [ggplot tips](#ggplot_tips) pour plus d'options.  

```{r, warning=F, message=F}
# filtrez la liste des lignes
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")

# créez un objet d'incidence en utilisant les données filtrées
central_outbreak <- incidence(
  central_data,
  date_index = "date_onset",
  interval = "week",
  groups = c("outcome"))

ggplot(central_outbreak) + 
     geom_col(aes(x = as.Date(date_index), y = count, fill =  outcome), # couleur de la boéte/barre
              alpha = 0.7, # transparence 
              color = "grey") +  #bordure de la boîte
     scale_y_continuous(
          breaks = seq(from = 0, to = 30, by = 5), # spécifie les incréments de l'axe des y par 5
    expand = c(0,0)) + # supprime l'espace excédentaire en dessous de 0 sur l'axe des y 
     scale_x_date(date_labels = "%a %d %b\n%Y (Week %W)") +
  
  # ajouter une légende dynamique
  
  # ajuster le caractère gras et la position de la légende
      theme_bw() +
      theme(
    legend.position="top",
    axis.title = element_text(size = 12, face = "bold"), # titres des axes plus grands et plus gras
    axis.text = element_text(size = 10, face = "bold"), # taille du texte de l'axe et gras
    plot.caption = element_text(hjust = 0, face = "italic"), # déplacer la légende vers la gauche
    axis.text.x = element_text(angle = 30, hjust=1)) +
     
     labs( 
          title = "Cas à l'hôpital central", # titre
          x = "Semaine d'apparition", # étiquette de l'axe des x
          y = "Incidence", # étiquette de l'axe des y
          fill = "Résultat pour le patient", # Titre de la légende
          caption = stringr::str_glue( # légende dynamique - voir la page sur les caractères et les chaîne de caractères pour plus de détails
      "n = {central_cases} de l'hôpital central
      L'apparition des cas va de {earliest_date} à {latest_date}. {missing_onset} cas n'ont pas de date d'apparition et ne sont pas représentés ici.",
      central_cases = nrow(central_data),
      earliest_date = format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y'),
      latest_date = format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y'),      
      missing_onset = nrow(central_data %>% filter(is.na(date_onset)))))


# tracer l'objet incidence
#plot(
#  central_outbreak,
#  fill = outcome, # couleur de la boîte/barre
#  legend = "top", # légende sur le dessus
#  title = "Cas à l'hôpital central", # titre
#  xlab = "Semaine d'apparition", # étiquette de l'axe des x
#  ylab = "Incidence", # étiquette de l'axe des y
#  show_cases = TRUE, # afficher chaque cas comme une boîte individuelle
#  alpha = 0.7, # transparence 
#  border = "grey", # bordure de la boîte
#  centre_dates = FALSE,                   
#  date_format = "%a %d %b\n%Y (Week %W)", 
#  angle = 30 # angle des étiquettes de date
#  )
```




### Changer les couleurs {.unnumbered}  

#### spécifier une palette {.unnumbered}  

Fournissez le nom d'une palette prédéfinie à l'argument `col_pal = ` dans `plot()`. Le paquet **incidence2** est livré avec 2 palettes prédéfinies : "vibrant" et "muted". Dans "vibrant" les 6 premiéres couleurs sont distinctes et dans "muted" les 9 premiéres couleurs sont distinctes. Aprés ces chiffres, les couleurs sont des interpolations/intermédiaires d'autres couleurs. Ces palettes prédéfinies sont disponibles sur [ce site](https://personal.sron.nl/~pault/#sec:qualitative). Les palettes excluent le gris, qui est réservé aux données manquantes (utilisez `na_color = ` pour modifier cette valeur par défaut).  

```{r out.width = c('50%', '50%'), fig.show='hold', warning = F, message = F}
# créez un objet incidence, avec des données groupées par catégorie d'âge  
age_outbreak <- incidence(
  linelist,
  date_index = "date_onset", # date d'apparition pour l'axe des x
  interval = "week", # agrégation hebdomadaire des cas
  groups = "age_cat")

# tracer l'épicurve avec la palette par défaut
#plot(age_outbreak, fill = age_cat, title = "'vibrant' incidence2 palette par défaut")

# tracer avec une palette de couleurs différente
#plot(age_outbreak, fill = age_cat, col_pal = muted, title = "'muted' incidence2 palette")
```

Vous pouvez également utiliser l'une des palettes **base** de R (mettez le nom de la palette *sans* guillemets).  

```{r out.width = c('50%', '50%'), fig.show='hold', warning = F, message = F}
# tracer avec la palette R de base
#plot(age_outbreak, fill = age_cat, col_pal = heat.colors, title = "base R heat.colors palette")

# tracé avec la palette R de base
#plot(age_outbreak, fill = age_cat, col_pal = rainbow, title = "base R rainbow palette")
```

Vous pouvez également ajouter une palette de couleurs à partir du paquet **viridis** ou du paquet **RColorBrewer**. Vous devez d'abord charger ces paquets, puis ajouter leurs fonctions respectives `scale_fill_*()` avec un `+`, comme indiqué ci-dessous.

```{r out.width = c('50%', '50%'), fig.show='hold', warning = F, message = F, eval=F}
pacman::p_load(RColorBrewer, viridis)

# tracer avec une palette de couleurs
plot(age_outbreak, fill = age_cat, title = "Palette Viridis")+
  scale_fill_viridis_d(
    option = "inferno", # schôma de couleurs, essayez aussi "plasma" ou la valeur par défaut
    name = "Age Category", # nom de la légende
    na.value = "grey")      # pour les valeurs manquantes

# tracer avec la palette de couleurs
plot(age_outbreak, fill = age_cat, title = "RColorBrewer palette")+
  scale_fill_brewer(
    palette = "Dark2", # palette de couleurs, essayez aussi Accent, Dark2, Paired, Pastel1, Pastel2, Set1, Set2, Set3
    name = "Age Category", # nom de la légende
    na.value = "grey")      # pour les valeurs manquantes
```


#### spécifier manuellement {.unnumbered}  

Pour spécifier les couleurs manuellement, ajoutez la fonction **ggplot2** `scale_fill_manual()` à la fonction `plot()` avec un `+` et fournissez le vecteur de noms de couleurs ou de codes HEX à l'argument `values = `. Le nombre de couleurs listées doit étre égal au nombre de groupes. Faites attention si les valeurs manquantes sont un groupe - elles peuvent étre converties en une valeur de caractère comme "Missing" pendant votre préparation de données avec la fonction `fct_explicit_na()` comme expliqué dans la page sur [les facteurs](#factors).  

```{r out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F, eval=F}
# couleurs manuelles
plot(age_outbreak, fill = age_cat, title = "Manually-specified colors")+
  scale_fill_manual(
    values = c("darkgreen", "darkblue", "purple", "grey", "yellow", "orange", "red", "lightblue"), # colors
    name = "Age category")      # Nom pour la légende
```

Comme mentionné dans la page [ggplot tips](#ggplot_tips), vous pouvez créer vos propres palettes en utilisant `colorRampPalette()` sur un vecteur de couleurs et en spécifiant le nombre de couleurs que vous voulez en retour. C'est un bon moyen d'obtenir de nombreuses couleurs dans une rampe en n'en spécifiant que quelques-unes.  

```{r}
my_cols <- c("darkgreen", "darkblue", "purple", "grey", "yellow", "orange")
my_palette <- colorRampPalette(my_cols)(12) # étendez les 6 couleurs ci-dessus à 12 couleurs
my_palette
```
          
          
### Ajuster l'ordre des niveaux {.unnumbered}  

Pour ajuster l'ordre d'apparition des groupes (sur le graphe et dans la légende), la colonne de regroupement doit étre de classe Facteur. Voir la page sur les [facteurs](#factors) pour plus d'informations.  

Tout d'abord, voyons une épicurve hebdomadaire par hôpital avec l'ordre par défaut :  

```{r, message=F, warning=F, eval=F}
# ORIGINAL - hôpital PAS comme facteur
###################################

# créer un objet d'incidence hebdomadaire, lignes groupées par hôpital et semaine
hospital_outbreak <- incidence(
  linelist,
  date_index = date_onset, 
  interval = "week", 
  groups = hospital)

# tracer l'objet incidence
plot(hospital_outbreak, fill = hospital, title = "ORIGINAL - l'hôpital n'est pas un facteur")
```

Maintenant, pour ajuster l'ordre de sorte que "Missing" et "Other" soient en haut de l'épicurve, nous pouvons faire ce qui suit :  

* Charger le paquet **forcats**, pour travailler avec les facteurs.  
* Ajustez l'ensemble de données - dans ce cas, nous définirons un nouvel ensemble de données (`plot_data`) dans lequel :  
  * la colonne `gender` est définie comme un facteur l'ordre des niveaux est défini avec `fct_relevel()` de sorte que "Other" et "Missing" soient les premiers, ils apparaissent donc en haut des barres  
* L'objet d'incidence est créé et tracé comme précédemment.  
* Nous ajoutons les modifications **ggplot2**.  
  * `scale_fill_manual()` pour assigner manuellement les couleurs de sorte que "Missing" soit gris et "Other" soit beige.  
 



```{r, message=F, warning=F, eval=F}
# MODIFIED - l'hôpital comme facteur
###############################

# charger le paquet forcats pour travailler avec les facteurs
pacman::p_load(forcats)

# Convertir la colonne hôpital en facteur et ajuster les niveaux
plot_data <- lineelist %>% 
  mutate(hospital = fct_relevel(hospital, c("Missing", "Other"))) # définir "Missing" et "Other" comme niveaux supérieurs.


# créez un objet d'incidence hebdomadaire, groupé par hôpital et par semaine.
hospital_outbreak_mod <- incidence(
  plot_data,
  date_index = date_onset, 
  interval = "week", 
  groups = hospital)

# tracer l'objet d'incidence
plot(hospital_outbreak_mod, fill = hospital)+
  
  # spécifier manuellement les couleurs
  scale_fill_manual(values = c("grey", "beige", "darkgreen", "green2", "orange", "red", "pink"))+                      

  # étiquettes ajoutées via ggplot
  labs(
      title = "MODIFIÉ - l'hôpital comme facteur", # titre du graphique
      subtitle = "Autre & Manquant en haut de l'épicurve",
      y = "Incidence hebdomadaire des cas ", # titre de l'axe des y  
      x = "Semaine d'apparition des symptômes", # titre de l'axe des x
      fill = "Hôpital") # titre de la légende     
```

<span style="color : darkgreen ;">**_ATTENTION:_** Si vous voulez inverser l'ordre de la légende uniquement, ajoutez cette commande **ggplot2** `guides(fill = guide_legend(reverse = TRUE))`.</span>  



### Lignes de grille verticales {.unnumbered}  

Si vous tracez avec les paramétres par défaut de **incidence2**, vous pouvez remarquer que les lignes de grille verticales apparaissent à chaque étiquette de date et une fois entre chaque étiquette de date. Cela peut entraîner l'intersection des lignes de grille avec le haut de certaines barres.  

<Remarque : ce paragraphe n'est pas applicable avec la version 1.0.0 de incidence2). Vous pouvez spécifier l'intervalle des lignes de grille en ajoutant la commande `scale_x_date()` de **ggplot2** à votre graphe **incidence2**. Dans celle-ci, spécifiez les intervalles pour `date_breaks = ` et `date_minor_breaks = ` (par exemple "semaines" ou "3 semaines" ou "mois"). Notez que l'utilisation de `scale_x_date()` remplacera tout formatage des étiquettes de date dans `plot()`, donc vous devrez spécifier tout format de chaîne à `date_labels = ` comme ci-dessous.   -->

Vous pouvez supprimer toutes les lignes de grille en ajoutant la commande **ggplot2** `theme_classic()`.  

```{r, warning=F, message=F, out.width = c('50%', '50%', '50%'), fig.show='hold'}
# fabrique un objet d'incidence
a <- incidence(
  central_data,
  date_index = "date_onset",
  interval = "weekly"
)

# Lignes de grille par défaut
plot(a, title = "Lignes par défaut")

# Intervalles de lignes de grille spécifiés
# NE FONCTIONNE PAS AVEC INCIDENCE2 1.0.0
# plot(a, title = "Lignes hebdomadaires")+
# scale_x_date(
# date_breaks = "4 weeks", # major vertical lines align on weeks
# date_minor_breaks = "weeks", # lignes verticales mineures chaque semaine
# date_labels = "%a\n%d\n%b") # format des étiquettes de date

# Pas de lignes de grille
plot(a, title = "Aucune ligne")+
  theme_classic() # supprime toutes les lignes du quadrillage
```

Notez cependant que si vous utilisez des semaines, les arguments `date_breaks` et `date_minor_breaks` ne fonctionnent que pour les semaines *lundi*. Si vos semaines sont d'un autre jour de la semaine, vous devrez fournir manuellement un vecteur de dates aux arguments `breaks = ` et `minor_breaks = ` à la place. Voir la section **ggplot2** pour des exemples de ceci en utilisant `seq.Date()`.

### Incidence cumulée {.unnumbered}  

Vous pouvez facilement produire un graphique de l'incidence cumulée en passant l'objet incidence à la commande **incidence2** `cumulate()` et ensuite à `plot()`. Cela fonctionne également avec `facet_plot()`.  

```{r}
# fabrique un objet d'incidence hebdomadaire
wkly_inci <- incidence(
  linelist,
  date_index = "date_onset",
  interval = "week")

# tracer l'incidence cumulée
wkly_inci %>% 
  cumulate() %>% 
  plot()
```


Voir la section plus bas sur cette page pour une méthode alternative pour tracer l'incidence cumulative avec **ggplot2** - par exemple pour superposer une ligne d'incidence cumulative sur une épicurve.  

### Moyenne glissante {.unnumbered}

Vous pouvez ajouter une moyenne glissante à un graphique **incidence2** facilement avec `add_rolling_average()` du paquet **i2extras**. Passez votre objet incidence2 à cette fonction, puis à `plot()`. définissez `before = ` comme le nombre de jours précédents que vous voulez inclure dans la moyenne glissante (par défaut, 2). Si vos données sont groupées, la moyenne mobile sera calculée par groupe. 

```{r, warning=F, message=F}
rolling_avg <- incidence( # make incidence object
  linelist,
  date_index = "date_onset",
  interval = "week",
  groups = "gender") %>% 
  
  i2extras::add_rolling_average(6) # ajouter des moyennes glissantes (dans ce cas, par sexe)

# tracer
plot(rolling_avg, n.breaks = 3) # facettes automatiquement car moyenne glissante sur les groupes
```

Pour apprendre à appliquer des moyennes glissantes de maniére plus générale sur des données, voir la page du manuel sur les [moyennes glissantes](#moving_average).  


<!-- ======================================================= -->
## Epicurves avec ggplot2 { }

L'utilisation de `ggplot()` pour construire votre épicurve permet plus de flexibilité et de personnalisation, mais nécessite plus d'efforts et de compréhension du fonctionnement de `ggplot()`.  

Contrairement à l'utilisation du paquet **incidence2**, vous devez *manuellement* contrôler l'agrégation des cas par temps (en semaines, mois, etc.) *et* les intervalles des étiquettes sur l'axe des dates. Ceci doit étre soigneusement géré.  

Ces exemples utilisent un sous-ensemble de l'ensemble de données `linelist` - seulement les cas de l'hôpital central.  


```{r, echo=F}
# Importez la linelist dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r}
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")
```

```{r, eval=F, echo=F}
detach("package:tidyverse", unload=TRUE)
library(tidyverse)
```


Pour produire une épicurve avec `ggplot()`, il y a trois éléments principaux :  

* Un histogramme, avec les cas de la liste de lignes agrégés en "bins" distingués par des points de "rupture" spécifiques.  
* Des échelles pour les axes et leurs étiquettes  
* Des thèmes pour l'apparence du graphique, y compris les titres, les étiquettes, les légendes, etc.


### spécifier les cas en bacs {.unnumbered}  

Nous montrons ici comment spécifier la façon dont les cas seront agrégés dans des cases d'histogramme ("barres"). Il est important de reconnaétre que l'agrégation des cas dans les cases de l'histogramme n'est **pas** nécessairement les mêmes intervalles que les dates qui apparaîtront sur l'axe des abscisses. 

Vous trouverez ci-dessous le code le plus simple pour produire des épicurves quotidiennes et hebdomadaires.  

Dans la commande globale `ggplot()`, le jeu de données est fourni avec `data = `. Sur cette base, la géométrie d'un histogramme est ajoutée avec un `+`. Dans la commande `geom_histogram()`, nous mappons l'esthétique de telle sorte que la colonne `date_onset` soit mappée sur l'axe des x. Toujours dans `geom_histogram()` mais *non* dans `aes()`, nous définissons la `binwidth =` des bins de l'histogramme, en jours. Si cette syntaxe **ggplot2** est confuse, revoyez la page sur [les bases de ggplot](#ggplot_basics).  

<span style="color : orange ;">**_CAUTION:_** Tracer des cas hebdomadaires en utilisant `binwidth = 7` fait démarrer le premier bin de 7 jours au premier cas, qui pourrait étre n'importe quel jour de la semaine ! Pour créer des semaines spécifiques, voir la section ci-dessous .</span>


```{r ggplot_simple, out.width = c('50%', '50%'), fig.show='hold', warning= F, message = F}
# quotidien 
ggplot(data = central_data) + # set data
  geom_histogram( # ajouter un histogramme
    mapping = aes(x = date_onset), # map date column to x-axis
    binwidth = 1)+ # cas groupés par 1 jour 
  labs(title = "Central Hospital - Quotidiennement") # titre

# hebdomadaire
ggplot(data = central_data) + # set data 
  geom_histogram( # ajouter un histogramme
      mapping = aes(x = date_onset), # mappage de la colonne date sur l'axe des x
      binwidth = 7)+ # cas classés tous les 7 jours, à partir du premier cas ( !) 
  labs(title = "Central Hospital - Tranches de 7 jours, à partir du premier cas") # titre
```

Notons que le premier cas de cet ensemble de données de l'hôpital Central a vu ses symptômes apparaître le :  

```{r}
format(min(central_data$date_onset, na.rm=T), "%A %d %b, %Y")
```

**Pour spécifier manuellement les ruptures des cases de l'histogramme, n'utilisez pas l'argument `binwidth = `, mais fournissez un vecteur de dates à `breaks = `.**.  

créez le vecteur de dates avec la fonction R **base** `seq.Date()`. Cette fonction attend les arguments `to = `, `from = `, et `by = `. Par exemple, la commande ci-dessous renvoie les dates mensuelles commençant le 15 janvier et se terminant le 28 juin.

```{r}
monthly_breaks <- seq.Date(from = as.Date("2014-02-01"),
                           to = as.Date("2015-07-15"),
                           by = "months")

monthly_breaks # print
```

Ce vecteur peut étre fourni à `geom_histogram()` sous la forme `breaks = ` :  

```{r, warning=F, message=F}
# mensuel 
ggplot(data = central_data) +  
  geom_histogram(
    mapping = aes(x = date_onset),
    breaks = monthly_breaks)+ # fournit le vecteur prédéfini de breaks                    
  labs(title = "Bins de cas mensuels") # titre
```

Une simple séquence de date hebdomadaire peut étre retournée en définissant `by = "week"`. Par exemple : 

```{r}
weekly_breaks <- seq.Date(from = as.Date("2014-02-01"),
                          to = as.Date("2015-07-15"),
                          by = "week")
```

 
Une alternative à la fourniture de dates de début et de fin spécifiques consiste à écrire un code *dynamique* pour que les bacs hebdomadaires commencent *le lundi précédant le premier cas*. **Nous utiliserons ces vecteurs de date dans les exemples ci-dessous.  
     
```{r}
# Séquence de dates hebdomadaires du lundi pour CENTRAL HOSPITAL
weekly_breaks_central <- seq.Date(
  from = floor_date(min(central_data$date_onset, na.rm=T), "week", week_start = 1), # lundi avant le premier cas
  to = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 1), # lundi aprés la derniére affaire
  by = "week")
```  

Décortiquons le code plutôt déconcertant ci-dessus :  

* La valeur "from" (date la plus ancienne de la séquence) est crée comme suit : la valeur minimale de la date (`min()` avec `na.rm=TRUE`) dans la colonne `date_onset` est introduite dans `floor_date()` du paquet **lubridate**. `floor_date()` défini sur "week" renvoie la date de début de la "semaine" de ce cas, étant donné que le jour de début de chaque semaine est un lundi (`week_start = 1`).  
* De même, la valeur "to" (date de fin de la séquence) est créée en utilisant la fonction inverse `ceiling_date()` pour retourner le lundi *aprés* le dernier cas.  
* L'argument "by" de `seq.Date()` peut étre défini sur un nombre quelconque de jours, de semaines ou de mois.   
* Utilisez `week_start = 7` pour les semaines de dimanche.  

Comme nous utiliserons ces vecteurs de date tout au long de cette page, nous en définissons également un pour l'ensemble du foyer (ce qui précéde ne concerne que l'hôpital central).  

```{r}
# Séquence pour l'ensemble du foyer
weekly_breaks_all <- seq.Date(
  from = floor_date(min(linelist$date_onset, na.rm=T), "week", week_start = 1), # lundi avant le premier cas
  to = ceiling_date(max(linelist$date_onset, na.rm=T), "week", week_start = 1), # lundi aprés la derniére affaire
  by = "week")
```

Ces sorties `seq.Date()` peuvent étre utilisées pour créer les ruptures des cases de l'histogramme, mais aussi les ruptures pour les étiquettes de date, qui peuvent étre indépendantes des cases. Vous en saurez plus sur les étiquettes de date dans les sections suivantes.  

<span style="color : darkgreen ;">**_TIP:_** Pour une commande `ggplot()` plus simple, sauvegardez à l'avance les ruptures de bacs et les ruptures d'étiquettes de dates en tant que vecteurs nommés, et fournissez simplement leurs noms à `breaks = `.</span>.  







### Exemple d'épicurve hebdomadaire {.unnumbered}  

**Vous trouverez ci-dessous un exemple de code détaillé pour produire des épicurves hebdomadaires pour les semaines du lundi, avec des barres alignées, des étiquettes de date et des lignes de grille verticales.** Cette section est destinée à l'utilisateur qui a besoin de code rapidement. Pour comprendre en profondeur chaque aspect (thèmes, étiquettes de date, etc.), passez aux sections suivantes. A noter :  

* Les *cassures de l'histogramme* sont définies avec `seq.Date()` comme expliqué ci-dessus pour commencer le lundi avant le premier cas et pour finir le lundi aprés le dernier cas.  
* L'intervalle des *étiquettes de date* est spécifié par `date_breaks =` dans `scale_x_date()`.  
* L'intervalle des petites lignes verticales entre les étiquettes de date est spécifié par `date_minor_breaks = `.  
* `expand = c(0,0)` dans les échelles x et y supprime l'espace excédentaire de chaque côte des axes, ce qui garantit également que les étiquettes de date commencent à partir de la premiére barre.  

```{r, warning=F, message=F}
# ALIGNEMENT TOTAL DE LA SEMAINE DU LUNDI
#############################
# définir la séquence des pauses hebdomadaires
weekly_breaks_central <- seq.Date(
      from = floor_date(min(central_data$date_onset, na.rm=T), "week", week_start = 1), # lundi avant la premiére affaire
      to = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 1), # lundi aprés la derniére affaire
      by = "week")    # les bins sont de 7 jours 


ggplot(data = central_data) + 
  
  # créer un histogramme : spécifier les points de rupture des bacs : commence le lundi avant le premier cas, se termine le lundi aprés le dernier cas
  geom_histogram(
    
    # esthétique de la mapping
    mapping = aes(x = date_onset), # colonne de date mappée sur l'axe des x
    
    # ruptures de la case de l'histogramme
    breaks = weekly_breaks_central, # pauses des cases de l'histogramme définies précédemment
    
    # barres
    color = "darkblue", # couleur des lignes autour des barres
    fill = "lightblue" # couleur de remplissage dans les barres
  )+ 
    
  # étiquettes de l'axe des x
  scale_x_date(
    expand = c(0,0), # suppression de l'espace excédentaire sur l'axe des x avant et aprés les barres de cas
    date_breaks = "4 weeks", # les étiquettes de date et les principales lignes de grille verticales apparaissent toutes les 3 semaines du lundi
    date_minor_breaks = "week", # les lignes verticales mineures apparaissent chaque lundi de semaine
    date_labels = "%a\n%d %b\n%Y")+ # format des étiquettes de date
  
  # Axe des y
  scale_y_continuous(
    expand = c(0,0))+ # suppression de l'espace excédentaire sur l'axe des y en dessous de 0 (alignement de l'histogramme sur l'axe des x)
  
  # thèmes esthétiques
  theme_minimal()+ # simplifie le fond du graphique
  
  theme(
    plot.caption = element_text(hjust = 0, # légende sur le côte gauche
                                face = "italic"), # légende en italique
    axis.title = element_text(face = "bold"))+ # titres des axes en gras
  
  # étiquettes incluant une légende dynamique
  labs(
    title = "Incidence hebdomadaire des cas (semaines de lundi)",
    subtitle = "Notez l'alignement des barres, des lignes de grille verticales et des étiquettes d'axe sur les semaines du lundi",
    x = "Semaine d'apparition des symptômes",
    y = "Incidence hebdomadaire des cas signalés",
    caption = stringr::str_glue("n = {nrow(central_data)} de Central Hospital ; Les occurrences de cas ranges de {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} à {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}{nrow(central_data %>% filter(is.na(date_onset)))} cas dont la date d'apparition est manquante et qui ne sont pas indiqués"))
```


#### Semaines de dimanche {.unnumbered}  

Pour obtenir le graphique ci-dessus pour les semaines de dimanche, quelques modifications sont nécessaires, car les `date_breaks = "weeks"` ne fonctionnent que pour les semaines de lundi.  

* Les points de rupture des *bins de l'histogramme* doivent étre fixés au dimanche (`week_start = 7`)  
* Dans `scale_x_date()`, les ruptures de date similaires doivent étre fournies à `breaks =` et `minor_breaks = ` pour s'assurer que les étiquettes de date et les lignes de grille verticales s'alignent sur les dimanches.  

Par exemple, la commande `scale_x_date()` pour les semaines du dimanche pourrait ressembler à ceci :  

```{r, eval=F}
scale_x_date(
    expand = c(0,0),
    
    # spécifie l'intervalle des étiquettes de date et des principales lignes de grille verticales
    breaks = seq.Date(
      from = floor_date(min(central_data$date_onset, na.rm=T), "week", week_start = 7), # dimanche avant la premiére affaire
      to = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7), # dimanche aprés la derniére affaire
      by = "4 weeks"),
    
    # spécifier l'intervalle de la ligne de grille verticale mineure 
    minor_breaks = seq.Date(
      from = floor_date(min(central_data$date_onset, na.rm=T), "week", week_start = 7), # dimanche avant le premier cas
      to = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7), # dimanche aprés la derniére affaire
      by = "week"),
   
    # format des étiquettes de date
    date_labels = "%a\n%d %b\n%Y")+ # jour, au-dessus abréviation du mois, au-dessus année à 2 chiffres

```



### Grouper/colorer par valeur {.unnumbered}

Les barres de l'histogramme peuvent étre colorées par groupe et "empilées". Pour désigner la colonne de regroupement, effectuez les modifications suivantes. Voir la page [ggplot basics](#ggplot_basics) pour plus de détails.  

* Dans le mappage esthétique de l'histogramme `aes()`, mettez en correspondance le nom de la colonne avec les arguments `group = ` et `fill = `.  
* Supprimez tout argument `fill = ` à l'extérieur de `aes()`, car il remplacera celui qui se trouve à l'intérieur.  
* Les arguments *inside* de `aes()` s'appliqueront *par groupe*, alors que les arguments *outside* s'appliqueront à toutes les barres (par exemple, vous pouvez toujours vouloir `color = ` à l'extérieur, pour que chaque barre ait la même bordure).  

Voici à quoi ressemblerait la commande `aes()` pour grouper et colorer les barres par sexe :  

```{r, eval=F}
aes(x = date_onset, group = gender, fill = gender)
```

Le voici appliqué :  

```{r, warning=F, message=F}
ggplot(data = linelist) + # commencer avec linelist (many hospitals)
  
  # faire un histogramme : spécifier les points de rupture de la benne : commence le lundi avant le premier cas, se termine le lundi aprés le dernier cas
  geom_histogram(
    mapping = aes(
      x = date_onset,
      group = hospital, # définir les données pour qu'elles soient groupées par hôpital
      fill = hospital), # remplissage des barres (couleur intérieure) par hôpital
    
    # les pauses sont les semaines de lundi
    breaks = weekly_breaks_all, # séquence de pauses hebdomadaires du lundi pour toute l'épidémie, définie dans le code précédent 
    
    closed = "left", # Compter les cas à partir du début du point d'arrêt
    # Couleur autour des barres
    color = "black")
```


### Ajuster les couleurs {.unnumbered}  

* Pour *manuellement* régler le remplissage pour chaque groupe, utilisez `scale_fill_manual()` (note : `scale_color_manual()` est différent !).
  * Utilisez l'argument `values = ` pour appliquer un vecteur de couleurs.  
  * Utilisez `na.value = ` pour spécifier une couleur pour les valeurs `NA`.  
  * Utilisez l'argument `labels = ` pour changer le texte des éléments de la légende. Pour étre sur, fournissez un vecteur nommé comme `c("old" = "new", "old" = "new")` ou ajustez les valeurs dans les données elles-mêmes.  
  * Utilisez `name = ` pour donner un titre correct à la légende.  
* Pour plus d'informations sur les échelles et les palettes de couleurs, consultez la page sur [les bases de ggplot](#ggplot_basics).  

```{r, warning=F, message=F}
ggplot(data = linelist)+ # commencer avec linelist (plusieurs hôpitaux)
  
  # faire un histogramme
  geom_histogram(
    mapping = aes(x = date_onset,
        group = hospital, # cas groupés par hôpital
        fill = hospital), # remplissage des barres par hôpital
    
    # bin breaks
    breaks = weekly_breaks_all, # séquence de bin breaks hebdomadaires du lundi, définie dans le code précédent
    
    closed = "left", # Compter les cas à partir du début du point d'arrêt
  
    color = "black")+ # couleur de la bordure de chaque barre
  
  # spécification manuelle des couleurs
  scale_fill_manual(
    values = c("black", "orange", "grey", "beige", "blue", "brown"),
    labels = c("St. Mark's Maternity Hospital (SMMH)" = "St.Mark's"),
    name = "Hospital") # spécifier les couleurs de remplissage ("values") - attention à l'ordre !

```




### Ajuster l'ordre des niveaux {.unnumbered}  

Le meilleur moyen d'ajuster l'ordre dans lequel les barres groupées sont empilées est de classer la colonne de groupage en tant que classe Facteur. Vous pouvez alors désigner l'ordre des niveaux de facteurs (et leurs étiquettes d'affichage). Voir la page sur [facteurs](#factors) ou [ggplot tips](#ggplot_tips) pour plus de détails.  

Avant de réaliser le tracé, utilisez la fonction `fct_relevel()` du paquet **forcats** pour convertir la colonne de regroupement en classe facteur et ajuster manuellement l'ordre des niveaux, comme détaillé dans la page sur les [facteurs](#factors).  

```{r}
# charger le paquet forcats pour travailler avec les facteurs
pacman::p_load(forcats)

# définir un nouvel ensemble de données avec l'hôpital comme facteur
plot_data <- linelist %>% 
  mutate(hospital = fct_relevel(hospital, c("Missing", "Other"))) # Convertir en facteur et définir "Manquant" et "Autre" comme niveaux supérieurs pour apparaétre sur le sommet de l'épicurve.

levels(plot_data$hospital) # Imprime les niveaux dans l'ordre
```

Dans le graphique ci-dessous, les seules différences par rapport au précédent sont que la colonne `hospital` a été consolidée comme ci-dessus, et que nous utilisons `guides()` pour inverser l'ordre de la légende, de sorte que "Missing" se trouve en bas de la légende.  

```{r, warning=F, message=F}
ggplot(plot_data) + # Utiliser le NOUVEL ensemble de données avec les hôpitaux comme facteurs réordonnés.
  
  # créer un histogramme
  geom_histogram(
    mapping = aes(x = date_onset,
        group = hospital, # cas groupés par hôpital
        fill = hospital), # remplissage des barres (couleur) par hôpital
    
    breaks = weekly_breaks_all, # séquence de pauses hebdomadaires du lundi pour toute l'épidémie, définie en haut de la section ggplot
    
    color = "black")+ # couleur de la bordure autour de chaque barre
    
  # étiquettes de l'axe des x
  scale_x_date(
    expand = c(0,0), # supprimer l'espace excédentaire sur l'axe des x avant et aprés les barres de cas
    date_breaks = "3 weeks", # les étiquettes apparaissent toutes les 3 semaines du lundi
    date_minor_breaks = "week", # les lignes verticales apparaissent tous les lundis de la semaine
    date_labels = "%d\n%b\n'%y")+ # format des étiquettes de date
  
  # Axe des y
  scale_y_continuous(
    expand = c(0,0))+ # suppression de l'espace excédentaire sur l'axe des y en dessous de 0
  
  # spécification manuelle des couleurs, ! attention à l'ordre
  scale_fill_manual(
    values = c("grey", "beige", "black", "orange", "blue", "brown"),
    labels = c("St. Mark's Maternity Hospital (SMMH)" = "St.Marks"),
    name = "Hospital")+ 
  
  # thèmes esthétiques
  theme_minimal()+ # simplifier le fond du graphique
  
  theme(
    plot.caption = element_text(face = "italic", # légende à gauche en italique
                                hjust = 0), 
    axis.title = element_text(face = "bold"))+ # titres des axes en gras
  
  # étiquettes
  labs(
    title = "Incidence hebdomadaire des cas par hôpital",
    subtitle = "Hospital as re-ordered factor",
    x = "Semaine d'apparition des symptômes",
    y = "Cas hebdomadaires")
```

<span style="color : darkgreen ;">**_TIP:_** Pour inverser l'ordre de la légende uniquement, ajoutez cette commande **ggplot2** : `guides(fill = guide_legend(reverse = TRUE))`.</span>  





### Ajuster la légende {.unnumbered}

Pour en savoir plus sur les légendes et les échelles, consultez la page [ggplot tips](#ggplot_tips). Voici quelques points saillants :  

* Modifiez le titre de la légende soit dans la fonction d'échelle, soit avec `labs(fill = "Legend title")` (si vous utilisez `color = ` esthétique, alors utilisez `labs(color = "")`).  
* `theme(legend.title = element_blank())` pour ne pas avoir de titre de légende  
* `theme(legend.position = "top")` ("bottom", "left", "right", ou "none" pour supprimer la légende)
* `theme(legend.direction = "horizontal")` légende horizontale 
* `guides(fill = guide_legend(reverse = TRUE))` pour inverser l'ordre de la légende  







### Bars side-by-side {.unnumbered} (barres côte à côte)  

L'affichage côte à côte des barres de groupe (par opposition à l'empilement) est spécifié dans `geom_histogram()` avec `position = "dodge"` placé en dehors de `aes()`.  

S'il y a plus de deux groupes de valeurs, ceux-ci peuvent devenir difficiles à lire. Envisagez plutôt d'utiliser un graphique à facettes (petits multiples). Pour améliorer la lisibilité dans cet exemple, les valeurs de sexe manquantes sont supprimées.  

```{r, warning=F, message=F}
ggplot(central_data %>% drop_na(gender))+ # Commencez par les cas de l'hôpital central en supprimant les valeurs manquantes pour le sexe.
    geom_histogram(
        mapping = aes(
          x = date_onset,
          group = gender, # cas groupés par sexe
          fill = gender), # barres remplies par sexe
        
        # histogramme bin breaks
        breaks = weekly_breaks_central, # séquence de dates hebdomadaires pour le foyer central - définie en haut de la section ggplot
        
        closed = "left",
        
        color = "black", # couleur du bord des barres
        
        position = "dodge")+ # barres SIDE-BY-SIDE
                      
  
  # Les étiquettes sur l'axe des x
  scale_x_date(expand = c(0,0), # supprimer l'espace excédentaire de l'axe des x sous et aprés les barres de cas
               date_breaks = "3 weeks", # les étiquettes apparaissent toutes les 3 semaines du lundi
               date_minor_breaks = "week", # les lignes verticales apparaissent tous les lundis de la semaine
               date_labels = "%d\n%b\n'%y")+ # format des étiquettes de date
  
  # Axe des y
  scale_y_continuous(expand = c(0,0))+ # supprime l'espace excédentaire sur l'axe des y entre le bas des barres et les étiquettes
  
  #échelle des couleurs et des étiquettes de légende
  scale_fill_manual(values = c("brown", "orange"), # spécifie les couleurs de remplissage ("values") - attention à l'ordre !
                    na.value = "grey" )+     

  # thèmes esthétiques
  theme_minimal()+ # un ensemble de thèmes pour simplifier le graphe
  theme(plot.caption = element_text(face = "italic", hjust = 0), # légende à gauche en italique
        axis.title = element_text(face = "bold"))+ # titres des axes en gras
  
  # étiquettes
  labs(title = "Incidence hebdomadaire des cas, par sexe",
       subtitle = "Sous-titre",
       fill = "Gender", # fournir un nouveau titre pour la légende
       x = "Semaine d'apparition des symptômes",
       y = "Incidence hebdomadaire des cas signalés")
```




### Limites de l'axe {.unnumbered}  

Il existe deux façons de limiter l'étendue des valeurs des axes.  

Généralement, la méthode préférée est d'utiliser la commande `coord_cartesian()`, qui accepte `xlim = c(min, max)` et `ylim = c(min, max)` (où vous fournissez les valeurs min et max). Ceci agit comme un "zoom" sans réellement supprimer de données, ce qui est important pour les statistiques et les mesures sommaires.  

Alternativement, vous pouvez définir les valeurs maximales et minimales de la date en utilisant `limits = c()` dans `scale_x_date()`. Par exemple :  

```{r eval=F}
scale_x_date(limits = c(as.Date("2014-04-01"), NA)) # définit une date minimum mais laisse la date maximum ouverte.  
```

Si vous souhaitez que l'axe des abscisses s'tende jusqu'é une date spécifique (par exemple, la date du jour), même si aucun nouveau cas n'a été signalé, vous pouvez utiliser :  

```{r eval=F}
scale_x_date(limits = c(NA, Sys.Date())) # garantit que l'axe des dates s'étendra jusqu'à la date du jour  
```

<span style="color : red ;">**_DANGER:_** Soyez prudent en fixant les ruptures d'échelle ou les limites de l'axe des y (par exemple, 0 à 30 par 5 : `seq(0, 30, 5)`). De tels nombres statiques peuvent couper votre tracé trop court si les données changent pour dépasser la limite !</span>.



### Libellés des axes de date / lignes de grille {.unnumbered} 

<span style="color : darkgreen ;">**_TIP:_** Rappelez-vous que les **étiquettes de l'axe des dates** sont indépendantes de l'agrégation des données en barres, mais visuellement, il peut étre important d'aligner les bacs, les étiquettes de date et les lignes de grille verticales.</span>

Pour **modifier les étiquettes de date et les lignes de grille**, utilisez `scale_x_date()` de l'une de ces façons :  

* **Si vos bins d'histogramme sont des jours, lundi des semaines, des mois ou des années** :  
  * Utilisez `date_breaks = ` pour spécifier l'intervalle des étiquettes et des lignes de grille principales (par exemple "jour", "semaine", "3 semaines", "mois" ou "année").
  * Utilisez `date_minor_breaks = ` pour spécifier l'intervalle des lignes verticales mineures (entre les étiquettes de date)  
  * Ajoutez `expand = c(0,0)` pour que les étiquettes commencent à la premiére barre.  
  * Utilisez `date_labels = ` pour spécifier le format des étiquettes de date - voir la page Dates pour des conseils (utilisez `\n` pour une nouvelle ligne).  
* **Si les cases de votre histogramme sont des semaines de dimanche** :  
  * Utilisez `breaks = ` et `minor_breaks = ` en fournissant une séquence de ruptures de date pour chacun d'entre eux.
  * Vous pouvez toujours utiliser `date_labels = ` et `expand = ` pour le formatage comme décrit ci-dessus.  

Quelques notes :  

* Voir la section ggplot d'ouverture pour des instructions sur la façon de créer une séquence de dates en utilisant `seq.Date()`.  
* Voir [cette page](https://rdrr.io/r/base/strptime.html) ou la page [travailler avec des dates](#working_dates) pour des conseils sur la création d'étiquettes de date.  




#### Démonstrations {.unnumbered}

Vous trouverez ci-dessous une démonstration de tracés où les bacs et les étiquettes/grilles sont alignés et non alignés :  

```{r fig.show='hold', class.source = 'fold-hide', warning=F, message=F}
# Bacs de 7 jours + étiquettes du lundi
#############################
ggplot(central_data) +
  geom_histogram(
    mapping = aes(x = date_onset),
    binwidth = 7, # bins de 7 jours avec début au premier cas
    color = "darkblue",
    fill = "lightblue") +
  
  scale_x_date(
    expand = c(0,0), # supprime l'espace excédentaire sur l'axe des x en dessous et aprés les barres de cas
    date_breaks = "3 weeks", # Lundi toutes les 3 semaines
    date_minor_breaks = "week", # lundi semaines
    date_labels = "%a\n%d\n%b\n'%y")+ # format des étiquettes
  
  scale_y_continuous(
    expand = c(0,0))+ # suppression de l'espace excédentaire sous l'axe des x, mise à plat
  
  labs(
    title = "MAL ALIGNÉ",
    subtitle = " ! ATTENTION : Les barres de 7 jours commencent le jeudi avec le premier cas.\n Grandes lignes de grille et étiquettes de date au 1er de chaque mois.\n Lignes de grille mineures chaque lundi.")



# Tranches de 7 jours + Mois
#####################
ggplot(central_data) +
  geom_histogram(
    mapping = aes(x = date_onset),
    binwidth = 7,
    color = "darkblue",
    fill = "lightblue") +
  
  scale_x_date(
    expand = c(0,0), # supprime l'espace excédentaire de l'axe des x sous et aprés les barres de cas
    date_breaks = "months", # 1er du mois
    date_minor_breaks = "week", # semaines de lundi
    date_labels = "%a\n%d %b\n%Y")+ # format des étiquettes
  
  scale_y_continuous(
    expand = c(0,0)) + # Suppression de l'espace excédentaire sous l'axe des x, mise à plat des données 
  
  labs(
    title = "MAL ALIGNÉ",
    subtitle = " ! ATTENTION : Les barres de 7 jours commencent le jeudi avec le premier cas\nLes lignes de grille principales et les étiquettes de date au 1er de chaque mois\nLes lignes de grille mineures sont hebdomadaires le lundi\nNotez l'espacement inégal de certaines lignes de grille et les tics non alignés avec les barres"
    )


# ALIGNEMENT TOTAL DU LUNDI : spécifier que les pauses manuelles sont des lundis
#################################################################
ggplot(central_data) + 
  geom_histogram(
    mapping = aes(x = date_onset),
    
    # les ruptures d'histogramme sont fixées à 7 jours commençant le lundi avant le premier cas
    breaks = weekly_breaks_central, # défini plus tôt dans cette page
    
    closed = "left",
    
    color = "darkblue",
    
    fill = "lightblue") + 
  
  scale_x_date(
    expand = c(0,0), # supprime l'espace excédentaire sur l'axe x en dessous et aprés les barres de cas
    date_breaks = "4 weeks", # Lundi toutes les 4 semaines
    date_minor_breaks = "week", # lundi semaines 
    date_labels = "%a\n%d %b\n%Y")+ # format des étiquettes
  
  scale_y_continuous(
    expand = c(0,0))+ # Suppression de l'espace excédentaire sous l'axe des x, mise à plat des données 
  
  labs(
    title = "Lundis ALIGNÉS",
    subtitle = "Les intervalles de 7 jours sont réglés manuellement pour commencer le lundi avant le premier cas (28 avr.)\n Les étiquettes de date et les lignes de grille sont aussi sur les lundis")


# ALIGNEMENT TOTAL DES LUNDIS AVEC LES étiQUETTES DE MOIS :
############################################
ggplot(central_data) + 
  geom_histogram(
    mapping = aes(x = date_onset),
    
    # ruptures d'histogramme fixées à 7 jours commençant le lundi avant le premier cas
    breaks = weekly_breaks_central, # défini plus tôt dans cette page
    
    color = "darkblue",
    
    closed = "left",
    
    fill = "lightblue") + 
  
  scale_x_date(
    expand = c(0,0), # supprime l'espace excédentaire sur l'axe x en dessous et aprés les barres de cas
    date_breaks = "months", # Lundi toutes les 4 semaines
    date_minor_breaks = "week", # lundi semaines 
    date_labels = "%b\n%Y")+ # format des étiquettes
  
  scale_y_continuous(
    expand = c(0,0))+ # suppression de l'espace excédentaire sous l'axe des x, mise à plat 
  
  theme(panel.grid.major = element_blank())+ # Suppression des lignes de grille principales (tombent le 1er du mois)
          
  labs(
    title = "Lundis ALIGNÉS avec étiquettes MONTHLY",
    subtitle = "Bacs de 7 jours réglés manuellement pour commencer le lundi avant le premier cas (28 avril) - étiquettes de date le 1er du mois - Suppression des principaux quadrillages mensuels")


# ALIGNEMENT TOTAL DU DIMANCHE : spécifier les ruptures manuelles des bacs ET les étiquettes pour les dimanches
############################################################################
ggplot(central_data) + 
  geom_histogram(
    mapping = aes(x = date_onset),
    
    # ruptures d'histogramme fixées à 7 jours commençant le dimanche avant le premier cas
    breaks = seq.Date(from = floor_date(min(central_data$date_onset, na.rm=T), "week", week_start = 7),
                      to = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7),
                      by = "7 days"),
    
    color = "darkblue",
    
    closed = "left",
    
    fill = "lightblue") + 
  
  scale_x_date(
    expand = c(0,0),
    # les ruptures de l'étiquette de date et les principales lignes de la grille sont fixées à toutes les 3 semaines, en commençant le dimanche avant le premier cas.
    breaks = seq.Date(from = floor_date(min(central_data$date_onset, na.rm=T), "week", week_start = 7),
                      to = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7),
                      by = "3 weeks"),
    
    # grilles mineures fixées à la semaine commençant le dimanche avant le premier cas
    minor_breaks = seq.Date(from = floor_date(min(central_data$date_onset, na.rm=T), "week", week_start = 7),
                            to = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7),
                            by = "7 days"),
    
    date_labels = "%a\n%d\n%b\n'%y")+ # format des étiquettes
  
  scale_y_continuous(
    expand = c(0,0))+ # suppression de l'espace excédentaire sous l'axe des x, mise à plat 
  
  labs(title = "Dimanches ALIGNÉS",
       subtitle = "Les intervalles de 7 jours ont été réglés manuellement pour commencer le dimanche avant le premier cas (27 avril)\n Les étiquettes de date et les lignes de grille ont également été réglées manuellement sur les dimanches")

```





### données agrégées {.unnumbered} 

Souvent, au lieu d'une liste de lignes, vous commencez par des comptages agrégés d'établissements, de districts, etc. Vous pouvez faire une épicurve avec `ggplot()` mais le code sera légérement différent. Cette section va utiliser le jeu de données `count_data` qui a été importé plus tôt, dans la section de préparation des données. Ce jeu de données est la `linelist` agrégée au nombre de jours d'hospitalisation. Les 50 premiéres lignes sont affichôes ci-dessous.  

```{r message=FALSE, warning=F, echo=F}
# affichez les données de la liste de diffusion sous forme de tableau
DT::datatable(head(count_data, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


#### Tracer des épicentres quotidiens {.unnumbered}  

Nous pouvons tracer une épicurve quotidienne à partir de ces *comptes quotidiens*. Voici les différences par rapport au code :  

* Dans le mappage esthétique `aes()`, spécifiez `y = ` comme colonne de comptage (dans ce cas, le nom de la colonne est `n_cases`).
* Ajoutez l'argument `stat = "identity"` dans `geom_histogram()`, qui spécifie que la hauteur de la barre doit étre la valeur `y = `, et non le nombre de lignes comme c'est le cas par défaut.  
* Ajoutez l'argument `width = ` pour éviter les lignes blanches verticales entre les barres. Pour les données quotidiennes, fixez la valeur à 1. Pour les données de comptage hebdomadaire, fixez la valeur à 7. Pour les données de comptage mensuel, les lignes blanches sont un probléme (chaque mois a un nombre de jours différent) - envisagez de transformer votre axe des x en un facteur ordonné catégorique (mois) et utilisez `geom_col()``.


```{r, message=FALSE, warning=F}
ggplot(data = count_data) +
  geom_histogram(
    mapping = aes(x = date_hospitalisation, y = n_cases),
    stat = "identity",
    width = 1) + # pour les comptages quotidiens, définir width = 1 pour éviter les espaces blancs entre les barres
  labs(
    x = "Date du rapport", 
    y = "Nombre de cas",
    title = "Incidence quotidienne des cas, à partir des données de comptage quotidiennes")
```

#### Tracer les comptages hebdomadaires {.unnumbered}

Si vos données sont déjà des comptages de cas par semaine, elles peuvent ressembler à cet ensemble de données (appelé `count_data_weekly`) :  

```{r, warning=F, message=F, echo=F}
# créer un jeu de données hebdomadaire avec la colonne epiweek
count_data_weekly <- count_data %>%
  mutate(epiweek = lubridate::floor_date(date_hospitalisation, "week")) %>% 
  group_by(hospital, epiweek, .drop=F) %>% 
  summarize(n_cases_weekly = sum(n_cases, na.rm=T))   
```

Les 50 premiéres lignes de `count_data_weekly` sont affichées ci-dessous. Vous pouvez voir que les comptes ont été agrégés en semaines. Chaque semaine est affichée par le premier jour de la semaine (lundi par défaut).  

```{r message=FALSE, echo=F}
# affiche les données de la liste des lignes sous forme de tableau
DT::datatable(count_data_weekly, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Maintenant, tracez le graphique de façon à ce que `x = ` la colonne epiweek. N'oubliez pas d'ajouter `y = ` la colonne des comptes à la mapping esthétique, et ajoutez `stat = "identity"` comme expliqué ci-dessus.  

```{r, warning=F, message=F}
ggplot(data = count_data_weekly)+
  
  geom_histogram(
    mapping = aes(
      x = epiweek, # l'axe des x est l'epiweek (en tant que classe Date)
      y = n_cases_weekly, # l'axe des y est la hauteur du nombre de cas hebdomadaires
      group = hospital, # nous regroupons les barres et les couleurs par hôpital
      fill = hospital),
    stat = "identity")+ # ceci est également nécessaire lorsque l'on trace des données de comptage
     
  # étiquettes pour l'axe des x
  scale_x_date(
    date_breaks = "2 months", # étiquettes tous les 2 mois 
    date_minor_breaks = "1 month", # grilles tous les mois
    date_labels = '%b\n%Y')+ # étiqueté par mois avec l'année en dessous
     
  # Choisissez la palette de couleurs (utilise le paquet RColorBrewer)
  scale_fill_brewer(palette = "Pastel2")+ 
  
  theme_minimal()+
  
  labs(
    x = "Semaine d'apparition", 
    y = "Incidence hebdomadaire des cas",
    fill = "Hospital",
    title = "Incidence hebdomadaire des cas, à partir des données de comptage agrégées par hôpital")
```




### Moyennes mobiles {.unnumbered}

Voir la page sur les [Moyennes mobiles](#moving_average) pour une description détaillée et plusieurs options. Vous trouverez ci-dessous une option pour calculer des moyennes mobiles avec le package **slider**. Dans cette approche, *la moyenne mobile est calculée dans l'ensemble de données avant le tracé* :  

1) Regroupez les données en comptes si nécessaire (quotidien, hebdomadaire, etc.) (voir la page sur [groupage des données](#grouping_data)).  
2) créez une nouvelle colonne pour contenir la moyenne mobile, créée avec `slide_index()` du paquet **slider**.  
3) Tracez la moyenne mobile comme une `geom_line()` au-dessus (aprés) l'histogramme épicurvien  

Voir l'utile [vignette pour le paquet **slider**](https://cran.r-project.org/web/packages/slider/vignettes/slider.html)  


```{r, warning=F, message=F}
# charger le paquet
pacman::p_load(slider) # slider utilisé pour calculer les moyennes mobiles

# créer un jeu de données de comptages quotidiens et de moyennes mobiles sur 7 jours
#######################################################
ll_counts_7day <- linelist %>% # commencer avec linelist
  
  ## compter les cas par date
  count(date_onset, name = "new_cases") %>% # nommer une nouvelle colonne avec les comptages comme "new_cases".
  drop_na(date_onset) %>% # supprime les cas dont la date_onset est manquante
  
  ## calculer le nombre moyen de cas dans la fenétre de 7 jours
  mutate(
    avg_7day = slider::slide_index( # créer une nouvelle colonne
      new_cases, # calcul basé sur la valeur de la colonne new_cases
      .i = date_onset, # l'index est la colonne date_onset, donc les dates non présentes sont incluses dans la fenétre 
      .f = ~mean(.x, na.rm = TRUE), # La fonction est mean() avec les valeurs manquantes supprimées.
      .before = 6, # la fenétre est le jour et les 6 jours précédents
      .complete = FALSE), # doit étre FALSE pour que unlist() fonctionne à l'étape suivante
    avg_7day = unlist(avg_7day)) # convertit la liste des classes en classes numériques


# tracer
######
ggplot(data = ll_counts_7day) + # commencer avec le nouvel ensemble de données défini ci-dessus 
    geom_histogram( # crée un histogramme épicurve
      mapping = aes(
        x = date_onset, # colonne de date comme axe des x
        y = new_cases), # la hauteur est le nombre de nouveaux cas quotidiens
        stat = "identity", # la hauteur est la valeur y
        fill="#92a8d1", # couleur froide pour les barres
        colour = "#92a8d1", # même couleur pour la bordure des barres
        )+ 
    geom_line( # créer une ligne pour la moyenne mobile
      mapping = aes(
        x = date_onset, # colonne de date pour l'axe des x
        y = avg_7day, # valeur y définie dans la colonne de la moyenne mobile
        lty = "7-day \nrolling avg"), # nom de la ligne dans la légende
      color="red", # couleur de la ligne
      size = 1) + # largeur de la ligne
    scale_x_date( # échelle de date
      date_breaks = "1 month",
      date_labels = '%d/%m',
      expand = c(0,0)) +
    scale_y_continuous( # échelle de l'axe des y
      expand = c(0,0),
      limits = c(0, NA)) +       
    labs(
      x="",
      y = "Nombre de cas confirmés",
      fill = "Legende")+ 
    theme_minimal()+
    theme(legend.title = element_blank()) # supprime le titre de la légende
```




### Facettes/petits-multiples {.unnumbered}

Comme pour les autres ggplots, vous pouvez créer des graphiques à facettes ("petits multiples"). Comme expliqué dans la page [ggplot tips](#ggplot_tips) de ce manuel, vous pouvez utiliser soit `facet_wrap()` soit `facet_grid()`. Ici, nous faisons une démonstration avec `facet_wrap()`. Pour les épicurves, `facet_wrap()` est typiquement plus facile car il est probable que vous n'ayez besoin de faire une facette que sur une seule colonne.  

La syntaxe générale est `facet_wrap(rows ~ cols)`, où à gauche du tilde (~) est le nom d'une colonne à répartir sur les "rows" du graphique à facettes, et à droite du tilde est le nom d'une colonne à répartir sur les "columns" du graphique à facettes. Plus simplement, il suffit d'utiliser un seul nom de colonne, à droite du tilde : `facet_wrap(~age_cat)`.  


**Axes libres**  
Vous devrez décider si les échelles des axes pour chaque facette sont "fixées" aux mêmes dimensions (par défaut), ou "libres" (ce qui signifie qu'elles changeront en fonction des données de la facette). Faites-le avec l'argument `scales = ` dans `facet_wrap()` en spécifiant "free_x" ou "free_y", ou "free".  


**Nombre de cols et de rangs de facettes**  
Cela peut étre spécifié avec `ncol = ` et `nrow = ` dans `facet_wrap()`. 


**Ordre des facettes**  
Pour changer l'ordre d'apparition, changez l'ordre sous-jacent des niveaux de la colonne de facteurs utilisée pour créer les facettes.  


**esthétique**  
La taille et le visage de la police, la couleur de la bande, etc. peuvent étre modifiés par `theme()` avec des arguments comme :  

* `strip.text = element_text()` (taille, couleur, face, angle...)
* `strip.background = element_rect()` (par exemple element_rect(fill="grey"))  
* `strip.position = ` (position de la bande "bas", "haut", "gauche" ou "droite")  


**Libellés des bandes**  
Les étiquettes des graphiques à facettes peuvent étre modifiées par les "étiquettes" de la colonne comme facteur, ou par l'utilisation d'un "labeller".  

Faites une étiquette comme celle-ci, en utilisant la fonction `as_labeller()` de **ggplot2**. Puis fournissez l'étiqueteuse à l'argument `labeller = ` de `facet_wrap()` comme indiqué ci-dessous.  

```{r, class.source = 'fold-show'}
my_labels <- as_labeller(c(
     "0-4" = "0-4 ans",
     "5-9" = "5-9 ans",
     "10-14" = "10-14 ans",
     "15-19" = "15-19 ans",
     "20-29" = "20-29 ans",
     "30-49" = "30-49 ans",
     "50-69" = "50-69 ans",
     "70+" = "Plus de 70 ans"))
```

**Un exemple de graphique à facettes** - facetté par la colonne `age_cat`.


```{r, warning=F, message=F}
# faire le graphe
###########
ggplot(central_data) + 
  
  geom_histogram(
    mapping = aes(
      x = date_onset,
      group = age_cat,
      fill = age_cat), # les arguments à l'intérieur de aes() s'appliquent par groupe
      
    color = "black", # les arguments hors aes() s'appliquent à toutes les données
        
    # ruptures d'histogramme
    breaks = weekly_breaks_central)+ # vecteur de date prédéfini (voir plus haut dans cette page)
                      
  # Les étiquettes sur l'axe des x
  scale_x_date(
    expand = c(0,0), # supprimez l'espace excédentaire sur l'axe des x en dessous et aprés les barres de cas
    date_breaks = "2 months", # les étiquettes apparaissent tous les 2 mois
    date_minor_breaks = "1 month", # les lignes verticales apparaissent tous les 1 mois 
    date_labels = "%b\n'%y")+ # format des étiquettes de date
  
  # Axe des y
  scale_y_continuous(expand = c(0,0))+ # supprime l'espace excédentaire sur l'axe des y entre le bas des barres et les étiquettes
  
  # thèmes esthétiques
  theme_minimal()+ # un ensemble de thèmes pour simplifier le graphique
  theme(
    plot.caption = element_text(face = "italic", hjust = 0), # légende à gauche en italique
    axis.title = element_text(face = "bold"),
    legend.position = "bottom",
    strip.text = element_text(face = "bold", size = 10),
    strip.background = element_rect(fill = "grey"))+ # titres des axes en gras
  
  # créer des facettes
  facet_wrap(
    ~age_cat,
    ncol = 4,
    strip.position = "top",
    labeller = my_labels)+             
  
  # étiquettes
  labs(
    title = "Incidence hebdomadaire des cas, par catégorie d'âge",
    subtitle = "Sous-titre",
    fill = "Catégorie d'âge", # fournir un nouveau titre pour la légende
    x = "Semaine d'apparition des symptômes",
    y = "Cas incidents hebdomadaires signalés",
    caption = stringr::str_glue("n = {nrow(central_data)} from Central Hospital ; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} à {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}{nrow(central_data %>% filter(is.na(date_onset)))} cas dont la date d'apparition est manquante et qui ne sont pas indiqués"))
```

Voir ce [lien](https://ggplot2.tidyverse.org/reference/labellers.html) pour plus d'informations sur les étiqueteuses.  




#### Épidémie totale dans l'arriére-plan de la facette {.nonnumbered}

Pour afficher l'épidémie totale en arriére-plan de chaque facette, ajoutez la fonction `gghighlight()` avec des parenthôses vides au ggplot. Cette fonction provient du paquet **gghighlight**. Notez que le maximum de l'axe des y dans toutes les facettes est maintenant basé sur le pic de l'épidémie entiére. Il y a plus d'exemples de ce package dans la page [ggplot tips](#ggplot_tips).  

```{r, warning=F, message=F}
ggplot(central_data) + 
  
  # Epicurves par groupe
  geom_histogram(
    mapping = aes(
      x = date_onset,
      group = age_cat,
      fill = age_cat), # les arguments à l'intérieur de aes() s'appliquent par groupe
    
    color = "black", # les arguments hors aes() s'appliquent à toutes les données
    
    # ruptures d'histogramme
    breaks = weekly_breaks_central)+ # vecteur de dates prédéfini (voir en haut de la section ggplot)                
  
  # ajoute une épidémie grise en arriére-plan à chaque facette
  gghighlight::gghighlight()+
  
  # étiquettes sur l'axe des x
  scale_x_date(
    expand = c(0,0), # Suppression de l'espace excédentaire sur l'axe des x sous et aprés les barres de cas
    date_breaks = "2 months", # les étiquettes apparaissent tous les 2 mois
    date_minor_breaks = "1 month", # les lignes verticales apparaissent tous les 1 mois 
    date_labels = "%b\n'%y")+ # format des étiquettes de date
  
  # Axe des y
  scale_y_continuous(expand = c(0,0))+ # supprime l'espace excédentaire de l'axe des y en dessous de 0
  
  # thèmes esthétiques
  theme_minimal()+ # un ensemble de thèmes pour simplifier le graphique
  theme(
    plot.caption = element_text(face = "italic", hjust = 0), # légende à gauche en italique
    axis.title = element_text(face = "bold"),
    legend.position = "bottom",
    strip.text = element_text(face = "bold", size = 10),
    strip.background = element_rect(fill = "white"))+ # titres des axes en gras
  
  # créer des facettes
  facet_wrap(
    ~age_cat, # chaque facette est une valeur de age_cat
    ncol = 4, # nombre de colonnes
    strip.position = "top", # position du titre/strip de la facette
    labeller = my_labels)+ # labeller définit ci-dessus
  
  # étiquettes
  labs(
    title = "Incidence hebdomadaire des cas, par catégorie d'âge",
    subtitle = "Sous-titre",
    fill = "Catégorie d'âge", # fournit un nouveau titre pour la légende
    x = "Semaine d'apparition des symptômes",
    y = "Cas incidents hebdomadaires signalés",
    caption = stringr::str_glue("n = {nrow(central_data)} from Central Hospital ; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} à {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}{nrow(central_data %>% filter(is.na(date_onset)))} cas dont la date d'apparition est manquante et qui ne sont pas indiqués"))
```


#### Une facette avec des données {.unnumbered}  

Si vous voulez avoir une seule boîte à facettes qui contient toutes les données, dupliquez l'ensemble des données et traitez les doublons comme une seule valeur de facette. La fonction "helper" `CreateAllFacet()` ci-dessous peut vous aider (gràce à cet [article de blog](https://stackoverflow.com/questions/18933575/easily-add-an-all-facet-to-facet-wrap-in-ggplot2)). Quand elle est exécutée, le nombre de lignes double et il y aura une nouvelle colonne appelée `facet` dans laquelle les lignes dupliquées auront la valeur "all", et les lignes originales auront la valeur originale de la colonne de facettes. Il ne vous reste plus qu'à effectuer la facette sur la colonne `facet`.   

Voici la fonction d'aide. Exécutez-la pour qu'elle soit disponible pour vous.  

```{r}
# définir la fonction d'aide
CreateAllFacet <- function(df, col){
     df$facet <- df[[col]]
     temp <- df
     temp$facet <- "all"
     merged <-rbind(temp, df)
     
     # s'assurer que la valeur de la facette est un facteur
     merged[[col]] <- as.factor(merged[[col]])
     
     return(merged)
}
```

Appliquez maintenant la fonction d'aide à l'ensemble de données, sur la colonne `age_cat` :  

```{r}
# créez un jeu de données dupliqué et avec une nouvelle colonne "facet" pour afficher "toutes" les catégories d'âge comme autre niveau de facette.
central_data2 <- CreateAllFacet(central_data, col = "age_cat") %>%
  
  # définir les niveaux de facteurs
  mutate(facet = fct_relevel(facet, "all", "0-4", "5-9",
                             "10-14", "15-19", "20-29",
                             "30-49", "50-69", "70+"))

# vérifier les niveaux
table(central_data2$facet, useNA = "always")
```

Les changements notables de la commande `ggplot()` sont les suivants :  

* Les données utilisées sont maintenant central_data2 (deux fois plus de lignes, avec une nouvelle colonne "facet").
* L'étiqueteuse devra étre mise à jour, si elle est utilisée.  
* Optionnel : pour obtenir des facettes empilées verticalement : la colonne facette est déplacée vers les lignes de l'équation et remplacée à droite par "." (`facet_wrap(facet~.)`), et `ncol = 1`. Vous pouvez aussi avoir besoin d'ajuster la largeur et la hauteur de l'image du graphique en png (voir `ggsave()` dans [ggplot tips](#ggplot_tips)).  

```{r, fig.height=12, fig.width=5, warning=F, message=F}
ggplot(central_data2) + 
  
  # Epicurves réelles par groupe
  geom_histogram(
        mapping = aes(
          x = date_onset,
          group = age_cat,
          fill = age_cat), # les arguments à l'intérieur de aes() s'appliquent par groupe
        color = "black", # les arguments hors aes() s'appliquent à toutes les données
        
        # ruptures d'histogramme
        breaks = weekly_breaks_central)+ # vecteur de dates prédéfini (voir en haut de la section ggplot)
                     
  # étiquettes sur l'axe des x
  scale_x_date(
    expand = c(0,0), # supprime l'espace excédentaire de l'axe des x sous et aprés les barres de cas
    date_breaks = "2 months", # les étiquettes apparaissent tous les 2 mois
    date_minor_breaks = "1 month", # les lignes verticales apparaissent tous les 1 mois 
    date_labels = "%b\n'%y")+ # format des étiquettes de date
  
  # Axe des y
  scale_y_continuous(expand = c(0,0))+ # supprime l'espace excédentaire sur l'axe des y entre le bas des barres et les étiquettes
  
  # thèmes esthétiques
  theme_minimal()+ # un ensemble de thèmes pour simplifier le graphique
  theme(
    plot.caption = element_text(face = "italic", hjust = 0), # légende à gauche en italique
    axis.title = element_text(face = "bold"),
    legend.position = "bottom")+               
  
  # créer des facettes
  facet_wrap(facet~. , # chaque parcelle est une valeur de la facette
             ncol = 1)+            

  # étiquettes
  labs(title = "Incidence hebdomadaire des cas, par catégorie d'âge",
       subtitle = "Sous-titre",
       fill = "Catégorie d'âge", # fournit un nouveau titre pour la légende
       x = "Semaine d'apparition des symptômes",
       y = "Cas incidents hebdomadaires signalés",
       caption = stringr::str_glue("n = {nrow(central_data)} from Central Hospital ; Case onsets range from
                                   {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} à
                                   {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\n{nrow(central_data %>% filter(is.na(date_onset)))} cas dont la date d'apparition est manquante et qui ne sont pas indiqués"))

```








## Données provisoires  


Les données les plus récentes présentées dans les épicurves doivent souvent étre marquées comme provisoires, ou sujettes à des retards de déclaration. Ceci peut étre fait en ajoutant une ligne verticale et/ou un rectangle sur un nombre de jours spécifié. Voici deux options :  

1) Utilisez `annotate()` :  
    + Pour une ligne, utilisez `annotate(geom = "segment")`. Fournissez `x`, `xend`, `y`, et `yend`. Ajustez la taille, le type de ligne (`lty`) et la couleur.  
    + Pour un rectangle, utilisez `annotate(geom = "rect")`. Fournissez xmin/xmax/ymin/ymax. Ajustez la couleur et l'alpha.  
2) Regroupez les données par statut provisoire et colorez ces barres différemment.  

<span style="color : orange ;">**_CAUTION:_** Vous pourriez essayer `geom_rect()` pour dessiner un rectangle, mais l'ajustement de la transparence ne fonctionne pas dans un contexte de linelist. Cette fonction superpose un rectangle pour chaque observation/rangée ! Utilisez soit un alpha trés faible (par exemple 0,01), soit une autre approche. </span>

### Utilisation de `annotate()` {.unnumbered}

* Dans `annotate(geom = "rect")`, les arguments `xmin` et `xmax` doivent recevoir des entrées de la classe Date.  
* Notez que, comme ces données sont agrégées en barres hebdomadaires et que la derniére barre s'étend jusqu'au lundi suivant le dernier point de données, la région ombrée peut sembler couvrir 4 semaines.  
* Voici un `annotate()` [exemple en ligne](https://ggplot2.tidyverse.org/reference/annotate.html)


```{r, warning=F, message=F}
ggplot(central_data) + 
  
  # histogramme
  geom_histogram(
    mapping = aes(x = date_onset),
    
    breaks = weekly_breaks_central, # vecteur de date prédéfini - voir en haut de la section ggplot
    
    color = "darkblue",
    
    closed = "left",
    
    fill = "lightblue") +

  # échelles
  scale_y_continuous(expand = c(0,0))+
  scale_x_date(
    expand = c(0,0), # Suppression de l'espace excédentaire sur l'axe des x sous et aprés les barres de cas
    date_breaks = "1 month", # 1er du mois
    date_minor_breaks = "1 month", # 1er du mois
    date_labels = "%b\n'%y")+ # format des étiquettes
  
  # étiquettes et theme
  labs(
    title = "Utilisant annotate()\nRectangle et ligne montrant que les données des 21 derniers jours sont provisoires",
    x = "Semaine d'apparition des symptômes",
    y = "Indication hebdomadaire des cas")+ 
  theme_minimal()+
  
  # ajoute un rectangle rouge semi-transparent aux données provisoires
  annotate(
    "rect",
    xmin = as.Date(max(central_data$date_onset, na.rm = T) - 21), # la note doit étre enveloppée dans as.Date()
    xmax = as.Date(Inf), # la note doit étre enveloppée dans as.Date()
    ymin = 0,
    ymax = Inf,
    alpha = 0.2, # alpha facile et intuitif à ajuster en utilisant annotate()
    fill = "red")+
  
  # ajoute une ligne verticale noire au-dessus des autres couches
  annotate(
    "segment",
    x = max(central_data$date_onset, na.rm = T) - 21, # 21 jours avant les derniéres données
    xend = max(central_data$date_onset, na.rm = T) - 21, 
    y = 0, # la ligne commence à y = 0
    yend = Inf, # ligne jusqu'au sommet du graphique
    size = 2, # taille de la ligne
    color = "black",
    lty = "solid")+ # type de ligne, par exemple "solid", "dashed".

  # ajouter du texte dans le rectangle
  annotate(
    "text",
    x = max(central_data$date_onset, na.rm = T) - 15,
    y = 15,
    label = "Sujet à des délais de déclaration",
    angle = 90)
```


La même ligne verticale noire peut étre obtenue avec le code ci-dessous, mais en utilisant `geom_vline()` vous perdez la possibilité de contrôler la hauteur :  

```{r, eval=F}
geom_vline(xintercept = max(central_data$date_onset, na.rm = T) - 21,
           size = 2,
           color = "black")
```



### Couleur des barres {.unnumbered}  

Une autre approche pourrait consister à ajuster la couleur ou l'affichage des barres de données provisoires elles-mêmes. Vous pouvez créer une nouvelle colonne dans l'étape de préparation des données et l'utiliser pour regrouper les données, de sorte que les " aes(fill = )`" des données provisoires puissent avoir une couleur ou un alpha différent des autres barres. 

```{r, message=F, warning=F}
# ajouter une colonne
############
plot_data <- central_data %>% 
  mutate(tentative = case_when(
    date_onset >= max(date_onset, na.rm=T) - 7 ~ "Tentative", # tenative si dans les 7 derniers jours
    TRUE ~ "Reliable")) # tout le reste est fiable

# tracé
######
ggplot(plot_data, aes(x = date_onset, fill = tentative)) + 
  
  # histogramme
  geom_histogram(
    breaks = weekly_breaks_central, # vecteur de données prédéfini, voir en haut de la page ggplot
    color = "black") +

  # échelles
  scale_y_continuous(expand = c(0,0))+
  scale_fill_manual(values = c("lightblue", "grey"))+
  scale_x_date(
    expand = c(0,0), # Supprimez l'espace excédentaire de l'axe des x sous et aprés les barres de cas
    date_breaks = "3 weeks", # Lundi toutes les 3 semaines
    date_minor_breaks = "week", # lundi semaines 
    date_labels = "%d\n%b\n'%y")+ # format des étiquettes
  
  # étiquettes et theme
  labs(title = "Afficher les jours de déclaration provisoire",
    subtitle = "")+ 
  theme_minimal()+
  theme(legend.title = element_blank()) # supprimer le titre de la légende
  
```


## Étiquettes de date à plusieurs niveaux  

Si vous voulez des étiquettes de date à plusieurs niveaux (par exemple, le mois et l'année) *sans dupliquer les niveaux d'étiquette inférieurs*, envisagez l'une des approches ci-dessous :  

Rappelez-vous - vous pouvez utiliser des outils comme `\n` *avec* les arguments `date_labels` ou `labels` pour mettre des parties de chaque étiquette sur une nouvelle ligne en dessous. Cependant, le code ci-dessous vous aide à prendre les années ou les mois (par exemple) sur une ligne inférieure *et seulement une fois*. Quelques notes sur le code ci-dessous :  



* Le nombre de cas est agrégé en semaines pour des raisons esthétiques. Voir la page Epicurves (onglet données agrégées) pour plus de détails.  
* Une ligne `geom_area()` est utilisée au lieu d'un histogramme, car l'approche par facettes ci-dessous ne fonctionne pas bien avec les histogrammes.  


**Aggréger en comptages hebdomadaires**

```{r out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F}

# créez un ensemble de données sur le nombre de cas par semaine.
#######################################
central_weekly <- linelist %>%
  filter(hospital == "Central Hospital") %>% # Filtrer linelist
  mutate(week = lubridate::floor_date(date_onset, unit = "weeks")) %>% # count(week) %>% # count(week)  
  count(week) %>% # résume le nombre de cas hebdomadaires
  drop_na(week) %>% # Suppression des cas dont la date d'apparition est manquante
  complete( # remplir toutes les semaines où aucun cas n'a été signalé
    week = seq.Date(
      from = min(week),   
      to = max(week),
      by = "week"),
    fill = list(n = 0))                        # convertir les nouvelles valeurs NA en comptage 0
```

**Faire des graphiques**  

```{r, warning=F, message=F}
# tracer avec une bordure de boîte sur l'année
##############################
ggplot(central_weekly) +
  geom_area(aes(x = week, y = n), # faire une ligne, spécifier x et y
            stat = "identity") + # car la hauteur de la ligne est un nombre compté
  scale_x_date(date_labels="%b", # le format des étiquettes de date montre le mois 
               date_breaks="month", # étiquettes de date au 1er de chaque mois
               expand=c(0,0)) + # suppression de l'espace excédentaire à chaque extrémité
  scale_y_continuous(
    expand = c(0,0))+ # Suppression de l'espace excédentaire sous l'axe des x
  facet_grid(~lubridate::year(week), # facette sur l'année (de la colonne de la classe Date)
             space="free_x",                
             scales="free_x", # les axes x s'adaptent à la plage de données (pas "fixe")
             switch="x") + # étiquettes des facettes (année) en bas de page
  theme_bw() +
  theme(strip.placement = "outside", # placement des étiquettes de facettes
        strip.background = element_rect(fill = NA, # facet labels no fill grey border
                                        color = "gray50"),
        panel.spacing = unit(0, "cm"))+ # pas d'espace entre les panneaux de facettes
  labs(title = "Étiquettes d'année imbriquées, bordure d'étiquette grise")


# tracé sans bordure sur l'année
#################################
ggplot(central_weekly,
       aes(x = week, y = n)) + # établir x et y pour tout le graphique
  geom_line(stat = "identity", # créer une ligne, la hauteur de la ligne est le nombre de comptage
            color = "#69b3a2") + # couleur de la ligne
  geom_point(size=1, color="#69b3a2") + # faire des points aux points de données hebdomadaires
  geom_area(fill = "#69b3a2", # zone de remplissage sous la ligne
            alpha = 0.4)+ # transparence du remplissage
  scale_x_date(date_labels="%b", # format de l'étiquette de la date pour montrer le mois 
               date_breaks="month", # étiquettes de date au 1er de chaque mois
               expand=c(0,0)) + # supprimer l'espace excédentaire
  scale_y_continuous(
    expand = c(0,0))+ # suppression de l'espace excédentaire sous l'axe des x
  facet_grid(~lubridate::year(week), # facette sur l'année (de la colonne de la classe Date)
             space="free_x",                
             scales="free_x", # les axes x s'adaptent à la plage de données (pas "fixe")
             switch="x") + # étiquettes des facettes (année) en bas de page
  theme_bw() +
  theme(strip.placement = "outside", # placement des étiquettes de facettes
          strip.background = element_blank(), # pas de fond d'étiquette de facette
          panel.grid.minor.x = element_blank(),          
          panel.border = element_rect(color="grey40"), # bordure grise du PANEL de la facette
          panel.spacing=unit(0, "cm"))+ # Pas d'espace entre les panneaux à facettes
  labs(title = "Étiquettes annuelles imbriquées - points, ombrées, pas de bordure d'étiquette")
```

Les techniques ci-dessus ont été adaptées de [this](https://stackoverflow.com/questions/44616530/axis-labels-on-two-lines-with-nested-x-variables-year-below-months) et [this](https://stackoverflow.com/questions/20571306/multi-row-x-axis-labels-in-ggplot-line-chart) post sur stackoverflow.com.  






<!-- ======================================================= -->
## Double axe { }  

Bien qu'il y ait des discussions acharnées sur la validité des doubles axes au sein de la communauté de visualisation des données, de nombreux superviseurs d'épi veulent toujours voir une épicurve ou un graphique similaire avec un pourcentage superposé à un deuxiéme axe. Ce sujet est abordé plus en détail dans la page [ggplot tips](#ggplot_tips), mais un exemple utilisant la méthode **cowplot** est présenté ci-dessous :  

* Deux graphiques distincts sont créés, puis combinés avec le paquet **cowplot**.  
* Les graphiques doivent avoir exactement le même axe des x (limites définies), sinon les données et les étiquettes ne seront pas alignées.  
* Chacun utilise `theme_cowplot()` et l'un d'entre eux a l'axe des y déplacé sur le côte droit du graphique  

```{r, warning=F, message=F}
#Chargez le paquet
pacman::p_load(cowplot)

# Faire le premier tracé de l'histogramme épicurve
#######################################
plot_cases <- linelist %>% 
  
  # Tracer les cas par semaine
  ggplot()+
  
  # créer un histogramme  
  geom_histogram(
    
    mapping = aes(x = date_onset),
    
    # bin breaks chaque semaine en commençant le lundi avant le premier cas, jusqu'au lundi aprés le dernier cas
    breaks = weekly_breaks_all)+ # vecteur prédéfini de dates hebdomadaires (voir en haut de la section ggplot)
        
  # spécifier le début et la fin de l'axe des dates pour l'aligner avec les autres graphiques
  scale_x_date(
    limits = c(min(weekly_breaks_all), max(weekly_breaks_all)))+ # min/max des pauses hebdomadaires prédéfinies de l'histogramme
  
  # étiquettes
  labs(
      y = "Cas quotidiens",
      x = "Date d'apparition des symptômes"
    )+
  theme_cowplot()


# faire un second tracé du pourcentage de décés par semaine
###########################################
plot_deaths <- linelist %>% # commence avec linelist
  group_by(week = floor_date(date_onset, "week")) %>% # créer une colonne semaine
  
  # résumer pour obtenir le pourcentage hebdomadaire de cas décédés
  summarise(n_cases = n(),
            died = sum(outcome == "Death", na.rm=T),
            pct_died = 100*died/n_cases) %>% 
  
  # commencer le tracé
  ggplot()+
  
  # ligne du pourcentage hebdomadaire de décés
  geom_line( # créer une ligne de pourcentage de décés
    mapping = aes(x = week, y = pct_died), # spécifie la hauteur des y comme colonne pct_died
    stat = "identity", # fixer la hauteur de la ligne à la valeur de la colonne pct_death, et non au nombre de lignes (par défaut)
    size = 2,
    color = "black")+
  
  # mêmes limites de l'axe des dates que l'autre graphique - alignement parfait
  scale_x_date(
    limits = c(min(weekly_breaks_all), max(weekly_breaks_all)))+ # min/max des pauses hebdomadaires prédéfinies de l'histogramme
  
  
  # ajustements de l'axe des y
  scale_y_continuous( # ajuster l'axe des y
    breaks = seq(0,100, 10), # définit les intervalles de rupture de l'axe des pourcentages
    limits = c(0, 100), # définit l'étendue de l'axe des pourcentages
    position = "right")+ # déplace l'axe des pourcentages vers la droite
  
  # étiquette pour l'axe des Y, pas d'étiquette pour l'axe des X
  labs(x = "",
       y = "Pourcentage décédé") +  # étiquette de l'axe des pourcentages
  
  theme_cowplot() # ajoutez ceci pour que les deux graphiques fusionnent bien ensemble
```

Utilisez maintenant **cowplot** pour superposer les deux graphiques. Une attention particuliére a été portée à l'alignement de l'axe des x, au côte de l'axe des y et à l'utilisation de `theme_cowplot()`.  

```{r, warning=F, message=F}

aligned_plots <- cowplot::align_plots(plot_cases, plot_deaths, align="hv", axis="tblr")

ggdraw(aligned_plots[[1]]) + draw_plot(aligned_plots[[2]])

```




## Incidence cumulée {}

Note : Si vous utilisez **incidence2**, consultez la section sur la façon dont vous pouvez produire l'incidence cumulée avec une fonction simple. Cette page traitera de la façon de calculer l'incidence cumulée et de la tracer avec `ggplot()`.  

Si vous commencez avec une liste de cas, créez une nouvelle colonne contenant le nombre cumulé de cas par jour dans une épidémie en utilisant `cumsum()` de **base** R :    

```{r}
cumulative_case_counts <- linelist %>% 
  count(date_onset) %>% # nombre de lignes par jour (retourné dans la colonne "n")   
  mutate(                         
    cumulative_cases = cumsum(n) # nouvelle colonne du nombre cumulé de lignes à chaque date
    )
```

Les 10 premiéres lignes sont affichôes ci-dessous :  

```{r message=FALSE, echo=F}
# affichez les données de la liste des lignes sous forme de tableau
DT::datatable(head(cumulative_case_counts, 10), rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```



Cette colonne cumulative peut ensuite étre tracée en fonction de la `date_onset`, en utilisant `geom_line()`` :

```{r, warning=F, message=F}
plot_cumulative <- ggplot()+
  geom_line(
    data = cumulative_case_counts,
    aes(x = date_onset, y = cumulative_cases),
    size = 2,
    color = "blue")

plot_cumulative
```


On peut aussi le superposer à l'épicurve, avec un double axe en utilisant la méthode **cowplot** décrite ci-dessus et dans la page [ggplot tips](#ggplot_tips) :

```{r, warning=F, message=F}
#charger le paquet

pacman::p_load(cowplot)


# Faire le premier tracé de l'histogramme épicurve
plot_cases <- ggplot()+
  geom_histogram(          
    data = linelist,
    aes(x = date_onset),
    binwidth = 1)+
  labs(
    y = "Cas quotidiens",
    x = "Date d'apparition des symptômes"
  ) +
  theme_cowplot()

# créer un second tracé de la ligne des cas cumulés
plot_cumulative <- ggplot()+
  geom_line(
    data = cumulative_case_counts,
    aes(x = date_onset, y = cumulative_cases),
    size = 2,
    color = "blue")+
  scale_y_continuous(
    position = "right")+
  labs(x = "",
       y = "Cas cumulés")+
  theme_cowplot()+
  theme(
    axis.line.x = element_blank(),
    axis.text.x = element_blank(),
    axis.title.x = element_blank(),
    axis.ticks = element_blank())

```

Utilisez maintenant **cowplot** pour superposer les deux graphiques. Une attention particuliére a été portée à l'alignement de l'axe des x, au côte de l'axe des y et à l'utilisation de `theme_cowplot()`.  

```{r, warning=F, message=F}

aligned_plots <- cowplot::align_plots(plot_cases, plot_cumulative, align="hv", axis="tblr")

ggdraw(aligned_plots[[1]]) + draw_plot(aligned_plots[[2]])
```


<!-- ======================================================= -->
## Ressources { }








```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/epicurves.Rmd-->

# Pyramides démographiques et échelles de Likert {#age_pyramid}

```{r, out.width = c('50%', '50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "pop_pyramid_baseline.png"))

knitr::include_graphics(here::here("images", "likert.png"))
```

Les pyramides démographiques sont utiles pour montrer les distributions d'âge et de sexe. Un code similaire peut être utilisé pour visualiser les résultats de questions d'enquête de type Likert (par exemple, "Tout à fait d'accord", "D'accord", "Neutre", "Pas d'accord", "Pas du tout d'accord"). Dans cette page, nous couvrons les points suivants :

-   Des pyramides rapides et faciles à réaliser grâce au package **apyramid**\
-   Des pyramides plus personnalisables grâce à `ggplot()`\
-   Affichage de données démographiques "de référence" à l'arrière-plan de la pyramide\
-   Utilisation de graphiques de type pyramide pour afficher d'autres types de données (par exemple, les réponses à des questions d'enquête de type **Likert**).

<!-- ======================================================= -->

## Preparation

### Charger les packages {.unnumbered}

Ce morceau de code montre le chargement des packages nécessaires aux analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le package si nécessaire et le charge pour l'utiliser. Vous pouvez également charger les packages installés avec `library()` à partir de la **base** R. Voir la page sur [les bases de R](#basics) pour plus d'informations sur les packages R.

```{r}
pacman::p_load(rio,       # Pour importer les données
               here,      # Pour localiser les fichiers
               tidyverse, # Pour nettoyer, traiter et représenter les données (inclut le package ggplot2)
               apyramid,  # Un package dédié à la création de pyramides des âges
               janitor,   # Tableaux et nettoyage des données
               stringr)   # Pour travailler avec des chaînes de caractères pour les titres, les légendes, etc.
```

### Importer les données {.unnumbered}

Pour commencer, nous importons la liste de cas nettoyée d'une épidémie d'Ebola simulée. Si vous voulez le faire en même temps, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la liste des cas "nettoyée".</a> (sous le format .rds). Importez des données à l'aide de la fonction `import()` du package **rio** (elle gère de nombreux types de fichiers tels que .xlsx, .csv, .rds - voir la page [Importation et exportation](#importing) pour plus de détails).

```{r, echo=F}
# Importer la liste des cas dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# Importer la liste des cas dans R
linelist <- import("linelist_cleaned.rds")
```

Les 50 premières lignes de la liste des cas sont affichées ci-dessous.

```{r, message=FALSE, echo=F}
# Afficher la liste des cas sous forme de tableau
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Nettoyage {.unnumbered}

Pour réaliser une pyramide démographique traditionnelle par âge/sexe, les données doivent d'abord être nettoyées de la manière suivante :

-   La colonne sexe doit être nettoyée.

-   Selon votre méthode, l'âge doit être stocké soit sous forme numérique, soit dans une colonne de catégorie d'âge.

Si vous utilisez des catégories d'âge, les valeurs de la colonne doivent être ordonnées correctement, soit par défaut en alphanumérique, soit intentionnellement en convertissant en facteur de classe.

Ci-dessous nous utilisons `tabyl()` du package **janitor** pour inspecter les colonnes `gender` et `age_cat5`.

```{r}
linelist %>% 
  tabyl(age_cat5, gender)
```

Nous effectuons également un rapide histogramme sur la colonne `age` pour nous assurer qu'elle est propre et correctement classée :

```{r}
hist(linelist$age)
```

<!-- ======================================================= -->

## Le package **apyramid**

Le package **apyramid** est un produit du projet [R4Epis](https://r4epis.netlify.com/). Vous pouvez en savoir plus sur ce paquet [ici](https://cran.r-project.org/web/packages/apyramid/vignettes/intro.html). Il vous permet de réaliser rapidement une pyramide des âges. Pour des situations plus nuancées, voir la section ci-dessous [utilisez `ggplot()`](#demo_pyr_gg). Vous pouvez en savoir plus sur le package **apyramid** dans sa page d'aide en entrant `?age_pyramid` dans votre console R.

### Données sous forme de liste des cas {.unnumbered}

En utilisant l'ensemble de données `linelist` nettoyées, nous pouvons créer une pyramide des âges avec une simple commande `age_pyramid()`. Dans cette commande :

-   Le paramètre `data =` est défini comme le tableau de données linelist.

-   Le paramètre `age_group =` (pour l'axe des ordonnées) est défini comme le nom de la colonne d'âge catégorique (entre guillemets).

-   Le paramètre `split_by =` (pour l'axe des abscisses) est défini comme la colonne sexe.

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "gender")
```

La pyramide peut être affichée avec le pourcentage de tous les cas sur l'axe des abscisses, au lieu du nombre, en incluant `proportional = TRUE`.

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "gender",
                      proportional = TRUE)
```

Lors de l'utilisation du package **apyramid**, si la colonne `split_by` est binaire (par exemple homme/femme, ou oui/non), le résultat apparaîtra comme une pyramide. Cependant, s'il y a plus de deux valeurs dans la colonne `split_by` (sans compter `NA`), la pyramide apparaîtra comme un diagramme à barres à facettes avec des barres grises dans le "fond" indiquant la plage des données non facettées pour ce groupe d'âge. Dans ce cas, les valeurs de `split_by =` apparaîtront comme des libellés en haut de chaque panneau de facettes. Par exemple, voici ce qui se passe si la colonne `hospital` est attribuée à `split_by =`.

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "hospital")  
```

#### Valeurs manquantes {.unnumbered}

Les lignes qui ont des valeurs manquantes `NA` dans les colonnes `split_by =` ou `age_group =`, si elles sont codées comme `NA`, ne déclencheront pas les facettes indiquées ci-dessus. Par défaut, ces lignes ne seront pas affichées. Cependant, vous pouvez demander à ce qu'elle apparaissent dans un graphique à barres adjacent et en tant que groupe d'âge distinct en haut, en spécifiant `na.rm = FALSE`.

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "gender",
                      na.rm = FALSE)         # Montre les patients dont l'âge ou le sexe est manquant
```

#### Proportions, couleurs et attributs graphiques {.unnumbered}

Par défaut, les barres affichent les nombres (pas les pourcentages), une ligne en pointillés au milieu de chaque groupe est affichée, et les couleurs sont vertes/violettes. Chacun de ces paramètres peut être ajusté, comme indiqué ci-dessous :

Vous pouvez également ajouter des commandes ggplot() supplémentaires au graphique en utilisant la syntaxe standard de `ggplot()` "+", comme des attributs graphiques et des ajustements de libellés :

```{r, warning=F, message=F}
apyramid::age_pyramid(
  data = linelist,
  age_group = "age_cat5",
  split_by = "gender",
  proportional = TRUE,              # afficher les pourcentages, pas les chiffres
  show_midpoint = FALSE,            # supprimer la ligne du milieu de la barre
  #pal = c("orange", "purple")      # permet de préciser des couleurs alternatives (mais pas des libellés différents)
  )+                 
  
  # commandes supplémentaires de ggplot
  theme_minimal()+                               # simplifier le fond
  scale_fill_manual(                             # préciser des couleurs ET des libellés
    values = c("orange", "purple"),              
    labels = c("m" = "Male", "f" = "Female"))+
  labs(y = "Percent of all cases",              # les libellés x et y sont inversées
       x = "Age categories",                          
       fill = "Gender", 
       caption = "My data source and caption here",
       title = "Title of my plot",
       subtitle = "Subtitle with \n a second line...")+
  theme(
    legend.position = "bottom",                          # légende en bas
    axis.text = element_text(size = 10, face = "bold"),  # polices/tailles
    axis.title = element_text(size = 12, face = "bold"))
```

### Données aggrégées {.unnumbered}

Les exemples ci-dessus supposent que vos données sont au format de liste de cas, avec une ligne par observation. Si vos données sont déjà agrégées par catégorie d'âge, vous pouvez toujours utiliser le package **apyramid**, comme indiqué ci-dessous.

Pour la démonstration, nous agrégeons les données de la liste de cas en nombre de cas par catégorie d'âge et par sexe, dans un format "large". Cela fera comme si vos données étaient aggrégées dès le départ. Pour en savoir plus sur le [Regroupement des données](#grouping_data) et le [Pivotage des données](#pivoting_data), consultez leurs pages respectives.

```{r, warning=F, message=F}
demo_agg <- linelist %>% 
  count(age_cat5, gender, name = "cases") %>% 
  pivot_wider(
    id_cols = age_cat5,
    names_from = gender,
    values_from = cases) %>% 
  rename(`missing_gender` = `NA`)
```

...ce qui fait que le jeu de données ressemble à ceci : avec des colonnes pour la catégorie d'âge, et le nombre d'hommes, le nombre de femmes, et le nombre de valeurs manquantes.

```{r, echo=F, warning=F, message=F}
# Voir les données agregées
DT::datatable(demo_agg, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Pour préparer ces données pour la pyramide des âges, nous allons faire pivoter les données pour qu'elles soient "longues" avec la fonction `pivot_longer()` de **dplyr**. Ceci est dû au fait que `ggplot()` préfère généralement les données "longues", et **apyramid** utilise `ggplot()`.

```{r, warning=F, message=F}
# Faire pivoter les données agrégées afin qu'elles soient "longues"
demo_agg_long <- demo_agg %>% 
  pivot_longer(
    col = c(f, m, missing_gender),            # Colonnes à "allonger"
    names_to = "gender",                # Nom de la nouvelle colonne de catégorie
    values_to = "counts") %>%           # Nom pour la nouvelle colonne de comptage
  mutate(
    gender = na_if(gender, "missing_gender")) # On convertit  "missing_gender" en NA
```

```{r, echo=F, warning=F, message=F}
# Voir les données aggrégées
DT::datatable(demo_agg_long, rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Utilisez ensuite les arguments `split_by =` et `count =` de `age_pyramid()` pour spécifier les colonnes respectives dans les données :

```{r, warning=F, message=F}
apyramid::age_pyramid(data = demo_agg_long,
                      age_group = "age_cat5",# Nom de la colonne pour la catégorie d'âge
                      split_by = "gender",   # Nom de la colonne pour le sexe
                      count = "counts")      # Nom de la colonne pour le nombre de cas
```

Notez dans l'exemple ci-dessus que l'ordre des facteurs "m" et "f" est différent (pyramide inversée). Pour ajuster l'ordre, vous devez redéfinir le sexe dans les données agrégées comme un facteur et ordonner les niveaux comme vous le souhaitez. Voir la page [Facteurs](#factors).

<!-- ======================================================= -->

## `ggplot()` {#demo_pyr_gg}

L'utilisation de `ggplot()` pour construire votre pyramide des âges offre plus de flexibilité, mais demande plus d'efforts et de compréhension du fonctionnement de `ggplot()`. Il est également plus facile de faire des erreurs accidentelles.

Pour utiliser `ggplot()` afin de créer des pyramides démographiques, vous devez créer deux diagrammes à barres (un pour chaque sexe), convertir les valeurs de l'un des diagrammes en valeurs négatives, et enfin inverser les axes x et y pour afficher les diagrammes à barres verticalement, leurs bases se rejoignant au milieu du diagramme.

### Préparation {.unnumbered}

Cette approche utilise une colonne d'âge sous forme *numérique*, et non la colonne *catégorielle* `age_cat5`. Nous allons donc vérifier que la classe de cette colonne est bien numérique.

```{r}
class(linelist$age)
```

Vous pourriez utiliser la même logique ci-dessous pour construire une pyramide à partir de données catégorielles en utilisant `geom_col()` au lieu de `geom_histogram()`.

<!-- ======================================================= -->

### Construction du graphe {.unnumbered}

Tout d'abord, il faut comprendre que pour réaliser une telle pyramide à l'aide de `ggplot()`, l'approche est la suivante :

-   Dans la fonction `ggplot()`, créez deux histogrammes en utilisant la colonne numérique de l'âge. Créez-en un pour chacune des deux valeurs de regroupement (dans ce cas, les sexes masculin et féminin). Pour ce faire, les données de chaque histogramme sont spécifiées dans leurs commandes `geom_histogram()` respectives, avec les filtres respectifs appliqués à `linelist`.

-   Un graphique aura des valeurs de comptage positives, tandis que l'autre aura ses comptages convertis en valeurs négatives - cela crée la "pyramide" avec la valeur 0 au milieu du graphique. Les valeurs négatives sont créées en utilisant un terme spécial de **ggplot2** `..count..` et en les multipliant par -1.

-   La commande `coord_flip()` permute les axes X et Y, ce qui a pour effet de rendre les graphiques verticaux et de créer la pyramide.

-   Enfin, les étiquettes des valeurs de l'axe des comptes doivent être modifiées pour qu'elles apparaissent comme des comptes "positifs" des deux côtés de la pyramide (bien que les valeurs sous-jacentes d'un côté soient négatives).

Une version **simple** de cette méthode, utilisant `geom_histogram()`, est présentée ci-dessous :

```{r, warning=F, message=F}
  # commencer le ggplot
  ggplot(mapping = aes(x = age, fill = gender)) +
  
  # histogramme femmes
  geom_histogram(data = linelist %>% filter(gender == "f"),
                 breaks = seq(0,85,5),
                 colour = "white") +
  
  # histogramme hommes (valeurs converties en négatif)
  geom_histogram(data = linelist %>% filter(gender == "m"),
                 breaks = seq(0,85,5),
                 mapping = aes(y = ..count..*(-1)),
                 colour = "white") +
  
  # Inversion des axes X et Y
  coord_flip() +
  
  # Ajustement de l'échelle de l'axe des nombres de cas
  scale_y_continuous(limits = c(-600, 900),
                     breaks = seq(-600,900,100),
                     labels = abs(seq(-600, 900, 100)))
```

[***DANGER:*** Si les **limites** de votre axe de nombre de cas sont trop basses, et qu'une barre de compte les dépasse, la barre disparaîtra entièrement ou sera artificiellement raccourcie ! Faites attention à ce phénomène si vous analysez des données qui sont régulièrement mises à jour. Pour éviter cela, les limites de votre axe de comptage doivent s'ajuster automatiquement à vos données, comme ci-dessous]{style="color: red;"}

Il y a beaucoup de choses que vous pouvez changer/ajoutez à cette version simple :

-   Ajuster automatiquement l'échelle de l'axe des comptes à vos données (éviter les erreurs discutées dans l'avertissement ci-dessous).\
-   Spécifier manuellement les couleurs et les étiquettes de légende

**Convertir les nombre de cas en pourcentages**

Pour convertir les nombres de cas en pourcentages (du total), faites-le dans vos données avant de les représenter. Ci-dessous, nous obtenons les comptes d'âge et de sexe, puis on utilise `ungroup()`, et enfin `mutate()` pour créer de nouvelles colonnes de pourcentage. Si vous voulez des pourcentages par sexe, sautez l'étape de dégroupage.

```{r, warning=F, message=F}
# créer un jeu de données avec la proportion du total
pyramid_data <- linelist %>%
  count(age_cat5,
        gender,
        name = "counts") %>% 
  ungroup() %>%                 # dégrouper de sorte à ce que les pourcentages ne soit pas par groupe
  mutate(percent = round(100*(counts / sum(counts, na.rm=T)), digits = 1), 
         percent = case_when(
            gender == "f" ~ percent,
            gender == "m" ~ -percent,     # convertir les hommes en négatif
            TRUE          ~ NA_real_))    # les valeur NA doivent aussi être numériques
```

Il est important de noter que nous enregistrons les valeurs max et min afin de connaître les limites de l'échelle. Elles seront utilisées dans la commande `ggplot()` ci-dessous.

```{r}
max_per <- max(pyramid_data$percent, na.rm=T)
min_per <- min(pyramid_data$percent, na.rm=T)

max_per
min_per
```

Enfin, nous effectuons le `ggplot()` sur les données en pourcentage. Nous spécifions `scale_y_continuous()` pour étendre les longueurs prédéfinies dans chaque direction ("positive" et "négative"). Nous utilisons `floor()` et `ceiling()` pour arrondir les décimales dans la direction appropriée (vers le bas ou vers le haut) pour le côté de l'axe.

```{r, warning=F, message=F}
# Commencer le ggplot
  ggplot()+  # par défaut l'axe X est l'âge en années;

  # graphe des données des cas
  geom_col(data = pyramid_data,
           mapping = aes(
             x = age_cat5,
             y = percent,
             fill = gender),         
           colour = "white")+       # contour blanc autour de chaque barre
  
  # Inverser les axes X et Y pour rendre la pyramide verticale
  coord_flip()+
  

  # Ajuster les échelles des axes
  # scale_x_continuous(breaks = seq(0,100,5), labels = seq(0,100,5)) +
  scale_y_continuous(
    limits = c(min_per, max_per),
    breaks = seq(from = floor(min_per),                # suite des valeurs deux par deux
                 to = ceiling(max_per),
                 by = 2),
    labels = paste0(abs(seq(from = floor(min_per),     # suite des valeurs absolues deux par deux, avec "%"
                            to = ceiling(max_per),
                            by = 2)),
                    "%"))+  

  # Préciser manuellement les couleurs et les étiquettes
  scale_fill_manual(
    values = c("f" = "orange",
               "m" = "darkgreen"),
    labels = c("Female", "Male")) +
  
  # Etiqueter les valeur (en se rappellant que X et Y sont inversés)
  labs(
    title = "Age and gender of cases",
    x = "Age group",
    y = "Percent of total",
    fill = NULL,
    caption = stringr::str_glue("Data are from linelist \nn = {nrow(linelist)} (age or sex missing for {sum(is.na(linelist$gender) | is.na(linelist$age_years))} cases) \nData as of: {format(Sys.Date(), '%d %b %Y')}")) +
  
  # Attributs graphiques et élements du thème
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank(),
    axis.line = element_line(colour = "black"),
    plot.title = element_text(hjust = 0.5), 
    plot.caption = element_text(hjust=0, size=11, face = "italic")
    )

```

<!-- ======================================================= -->

### Comparer à une référence {.unnumbered}

Grâce à la flexibilité de `ggplot()`, vous pouvez avoir une deuxième couche de barres en arrière-plan qui représente la pyramide des âges "réelle" ou "de référence". Cela peut fournir une visualisation agréable pour comparer les données observées avec les données de référence.

Importez et visualisez les données de population (voir la page [Télécharger le manuel et les données](#download_book_data)) :

```{r echo=F}
#  Import des données démographiques d'une population
pop <- rio::import(here::here("data", "standardization", "country_demographics.csv"))
```

```{r eval=F}
# i Import des données démographiques d'une population
pop <- rio::import("country_demographics.csv")
```

```{r, echo=F, warning=F, message=F}
# Afficher la liste des cas sous forme de tableau
DT::datatable(pop, rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```

Tout d'abord, quelques étapes de gestion des données :

Nous enregistrons ici l'ordre des catégories d'âge que nous voulons voir apparaître. En raison de certaines bizarreries dans l'implémentation de `ggplot()`, dans ce scénario spécifique, il est plus facile de stocker ces données comme un vecteur de caractères et de les utiliser plus tard dans la fonction graphique.

```{r}
# enregistrer les niveaux d'âge catégoriel corrects
age_levels <- c("0-4","5-9", "10-14", "15-19", "20-24",
                "25-29","30-34", "35-39", "40-44", "45-49",
                "50-54", "55-59", "60-64", "65-69", "70-74",
                "75-79", "80-84", "85+")
```

Combinez les données de population et de cas à l'aide de la fonction du paquet **dplyr**,  `bind_rows()` :

-   Tout d'abord, assurez-vous qu'elles ont *exactement les mêmes* noms de colonnes, valeurs de catégories d'âge et valeurs de sexe.\
-   Faites en sorte qu'ils aient la même structure de données : colonnes de catégorie d'âge, de sexe, de nombre et de pourcentage du total.\
-   Liez-les ensemble, l'un au-dessus de l'autre (`bind_rows()`).

```{r, warning=F, message=F}
# Créer/transformer les données de population, avec le pourcentage du total
########################################################
pop_data <- pop %>% 
  pivot_longer(      # On faire "pivoter" la colonne sexe à l'aide de pivot_longer
    cols = c(m, f),
    names_to = "gender",
    values_to = "counts") %>% 
  
  mutate(
    percent  = round(100*(counts / sum(counts, na.rm=T)),1),  # % du total
    percent  = case_when(                                                        
     gender == "f" ~ percent,
     gender == "m" ~ -percent,               # Pour les hommes on converti le pourcentage en négatif
     TRUE          ~ NA_real_))
```

Examiner les données de population modifiées

```{r, echo=F, warning=F, message=F}
# Afficher les données de populationn sous forme de tableau
DT::datatable(pop_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

On peut maintenant faire la même chose pour les données des cas.  C'est légèrement différent car elle commence par les lignes de cas et non par des totaux.

```{r, warning=F, message=F}
# créer des données sur les cas par âge/sexe, avec le pourcentage du total
#######################################################
case_data <- linelist %>%
  count(age_cat5, gender, name = "counts") %>%  # nombre par groupes age-sexe
  ungroup() %>% 
  mutate(
    percent = round(100*(counts / sum(counts, na.rm=T)),1),  # calculer le % du total pour les groupes d'âge et de sexe
    percent = case_when(                                     # our les hommes on converti le pourcentage en négatif
      gender == "f" ~ percent,
      gender == "m" ~ -percent,
      TRUE          ~ NA_real_))
```

Examiner les données des cas modifiées

```{r, message=FALSE, echo=F}
# Afficher les données des cas sous forme de tableau
DT::datatable(case_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Les deux jeux de données sont maintenant combinés, l'un au-dessus de l'autre (ils ont les mêmes noms de colonnes). Nous pouvons "nommer" chaque jeux de données, et utiliser l'argument `.id =` pour créer une nouvelle colonne "data_source" qui indiquera de quel cadre de données chaque ligne provient. Nous pouvons utiliser cette colonne pour filtrer dans la fonction `ggplot()`.

```{r, warning=F, message=F}
# combiner les données de cas et de population (mêmes noms de colonnes, valeurs age_cat et valeurs de sexe)
pyramid_data <- bind_rows("cases" = case_data, "population" = pop_data, .id = "data_source")
```

Enregistre les valeurs maximales et minimales en pourcentage, utilisées dans la fonction de traçage pour définir l'étendue du tracé (et ne pas couper les barres !).

```{r}
# Définir l'étendue de l'axe des pourcentages, utilisé pour les limites du graphe.
max_per <- max(pyramid_data$percent, na.rm=T)
min_per <- min(pyramid_data$percent, na.rm=T)
```

Le graphique est maintenant réalisé avec `ggplot()` :

-   Un graphique en barres des données de population (barres plus larges et plus transparentes)
-   Un histogramme des données de cas (petites barres, plus solides)

```{r, warning=F, message=F}

# Entamer le ggplot
##############
ggplot()+  # l'axe des x par défaut est l'âge en années;

  # Graphe des données de population
  geom_col(
    data = pyramid_data %>% filter(data_source == "population"),
    mapping = aes(
      x = age_cat5,
      y = percent,
      fill = gender),
    colour = "black",                               # Contour noir autour des barres
    alpha = 0.2,                                    # plus transparent
    width = 1)+                                     # plein largeur
  
  # Graphe des données des cas
  geom_col(
    data = pyramid_data %>% filter(data_source == "cases"), 
    mapping = aes(
      x = age_cat5,                               # Catégorie d'âge comme axes des x d'origine
      y = percent,                                # % comme axe des Y d'origine
      fill = gender),                             # Couleur de remplissage des barres en fonctio ns du sexe
    colour = "black",                               # Contour noir autour des barres
    alpha = 1,                                      # pas transparent 
    width = 0.3)+                                   # largeur réduite
  
  # inversersion des axes X et Y pour rendre la pyramide verticale
  coord_flip()+
  
  # s'assurer à la main que l'axe de l'âge est ordonné correctement
  scale_x_discrete(limits = age_levels)+     # défini dans le morceua de code ci-dessus
  
  # définir l'axe des pourcentages 
  scale_y_continuous(
    limits = c(min_per, max_per),                                          # Le min et le max sont définis ci-dessus
    breaks = seq(floor(min_per), ceiling(max_per), by = 2),                #  De min_per (pourcentage minimum) à max_per (pourcentage maximum) par 2 
    labels = paste0(                                                       # Pour les libellés, coller ensemble... 
              abs(seq(floor(min_per), ceiling(max_per), by = 2)), "%"))+                                                  

  # Définir manuellement les couleurs et les étiquettes de légende
  scale_fill_manual(
    values = c("f" = "orange",         # attribuer des couleurs aux valeurs des données
               "m" = "darkgreen"),
    labels = c("f" = "Female",
               "m"= "Male"),      # modifier les libellés qui apparaissent dans la légende, noté l'ordre
  ) +

  # Ajouter au graphes les libellés et les titres 
  labs(
    title = "Case age and gender distribution,\nas compared to baseline population",
    subtitle = "",
    x = "Age category",
    y = "Percent of total",
    fill = NULL,
    caption = stringr::str_glue("Cases shown on top of country demographic baseline\nCase data are from linelist, n = {nrow(linelist)}\nAge or gender missing for {sum(is.na(linelist$gender) | is.na(linelist$age_years))} cases\nCase data as of: {format(max(linelist$date_onset, na.rm=T), '%d %b %Y')}")) +
  
  # Paramètres graphiques optionnels
  theme(
    legend.position = "bottom",                             # Deplacer la légende en bas du graphe
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank(),
    axis.line = element_line(colour = "black"),
    plot.title = element_text(hjust = 0), 
    plot.caption = element_text(hjust=0, size=11, face = "italic"))

```

<!-- ======================================================= -->

## Échelle de Likert

Les techniques utilisées pour réaliser une pyramide des âges avec `ggplot()` peuvent également être utilisées pour réaliser des graphiques de données d'enquêtes à échelle de Likert.

```{r, eval=F, echo=F}
data_raw <- import("P:/Shared/equateur_mve_2020/lessons learned/Ebola After-Action Survey - HQ epi team (form responses).csv")


likert_data <- data_raw %>% 
  select(2, 4:11) %>% 
  rename(status = 1,
         Q1 = 2,
         Q2 = 3,
            Q3 = 4,
            Q4 = 5,
            Q5 = 6,
            Q6 = 7,
            Q7 = 8,
            Q8 = 9) %>% 
  mutate(status = case_when(
           stringr::str_detect(status, "Mar") ~ "Senior",
           stringr::str_detect(status, "Jan") ~ "Intermediate",
           stringr::str_detect(status, "Feb") ~ "Junior",
           TRUE ~ "Senior")) %>% 
  mutate(Q4 = recode(Q4, "Not applicable" = "Very Poor"))

table(likert_data$status)

rio::export(likert_data, here::here("data", "likert_data.csv"))
```

Importez les données (voir la page [Télécharger le manuel et les données](#download_book_data) si vous le souhaitez).

```{r echo=F}
# importer les données de réponse de l'enquête likert
likert_data <- rio::import(here::here("data", "likert_data.csv"))
```

```{r, eval=F}
# importer les données de réponse de l'enquête likert
likert_data <- rio::import("likert_data.csv")
```

Commencez avec des données qui ressemblent à ceci, avec une classification catégorielle de chaque répondant (`statut`) et leurs réponses à 8 questions sur une échelle de type Likert à 4 points ("Très mauvais", "Mauvais", "Bon", "Très bon").

```{r, echo=F, message=FALSE}
# afficher les données de la liste des lignes sous forme de tableau
DT::datatable(likert_data, rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```

Tout d'abord, quelques étapes de gestion des données :

-   Pivoter les données afin qu'elles soient "longues" plutôt que "larges"
-   Créer une nouvelle colonne `direction` qui indique si une réponse était globalement "positive" ou "négative"
-   Définisser l'ordre du niveau de Facteur pour la colonne `status` et la colonne `Response`
-   Enregistrez la valeur maximale pour que les limites du graphique soient appropriées.

```{r, warning=F, message=F}
melted <- likert_data %>% 
  pivot_longer(
    cols = Q1:Q8,
    names_to = "Question",
    values_to = "Response") %>% 
  mutate(
    
    direction = case_when(
      Response %in% c("Poor","Very Poor")  ~ "Negative",
      Response %in% c("Good", "Very Good") ~ "Positive",
      TRUE                                 ~ "Unknown"),
    
    status = fct_relevel(status, "Junior", "Intermediate", "Senior"),
    
    # On inverse  'Very Poor' et 'Poor' pour que l'ordre soit le bon
    Response = fct_relevel(Response, "Very Good", "Good", "Very Poor", "Poor")) 

# Permet d'obtenir la plus grande valeur pour les limites d'échelle
melted_max <- melted %>% 
  count(status, Question) %>% # nombre de ligne
  pull(n) %>%                 # Colonne 'n'
  max(na.rm=T)                # Maximum
```

Maintenant, créez le graphique. Comme dans les pyramides des âges ci-dessus, nous créons deux graphes à barres et inversons les valeurs de l'un d'entre eux en négatif.

On utilise `geom_bar()` parce que nos données sont une ligne par observation, et non pas des comptes agrégés. Nous utilisons le terme spécial de **ggplot2** `..count..` dans l'un des graphiques en barres pour inverser les valeurs négatives (*-1*), et nous définissons `position = "stack"` pour que les valeurs s'empilent les unes sur les autres.

```{r, warning=F, message=F}
# Créer le graphe
ggplot()+
     
  # graphique à barres des réponses "négatives" 
     geom_bar(
       data = melted %>% filter(direction == "Negative"),
       mapping = aes(
         x = status,
         y = ..count..*(-1),    # counts inverted to negative
         fill = Response),
       color = "black",
       closed = "left",
       position = "stack")+
     
     #  graphique à barres des réponses "positives" 
     geom_bar(
       data = melted %>% filter(direction == "Positive"),
       mapping = aes(
         x = status,
         fill = Response),
       colour = "black",
       closed = "left",
       position = "stack")+
     
     # inversion des axes x et y
     coord_flip()+
  
     # Ligne noire verticale à 0
     geom_hline(yintercept = 0, color = "black", size=1)+
     
    # On convertit les libellés pour qu'il n'y ai que des chiffres positifs
    scale_y_continuous(
      
      # limites de l'échelle des x
      limits = c(-ceiling(melted_max/10)*11,    # séquence de négatif à positif par 10, bords arrondis vers l'extérieur au 5 le plus proche
                 ceiling(melted_max/10)*10),   
      
      # valeurs de l'échelle de l'axe des x
      breaks = seq(from = -ceiling(melted_max/10)*10,
                   to = ceiling(melted_max/10)*10,
                   by = 10),
      
      # libellés de l'échelle de l'axe des x
      labels = abs(unique(c(seq(-ceiling(melted_max/10)*10, 0, 10),
                            seq(0, ceiling(melted_max/10)*10, 10))))) +
     
    # échelle de couleur attribuées manuellement
    scale_fill_manual(
      values = c("Very Good"  = "green4", # attribue les couleurs
                "Good"      = "green3",
                "Poor"      = "yellow",
                "Very Poor" = "red3"),
      breaks = c("Very Good", "Good", "Poor", "Very Poor"))+ # donne l'ordre de la légende
     
    
     
    # Convertit le graphe de sorte à ce que chaque sous graphe corresponde à une question
    facet_wrap( ~ Question, ncol = 3)+
     
    # libellés, titres, légende
    labs(
      title = str_glue("Likert-style responses\nn = {nrow(likert_data)}"),
      x = "Respondent status",
      y = "Number of responses",
      fill = "")+

     # Paramètres graphiques 
     theme_minimal()+
     theme(axis.text = element_text(size = 12),
           axis.title = element_text(size = 14, face = "bold"),
           strip.text = element_text(size = 14, face = "bold"),  # Titre de chaque sous graphique
           plot.title = element_text(size = 20, face = "bold"),
           panel.background = element_rect(fill = NA, color = "black")) # Cadre noir autour de chaque sous graphique
```

<!-- ======================================================= -->

## Resources

[Documentation de apyramid](https://cran.r-project.org/web/packages/apyramid/vignettes/intro.html)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/age_pyramid.Rmd-->


# Graphiques thermiques {#heatmaps}  


Les diagrammes de chaleur, également connus sous le nom de "cartes de chaleur" ou "tuiles de chaleur", peuvent être des visualisations utiles lorsqu'on essaie d'afficher 3 variables (axe des x, axe des y et remplissage). Nous présentons ci-dessous deux exemples :  

* Une matrice visuelle des événements de transmission par âge ("qui a infecté qui").  
* Le suivi des métriques de déclaration dans de nombreux établissements/juridictions au fil du temps.  


```{r, out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "transmission_matrix.png"))

knitr::include_graphics(here::here("images", "heat_tile.png"))

```





<!-- ======================================================= -->
## Préparation { }

### Chargement des paquets {.unnumbered}  

Ce morceau de code montre le chargement des paquets nécessaires aux analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* le charge pour l'utiliser. Vous pouvez également charger les paquets installés avec `library()` de **base** R. Voir la page sur [bases de R](# rbasics) pour plus d'informations sur les paquets R.  

```{r}
pacman::p_load(
  tidyverse, # manipulation et visualisation de données
  rio, # importation de données 
  lubridate # travail avec les dates
  )
```

**Ensembles de données**  

Cette page utilise la liste de cas d'une épidémie simulée pour la section de la matrice de transmission, et un jeu de données séparé du nombre quotidien de cas de paludisme par établissement pour la section du suivi des mesures. Ils sont chargés et nettoyés dans leurs sections individuelles.  







## Matrice de transmission  

Les tuiles thermiques peuvent être utiles pour visualiser les matrices. Un exemple est d'afficher "qui a infecté qui" dans une épidémie. Cela suppose que vous disposiez d'informations sur les événements de transmission.  

Notez que la page [Recherche des contacts](#contact_tracing) contient un autre exemple de création d'une matrice de contacts en tuiles thermiques, à l'aide d'un ensemble de données différent (peut-être plus simple) où les âges des cas et leurs sources sont soigneusement alignés sur la même ligne du cadre de données. Ces mêmes données sont utilisées pour réaliser une carte de *densité* dans la page [Astuces de ggplot](#ggplot_tips). L'exemple ci-dessous part d'une liste de cas et implique donc une manipulation considérable des données avant d'obtenir un cadre de données traçable. Il existe donc de nombreux scénarios parmi lesquels choisir...  


Nous commençons à partir de la liste de cas d'une épidémie d'Ebola simulée. Si vous souhaitez nous suivre, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la linelist "propre"</a> (en tant que fichier .rds). Importez vos données avec la fonction `import()` du paquet **rio** (elle accepte de nombreux types de fichiers comme .xlsx, .rds, .csv - voir la page [Importation et exportation](#import_export) pour plus de détails).  


Les 50 premières lignes de la liste de lignes sont présentées ci-dessous à titre de démonstration :  


```{r, echo=F}
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```


```{r, eval=F}
linelist <- import("linelist_cleaned.rds")
```


Dans cette linelist :  

* Il y a une ligne par cas, identifié par `case_id`.  
* Il y a une colonne ultérieure `infector` qui contient le `case_id` de l'*infector*, qui est aussi un cas dans la linelist  


```{r message=FALSE, echo=F}
# affichez la population sous forme de tableau
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space : nowrap' )
```



### Préparation des données {.unnumbered}  

**Objectif** : Nous devons obtenir un cadre de données de type "long" qui contient une ligne par route de transmission âge-âge possible, avec une colonne numérique contenant la proportion de cette ligne de tous les événements de transmission observés dans la liste de lignes.  

Il faudra plusieurs étapes de manipulation des données pour y parvenir :  


#### Créer un cadre de données pour les cas {.unnumbered} 

Pour commencer, nous créons un cadre de données des cas, de leurs âges, et de leurs infecteurs - nous appelons ce cadre de données `case_ages`. Les 50 premières lignes sont affichées ci-dessous.  

```{r}
case_ages <- linelist %>% 
  select(case_id, infector, age_cat) %>% 
  rename("case_age_cat" = "age_cat")
```

```{r message=FALSE, echo=F}
# afficher le fichier de forme sous forme de tableau
DT::datatable(head(case_ages, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space : nowrap' )
```

#### Création d'un cadre de données d'infecteurs {.unnumbered}  

Ensuite, nous créons un cadre de données des infecteurs - pour l'instant, il est constitué d'une seule colonne. Il s'agit des identifiants des infecteurs de la liste de diffusion. Tous les cas n'ont pas un infecteur connu, nous supprimons donc les valeurs manquantes. Les 50 premières lignes sont affichées ci-dessous.  


```{r}
infectors <- linelist %>% 
  select(infector) %>% 
  drop_na(infector)
```

```{r message=FALSE, echo=F}
# afficher le fichier de forme sous forme de tableau
DT::datatable(head(infectors, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space : nowrap' )
```

Ensuite, nous utilisons des jointures pour obtenir l'âge des infecteurs. Ce n'est pas simple, car dans la `linelist`, les âges des infecteurs ne sont pas listés en tant que tels. Nous obtenons ce résultat en joignant le cas `linelist` aux infecteurs. Nous commençons par les infecteurs, et `left_join()` (ajoutons) la case `linelist` de sorte que la colonne `infector` id du cadre de données "baseline" de gauche rejoint la colonne `case_id` du cadre de données `linelist` de droite.  

Ainsi, les données de l'enregistrement du cas de l'infecteur dans la linelist (y compris l'âge) sont ajoutées à la ligne de l'infecteur. Les 50 premières lignes sont affichées ci-dessous.  

```{r}
infector_ages <- infectors %>% # commence par infectors
  left_join( # ajoute les données de la linelist à chaque infecteur  
    linelist,
    by = c("infector" = "case_id")) %>% # faire correspondre l'infector à ses informations en tant que cas
  select(infector, age_cat) %>% # ne conserve que les colonnes d'intérêt
  rename("infector_age_cat" = "age_cat") # renommer pour plus de clarté
```

```{r message=FALSE, echo=F}
# afficher le fichier de forme sous forme de tableau
DT::datatable(head(infector_ages, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space : nowrap' )
```

Ensuite, nous combinons les cas et leurs âges avec les infecteurs et leurs âges. Chacun de ces cadres de données possède la colonne `infector`, elle est donc utilisée pour la jointure. Les premières lignes sont affichées ci-dessous :    

```{r}
ages_complete <- case_ages %>%  
  left_join(
    infector_ages,
    by = "infector") %>% # chacun a la colonne infector
  drop_na() # supprime les lignes avec des données manquantes
```


```{r message=FALSE, echo=F}
# afficher le fichier de forme sous forme de tableau
DT::datatable(head(ages_complete, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space : nowrap' )
```

Ci-dessous, un simple tableau croisé des chiffres entre les groupes d'âge des cas et des infecteurs. Des étiquettes ont été ajoutées pour plus de clarté.  

```{r}
table(cases = ages_complete$case_age_cat,
      infectors = ages_complete$infector_age_cat)
```


Nous pouvons convertir ce tableau en un cadre de données avec `data.frame()` de **base** R, qui le convertit aussi automatiquement au format "long", ce qui est souhaité pour le `ggplot()`. Les premières lignes sont présentées ci-dessous.  

```{r}
long_counts <- data.frame(table(
    cas = ages_complete$case_age_cat,
    infectors = ages_complete$infector_age_cat))
```

```{r message=FALSE, echo=F}
# afficher le fichier de forme sous forme de tableau
DT::datatable(head(long_counts, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space : nowrap' )
```


Maintenant, nous faisons la même chose, mais nous appliquons `prop.table()` de **base** R au tableau pour qu'au lieu de compter, nous obtenions des proportions du total. Les 50 premières lignes sont affichées ci-dessous.    

```{r}
long_prop <- data.frame(prop.table(table(
    cases = ages_complete$case_age_cat,
    infectors = ages_complete$infector_age_cat)))
```

```{r message=FALSE, echo=F}
# afficher le fichier de forme sous forme de tableau
DT::datatable(head(long_prop, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space : nowrap' )
```




### Créer un diagramme de chaleur {.unnumbered}

Maintenant, nous pouvons enfin créer le graphique de chaleur avec le paquet **ggplot2**, en utilisant la fonction `geom_tile()`. Consultez la page [Astuces de ggplot](#ggplot_tips) pour en savoir plus sur les échelles de couleur et de remplissage, en particulier la fonction `scale_fill_gradient()`.  

* Dans l'esthétique `aes()` de `geom_tile()`, définissez x et y comme l'âge du cas et l'âge de l'infecteur.  
* De plus, dans `aes()`, mettez l'argument `fill = ` dans la colonne `Freq` - c'est la valeur qui sera convertie en une couleur de tuile.  
* Définissez une couleur d'échelle avec `scale_fill_gradient()` - vous pouvez spécifier les couleurs hautes et basses.  
  * Notez que `scale_color_gradient()` est différent ! Dans ce cas, vous voulez le remplissage  
* Comme la couleur est faite via "fill", vous pouvez utiliser l'argument `fill = ` dans `labs()` pour changer le titre de la légende.  

```{r}
ggplot(data = long_prop)+ # utilise des données longues, avec des proportions comme Freq
  geom_tile( # visualisation en tuiles
    aes(
      x = cases, # l'axe des x est l'âge du cas
      y = infectors, # l'axe des y est l'âge de l'infecteur
      fill = Freq))+ # la couleur de la tuile correspond à la colonne Freq dans les données
  scale_fill_gradient( # ajuste la couleur de remplissage des tuiles
    low = "blue",
    high = "orange")+
  labs( # étiquettes
    x = "Âge du cas",
    y = "Âge du contaminateur",
    title = "Qui a infecté qui",
    subtitle = "Matrice de fréquence des événements de transmission",
    fill = "Proportion de tous les événements de transmission" # titre de la légende
  )
  
```



<!-- ======================================================= -->
## Rapport sur les mesures dans le temps { }

Souvent, en santé publique, un objectif est d'évaluer les tendances dans le temps pour de nombreuses entités (établissements, juridictions, etc.). Une façon de visualiser ces tendances dans le temps est un diagramme de chaleur où l'axe des x est le temps et l'axe des y sont les nombreuses entités.  



### Préparation des données {.unnumbered}

Nous commençons par importer un jeu de données de rapports quotidiens sur le paludisme provenant de nombreux établissements. Les rapports contiennent une date, une province, un district et un nombre de cas de paludisme. Voir la page [Télécharger le manuel et les données](#download_book_data) pour savoir comment télécharger ces données. Voici les 30 premières lignes :  

```{r, echo=F}
facility_count_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) %>% 
  select(location_name, data_date, District, malaria_tot)
```

```{r, eval=F}
facility_count_data <- import("malaria_facility_count_data.rds")
```


```{r, echo=F}
DT::datatable(head(facility_count_data,30), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space : nowrap')
```


#### Agréger et résumer {.unnumbered}

**L'objectif de cet exemple** est de transformer les comptages quotidiens de cas de paludisme *totaux* de l'établissement (vus dans l'onglet précédent) en statistiques récapitulatives *hebdomadaires* des performances de déclaration de l'établissement - dans ce cas, *la proportion de jours par semaine où l'établissement a déclaré des données*. Pour cet exemple, nous allons montrer les données uniquement pour le **District de Spring**.  

Pour ce faire, nous allons procéder aux étapes suivantes de gestion des données :  

1) Filtrer les données comme il convient (par lieu, par date).  
2) Créer une colonne hebdomadaire en utilisant `floor_date()` du package **lubridate**.  
    + Cette fonction renvoie la date de début de la semaine d'une date donnée, en utilisant une date de début spécifiée de chaque semaine (par exemple "Lundi")  
3) Les données sont regroupées par les colonnes "lieu" et "semaine" pour créer des unités d'analyse de "semaine d'installation".  
4) La fonction `summarise()` crée de nouvelles colonnes pour refléter les statistiques sommaires par groupe de semaine d'installation :  
    + Nombre de jours par semaine (7 - une valeur statique)  
    + Nombre de rapports reçus de la semaine d'installation (peut être plus de 7 !)  
    + Somme des cas de malaria rapportés par la semaine d'installation (juste pour l'intérêt)  
    + Nombre de jours *uniques* dans la semaine d'installation pour lesquels des données ont été rapportées.  
    **Pourcentage des 7 jours par semaine d'installation pour lesquels des données ont été déclarées**.  
5) Le cadre de données est joint avec `right_join()` à une liste complète de toutes les combinaisons possibles de semaine d'installation, pour rendre l'ensemble de données complet. La matrice de toutes les combinaisons possibles est créée en appliquant `expand()` aux deux colonnes du cadre de données tel qu'il est à ce moment-là dans la chaîne de production (représenté par `.`). Comme un `right_join()` est utilisé, toutes les lignes du cadre de données `expand()` sont conservées, et ajoutées à `agg_weeks` si nécessaire. Ces nouvelles lignes apparaissent avec des valeurs résumées `NA` (manquantes).  


Nous faisons ci-dessous une démonstration étape par étape :  

```{r, message=FALSE, warning=FALSE}
# Créer un ensemble de données de résumé hebdomadaire
agg_weeks <- facility_count_data %>% 
  
  # Filtrez les données comme il se doit
  filter(
    District == "Spring",
    data_date < as.Date("2020-08-01")) 
```

Maintenant, le jeu de données a `r nrow(agg_weeks)` lignes, alors qu'il avait précédemment `r nrow(facility_count_data)`.  

Ensuite, nous créons une colonne `week` reflétant la date de début de la semaine pour chaque enregistrement. Ceci est réalisé avec le package **lubridate** et la fonction `floor_date()`, qui est définie sur "week" et pour que les semaines commencent le lundi (jour 1 de la semaine - le dimanche serait le 7). Les lignes du haut sont présentées ci-dessous.  

```{r}
agg_weeks <- agg_weeks %>% 
  # Créez une colonne semaine à partir de data_date
  mutate(
    week = lubridate::floor_date( # créer une nouvelle colonne de semaines
      data_date, # colonne de date
      unit = "week", # donne le début de la semaine
      week_start = 1))                                # les semaines commencent le lundi 
```

La nouvelle colonne de semaine est visible à l'extrême droite du cadre de données.  

```{r, echo=F}
DT::datatable(head(agg_weeks,30), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space : nowrap')
```

Maintenant, nous regroupons les données en semaines d'installation et les résumons pour produire des statistiques par semaine d'installation. Consultez la page sur les [Tableaux descriptifs](#descriptive_tables) pour obtenir des conseils. Le regroupement en lui-même ne modifie pas la trame de données, mais il a un impact sur la façon dont les statistiques récapitulatives suivantes sont calculées.  

Les lignes du haut sont présentées ci-dessous. Notez comment les colonnes ont complètement changé pour refléter les statistiques récapitulatives souhaitées. Chaque ligne reflète une semaine d'installation. 

```{r, warning=F, message=F}
agg_weeks <- agg_weeks %>%   

  # Regroupement en semaines d'installation
  group_by(location_name, week) %>%
  
  # Créez des colonnes de statistiques récapitulatives sur les données groupées
  summarize(
    n_days = 7, # 7 jours par semaine           
    n_reports = dplyr::n(), # nombre de rapports reçus par semaine (peut être >7)
    malaria_tot = sum(malaria_tot, na.rm = T), # nombre total de cas de paludisme signalés
    n_days_reported = length(unique(data_date)), # nombre de jours uniques de déclaration par semaine
    p_days_reported = round(100*(n_days_reported / n_days))) %>%      # pourcentage de jours de déclaration
  ungroup(location_name, week)    #ungroup() alors expand() marche dans les prochaines etapes
```

```{r, echo=F}
DT::datatable(head(agg_weeks,30), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space : nowrap')
```

Enfin, nous exécutons la commande ci-dessous pour nous assurer que TOUTES les semaines d'installation possibles sont présentes dans les données, même si elles étaient absentes auparavant.  

Nous utilisons un `right_join()` sur lui-même (l'ensemble de données est représenté par ".") mais il a été étendu pour inclure toutes les combinaisons possibles des colonnes `week` et `location_name`. Voir la documentation sur la fonction `expand()` dans la page sur [Pivoter les donnees](#pivoting_data). Avant d'exécuter ce code, l'ensemble de données contient `r nrow(agg_weeks)` lignes.   

```{r, message=F, warning=F}
# Créez un cadre de données pour chaque semaine d'installation possible.
expanded_weeks <- agg_weeks %>% 
  tidyr::expand(location_name, week) # étendre le cadre de données pour inclure toutes les combinaisons possibles établissement-semaine
                                          
```

Voici `expanded_weeks`, avec `r nrow(expanded_weeks)` lignes:  

```{r, echo=F}
DT::datatable(expanded_weeks, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space : nowrap')
```

Avant d'exécuter ce code, `agg_weeks` contient `r nrow(agg_weeks)` lignes.   

```{r}
# Utilisez une jointure à droite avec la liste étendue des semaines d'installation pour combler les lacunes dans les données.
agg_weeks <- agg_weeks %>%      
  right_join(expanded_weeks) %>% # Assurez-vous que toutes les combinaisons possibles de semaines d'installation apparaissent dans les données.
  mutate(p_days_reported = replace_na(p_days_reported, 0))  # Convertir les valeurs manquantes en 0                           
```

Après avoir exécuté ce code, `agg_weeks` contient `r nrow(agg_weeks)` lignes.   


<!-- ======================================================= -->
### Créer un graphique de chaleur {.unnumbered}


Le `ggplot()` est réalisé en utilisant `geom_tile()` du paquet **ggplot2** :  

* Les semaines sur l'axe des x sont transformées en dates, permettant l'utilisation de `scale_x_date()`.  
* L'axe des ordonnées affiche tous les noms des établissements.  
* Le "remplissage" est "p_days_reported", la performance pour cette semaine d'installation (numérique).  
* `scale_fill_gradient()` est utilisé sur le remplissage numérique, en spécifiant des couleurs pour le haut, le bas, et `NA`.  
* La fonction `scale_x_date()` est utilisée sur l'axe des x pour spécifier les étiquettes toutes les 2 semaines et leur format.  
* Les thèmes d'affichage et les étiquettes peuvent être ajustés si nécessaire.




<!-- ======================================================= -->
### Basique {.unnumbered}  

Un graphique thermique de base est produit ci-dessous, en utilisant les couleurs, les échelles, etc. par défaut. Comme expliqué ci-dessus, dans le `aes()` pour le `geom_tile()` vous devez fournir une colonne pour l'axe des x, une colonne pour l'axe des y, **et** une colonne pour le `fill = `. Le remplissage est la valeur numérique qui présente comme couleur de tuile.  

```{r}
ggplot(data = agg_weeks)+
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported))
```

### Tracé nettoyé {.unnumbered}

Nous pouvons améliorer l'apparence de ce graphique en ajoutant des fonctions **ggplot2** supplémentaires, comme indiqué ci-dessous. Voir la page sur les [astuces de ggplot](#ggplot_tips) pour plus de détails.  

```{r, message=FALSE, warning=FALSE}
ggplot(data = agg_weeks)+ 
  
  # affiche les données sous forme de tuiles
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported),      
    color = "white")+ # lignes de grille blanches
  
  scale_fill_gradient(
    low = "orange",
    high = "darkgreen",
    na.value = "grey80")+
  
  # axe des dates
  scale_x_date(
    expand = c(0,0), # supprimer l'espace supplémentaire sur les côtés
    date_breaks = "2 weeks", # étiquettes toutes les 2 semaines
    date_labels = "%d\n%b")+ # le format est jour sur mois (\n dans la nouvelle ligne)
  
  # thèmes esthétiques
  theme_minimal()+ # simplifier l'arrière-plan
  
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1, "cm"), # hauteur de la clé de légende
    legend.key.width = grid::unit(0.6, "cm"), # largeur de la clé de légende
    
    axis.text.x = element_text(size=12), # taille du texte de l'axe
    axis.text.y = element_text(vjust=0.2), # alignement du texte de l'axe
    axis.ticks = element_line(size=0.4),               
    axis.title = element_text(size=12, face="bold"), # taille et gras du titre de l'axe
    
    plot.title = element_text(hjust=0,size=14,face="bold"), # titre aligné à droite, large, gras
    plot.caption = element_text(hjust = 0, face = "italic"), # légende alignée à droite et en italique
    )+
  
  # étiquettes du graphique
  labs(x = "Semaine",
       y = "Nom de l'établissement",
       fill = "Reporting\nperformance (%)", # titre de la légende, car la légende montre le remplissage
       title = "Pourcentage de jours par semaine où l'établissement a déclaré des données",
       subtitle = "Établissements de santé du district, mai-juillet 2020",
       caption = "Semaines de 7 jours commençant le lundi")
```





<!-- ======================================================= -->
### Axe des y ordonné {.unnumbered}  

Actuellement, les installations sont ordonnées "alpha-numériquement" de bas en haut. Si vous voulez ajuster l'ordre des installations de l'axe des y, convertissez-les en facteur de classe et fournissez l'ordre. Voir la page sur les [Facteurs](#factors) pour des conseils.  

Puisqu'il y a beaucoup d'installations et que nous ne voulons pas les écrire toutes, nous allons essayer une autre approche - classer les installations dans un cadre de données et utiliser la colonne de noms résultante comme ordre de niveau de facteur. Ci-dessous, la colonne `location_name` est convertie en un facteur, et l'ordre de ses niveaux est établi sur la base du nombre total de jours de déclaration déposés par l'installation sur l'ensemble de la période.  

Pour ce faire, nous créons un cadre de données qui représente le nombre total de rapports par établissement, classés par ordre croissant. Nous pouvons utiliser ce vecteur pour ordonner les niveaux de facteurs dans le graphique.   

```{r}
facility_order <- agg_weeks %>% 
  group_by(location_name) %>% 
  summarize(tot_reports = sum(n_days_reported, na.rm=T)) %>% 
  arrange(tot_reports) # ordre ascendant
```

Voir le cadre de données ci-dessous :  

```{r, echo=F}
DT::datatable(facility_order, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space : nowrap')
```




Utilisez maintenant une colonne du cadre de données ci-dessus (`facility_order$location_name`) comme ordre des niveaux de facteur de `location_name` dans le cadre de données `agg_weeks` :  

```{r, warning=F, message=F}
# charger le paquet 
pacman::p_load(forcats)

# créer le facteur et définir les niveaux manuellement
agg_weeks <- agg_weeks %>% 
  mutate(location_name = fct_relevel(
    location_name, facility_order$location_name)
    )
```

Et maintenant, les données sont à nouveau tracées, le nom de l'emplacement étant un facteur ordonné :  

```{r, message=FALSE, warning=FALSE}
ggplot(data = agg_weeks)+ 
  
  # afficher les données sous forme de tuiles
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported),      
    color = "white")+ # lignes de grille blanches
  
  scale_fill_gradient(
    low = "orange",
    high = "darkgreen",
    na.value = "grey80")+
  
  # axe des dates
  scale_x_date(
    expand = c(0,0), # supprimer l'espace supplémentaire sur les côtés
    date_breaks = "2 weeks", # étiquettes toutes les 2 semaines
    date_labels = "%d\n%b")+ # le format est jour sur mois (\n dans la nouvelle ligne)
  
  # thèmes esthétiques
  theme_minimal()+ # simplifier l'arrière-plan
  
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1, "cm"), # hauteur de la clé de légende
    legend.key.width = grid::unit(0.6, "cm"), # largeur de la clé de légende
    
    axis.text.x = element_text(size=12), # taille du texte de l'axe
    axis.text.y = element_text(vjust=0.2), # alignement du texte de l'axe
    axis.ticks = element_line(size=0.4),               
    axis.title = element_text(size=12, face="bold"), # taille et gras du titre de l'axe
    
    plot.title = element_text(hjust=0,size=14,face="bold"), # titre aligné à droite, large, gras
    plot.caption = element_text(hjust = 0, face = "italic"), # légende alignée à droite et en italique
    )+
  
  # étiquettes du graphique
  labs(x = "Semaine",
       y = "Nom de l'établissement",
       fill = "Reporting\nperformance (%)", # titre de la légende, car la légende montre le remplissage
       title = "Pourcentage de jours par semaine où l'établissement a déclaré des données",
       subtitle = "Établissements de santé du district, mai-juillet 2020",
       caption = "Semaines de 7 jours commençant le lundi")
```





<!-- ======================================================= -->
### Afficher les valeurs {.unnumbered}  


Vous pouvez ajouter une couche `geom_text()` au dessus des tuiles, pour afficher les numéros réels de chaque tuile. Attention, cela peut ne pas être joli si vous avez beaucoup de petites tuiles !  

Le code suivant a été ajouté : `geom_text(aes(label = p_days_reported))`. Ceci ajoute du texte sur chaque tuile. Le texte affiché est la valeur assignée à l'argument `label =`, qui dans ce cas a été fixé à la même colonne numérique `p_days_reported` qui est aussi utilisée pour créer le gradient de couleur.  



  
```{r, message=FALSE, warning=FALSE}
ggplot(data = agg_weeks)+ 
  
  # affiche les données sous forme de tuiles
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported),      
    color = "white")+ # lignes de grille blanches
  
  # Texte
  geom_text(
    aes(
      x = week,
      y = location_name,
      label = p_days_reported))+ # ajouter le texte au dessus de la tuile
  
  # remplir l'échelle
  scale_fill_gradient(
    low = "orange",
    high = "darkgreen",
    na.value = "grey80")+
  
  # axe des dates
  scale_x_date(
    expand = c(0,0), # supprimer l'espace supplémentaire sur les côtés
    date_breaks = "2 weeks", # étiquettes toutes les 2 semaines
    date_labels = "%d\n%b")+ # le format est jour sur mois (\n dans la nouvelle ligne)
  
  # thèmes esthétiques
  theme_minimal()+ # simplifier l'arrière-plan
  
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1, "cm"), # hauteur de la clé de légende
    legend.key.width = grid::unit(0.6, "cm"), # largeur de la clé de légende
    
    axis.text.x = element_text(size=12), # taille du texte de l'axe
    axis.text.y = element_text(vjust=0.2), # alignement du texte de l'axe
    axis.ticks = element_line(size=0.4),               
    axis.title = element_text(size=12, face="bold"), # taille et gras du titre de l'axe
    
    plot.title = element_text(hjust=0,size=14,face="bold"), # titre aligné à droite, large, gras
    plot.caption = element_text(hjust = 0, face = "italic"), # légende alignée à droite et en italique
    )+
  
  # étiquettes du graphique
  labs(x = "Semaine",
       y = "Nom de l'établissement",
       fill = "Reporting\nperformance (%)", # titre de la légende, car la légende montre le remplissage
       title = "Pourcentage de jours par semaine où l'établissement a déclaré des données",
       subtitle = "Établissements de santé du district, mai-juillet 2020",
       caption = "Semaines de 7 jours commençant le lundi")
```




<!-- ======================================================= -->
## Ressources { }

[scale_fill_gradient()](https://ggplot2.tidyverse.org/reference/scale_gradient.html)  

[Galerie de graphiques R - carte thermique](https://ggplot2.tidyverse.org/reference/scale_gradient.html) 
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/heatmaps.Rmd-->


# Diagrammes et schémas {#diagrams}  



```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "flow_chart.png"))
knitr::include_graphics(here::here("images", "sankey_diagram.png"))
```


Cette page passe en revue le code pour produire:

* Des diagrammes de flux en utilisant **DiagrammemeR** et le langage DOT.  
* Diagrammes Alluviaux/Sankey  
* Des chronologies d'événements  

<!--DAGs (Directed Acyclic Graphs) -->
<!-- Graphiques de GANTT -->


<!-- ======================================================= -->
## Préparation { }

### Chargement des paquets {.unnumbered}  

Ce chunk de code montre le chargement des paquets nécessaires pour les analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* le charge pour l'utiliser. Vous pouvez également charger les paquets installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les paquets bases de R.  

```{r}
pacman::p_load(
  DiagrammeR, # pour les diagrammes de flux
  networkD3, # pour les diagrammes alluviaux/Sankey
  tidyverse) # gestion et visualisation des données
```

### Importer des données {.unnumbered}  

La plupart du contenu de cette page ne nécessite pas de jeu de données. Cependant, dans la section sur le diagramme de Sankey, nous utiliserons la liste de cas d'une simulation d'épidémie d'Ebola. Si vous souhaitez suivre cette partie, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la liste de cas "propre"</a> (en fichier format .rds). Importez les données avec la fonction `import()` du paquet **rio** (elle gère de nombreux types de fichiers comme .xlsx, .csv, .rds - voir la page [Importation et exportation](#import_export) pour plus de détails).  

```{r, echo=F}
# Importez la liste de diffusion dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# Importez la liste de cas
linelist <- import("linelist_cleaned.rds")
```

Les 50 premières lignes de la linelist sont affichées ci-dessous.

```{r, message=FALSE, echo=F}
# affiche les données de la liste de diffusion sous forme de tableau
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space : nowrap' )
```



<!-- ======================================================= -->
## Diagrammes de flux { }

On peut utiliser le paquet R **DiagrammeR** pour créer des diagrammes/schémas de flux. Ils peuvent être statiques, ou s'ajuster  dynamiquement en fonction des changements dans un ensemble de données.  

**Outils**  

La fonction `grViz()` est utilisée pour créer un diagramme "Graphviz". Cette fonction accepte une *chaîne de caractères en entrée contenant les instructions* pour réaliser le diagramme. Dans cette chaîne de caractères, les instructions sont écrites dans un langage différent, [DOT](https://graphviz.org/doc/info/lang.html); il est assez facile d'apprendre les bases de le langage DOT.

**Structure de base**  

1) Ouvrez les instructions `grViz("`  
2) Spécifiez la direction et le nom du graphique, et ouvrez les parenthèses, par exemple `digraph mon_diagramme_de_flux {`
3) Déclaration du graphique (disposition, direction du rang)  
4) Déclaration des noeuds (crée les noeuds)
5) Déclarations de contours/bords (donne les liens entre les noeuds)  
6) Fermer les instructions `}")`  

### Exemples simples {.unnumbered} 

Vous trouverez ci-dessous deux exemples simples  

Un exemple très minimal :  

```{r out.width='50%'}
## Un tracé minimal
DiagrammeR::grViz("digraph {
  
graph[layout = dot, rankdir = LR]

a
b
c

a -> b -> c
}")
```

Un exemple avec un contexte de santé publique un peu plus appliqué :  

```{r out.width='50%'}
# Toutes les instructions se trouvent dans une grande chaîne de caractères
# 'digraph' signifie 'graphique directionnel', puis le nom du graphique 

# déclaration du graphique, déclaration des noueds, forme et largeur de noueds, noms des noeuds, bords ou contours
  #######
  #################

grViz("    
digraph surveillance_diagram { 
  
  
  graph [layout = dot,
         rankdir = TB,
         overlap = true,
         fontsize = 10]
  
  
  node [shape = circle, 
       fixedsize = true
       width = 1.3]               
  
  Primary
  Secondary
  Tertiary

  
  Primary -> Secondary [label = ' transfert de cas']
  Secondary -> Tertiary [label = ' transfert de cas']
}
")
```

### Syntaxe {.unnumbered}

**Syntaxe de base**  

Les noms de nouds, ou les déclarations de bords, peuvent être séparés par des espaces, des points-virgules ou des nouvelles lignes.  

**Direction du rang**  

Un graphique peut être réorienté pour se déplacer de gauche à droite en ajustant l'argument `rankdir` dans la déclaration du graphique. Le défaut est TB (top-to-bottom; de haut en bas), mais il peut être LR (left-to-right, gauche-à-droite), ou l'inverse
 (RL,BT).  

**Noms de nouds**  

Les noms de noeuds peuvent être des mots simples, comme dans l'exemple ci-dessus. Pour utiliser des noms de plusieurs mots ou des caractères spéciaux (par exemple, parenthèses, tirets), placez le nom du noud entre guillemets simples (' '). Il peut être plus facile d'avoir un nom de nœud court et d'attribuer un *label*, comme indiqué ci-dessous entre crochets [ ]. Si vous voulez avoir une nouvelle ligne dans le nom du nœud, vous devez le faire via une étiquette. Utilisez `\n` dans l'étiquette du nœud entre guillemets simples, comme indiqué ci-dessous.  

**Sous-groupes**  
Dans les déclarations des bords/contours, des sous-groupes peuvent être créés de chaque côté de le bords avec des crochets ({ }). Le bord s'applique alors à tous les nouds entre crochets. Ceci est un raccourci.  


**Mise en page**  

* dot (définir `rankdir` comme soit TB, LR, RL, ou BT)
* neato  
* twopi  
* circo  


**Noeuds - attributs modifiables**  

* `label` (texte, entre guillemets simples si plusieurs mots)  
* `fillcolor` (plusieurs couleurs possibles)  
* `fontcolor`  
* `alpha` (transparence 0-1)  
* `shape` (ellipse, ovale, diamant, ouf, texte en clair, point, carré, triangle)  
* `style`  
* `sides`  
* `peripheries`  
* `fixedsize` (h x l)  
* `height`  
* `width`  
* `distortion`  
* `penwidth` (largeur de la bordure de la forme)  
* `x` (déplacement gauche/droite)  
* `y` (déplacement haut/bas)  
* `fontname`  
* `fontsize`  
* `icon`  


**Bords - attributs modifiables**  

* `arrowsize`  
* `arrowhead` (normal, box, crow, curve, diamond, dot, inv, none, tee, vee)  
* `arrowtail`  
* `dir` (direction, )  
* `style` (pointillé, ...)  
* `color`  
* `alpha`  
* `headport` (texte devant la tête de la flèche)  
* `tailport` (texte situé derrière la queue de flèche)  
* `fontname`  
* `fontsize`  
* `fontcolor` (couleur de la police)  
* `penwidth` (largeur de la flèche)  
* `minlen` (longueur minimale)

**Noms de couleurs** : valeurs hexadécimales ou noms de couleurs 'X11', voir [ici pour les détails sur X11](http://rich-iannone.github.io/DiagrammeR/graphviz_and_mermaid.html) 


### Exemples complexes {.unnumbered}

L'exemple ci-dessous développe le diagramme de surveillance, en ajoutant des noms de noeuds complexes, des bords groupées, des couleurs et un style spécifique.




```{r out.width='50%'}
# Toutes les instructions se trouvent dans une grande chaîne de caractères
# 'digraph' signifie 'graphique directionnel', puis le nom du graphique 
# déclaration du graphique
# disposition de haut en bas
  #################
# nouds (formes cercles)
  #################
  #bords et bord groupé



DiagrammeR::grViz(" 
digraph surveillance_diagram { 
  
  
  graph [layout = dot,
         rankdir = TB, 
         fontsize = 10]
  

  
  node [shape = circle, 
       fixedsize = true
       width = 1.3]                      
  
  Primary [label = 'Site Primaire'] 
  Secondary [label = 'Site Secondaire'] 
  Tertiary [label = 'Site Tertiaire'] 
  SC [label = 'Coordination de\nla Surveillance',
             fontcolor = darkgreen] 


  Primary -> Secondary [label = 'Transfert de cas',
                          fontcolor = red,
                          color = red]
  Secondary -> Tertiary [label = 'Transfert de cas',
                          fontcolor = red,
                          color = red]
  

  {Primary Secondary Tertiary} -> SC [label = 'déclaration des cas',
                                      fontcolor = darkgreen,
                                      couleur = darkgreen,
                                      style = dashed]
}
")
```

**Groupements de sous-graphiques**  

Pour regrouper les noeuds dans des clusters encadrés, placez-les dans le même sous-graphique nommé (`subgraph name {}`). Pour que chaque sous-graphe soit identifié dans une boîte de délimitation, commencez le nom du sous-graphique par "cluster", comme le montrent les 4 boîtes ci-dessous.  


```{r out.width='120%'}
DiagrammeR::grViz("             # All instructions are within a large character string
digraph surveillance_diagram {  # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,            
         overlap = true,
         fontsize = 10]
  
  # nodes (circles)
  #################
  node [shape = circle,                  # shape = circle
       fixedsize = true
       width = 1.3]                      # width of circles
  
  subgraph cluster_passive {
    Primary   [label = 'Site Primaire'] 
    Secondary [label = 'Site Secondaire'] 
    Tertiary  [label = 'Site Tertiaire'] 
    SC        [label = 'Coordination de\nla Surveillance',
               fontcolor = darkgreen] 
  }
  
  # nodes (boxes)
  ###############
  node [shape = box,                     # node shape
        fontname = Helvetica]            # text font in node
  
  subgraph cluster_active {
    Active [label = 'Surveillance\nActive'] 
    HCF_active [label = 'HCF\nRecherche Active']
  }
  
  subgraph cluster_EBD {
    EBS [label = 'Surveillance basée sur\n les événements (SBE)'] 
    'Social Media'
    Radio
  }
  
  subgraph cluster_CBS {
    CBS [label = 'Surveillance basée sur\n les communautés(SBC)']
    RECOs
  }
  
  # edges
  #######
  {Primary Secondary Tertiary} -> SC [label = 'déclaration des cas']
  Primary   -> Secondary [label = 'transfert de cas',
                          fontcolor = red]
  Secondary -> Tertiary [label = 'transfert de cas',
                          fontcolor = red]
  
  HCF_active -> Active
  
  {'Social Media' Radio} -> EBS
  
  RECOs -> CBS
}
")
```



**Formes des nouds** 

L'exemple ci-dessous, emprunté à [ce tutoriel](http://rich-iannone.github.io/DiagrammeR/), montre les formes de nouds appliquées et une abréviation pour les connexions de bords en série.  

```{r out.width='75%'}

# définir les styles globaux des noeuds. Nous pouvons les remplacer dans la boîte si nous le souhaitons.
# définitions des bords avec les ID des nouds
DiagrammeR::grViz("digraph {

graph [layout = dot, rankdir = LR]


node [shape = rectangle, style = filled, fillcolor = Linen]

data1 [label = 'Dataframe 1', shape = folder, fillcolor = Beige]
data2 [label = 'Dataframe 2', shape = folder, fillcolor = Beige]
process [label = 'Process \n Data']
statistical [label = 'Analyse\nStatistique'] 
results [label= 'Résultats']


{data1 data2} -> process -> statistical -> results
}")
```


### Sorties {.unnumbered}

Comment gérer et sauvegarder les sorties  

* Les résultats apparaîtront dans le volet de visualisation de RStudio, par défaut dans le coin inférieur droit, à côté de Files, Plots, Packages et Help.  
* Pour exporter, vous pouvez "Enregistrer en tant qu'image" ou "Copier a le presse-papiers" à partir de la Viewer. Le graphique s'ajustera à la taille spécifiée.  




### Figures paramétrées {.unnumbered} 

Voici une citation de ce tutoriel : https://mikeyharper.uk/flowcharts-in-r-using-diagrammer/  

"Figures paramétrées : L'un des grands avantages de la conception de figures dans R est que nous sommes en mesure de connecter les figures directement à notre analyse en lisant les valeurs R directement dans nos schemas de flux. Par exemple, supposons que vous ayez créé un processus de filtrage qui supprime les valeurs après chaque étape d'un processus, vous pouvez avoir une figure montrant le nombre de valeurs restantes dans l'ensemble de données après chaque étape de votre processus. Pour ce faire, vous pouvez utiliser le symbole @@X directement dans la figure, puis y faire référence dans le pied de page du graphique en utilisant [X] :, où X est un indice numérique unique."  

Nous vous encourageons à revoir ce tutoriel si le paramétrage est quelque chose qui vous intéresse.  


<!-- />Et ci-dessous, vous trouverez un exemple de code tiré de ce tutoriel. -->

<!-- ``{r, eval=F} -->
<!-- # Définir quelques exemples de données -->
<!-- data <- list(a=1000, b=800, c=600, d=400) -->


<!-- DiagrammeR::grViz(" -->
<!-- digraphe graph2 { -->

<!-- graphe [disposition = point] -->

<!-- # définitions de nouds avec texte d'étiquette substitué -->
<!-- noeud [forme = rectangle, largeur = 4, couleur de remplissage = Biege] -->
<!-- a [label = '@@1'] -->
<!-- b [label = '@@2'] -->
<!-- c [label = '@@3'] --> <!-- c [label = '@@3'] -->
<!-- d [label = '@@4'] --> <!-- d [label = '@@4'] -->

<!-- a -> b -> c -> d -->

<!-- } -->

<!-- [1] : paste0('Données brutes (n = ', data$a, ')') -->
<!-- [2] : paste0('Suppression des erreurs (n = ', data$b, ')') -->
<!-- [3] : paste0('Identifier les clients potentiels (n = ', data$c, ')') -->
<!-- [4] : paste0('Sélectionner les principales priorités (n = ', data$d, ')') -->
<!-- ") -->

<!-- ``` -->



<!-- ### Diagramme CONSORT {.unnumbered} -->

<!-- CETTE SECTION EST EN CONSTRUCTION -->

<!-- https://scriptsandstatistics.wordpress.com/2017/12/22/how-to-draw-a-consort-flow-diagram-using-r-and-graphviz/ -->

<!-- La note ci-dessus est périmée via DiagrammeR -->




<!-- ======================================================= -->
## Diagrammes Alluvial/Sankey { }

### Chargement des paquets {.unnumbered}  

Ce morceau de code montre le chargement des paquets nécessaires aux analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* le charge pour l'utiliser. Vous pouvez également charger les paquets installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les paquets R.  

Nous chargeons le paquet **networkD3** pour produire le diagramme, et aussi **tidyverse** pour les étapes de préparation des données.  

```{r}
pacman::p_load(
  networkD3,
  tidyverse)
```

### Graphique à partir d'un ensemble de données {.unnumbered} 

Tracer les connexions dans un jeu de données. Nous démontrons ci-dessous l'utilisation de le package **networkD3** sur le cas `linelist`. Voici un [tutoriel en ligne](https://www.r-graph-gallery.com/321-introduction-to-interactive-sankey-diagram-2.html).    

Nous commençons par obtenir le nombre de cas pour chaque combinaison unique de catégorie d'âge et d'hôpital. Pour plus de clarté, nous avons supprimé les valeurs dont la catégorie d'âge est manquante. Nous renommons également les colonnes `hospital` et `age_cat` en `source` et `target` respectivement. Ce seront les deux côtés du diagramme alluvial.  

```{r}
# comptes par hôpital et par catégorie d'âge
links <- linelist %>% 
  drop_na(age_cat) %>% 
  select(hospital, age_cat) %>%
  count(hospital, age_cat) %>% 
  rename(source = hospital,
         target = age_cat)
```

L'ensemble de données ressemble maintenant à ceci :  

```{r message=FALSE, echo=F}
DT::datatable(links, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```


Maintenant, nous créons un jeu de données de tous les noeuds du diagramme, sous la colonne `name`. Il s'agit de toutes les valeurs de `hospital` et `age_cat`. Notez que nous nous assurons qu'elles sont toutes de classe caractères avant de les combiner, et ajustons les colonnes `ID` pour qu'elles soient des numeros au lieu d'étiquettes :  

```{r}
# Les noms uniques des noeuds
nodes <- data.frame(
  name=c(as.character(links$source), as.character(links$target)) %>% 
    unique()
  )

nodes # imprimer
```

Nous éditons le cadre de données `links`, que nous avons créé ci-dessus avec `count()`. Nous ajoutons deux colonnes numériques, `IDsource` et `IDtarget`, qui reflèteront/créeront réellement les liens entre les noeuds. Ces colonnes contiendront les numéros numéros de rangs (position) des noeuds de source et de target. On soustrait 1 pour que ces numéros de position commencent à 0 (et pas à 1).  

```{r}
# correspond aux nombres, pas aux noms
links$IDsource <- match(links$source, nodes$name)-1 
links$IDtarget <- match(links$target, nodes$name)-1
```

Le jeu de données des liens ressemble maintenant à ceci :  

```{r message=FALSE, echo=F}
DT::datatable(links, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

Tracez maintenant le diagramme de Sankey avec `sankeyNetwork()`. Vous pouvez en savoir plus sur chaque argument en exécutant `?sankeyNetwork` dans la console. Notez que si vous ne définissiez pas `iterations = 0`, l'ordre de vos noeuds ne serait pas celui attendu. 


```{r}

# graphique
######
p <- sankeyNetwork(
  Links = links,
  Nodes = nodes,
  Source = "IDsource",
  Target = "IDtarget",
  Value = "n",
  NodeID = "name",
  units = "TWh",
  fontSize = 12,
  nodeWidth = 30,
  iterations = 0) # Assurez-vous que l'ordre des noeuds est celui des données.
p
```



Voici un exemple où le résultat du patient est également inclus. Notez que dans l'étape de préparation des données, nous devons calculer le nombre de cas entre l'âge et l'hôpital, et séparément entre l'hôpital et le résultat - puis lier tous ces comptes ensemble avec `bind_rows()`.  

```{r}
# Nombre de cas par hôpital et par catégorie d'âge
age_hosp_links <- linelist %>% 
  drop_na(age_cat) %>% 
  select(hospital, age_cat) %>%
  count(hospital, age_cat) %>% 
  rename(source = age_cat, 
         target = hospital)

hosp_out_links <- linelist %>% 
    drop_na(age_cat) %>% 
    select(hospital, outcome) %>% 
    count(hospital, outcome) %>% 
    rename(source = hospital, 
           target = outcome)

# combiner les liens
links <- bind_rows(age_hosp_links, hosp_out_links)

# Les noms uniques des noeuds
nodes <- data.frame(
  name=c(as.character(links$source), as.character(links$target)) %>% 
    unique()
  )

# Créer des numéros d'identification
links$IDsource <- match(links$source, nodes$name)-1 
links$IDtarget <- match(links$target, nodes$name)-1

# graphique
######
p <- sankeyNetwork(Links = links,
                   Nodes = nodes,
                   Source = "IDsource",
                   Target = "IDtarget",
                   Value = "n",
                   NodeID = "name",
                   units = "TWh",
                   fontSize = 12,
                   nodeWidth = 30,
                   iterations = 0)
p

```


https://www.displayr.com/sankey-diagrams-r/



<!-- ======================================================= -->
## Chronologie des événements { }

Pour faire une timeline montrant des événements spécifiques, vous pouvez utiliser le paquet `vistime`.

Voir cette [vignette](https://cran.r-project.org/web/packages/vistime/vignettes/vistime-vignette.html#ex.-2-project-planning)

```{r}
# charger le paquet
pacman::p_load(vistime, # créer la ligne de temps
               plotly # pour la visualisation interactive
               )
```

```{r, echo=F}
# référence : https://cran.r-project.org/web/packages/vistime/vignettes/vistime-vignette.html#ex.-2-project-planning

data <- read.csv(text="event, group, start, end, color
                       Event 1, Group A,2020-01-22,2020-01-22, #90caf9
                       Event 1, Group B,2020-01-23,2020-01-23, #90caf9
                       Event 1, Group C,2020-01-23,2020-01-23, #1565c0
                       Event 1, Group D,2020-01-25,2020-01-25, #f44336
                       Event 1, Group E,2020-01-25,2020-01-25, #90caf9
                       Event 1, Group F,2020-01-26,2020-01-26, #8d6e63
                       Event 1, Group G,2020-01-27,2020-01-27, #1565c0
                       Event 1, Group H,2020-01-27,2020-01-27, #90caf9
                       Event 1, Group I,2020-01-27,2020-01-27, #90a4ae
                       Event 2, Group A,2020-01-28,2020-01-28, #fc8d62
                       Event 2, Group C,2020-01-28,2020-01-28, #6a3d9a
                       Event 2, Group J,2020-01-28,2020-01-28, #90caf9
                       Event 2, Group J,2020-01-28,2020-01-28, #fc8d62
                       Event 2, Group J,2020-01-28,2020-01-28, #1565c0
")
```

Voici l'ensemble de données d'événements avec lequel nous commençons :  

```{r message=FALSE, echo=F}
DT::datatable(data, rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```



```{r}
p <- vistime(data) # appliquer vistime

library(plotly)

# étape 1 : transformation en liste
pp <- plotly_build(p)

# étape 2 : taille des marqueurs
for(i in 1:length(pp$x$data)){
  if(pp$x$data[[i]]$mode == "markers") pp$x$data[[i]]$marker$size <- 10
}

# étape 3 : taille du texte
for(i in 1:length(pp$x$data)){
  if(pp$x$data[[i]]$mode == "text") pp$x$data[[i]]$textfont$size <- 10
}


# étape 4 : position du texte
for(i in 1:length(pp$x$data)){
  if(pp$x$data[[i]]$mode == "text") pp$x$data[[i]]$textposition <- "right"
}

#imprimer
pp

```



<!-- ======================================================= -->
## DAGs { }

Vous pouvez construire un DAG manuellement en utilisant le paquet **DiagammeR** et le langage DOT comme décrit ci-dessus.  

Alternativement, il existe des paquets comme **ggdag** et **daggity**.

[Introduction aux DAGs - vignette ggdag](https://cran.r-project.org/web/packages/ggdag/vignettes/intro-to-dags.html)   

[Inférence causale avec les dags dans R](https://www.r-bloggers.com/2019/08/causal-inference-with-dags-in-r/#:~:text=En%20a%20DAG%20tout%20le,pour%20 dessiner%20et%20analyser%20DAGs.)  





<!-- ======================================================= -->
## Ressources { }



Une grande partie de ce qui précède concernant le langage DOT est adaptée du tutoriel [sur ce site](https://mikeyharper.uk/flowcharts-in-r-using-diagrammer/).  

Un autre [tutoriel sur DiagammeR](http://rich-iannone.github.io/DiagrammeR/) plus approfondi.

Ici, un page sur les [diagrammes de Sankey](https://www.displayr.com/sankey-diagrams-r/).
  

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/diagrams.Rmd-->


# Analyse des combinaisons {#combination_analysis}  

```{r echo=F, out.width="75%", warning=F, message=F}
pacman::p_load(tidyverse,
               UpSetR,
               ggupset)

# Ajoute de nouvelles variables de symptôme à la linelist, avec des valeurs aléatoires "oui" ou "non". 
linelist_sym <- linelist %>% 
  mutate(fever = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.80, 0.20)),
         chills = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.20, 0.80)),
         cough = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.9, 0.15)),
         aches = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.10, 0.90)),
         vomit = sample(c("yes", "no"), nrow(linelist), replace = T))

linelist_sym_2 <- linelist_sym %>% 
  
  # convertir les valeurs "oui" et "non" en le nom du symptôme lui-même
   mutate(across(.cols = c(fever, chills, cough, aches, vomit),
                 .fns = ~+(.x == "yes")))   
     
  #mutate(across(c("fever", "chills", "cough", "aches", "vomit"), ~ifelse(.x = "yes", colnames(.)[which(colnames(.) == "fever")], 0)))  

# Créer le graphique
UpSetR::upset(
  select(linelist_sym_2, fever, chills, cough, aches, vomit),
  sets = c("fever", "chills", "cough", "aches", "vomit"),
  order.by = "freq",
  sets.bar.color = c("blue", "red", "yellow", "darkgreen", "orange"), # couleurs optionnelles
  empty.intersections = "on",
  # nsets = 3,
  number.angles = 0,
  point.size = 3.5,
  line.size = 2, 
  mainbar.y.label = "Symptoms Combinations",
  sets.x.label = "Patients with Symptom")

```



Cette analyse représente la fréquence des différentes **combinaisons** de valeurs/réponses. Dans cet exemple, nous traçons la fréquence à laquelle les cas ont présenté diverses combinaisons de symptômes.  

Cette analyse est aussi souvent appelée :  

* **"Analyse des réponses multiples"**  
* **"Analyse des ensembles"**  
* **"Analyse des combinaisons"**  

Dans l'exemple de graphique ci-dessus, cinq symptômes sont représentés. Sous chaque barre verticale se trouve une ligne et des points indiquant la combinaison de symptômes reflétée par la barre ci-dessus. À droite, des barres horizontales reflètent la fréquence de chaque symptôme individuel.  

La première méthode que nous montrons utilise le paquet **ggupset**, et la seconde utilise le paquet **UpSetR**. 




  



<!-- ======================================================= -->
## Préparation { }

### Chargement des paquets {.unnumbered}

Ce chunk de code montre le chargement des paquets nécessaires aux analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* le charge pour l'utiliser. Vous pouvez également charger les paquets installés avec `library()` de **base** R. Voir la page sur [R - les bases](#rbasics) pour plus d'informations sur les paquets R.  

```{r, warning=F, message=F}
pacman::p_load(
  tidyverse, # gestion et visualisation de données
  UpSetR,    # paquet spécial pour les graphiques combinés
  ggupset)   # paquet spécial pour les tracés combinés
```

<!-- ======================================================= -->
### Importer les données {.unnumbered}  


Pour commencer, nous importons la linelist nettoyée des cas d'une épidémie d'Ebola simulée. Si vous voulez suivre, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la linelist "propre"</a> (en tant que fichier .rds). Importez des données avec la fonction `import()` du paquet **rio** (elle gère de nombreux types de fichiers comme .xlsx, .csv, .rds - voir la page [Importer et exporter des données](#import_export) pour plus de détails).  



```{r, echo=F}
# Importez la liste de lignes dans R
linelist_sym <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# importer la liste de cas linelist 
linelist_sym <- import("linelist_cleaned.rds")
```


Cette linelist comprend cinq variables "oui/non" sur les symptômes déclarés. Nous devrons transformer un peu ces variables pour utiliser le paquet **ggupset** afin de réaliser notre tracé. Visualisez les données (faites défiler vers la droite pour voir les variables de symptômes).  

```{r, message=FALSE, echo=F}
# affichez les données de la liste des lignes sous forme de tableau
DT::datatable(head(linelist_sym, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
### Re-formatage des valeurs {.unnumbered}  

Pour s'aligner sur le format attendu par **ggupset**, nous convertissons les "yes" ("oui") et "no" ("non") en nom de symptôme réel, en utilisant `case_when()` de **dplyr**. Si "non", nous mettons la valeur en blanc, donc les valeurs sont soit `NA` ou le symptôme.  
 

```{r, warning=F, message=F}
# crée une colonne avec les symptômes nommés, séparés par des points-virgules
linelist_sym_1 <- linelist_sym %>% 
  
  # convertissez les valeurs "oui" et "non" dans le nom du symptôme lui-même
  mutate(fever = ifelse(fever == "yes", "fever", NA), 
       chills = ifelse(chills == "yes", "chills", NA),
       cough = ifelse(cough == "yes", "cough", NA),
       aches = ifelse(aches == "yes", "aches", NA),
       vomit = ifelse(vomit == "yes", "vomit", NA))
```

Maintenant, nous faisons deux dernières colonnes :  

1. Concaténation (collage) de tous les symptômes du patient (une colonne de caractères)  
2. Convertir la colonne ci-dessus en classe *list*, afin qu'elle puisse être acceptée par **ggupset** pour faire le graphe.  

Voir la page sur [Caractères et chaînes de caractères](#character_strings) pour en savoir plus sur la fonction `unite()` de **stringr**.

```{r, warning=F, message=F}
linelist_sym_1 <- linelist_sym_1 %>% 
  unite(col = "all_symptoms",
        c(fever, chills, cough, aches, vomit), 
        sep = " ; ",
        remove = TRUE,
        na.rm = TRUE) %>% 
  mutate(
    # Faites une copie de la colonne all_symptoms, mais de la classe "list" (qui est nécessaire pour utiliser ggupset() à l'étape suivante).
    all_symptoms_list = as.list(strsplit(all_symptoms, " ; "))
    )
```

Visualisez les nouvelles données. Notez les deux colonnes vers l'extrémité droite - les valeurs combinées collées, et la liste

```{r, echo=F, , warning=F, message=F}
DT::datatable(head(linelist_sym_1,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```


<!-- ======================================================= -->
## **ggupset** { }

Charger le paquet

```{r}
pacman::p_load(ggupset)
```


Créez le graphique. Nous commençons par un `ggplot()` et un `geom_bar()`, mais ensuite nous ajoutons la fonction spéciale `scale_x_upset()` du **ggupset**.  

```{r, warning=F, message=F}
ggplot(
  data = linelist_sym_1,
  mapping = aes(x = all_symptoms_list)) +
geom_bar() +
scale_x_upset(
  reverse = FALSE,
  n_intersections = 10,
  sets = c("fever", "chills", "cough", "aches", "vomit"))+
labs(
  title = "Signes et symptômes",
  subtitle = "Les 10 combinaisons les plus fréquentes de signes et de symptômes",
  caption = "Caption ici",
  x = "Combinaison de symptômes",
  y = "Fréquence dans l'ensemble de données")

```
  
De plus amples informations sur **ggupset** peuvent être trouvées [en ligne](https://rdrr.io/cran/ggupset/man/scale_x_upset.html) ou hors ligne dans la documentation du paquet dans votre onglet d'aide RStudio `?ggupset`.  


<!-- ======================================================= -->
## `UpSetR` { }

Le paquet **UpSetR** permet de personnaliser davantage le tracé, mais il peut être plus difficile à exécuter :


**Chargez le paquet**  

```{r}
pacman::p_load(UpSetR)
```

**Nettoyage des données**  

Nous devons convertir les valeurs des symptômes de la `linelist` en 1 / 0. 

```{r}
linelist_sym_2 <- linelist_sym %>% 
  
  # convertissez les valeurs "oui" et "non" dans le nom du symptôme lui-même
     mutate(fever = ifelse(fever == "yes", 1, 0), 
            chills = ifelse(chills == "yes", 1, 0),
            cough = ifelse(cough == "yes", 1, 0),
            aches = ifelse(aches == "yes", 1, 0),
            vomit = ifelse(vomit == "yes", 1, 0))

```

Si vous êtes intéressé par une commande plus efficace, vous pouvez profiter de la fonction `+()`, qui convertit les 1 et les 0 en fonction d'une déclaration logique. Cette commande utilise la fonction `across()` pour modifier plusieurs colonnes à la fois (pour en savoir plus, lisez la section [Nettoyage des données et des fonctions de base](#clean_across)).  

```{r, eval=F, echo=T}
# convertissez "oui" et "non" a 1 et 0
linelist_sym_2 <- linelist_sym %>% 
  
  mutate(across(c(fever, chills, cough, aches, vomit), .fns = ~+(.x == "yes")))

```

Maintenant, faites le graphique en utilisant la fonction personnalisée `upset()` - en utilisant seulement les colonnes de symptômes. Vous devez désigner les "ensembles" à comparer (les noms des colonnes de symptômes). Vous pouvez aussi utiliser `nsets = ` et `order.by = "freq"` pour n'afficher que les X combinaisons les plus importantes.  

```{r, warning=F, message=F}

# Créer le graphique
UpSetR::upset(
  select(linelist_sym_2, fever, chills, cough, aches, vomit),
  sets = c("fever", "chills", "cough", "aches", "vomit"),
  order.by = "freq",
  sets.bar.color = c("blue", "red", "yellow", "darkgreen", "orange"), # couleurs optionnelles
  empty.intersections = "on",
  # nsets = 3,
  number.angles = 0,
  point.size = 3.5,
  line.size = 2, 
  mainbar.y.label = "Symptoms Combinations",
  sets.x.label = "Patients with Symptom")

```


<!-- ======================================================= -->
## Ressources { }

[La page github de UpSetR](https://github.com/hms-dbmi/UpSetR)  

[Une version Shiny App - vous pouvez télécharger vos propres données](https://gehlenborglab.shinyapps.io/upsetr/)  

[*documentation - difficile à interpréter](https://cran.r-project.org/web/packages/UpSetR/UpSetR.pdf) 
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/combination_analysis.Rmd-->


# Chaînes de transmission {#transmission_chains}


<!-- ======================================================= -->
## Aperçu { }

L'outil principal pour manipuler, analyser, et visualiser les chaînes de transmission et les données de recherche de contact est le paquet **epicontacts**, développé par [RECON](https://www.repidemicsconsortium.org/). Essayez le graphique interactif ci-dessous en passant la souris sur les noeuds pour obtenir plus d'informations et en cliquant dessus pour surligner les cas descendants.

```{r out.width=c('25%', '25%'), fig.show='hold', echo=F}

## installer la version de développement de epicontacts
if(
  !"epicontacts" %in% rownames(installed.packages()) |
  packageVersion("epicontacts") != "1.2.0"
) remotes::install_github("reconhub/epicontacts@timeline")

## installer et charger les paquets
pacman::p_load(tidyverse, epicontacts, magrittr, here, webshot, visNetwork)

## charger la liste de diffusion
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds")) %>%
  filter(!duplicated(case_id))

## générer des contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id,
    location = sample(c("Community", "Nosocomial"), n(), TRUE),
    duration = sample.int(10, n(), TRUE)
  ) %>%
  drop_na(from)

## générer des epicontacts
epic <- epicontacts::make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)

## objet subset
epic %<>% subset(
  node_attribute = list(date_onset = c(as.Date(c("2014-06-01", "2014-07-01"))))
) %>%
  thin("contacts")

## Graphique avec la date d'apparition en axe x
plot(
  epic,
  x_axis = "date_onset",
  label = FALSE,
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  node_shape = "gender",
  shapes = c(f = "female", m = "male"),
  unlinked_pos = "bottom",
  date_labels = "%b %d %Y",
  node_size = 35,
  font_size = 20,
  arrow_size = 0.5,
  height = 800,
  width = 700,
  edge_linetype = "location",
  legend_width = 0.15,
  highlight_downstream = TRUE,
  selector = FALSE
)

```

<!-- ======================================================= -->
## Préparation { }

### Charger les paquets {.unnumbered}  

Commencez par charger les paquets standards nécessaires à l'importation et à la manipulation des données. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* le charge pour l'utiliser. Vous pouvez également charger des paquets avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les paquets R.  
 
	
```{r transmission_chains_packages, eval = FALSE}
pacman::p_load(
   rio, # Importation de fichiers
   here, # Localisation de fichiers
   tidyverse, # Gestion des données + graphiques ggplot2
   remotes # Installation de paquets depuis github
)
```
	
Vous aurez besoin de la version de développement de **epicontacts**, qui peut être installée de github en utilisant la fonction `p_install_github()` de **pacman**. Vous n'avez besoin d'exécuter cette commande ci-dessous qu'une seule fois, et pas à chaque fois que vous utilisez le paquet (par la suite, vous pouvez utiliser `p_load()` comme d'habitude).

```{r transmission_chains_epicontacts_install, eval = FALSE}
pacman::p_install_gh("reconhub/epicontacts@timeline")
```


### Importer les données {.unnumbered}

Nous importons le jeu de données des cas d'une épidémie d'Ebola simulée. Si vous souhaitez télécharger les données pour suivre le code, consultez les instructions de la page [Télécharger le manuel et les données](#download_book_data). Le jeu de données est importé à l'aide de la fonction `import()` du paquet **rio**. Voir la page [Importation et exportation](#import_export) pour connaître les différentes methodes d'importer des données.

```{r, echo=F}
# Importez la linelist dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# Importez la liste de cas
linelist <- import("linelist_cleaned.xlsx")
```

Les 50 premières lignes de la linelist sont affichées ci-dessous. Les colonnes `case_id`, `generation`, `infector`, et `source` sont particulièrement intéressantes.  

```{r, message=FALSE, echo=F}
# affiche les données de la liste de diffusion sous forme de tableau
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Création d'un objet epicontacts {.unnumbered}

Nous devons ensuite créer un objet **epicontacts**, qui nécessite deux types de données:

* une linelist documentant les cas où les colonnes sont des variables et les lignes correspondent à des cas uniques.
* une liste de bords définissant les liens entre les cas sur la base de leurs identifiants uniques (il peut s'agir de contacts,
  des événements de transmission, etc.)

Comme nous avons déjà une linelist, il nous suffit de créer une liste de bord entre les cas, plus précisément entre leurs ID. Nous pouvons extraire les liens de transmission de la linelist en liant la colonne `infector` avec la colonne `case_id`. A ce stade, nous pouvons également ajouter des "propriétés de bords", c'est-à-dire toute variable décrivant le lien entre les deux cas, mais pas les cas eux-mêmes. Pour illustration, nous allons ajouter une variable `location` décrivant l'emplacement de l'événement de transmission, et une variable `duration` (durée) décrivant la durée du contact en jours.

Dans le code ci-dessous, la fonction `transmute` de le paquet **dplyr** est similaire à `mutate`, sauf qu'elle ne conserve que les colonnes que nous avons spécifiées dans la fonction. La fonction `drop_na` enlevera toutes les lignes où les colonnes spécifiées ont une valeur `NA`. Dans ce cas, nous ne voulons conserver que les lignes où l'infecteur est connu.

```{r transmission_chains_create_contacts,}
## générer des contacts
contacts <- linelist %>%
  transmute(
    infector = infector,
    case_id = case_id,
    location = sample(c("Community", "Nosocomial"), n(), TRUE),
    duration = sample.int(10, n(), TRUE)
  ) %>%
  drop_na(infector)
```

Nous pouvons maintenant créer l'objet **epicontacts** en utilisant la fonction `make_epicontacts` . Nous devons spécifier quelle colonne de la linelist correspond à l'identifiant unique du cas, ainsi que les colonnes des contacts qui pointent vers les identifiants uniques des cas impliqués dans chaque lien. Ces liens sont directionnels en le sens que l'infection va _de_ l'infecteur _à_ le cas, les arguments `from` et `to` en conséquence. Nous définissons donc l'argument `directed` (direction) à `TRUE` (VRAI), ce qui affectera les opérations futures.

```{r transmission_chains_create_epicontacts,}
## générer un objet epicontacts
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts,
  id = "case_id",
  from = "infector",
  to = "case_id",
  directed = TRUE
)
```
En examinant les objets **epicontacts**, on peut voir que la colonne `case_id` de la linelist a été renommée à `id` et que les colonnes `case_id` et `infector` des contacts ont été renommées à `from` et `to`. Cela garantit la cohérence dans le traitement, visualisation et analyse de l'objet **epicontacts**.

```{r transmission_chains_view_epicontacts,}
## visualiser l'objet epicontacts
epic
```

<!-- ======================================================= -->
## Manipulation { }

### Sous-ensemble {.unnumbered}

La méthode `subset()` pour les objets `epicontacts` permet, entre autres, de filtrer les réseaux en fonction des propriétés de la linelinst ("attributs de noeuds") et de la jeu de données de contacts ("attributs de bords").Ces valeurs doivent être passées comme des listes nommées à l'argument respectif. Par exemple, dans le code ci-dessous, nous ne gardons dans la linelist que les cas masculins qui ont une date d'infection entre avril et juillet 2014 (les dates sont spécifiées en tant que plages) et des liens de transmission qui ont eu lieu dans l'hôpital.

```{r transmission_chains_subset_nodes,}
sub_attributes <- subset(
  epic,
  node_attribute = list(
    gender = "m",
    date_infection = as.Date(c("2014-04-01", "2014-07-01"))
  ), 
  edge_attribute = list(location = "Nosocomial")
)
sub_attributes
```

Nous pouvons utiliser la fonction `thin` pour filtrer la linelist afin d'inclure les cas trouvés dans les contacts en définissant l'argument `what = "linelist"`, ou pour filtrer les contacts pour inclure les cas qui sont trouvés dans la linelist en définissant l'argument `what = "contacts"`. Dans le code ci-dessous, nous filtrons davantage l'objet **epicontacts** pour ne garder que les liens de transmission impliquant les cas masculins infectés entre avril et juillet que nous avons filtrés ci-dessus. Nous pouvons voir que seulement deux liens de transmission correspondent à cette spécification.

```{r transmission_chains_thin,}
sub_attributes <- thin(sub_attributes, what = "contacts")
nrow(sub_attributes$contacts)
```

Les réseaux peuvent être élagués pour n'inclure que les composants qui sont connectés à certains noeuds. L'argument `cluster_id`
prend un vecteur d'identifiants de cas et renvoie la linelist des individus qui sont liés, directement ou indirectement, à ces IDs. Dans le code ci-dessous, nous pouvons voir qu'un total de 13 cas de la linelist sont impliqués dans les clusters contenant `2ae019` et `71577a`.

```{r}
sub_id <- subset(epic, cluster_id = c("2ae019", "71577a"))
nrow(sub_id$linelist)
```

La méthode `subset()` pour les objets `epicontacts` permet aussi de filtrer par la taille des cluster en utilisant les arguments `cs`, `cs_min` et `cs_max`. Dans le code ci-dessous, nous gardons seulement les cas liés à des clusters de 10 cas ou plus, et nous pouvons voir que 271 cas de la linelist sont impliqués dans de tels clusters.
    
```{r}   
sub_cs <- subset(epic, cs_min = 10)
nrow(sub_cs$linelist)
```

### Accéder les IDs {.unnumbered}

La fonction `get_id()` récupère les informations sur les IDs des cas dans les
données, et peut être paramétrée comme la suite:

- **linelist** : IDs dans les données de la linelist
- **contacts** : IDs dans la jeu de données des contacts ("from" et "to" combinés)
- **from** : IDs dans la colonne "from" de la base de données des contacts.
- **to** : IDs dans la colonne "to" du jeu de données des contacts
- **all** : IDs qui apparaissent n'importe où dans l'un ou l'autre des jeu de données.
- **common** : IDs qui apparaissent à la fois dans la jeu de données des contacts et dans la linelist.
    
Par exemple, quels sont les dix premiers ID dans la jeu de données des contacts ?
```{r transmission_chains_get_ids,}
contacts_ids <- get_id(epic, "contacts")
head(contacts_ids, n = 10)
```

Combien d'identifiants sont trouvés à la fois dans la linelist et dans les contacts ?
```{r transmission_chains_get_both,}
length(get_id(epic, "common"))
```

<!-- ======================================================= -->
## Visualisation { }

### Graphique de base {.unnumbered}

Toutes les visualisations des objets **epicontacts** sont gérées par la fonction `plot`. Nous allons d'abord filtrer l'objet **epicontacts** pour n'inclure que les cas ayant une date d'apparition en juin 2014, en utilisant la fonction `subset`, et filtrer seulement les contacts liés à ces cas à l'aide de la fonction `thin`.
	
```{r transmission_chains_basic_plot_sub,}
## sous-ensemble objet epicontacts
sub <- epic %>%
  subset(
    node_attribute = list(date_onset = c(as.Date(c("2014-06-30", "2014-06-01"))))
  ) %>%
 thin("contacts")
```

Nous pouvons ensuite créer le graphique interactif de base très simplement comme suit :

```{r transmission_chains_basic_plot,}
## tracer l'objet epicontacts
plot(
  sub,
  width = 700,
  height = 700
)
```

Vous pouvez déplacer les noeuds en les faisant glisser, les survoler pour obtenir plus d'informations et cliquer dessus pour subligner les cas connectés.

Il existe un grand nombre d'arguments pour modifier ce graphique. Nous allons couvrir les principaux ici, mais consultez la documentation via `?vis_epicontacts` (la fonction appelée lors de l'utilisation de `plot` sur un objet **epicontacts**) pour obtenir une description complète des arguments de la fonction.

#### Visualiser les attributs des noeuds {.unnumbered}

La couleur, la forme et la taille d'un noeud peuvent être associées à une colonne specifiée de la linelist, en utilisant les arguments `node_color`, `node_shape` et `node_size`. Ceci est similaire à la syntaxe `aes` de **ggplot2**. 

Les couleurs, formes et tailles spécifiques des noeuds peuvent être spécifiées comme suit :

* **Couleurs** via l'argument `col_pal`, soit en fournissant une liste de noms pour la spécification manuelle de chaque couleur comme fait ci-dessous, ou en fournissant une fonction de palette de couleurs, telle que `colorRampPalette(c("black", "red", "orange"))` fournira un gradient de couleurs entre les trois spécifiées.

* **Shapes** en passant une liste nommée à l'argument `shapes`, et en spécifiant une forme pour chaque élément unique dans la colonne de la linelist spécifiée avec l'argument `node_shape`. Voir `codeawesome` pour les formes disponibles.

* **Taille** en passant une gamme de taille des noeuds à l'argument `size_range`.

Voici un exemple, où la couleur représente le résultat, la forme le sexe et la taille l'âge :

```{r transmission_chains_node_attribute,}
plot(
  sub, 
  node_color = "outcome",
  node_shape = "gender",
  node_size = "age",
  col_pal = c(Death = "firebrick", Recover = "green"),
  shapes = c(f = "female", m = "male"),
  size_range = c(40, 60),
  height = 700,
  width = 700
)
```

#### Visualisation des attributs de bords {.unnumbered}

La couleur, la largeur et le type de ligne de le bords peuvent être associés à une colonne du jeu de données contacts en utilisant les arguments `edge_color`, `edge_width` et `edge_linetype`, comme la suite:

* **Couleurs** via l'argument `edge_col_pal`, de la même manière que pour `col_pal`.

* **Largeurs** en passant une gamme de taille des noeuds à l'argument `width_range`.

Voici un exemple :

```{r transmission_chains_edge_attribute,}

plot(
  sub, 
  node_color = "outcome",
  node_shape = "gender",
  node_size = "age",
  col_pal = c(Death = "firebrick", Recover = "green"),
  shapes = c(f = "female", m = "male"),
  size_range = c(40, 60),
  edge_color = 'location',
  edge_linetype = 'location',
  edge_width = 'duration',
  #edge_col_pal = c(Community = "orange", Nosocomial = "violet"),
  width_range = c(1, 3),
  height = 700,
  width = 700
)

```

### Axe temporel {.unnumbered}

Nous pouvons également visualiser le réseau selon un axe temporel en faisant correspondre l'argument `x_axis` à une colonne de la linelist. Dans l'exemple ci-dessous, l'axe des x représente la date d'apparition des symptômes. Nous avons également spécifié l'argument `arrow_size` pour nous assurer que les flèches ne sont pas trop grandes, et nous avons défini `label = FALSE` pour rendre la figure moins encombrée.

```{r transmission_chains_x_axis,}
plot(
  sub,
  x_axis = "date_onset",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
)
```

Il existe un grand nombre d'arguments supplémentaires pour spécifier d'avantage la façon dont ce réseau est visualisé le long d'un axe temporel, que vous pouvez vérifier via `?vis_temporal_interactive` (la fonction appelée lors de l'utilisation de `plot` sur un objet **epicontacts** avec `x_axis` spécifié). Nous allons voir quelques examples ci-dessous.

#### Spécifier la forme de l'arbre de transmission {.unnumbered}

Il y a deux formes principales que l'arbre de transmission peut prendre, spécifiées en utilisant l'argument `network_shape`. La première est une forme `branchée` comme indiqué ci-dessus, où un bord droite relie deux noeuds connectes. C'est la représentation la plus intuitive mais elle peut donner lieu à des bords qui se chevauchent dans un réseau dense. La deuxième forme est le `rectangle`, qui produit un arbre ressemblant à une phylogénie. Par exemple :

```{r transmission_chains_rectangle,}
plot(
  sub,
  x_axis = "date_onset",
  network_shape = "rectangle",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
)
```

On peut assigner à chaque noud de cas une position verticale unique en modifiant l'argument `position_dodge`. La position des cas non liés (c'est-à-dire sans contacts signalés) est spécifiée à l'aide de l'argument `unlinked_pos`.

```{r transmission_chains_dodge,}
plot(
  sub,
  x_axis = "date_onset",
  network_shape = "rectangle",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  position_dodge = TRUE,
  unlinked_pos = "bottom",
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  hieght = 700,
  width = 700
)
```

La position du noeud parent par rapport aux noeuds enfants peut être spécifiée en utilisant l'argument `parent_pos`. L'option par défaut est de placer le noeud parent au milieu, mais il peut être placé en bas (`parent_pos = 'bottom'`) ou en haut (`parent_pos = 'top'`).

```{r transmission_chains_parent_pos,}
plot(
  sub,
  x_axis = "date_onset",
  network_shape = "rectangle",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  parent_pos = "top",
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
)
```

#### Enregistrement des graphiques et des figures {.unnumbered}

Vous pouvez enregistrer un graphique sous forme de fichier html interactif et autonome avec la fonction `visSave` du paquet **VisNetwork** :

```{r transmission_chaînes_save, eval=F}

plot(
  sub,
  x_axis = "date_onset",
  network_shape = "rectangle",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  parent_pos = "top",
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
) %>%
  visNetwork::visSave("network.html")

```

L'enregistrement de ces sorties de réseau sous forme d'image est malheureusement moins facile et nécessite d'enregistrer le fichier en tant que html et ensuite de faire une capture d'écran utilisant le paquet `webshot`. Dans le code ci-dessous, nous convertissons le fichier html sauvegardé ci-dessus en un PNG :

```{r transmission_chains_webshot, eval=F}
webshot(url = "network.html", file = "network.png")
```

### Ligne chronologique {.unnumbered}

Vous pouvez également ajouter les chronologie de cas sur le réseau, qui sont représentées sur l'axe des x de chaque cas. Ceci peut être utilisé pour visualiser localisations des cas, par exemple, ou le temps jusqu'au résultat. Pour générer une ligne chronologique, nous devons créer un dataframe d'au moins trois colonnes indiquant l'ID du cas, la date de début de l'"événement" et la date de fin de l'"événement". Vous pouvez également ajouter n'importe quel nombre d'autres colonnes qui peuvent ensuite être mappées aux noeuds et aux bords. Dans le code ci-dessous, nous générons une ligne chronologique allant de la date de l'apparition des symptômes à la date du résultat. Nous conservons les variables de résultat et d'hôpital que nous utilisons pour définir la forme et la couleur des noeuds. Notez que vous pouvez avoir plus qu'une ligne/événement chronologique par cas, par exemple si un cas a etait transféré entre plusieurs hôpitaux.

```{r transmission_chains_create_timeline,}

## générer une ligne chronologique
timeline <- linelist %>%
  transmute(
    id = case_id,
    start = date_onset,
    end = date_outcome,
    outcome = outcome,
    hospital = hospital
  )

```

Nous passons ensuite l'élément chronologique à l'argument `timeline`. Nous pouvons faire correspondre les attributs de la ligne chronologique aux couleurs, formesm et tailles des noeuds de la même manière que celle définie dans les sections précédentes, sauf que nous avons _deux_ noeuds: le noeud de début et de fin de chaque ligne chronologique qui ont des arguments distincts. Par exemple, `tl_start_node_color` définit quelle colonne de la ligne chronologique est mappée à la couleur du noeud de départ, tandis que `tl_end_node_shape` définit quelle colonne de la ligne chronologique est utilise pour la forme du noeud final. Nous pouvons également faire correspondre la couleur, la largeur, le type de ligne et les étiquettes de _bord_ de la ligne chronologique via les arguments `tl_edge_*`. 

Voir `?vis_temporal_interactive` (la fonction appelée de plot() avec un objet **epicontacts**) pour plus de détails. Chaque argument est également annoté dans le code ci-dessous :

```{r transmission_chains_vis_timeline,}

## définir les formes
shapes <- c(
  f = "female",
  m = "male",
  Death = "user-times",
  Recover = "heartbeat",
  "NA" = "question-circle"
)

## définir les couleurs
colours <- c(
  Death = "firebrick",
  Recover = "green",
  "NA" = "grey"
)

## faire un graphique
plot(
  sub,
  ## coordonnée x maximale de la date d'apparition de la maladie
  x_axis = "date_onset",
  ## utiliser une forme de réseau rectangulaire
  network_shape = "rectangle",
  ## mappe les formes de noeuds de cas à la colonne de sexe
  node_shape = "gender",
  ## nous ne voulons pas mapper la couleur des noeuds à aucune colonne, cela est important car la valeur par défaut est de mapper à l'id du noeud, ce qui va perturber le schéma de couleurs
  node_color = NULL,
  ## définir la taille du noeud de cas à 30 (comme il ne s'agit pas d'un caractère, node_size n'est pas mappée à une colonne mais interprétée comme la taille réelle du noeud)
  node_size = 30,
  ## définir la largeur du lien de transmission à 4 (comme il ne s'agit pas d'un caractère, edge_width n'est pas affectée à une colonne mais interprétée comme la largeur réelle du bord)
  edge_width = 4,
  ## fournir l'objet ligne chronologique
  timeline = timeline,
  ## mappe la forme du noeud de fin à la colonne de résultat dans l'objet de ligne chronologique
  tl_end_node_shape = "outcome",
  ## définir la taille du noeud final à 15 (comme il ne s'agit pas d'un caractère, cet argument n'est pas associé à la colonne des résultats dans l'objet ligne  chronologique).
  tl_end_node_size = 15,
  ## mappez la couleur du bord de la ligne de temps à la colonne de l'hôpital
  tl_edge_color = "hospital",
  ## Définir la largeur du bord de la ligne de temps à 2 (comme il ne s'agit pas d'un caractère, cet argument n'est pas associé à la colonne de l'hôpital).
  tl_edge_width = 2,
  ## mappez les étiquettes des bords à la variable hospital
  tl_edge_label = "hospital",
  ## spécifier la forme pour chaque attribut de noeud (défini ci-dessus)
  shapes = shapes,
  ## spécifier la palette de couleurs (définie ci-dessus)
  col_pal = colours,
  ## définir la taille de la flèche à 0.5
  arrow_size = 0.5,
  ## utiliser deux colonnes dans la légende
  legend_ncol = 2,
  ## définir la taille de la police
  font_size = 15,
  ## définir le formatage des dates
  date_labels = c("%d %b %Y"),
  ## ne pas tracer les étiquettes d'identification sous les noeuds
  label = FALSE,
  ## spécifier la hauteur
  height = 1000,
  ## spécifier la largeur
  width = 1200,
  ## assurez-vous que chaque noeud de cas a une coordonnée y unique, ceci est très important
  ## lors de l'utilisation de lignes chronologiques, sinon les lignes chronologiques se chevauchant de différents cas
  position_dodge = TRUE
)

```

<!-- ======================================================= -->
## Analyse { }

### Résumé {.unnumbered}

Nous pouvons obtenir un aperçu de certaines propriétés du réseau en utilisant la fonction `summary`.

```{r transmission_chains_summarise_epicontacts,}
## résumer l'objet epicontacts
summary(epic)
```

Par exemple, nous pouvons voir que seulement 57% des contacts ont les deux cas dans la linelist ; cela signifie que nous ne disposons pas de données de le linelist sur un nombre significatif de cas impliqués dans ces chaînes de transmission.

### Caractéristiques par paires {.unnumbered}

La fonction `get_pairwise()` permet de traiter les variables de la linelist en fonction de chaque paire dans l'ensemble de données de contact. Dans l'exemple suivant, la date d'apparition de la maladie est extraite de la liste de lignes afin de calculer la différence entre la date d'apparition de la maladie pour chaque paire dans l'ensemble de données de contact. La valeur produite par cette comparaison représente l' **intervalle de série (si)**.

```{r transmission_chains_pairwise,}
si <- get_pairwise(epic, "date_onset")   
summary(si)
tibble(si = si) %>%
  ggplot(aes(si)) +
  geom_histogram() +
  labs(
    x = "Intervalle de série",
    y = "Fréquence"
  )
```

La fonction `get_pairwise()` va interpréter la classe de la colonne utilisée pour la comparaison, et adaptera sa méthode de comparaison des valeurs en conséquence. Pour les nombres et les dates (comme l'exemple **si** ci-dessus), la fonction va soustraire les valeurs. Lorsqu'elle est appliquée à des colonnes qui sont des caractères ou des catégories,
`get_pairwise()` collera les valeurs ensemble. Comme la fonction permet également un traitement arbitraire (voir l'argument "f"), ces combinaisons discrètes peuvent être facilement mises en tableau et analysées.
    
```{r transmission_chains_pairwise_2,}
head(get_pairwise(epic, "gender"), n = 10)
get_pairwise(epic, "gender", f = table)
fisher.test(get_pairwise(epic, "gender", f = table))
```

Ici, nous voyons une association significative entre les liens de transmission et le sexe.

### Identifier les clusters {.unnumbered}

La fonction `get_clusters()` peut être utilisée pour identifier les composants connectés dans un objet `epicontacts`. Tout d'abord, nous l'utilisons pour récupérer un `data.frame` contenant les informations sur les clusters :

```{r transmission_chains_cluster,}
clust <- get_clusters(epic, output = "data.frame")
table(clust$cluster_size)
ggplot(clust, aes(cluster_size)) +
  geom_bar() +
  labs(
    x = "Taille des clusters",
    y = "Fréquence"
  )
```

Examinons les plus grands clusters. Pour cela, nous ajoutons des informations sur les clusters à l'objet `epicontacts`, puis nous le sous-ensemblons pour ne garder que les plus grands clusters :

```{r transmission_chains_cluster_2,}
epic <- get_clusters(epic)
max_size <- max(epic$linelist$cluster_size)
plot(subset(epic, cs = max_size))
```

### Calcul des degrés {.unnumbered}

Le degré d'un noeud correspond à son nombre de bords ou de connexions avec d'autres noeuds. `get_degree()` fournit une méthode simple pour calculer cette valeur pour les objets `epicontacts`. Un degré élevé dans ce contexte indique un individu qui était en contact avec beaucoup d'autres personnes. L'argument `type` indique que nous souhaitons compter à la fois le degré d'entrée et le degré de sortie, l'argument `only_linelist` indique que nous voulons calculer le degré pour les cas de la linelist.

```{r transmission_chains_degree,}
deg_both <- get_degree(epic, type = "both", only_linelist = TRUE)
```

Quels sont les individus qui ont les dix plus grands contacts ?

```{r}
head(sort(deg_both, decreasing = TRUE), 10)
```

Quel est le nombre moyen de contacts ?

```{r}
mean(deg_both)
```

<!-- ======================================================= -->
## Ressources { }

Le site pour le paquet [epicontacts](https://www.repidemicsconsortium.org/epicontacts/index.html) fournit une vue d'ensemble des fonctions du paquet et contient quelques vignettes plus approfondies.

La page [github](http://github.com/reconhub/epicontacts) peut être utilisée pour soulever des
problèmes et demander des fonctionnalités. 

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/transmission_chains.Rmd-->

# Les arbres phylogénétiques {#phylogenetic_trees}

<!-- ======================================================= -->

## Aperçu

**Les arbres phylogénétiques** sont utilisés pour visualiser et décrire la parenté et l'évolution des organismes à partir de la séquence de leur code génétique.

On peut les construire à partir de séquences génétiques en utilisant des méthodes basées sur la distance (comme la méthode neighbor-joining) ou les méthodes probabiliste (comme la méthode de maximum de vraisemblance et la méthode Bayesian Markov Chain Monte Carlo). Le séquençage de nouvelle génération (NGS) devient de plus en plus abordable et populaire en santé publique pour caractériser des agents pathogènes à l'origine des maladies infectieuses. Les appareils de séquençage portables réduisent le délai d'exécution et offrent la possibilité de rendre les résultats disponible en temps réel pour les enquêtes sur les épidémies. Les données NGS peuvent être utilisées pour identifier l'origine ou la source d'une souche épidémique ainsi que sa propagation, et pour déterminer la présence de gènes de résistance antimicrobienne. Pour visualiser la parenté génétique entre les échantillons, un arbre phylogénétique est construit.

Dans cette page, nous allons apprendre à utiliser le package **ggtree**, qui permet la visualisation combinée d'arbres phylogénétiques avec des données d'échantillons supplémentaires sous la forme de tableaux de données. Cela nous permettra d'observer les tendances et de comprendre la dynamique de l'épidémie.

```{r, phylogenetic_trees_overview_graph, out.width=c('80%'), fig.align='center', fig.show='hold', echo = FALSE}

pacman::p_load(here, ggplot2, dplyr, ape, ggtree, treeio, ggnewscale)

tree <- ape::read.tree(here::here("data", "phylo", "Shigella_tree.txt"))

sample_data <- read.csv(here::here("data","phylo", "sample_data_Shigella_tree.csv"),sep=",", na.strings=c("NA"), head = TRUE, stringsAsFactors=F)


ggtree(tree, layout="circular", branch.length='none') %<+% sample_data + # l'operateur %<+% est utilisé pour ajouter à l'arbre votre tableau de données 
  aes(color=I(Belgium))+ # colore les branches a partir d'une variable de votre tableau de données
  scale_color_manual(name = "Sample Origin", # nom de votre palette de couleur (qui apparaîtra dans la légende comme ceci)
                    breaks = c("Yes", "No"), # les différentes options de votre variable
                   labels = c("NRCSS Belgium", "Other"), # comment vous souhaitez que les différentes options soient désignées dans votre légende, permet de les formater.
                 values= c("blue", "black"), # la couleur que vous souhaitez attribuer à la variable 
                 na.value = "black") + # colore les valeurs NA en noir également
  new_scale_color()+ # permet d'ajouter une palette de couleurs supplémentaire pour une autre variable
     geom_tippoint(aes(color=Continent), size=1.5)+ # colore le point d'extrémité par continent, vous pouvez changer la forme en ajoutant "shape = "
scale_color_brewer(name = "Continent",  # nom de la palette de couleurs (qui apparaîtra dans la légende comme ceci)
                       palette="Set1", # nous utilisons un ensemble de couleurs fournies avec le package brewer
                   na.value="grey")+ # pour les valeurs NA nous utilisons la couleur grise
  theme(legend.position= "bottom")

```

<!-- ======================================================= -->

## Préparation

### Importation des packages {.unnumbered}

Ces lignes de code importe les packages necessaire pour l'analyse. Dans ce guide, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le package si nécessaire *puis* l'importe pour l'utiliser dans la session de Rstudio. Vous pouvez également charger les packages installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les packages en R.

```{r, phylogenetic_trees_loading_packages, warning=FALSE, message=FALSE}
pacman::p_load(
  rio,             # import/export
  here,            # gestion des chemins d'accès
  tidyverse,       # gestion des données + graphiques (ggplot2)
  ape,             # pour importer and exporter les fichiers phylogénétiques
  ggtree,          # pour visualiser les fichiers phylogénétiques
  treeio,          # pour visualiser les fichiers phylogénétiques
  ggnewscale)      # pour ajouter les palettes de couleurs supplémentaire

```

### Import data {.unnumbered}

Les données pour cette page peuvent être téléchargées suivant les instructions de la page [Télécharger le manuel et les données](#download_book_data).

Il existe plusieurs formats différents dans lesquels un arbre phylogénétique peut être enregistré (par exemple, Newick, NEXUS, Phylip). Un format couramment utilisés est celui des fichiers Newick (.nwk), qui est la norme pour représenter les arbres sous une forme facilement exploitable par ordinateur. Cela signifie qu'un arbre entier peut être représenté par un format de chaîne de caractères tel que "((t2:0.04,t1:0.34):0.89,(t5:0.37,(t4:0.03,t3:0.67):0.9):0.59) ; ", énumérant tous les nœuds, les extrémités et leur relation (longueur de branche) les uns avec les autres.

Note : Il est important de comprendre que le fichier de l'arbre phylogénétique en lui-même ne contient pas de données de séquençage, mais est simplement le résultat des distances génétiques entre les séquences. Nous ne pouvons donc pas extraire les données de séquençage d'un fichier arbre.

Tout d'abord, nous utilisons la fonction read.tree() du package **ape** pour importer un fichier d'arbre phylogénétique de Newick au format .txt, et le enregistrer dans un objet liste de classe "phylo". Si nécessaire, utilisez la fonction `here()` du package **here** pour spécifier le chemin relatif du fichier.

Note : Dans ce cas, l'arbre Newick est enregistré au format .txt pour faciliter sa manipulation et son téléchargement de Github.

```{r, echo=F}
tree <- ape::read.tree(here::here("data", "phylo", "Shigella_tree.txt"))
```

```{r, echo=T, eval=F}
tree <- ape::read.tree("Shigella_tree.txt")
```

Nous inspectons notre objet arbre et voyons qu'il contient 299 pointes (ou échantillons) et 236 nœuds.

```{r}
tree
```

Ensuite, nous importons un fichier en format .csv contenant des informations supplémentaires pour chaque échantillon séquencé, tel que le sexe, le pays d'origine et les attributs de résistance antimicrobienne, en utilisant la fonction `import()` du package **rio** :

```{r, echo=F}
sample_data <- import(here("data", "phylo", "sample_data_Shigella_tree.csv"))
```

```{r, echo=T, eval=F}
sample_data <- import("sample_data_Shigella_tree.csv")
```

Ci-dessous, vous trouverez les 50 premières lignes de données :

```{r message=FALSE, echo=F}
DT::datatable(head(sample_data,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Nettoyer et inspecter {.unnumbered}

Nous nettoyons et inspectons nos données : Pour pouvoir associer les données échantillon à l'arbre phylogénétique, les valeurs de la colonne `Sample_ID` dans le tableau de données `sample_data` doit correspondre aux valeurs `tip.labels` dans le fichier `tree` :

Nous vérifions le format du `tip.labels` dans le fichier `tree` en regardant les 6 premières entrées en utilisant `head()` de **base** R.

```{r, phylogenetic_trees_inspect_sampledata}
head(tree$tip.label) 
```

Nous nous assurons également que la première colonne de notre tableau de données `sample_data` est `Sample_ID`. Nous regardons les noms des colonnes de notre jeu de données en utilisant `colnames()` de **base** R.

```{r}
colnames(sample_data)   
```

Nous regardons les `Sample_IDs` dans le jeu de données pour nous assurer que le format est le même que dans le `tip.label` (par exemple, les lettres sont en majuscules, pas de soulignement supplémentaire `_` entre les lettres et les chiffres, etc.)

```{r}
head(sample_data$Sample_ID) # Nous vérifions les 6 premières entrées en utilisant head()
```

Nous pouvons aussi vérifier si tous les échantillons sont présents dans le fichier `tree` et vice versa en générant un vecteur logique de VRAI ou FAUX là où ils correspondent ou non. Ceux-ci ne sont pas imprimés ici, pour plus de simplicité.

```{r, eval=F}
sample_data$Sample_ID %in% tree$tip.label

tree$tip.label %in% sample_data$Sample_ID
```

Nous pouvons utiliser ces vecteurs pour démontrer les ID des échantillons qui ne sont pas sur l'arbre (il n'y en a pas).

```{r}
sample_data$Sample_ID[!tree$tip.label %in% sample_data$Sample_ID]
```

Après inspection, nous pouvons voir que le format de `Sample_ID` dans le dataframe correspond au format des noms d'échantillons dans le `tip.labels`. Ceux-ci n'ont pas besoin d'être triés dans le même ordre pour être appariés.

Voilà on est prêts à partir !

<!-- ======================================================= -->

## Visualisation simple de l'arbre

### Diverses configurations d'arbres {.unnumbered}

**ggtree** offre nombreux formats de configuration d'arbres différents et certains peuvent être plus adéquats que d'autres pour votre usage spécifique. Ci-dessous, vous trouverez quelques illustrations. Pour d'autres options, voir ce [livre en ligne](http://yulab-smu.top/treedata-book/chapter4.html).

Voici quelques exemples des configurations d'arbres :

```{r, phylogenetic_trees_example_formats, out.width=c('50%'), fig.show='hold'}

ggtree(tree)                                            # arbre linéaire simple
ggtree(tree,  branch.length = "none")                   # arbre linéaire simple avec toutes les pointes alignées 
ggtree(tree, layout="circular")                         # arbre circulaire simple
ggtree(tree, layout="circular", branch.length = "none") # arbre circulaire simple avec toutes les pointes alignées 

```

### Arbre simple plus données échantillons {.unnumbered}

L'opérateur **%\<+%** est utilisé pour connecter le tableau de donnees `sample_data` avec le fichier `arbre`. L'annotation la plus facile de votre arbre est l'ajout des noms des échantillons aux extrémités, ainsi que la coloration des points d'extrémité et si désiré des branches :

Voici un exemple d'arbre circulaire :

```{r, phylogenetic_trees_adding_sampledata, fig.align='center', warning=F, message=F}

ggtree(tree, layout = "circular", branch.length = 'none') %<+% sample_data + # %<+% ajoute le tableau des données l'échantillon à l'arbre
  aes(color = (Belgium))+                       # colore les branches en fonction d'une variable de votre tableau de données 
  scale_color_manual(
    name = "Sample Origin",                      # le nom de votre palette de couleurs (qui apparaîtra dans la légende comme ceci)
    breaks = c("Yes", "No"),                     # les différentes options de votre variable
    labels = c("NRCSS Belgium", "Other"),        # comment vous voulez que les différentes options soient nommées dans votre légende, ce qui permet de les formater
    values = c("blue", "black"),                  # la couleur que vous voulez attribuer à la variable
    na.value = "black") +                        # colorer les valeurs NA en noir également
  new_scale_color()+                             # permet d'ajouter une couleur supplémentaire pour une autre variable
    geom_tippoint(
      mapping = aes(color = Continent),          # colorer les pointes par continent. Vous pouvez changer la forme en ajoutant "shape = "
      size = 1.5)+                               # Définit la taille du point à la extremite 
  scale_color_brewer(
    name = "Continent",                    # nom de votre palette de couleurs (qui apparaîtra dans la légende comme ceci)
    palette = "Set1",                      # nous choisissons un ensemble de couleurs fournies avec le package brewer 
    na.value = "grey") +                    # pour les valeurs NA nous choisissons la couleur grise
  geom_tiplab(                             # ajoute le nom de l'échantillon à l'extrémité de sa branche
    color = 'black',                       # (ajoutez autant de lignes de texte que vous souhaitez avec + , mais vous devrez ajuster la valeur du décalage pour les placer les à côté des autres) 
    offset = 1,
    size = 1,
    geom = "text",
    align = TRUE) +    
  ggtitle("Phylogenetic tree of Shigella sonnei")+       # titre de votre graphique
  theme(
    axis.title.x = element_blank(), # supprime le titre de l'axe x
    axis.title.y = element_blank(), # supprime le titre de l'axe y
    legend.title = element_text(    # définit la taille de la police et le format du titre de la légende
      face = "bold",
      size = 12),   
    legend.text=element_text(       # définit la taille de la police et le format du texte de la légende
      face = "bold",
      size = 10),  
    plot.title = element_text(      # définit la taille de la police et le format du titre du graphique
      size = 12,
      face = "bold"),  
    legend.position = "bottom",     # # définit le position du légende
    legend.box = "vertical",        # # définit le position du légende
    legend.margin = margin())   
```

Vous pouvez exporter le graphique de votre arbre avec `ggsave()` comme vous le feriez avec n'importe quel autre objet ggplot. Écrit de cette façon, `ggsave()` enregistre la dernière image produite dans le chemin de fichier que vous spécifiez. Rappelez-vous que vous pouvez utiliser `here()` et les chemins de fichiers relatifs pour sauvegarder facilement dans des sous-dossiers, etc.

```{r, eval=F}
ggsave("example_tree_circular_1.png", width = 12, height = 14)

```

<!-- ======================================================= -->

## Manipulation de l'arbre

Parfois, vous pouvez avoir un très grand arbre phylogénétique et vous n'êtes intéressé que par une partie de l'arbre. Par exemple, si vous avez produit un arbre incluant des échantillons historiques ou internationaux afin d'obtenir un aperçu de la place de l'ensemble de vos données dans le schema général. Mais ensuite, pour examiner vos données de plus près, vous ne voulez inspecter que cette partie du grand arbre.

Comme le fichier de l'arbre phylogénétique n'est que le résultat de l'analyse des données de séquençage, nous ne pouvons pas manipuler l'ordre des nœuds et des branches dans le fichier lui-même. Ceux-ci ont déjà été déterminés dans une analyse précédente à partir des données de séquençage brutes. Nous sommes cependant en mesure de zoomer sur certaines parties, de cacher certaines parties et même de sous-ensembler une partie de l'arbre.

### Zoomer pour agrandir {.unnumbered}

Si vous ne voulez pas "couper" votre arbre, mais seulement en inspecter une partie de plus près, vous pouvez zoomer pour voir une partie spécifique.

Tout d'abord, nous traçons l'arbre entier au format linéaire et ajoutons des étiquettes numériques à chaque nœud de l'arbre.

```{r, phylogenetic_trees_zoom_in, out.width=c('50%'), fig.show='hold', fig.align='center'}

p <- ggtree(tree,) %<+% sample_data +
  geom_tiplab(size = 1.5) +                # étiquette les extrémités de toutes les branches avec le nom de l'échantillon dans le fichier de l'arbre
  geom_text2(
    mapping = aes(subset = !isTip,
                  label = node),
    size = 5,
    color = "darkred",
    hjust = 1,
    vjust = 1)                            # étiquette tous les nœuds de l'arbre

p  # imprime

```

Pour zoomer sur une branche particulière (qui déborde à droite), utilisez `viewClade()` sur l'objet ggtree `p` et fournissez le numéro du noeud pour obtenir une vue plus détaillée :

```{r phylogenetic_trees_zoom_in_452, out.width=c('50%'), fig.show='hold', fig.align='center'}

viewClade(p, node = 452)

```

### Réduire les branches {.unnumbered}

Cependant, nous pouvons vouloir ignorer cette branche et la réduire à ce même noeud (noeud n° 452) en utilisant `collapse()`. Cet arbre est défini comme `p_collapsed`.

```{r phylogenetic_trees_collapse_452, out.width=c('50%'), fig.show='hold', fig.align='center'}

p_collapsed <- collapse(p, node = 452)
p_collapsed
```

Pour plus de clarté, lorsque nous imprimons `p_collapsed`, nous ajoutons un `geom_point2()` (un diamant bleu) au noeud de la branche réduite.

```{r}
p_collapsed + 
geom_point2(aes(subset = (node == 452)),  # nous attribuons un symbole au nœud réduit
            size = 5,                     # définir la taille du symbole
            shape = 23,                   # définir la forme du symbole
            fill = "steelblue")           # définir la coleur du symbole
```

### Sous-ensembler un arbre {.unnumbered}

Si nous voulons faire un changement plus permanent et créer un nouvel arbre réduit avec lequel travailler, nous pouvons sous-ensembler une partie avec `tree_subset()`. Vous pouvez ensuite le sauvegarder comme un nouveau fichier newick tree ou un fichier .txt.

Tout d'abord, nous inspectons les noeuds de l'arbre et les étiquettes des extrémités afin de décider ce qu'il faut sous-ensembler.

```{r, phylogenetic_trees_subsetting, out.width=c('50%'), fig.show='hold', fig.align='center'}
ggtree(
  tree,
  branch.length = 'none',
  layout = 'circular') %<+% sample_data +               # nous ajoutons les données de l'échantillon en utilisant l'opérateur %<+%.
  geom_tiplab(size = 1)+                                # étiqueter les extrémités de toutes les branches avec le nom de l'échantillon dans le fichier arbre
  geom_text2(
    mapping = aes(subset = !isTip, label = node),
    size = 3,
    color = "darkred") +                                # étiquette tous les noeuds de l'arbre 
 theme(
   legend.position = "none",                            # supprime la légende tous ensemble
   axis.title.x = element_blank(),
   axis.title.y = element_blank(),
   plot.title = element_text(size = 12, face="bold"))
```

Maintenant, disons que nous avons décidé de sous-ensembler l'arbre au noeud 528 (ne garder que les pointes dans cette branche après le noeud 528) et nous le sauvegardons comme un nouvel objet `sub_tree1` :

```{r}
sub_tree1 <- tree_subset(
  tree,
  node = 528)                                            # nous sous-ensemblons l'arbre au nœud 528
```

Examinons l'arbre subset tree 1:

```{r}
ggtree(sub_tree1) +
  geom_tiplab(size = 3) +
  ggtitle("Subset tree 1")
```

Vous pouvez également effectuer un sous-ensemble basé sur un échantillon particulier, en spécifiant le nombre de noeuds "en arrière" que vous souhaitez inclure. Sous-ensemble la même partie de l'arbre basé sur un échantillon, dans ce cas S17BD07692, en remontant de 9 noeuds et nous le sauvegardons comme un nouvel objet `sub_tree2` :

```{r}
sub_tree2 <- tree_subset(
  tree,
  "S17BD07692",
  levels_back = 9) # levels_back définit le nombre de nœuds en arrière de la pointe de l'échantillon que vous voulez atteindre.
```

Examinons l'arbre subset tree 2:

```{r}
ggtree(sub_tree2) +
  geom_tiplab(size =3)  +
  ggtitle("Subset tree 2")

```

Vous pouvez également sauvegarder votre nouvel arbre soit comme un type Newick ou même un fichier texte en utilisant la fonction `write.tree()` du package **ape** :

```{r, eval=F, phylogenetic_trees_write_tree}
# pour sauvegarder en format .nwk format
ape::write.tree(sub_tree2, file='data/phylo/Shigella_subtree_2.nwk')

# pour sauvegarder en format .txt format
ape::write.tree(sub_tree2, file='data/phylo/Shigella_subtree_2.txt')

```

### Rotation des nœuds dans un arbre {.unnumbered}

Comme mentionné précédemment, nous ne pouvons pas modifier l'ordre des pointes ou des noeuds dans l'arbre, car cela est basé sur leur parenté génétique et ne peut pas être manipulé de manière visuelle. Mais nous pouvons roter des branches autour des nœuds si cela facilite notre visualisation.

Tout d'abord, nous traçons notre nouveau subset tree 2 avec des étiquettes de nœuds pour choisir le nœud que nous voulons manipuler et nous enregistrons dans un objet ggtree plot `p`.

```{r, phylogenetic_trees_rotating_1, out.width=c('50%'), fig.show='hold', fig.align='center'}

p <- ggtree(sub_tree2) +  
  geom_tiplab(size = 4) +
  geom_text2(aes(subset=!isTip, label=node), # étiquette tous les nœuds de l'arbre
             size = 5,
             color = "darkred", 
             hjust = 1, 
             vjust = 1) 
p
```

Nous pouvons ensuite manipuler les nœuds en appliquant **ggtree::rotate()** ou **ggtree::flip()** : Note : pour illustrer quels noeuds nous manipulons, nous appliquons d'abord la fonction **geom_hilight()** de **ggtree** pour mettre en évidence les échantillons dans les noeuds qui nous intéressent et nous enregistrons cet objet ggtree plot dans un nouvel objet `p1`.

```{r, phylogenetic_trees_rotating_2, out.width=c('50%'), fig.show='hold', fig.align='center'}

p1 <- p + geom_hilight(  # met en évidence le nœud 39 en bleu, "extend =" nous permet de définir la longueur du bloc de couleur
  node = 39,
  fill = "steelblue",
  extend = 0.0017) +  
geom_hilight(            # met en évidence le nœud 39 en jaune
  node = 37,
  fill = "yellow",
  extend = 0.0017) +               
ggtitle("Original tree")


p1 # imprime
```

Maintenant nous pouvons faire tourner le noeud 37 dans l'objet `p1` de sorte que les échantillons sur le noeud 38 se déplacent vers le haut. Nous enregistrons l'arbre pivoté dans un nouvel objet `p2`.

```{r}
p2 <- ggtree::rotate(p1, 37) + 
      ggtitle("Rotated Node 37")


p2   # imprime
```

Ou nous pouvons utiliser la commande flip pour faire pivoter le noeud 36 de l'objet `p1` et faire passer le noeud 37 en haut et le noeud 39 en bas. Nous enregistrons l'arbre retourné dans un nouvel objet `p3`.

```{r}

p3 <-  flip(p1, 39, 37) +
      ggtitle("Rotated Node 36")


p3   # imprime
```

### Exemple de sous-arbre avec annotation d'échantillon de données {.unnumbered}

Disons que nous investiguons le cluster de cas avec expansion clonale qui s'est produit en 2017 et 2018 au nœud 39 de notre sous-arbre. Nous ajoutons l'année d'isolement de la souche ainsi que l'historique des voyages et la couleur par pays pour voir l'origine d'autres souches étroitement liées :

```{r, phylogenetic_trees_inspect_subset_example, out.width=c('80%'), fig.show='hold', fig.align='center', warning=F, message=F}

ggtree(sub_tree2) %<+% sample_data +     # nous utilisons l'opérateur %<+% pour faire le lien avec le fichier sample_data 
  geom_tiplab(                          # étiquette les extrémités de toutes les branches avec le nom de l'échantillon dans le fichier arbre
    size = 2.5,
    offset = 0.001,
    align = TRUE) + 
  theme_tree2()+
  xlim(0, 0.015)+                       # définir les limites de l'axe x de notre arbre 
  geom_tippoint(aes(color=Country),     # colorer le point d'extrémité par continent
                size = 1.5)+ 
  scale_color_brewer(
    name = "Country", 
    palette = "Set1", 
    na.value = "grey")+
  geom_tiplab(                          # ajouter l'année d'isolation comme étiquette de texte aux extrémités
    aes(label = Year),
    color = 'blue',
    offset = 0.0045,
    size = 3,
    linetype = "blank" ,
    geom = "text",
    align = TRUE)+ 
  geom_tiplab(                          # ajouter l'historique de voyage comme étiquette de texte aux extrémités, en couleur rouge
    aes(label = Travel_history),
    color = 'red',
    offset = 0.006,
    size = 3,
    linetype = "blank",
    geom = "text",
    align = TRUE)+ 
  ggtitle("Phylogenetic tree of Belgian S. sonnei strains with travel history")+  # ajouter le titre du graphique
  xlab("genetic distance (0.001 = 4 nucleotides difference)")+                    # ajouter une étiquette à l'axe x 
  theme(
    axis.title.x = element_text(size = 10),
    axis.title.y = element_blank(),
    legend.title = element_text(face = "bold", size = 12),
    legend.text = element_text(face = "bold", size = 10),
    plot.title = element_text(size = 12, face = "bold"))

```

Notre observation pointe vers un événement d'importation de souches en provenance d'Asie, qui ont ensuite circulé en Belgique au fil des ans et semblent avoir causé notre dernière épidémie.

<!-- ======================================================= -->

## Arbres plus complexes : ajout de cartes thermiques des données de l'échantillon {.unnumbered}

Nous pouvons ajouter des informations plus complexes, telles que la présence catégorielle de gènes de résistance aux antimicrobiens et des valeurs numériques pour la résistance aux antimicrobiens effectivement mesurée sous la forme d'une carte thermique en utilisant la fonction **ggtree::gheatmap()**.

Tout d'abord, nous devons tracer notre arbre (qui peut être linéaire ou circulaire) et le enregistrer dans un nouvel objet ggtree plot `p` : Nous allons utiliser le sub_tree de la partie 3).

```{r, phylogenetic_trees_sampledata_heatmap, out.width=c('60%'), fig.align='center', fig.show='hold'}

p <- ggtree(sub_tree2, branch.length='none', layout='circular') %<+% sample_data +
  geom_tiplab(size =3) + 
 theme(
   legend.position = "none",
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    plot.title = element_text(
      size = 12,
      face = "bold",
      hjust = 0.5,
      vjust = -15))
p

```

Ensuite, nous préparons nos données. Pour visualiser différentes variables avec de nouveaux schémas de couleurs, nous sous-ensemblons notre dataframe à la variable désirée. Il est important d'ajouter le `Sample_ID` comme rownames sinon il ne pourra pas faire correspondre les données à l'arbre `tip.labels` :

Dans notre exemple, nous voulons examiner le sexe et les mutations qui pourraient conférer une résistance à la Ciprofloxacine, un important antibiotique de première ligne utilisé pour traiter les infections à Shigella.

Nous créons un tableau de données pour le sexe :

```{r, phylogenetic_trees_sampledata_heatmap_data}
gender <- data.frame("gender" = sample_data[,c("Gender")])
rownames(gender) <- sample_data$Sample_ID
```

Nous créons un tableau de données pour les mutations du gène gyrA, qui confèrent une résistance à la ciprofloxacine :

```{r}
cipR <- data.frame("cipR" = sample_data[,c("gyrA_mutations")])
rownames(cipR) <- sample_data$Sample_ID

```

Nous créons un tableau de données pour la concentration minimale inhibitrice (CMI) mesurée pour la Ciprofloxacine en provenance du laboratoire :

```{r}
MIC_Cip <- data.frame("mic_cip" = sample_data[,c("MIC_CIP")])
rownames(MIC_Cip) <- sample_data$Sample_ID
```

Nous créons un premier graphique en ajoutant une carte thermique binaire pour le sexe à l'arbre phylogénétique et en la sauvegardant dans un nouvel objet graphique ggtree `h1` :

```{r, phylogenetic_trees_sampledata_heatmap_gender, out.width=c('70%'), fig.show='hold', fig.align='center'}

h1 <-  gheatmap(p, gender,                                 # Nous ajoutons une couche de carte thermique du tableau de données de sexe à notre graphique arbre.
                offset = 10,                               # l'offset déplace la carte thermique vers la droite,
                width = 0.10,                              # la largeur définit la largeur de la colonne de la carte thermique,
                color = NULL,                              # la couleur définit la bordure des colonnes de la carte thermique.
         colnames = FALSE) +                               # cache les noms des colonnes de la carte thermique
  scale_fill_manual(name = "Gender",                       # définit le schéma de coloration et la légende pour le sexe
                    values = c("#00d1b1", "purple"),
                    breaks = c("Male", "Female"),
                    labels = c("Male", "Female")) +
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())
h1

```

Nous ajoutons ensuite des informations sur les mutations du gène gyrA, qui confèrent une résistance à la Ciprofloxacine :

Note : La présence de mutations ponctuelles chromosomiques dans les données WGS a été déterminée au préalable à l'aide de l'outil PointFinder développé par Zankari et al. (voir la référence dans la section des références supplémentaires)

Tout d'abord, nous attribuons un nouveau schéma de couleurs à notre objet de tracé existant `h1` et le sauvegardons dans un nouvel objet `h2`. Cela nous permet de définir et de modifier les couleurs de notre deuxième variable dans la carte thermique.

```{r}
h2 <- h1 + new_scale_fill() 
```

Ensuite, nous ajoutons la deuxième couche de carte thermique à `h2` et nous enregistrons les graphiques combinés dans un nouvel objet `h3` :

```{r, phylogenetic_trees_sampledata_heatmap_cip_genes, out.width=c('80%'), fig.show='hold', fig.align='center'}

h3 <- gheatmap(h2, cipR,         # ajoute la deuxième ligne de la carte thermique décrivant les mutations de résistance à la Ciprofloxacine
               offset = 12, 
               width = 0.10, 
               colnames = FALSE) +
  scale_fill_manual(name = "Ciprofloxacin resistance \n conferring mutation",
                    values = c("#fe9698","#ea0c92"),
                    breaks = c( "gyrA D87Y", "gyrA S83L"),
                    labels = c( "gyrA d87y", "gyrA s83l")) +
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())+
  guides(fill = guide_legend(nrow = 2,byrow = TRUE))
h3
```

Nous répétons le processus ci-dessus, en ajoutant d'abord une nouvelle couche d'échelle de couleurs à notre objet existant `h3`, puis en ajoutant les données continues sur la concentration minimale inhibitrice (CMI) de la Ciprofloxacine pour chaque souche à l'objet résultant `h4` pour produire l'objet final `h5` :

```{r, phylogenetic_trees_sampledata_heatmap_cip_MIC, out.width=c('90%'), fig.show='hold', fig.align='center'}
# D'abord nous ajoutons le nouveau schéma de coloration :
h4 <- h3 + new_scale_fill()

# puis nous combinons les deux en une nouvelle graphique :
h5 <- gheatmap(h4, MIC_Cip,  
               offset = 14, 
               width = 0.10,
                colnames = FALSE)+
  scale_fill_continuous(name = "MIC for Ciprofloxacin",  # nous définissons ici un schéma de couleurs de gradient pour la variable continue de CMI
                      low = "yellow", high = "red",
                      breaks = c(0, 0.50, 1.00),
                      na.value = "white") +
   guides(fill = guide_colourbar(barwidth = 5, barheight = 1))+
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())
h5

```

We can do the same exercise for a linear tree:

```{r, phylogenetic_trees_sampledata_heatmap_linear_1, out.width=c('80%'), fig.show='hold', fig.align='center'}

p <- ggtree(sub_tree2) %<+% sample_data +
  geom_tiplab(size = 3) + # etiquetter les pointes
  theme_tree2()+
  xlab("genetic distance (0.001 = 4 nucleotides difference)")+
  xlim(0, 0.015)+
 theme(legend.position = "none",
      axis.title.y = element_blank(),
      plot.title = element_text(size = 12, 
                                face = "bold",
                                hjust = 0.5,
                                vjust = -15))
p
```

Tout d'abord, nous ajoutons le sexe :

```{r, phylogenetic_trees_sampledata_heatmap_linear_2, out.width=c('80%'), fig.show='hold', fig.align='center'}

h1 <-  gheatmap(p, gender, 
                offset = 0.003,
                width = 0.1, 
                color="black", 
         colnames = FALSE)+
  scale_fill_manual(name = "Gender",
                    values = c("#00d1b1", "purple"),
                    breaks = c("Male", "Female"),
                    labels = c("Male", "Female"))+
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())
h1
```

Puis nous ajoutons les mutations de résistance à la Ciprofloxacine après avoir ajouté une autre couche de schéma de couleurs :

```{r, phylogenetic_trees_sampledata_heatmap_linear_3, out.width=c('80%'), fig.show='hold', fig.align='center'}

h2 <- h1 + new_scale_fill()
h3 <- gheatmap(h2, cipR,   
               offset = 0.004, 
               width = 0.1,
               color = "black",
                colnames = FALSE)+
  scale_fill_manual(name = "Ciprofloxacin resistance \n conferring mutation",
                    values = c("#fe9698","#ea0c92"),
                    breaks = c( "gyrA D87Y", "gyrA S83L"),
                    labels = c( "gyrA d87y", "gyrA s83l"))+
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())+
  guides(fill = guide_legend(nrow = 2,byrow = TRUE))
 h3
```

On ajoute ensuite la concentration minimale inhibitrice déterminée par le laboratoire (CMI) :

```{r, phylogenetic_trees_sampledata_heatmap_linear_4, out.width=c('80%'), fig.show='hold', fig.align='center'}

h4 <- h3 + new_scale_fill()
h5 <- gheatmap(h4, MIC_Cip, 
               offset = 0.005,  
               width = 0.1,
               color = "black", 
                colnames = FALSE)+
  scale_fill_continuous(name = "MIC for Ciprofloxacin",
                      low = "yellow", high = "red",
                      breaks = c(0,0.50,1.00),
                      na.value = "white")+
   guides(fill = guide_colourbar(barwidth = 5, barheight = 1))+
   theme(legend.position = "bottom",
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 8),
        legend.box = "horizontal", legend.margin = margin())+
  guides(shape = guide_legend(override.aes = list(size = 2)))
h5

```

<!-- ======================================================= -->

## Resources

<http://hydrodictyon.eeb.uconn.edu/eebedia/index.php/Ggtree#> Clade_Colors <https://bioconductor.riken.jp/packages/3.2/bioc/vignettes/ggtree/inst/doc/treeManipulation.html> <https://guangchuangyu.github.io/ggtree-book/chapter-ggtree.html> <https://bioconductor.riken.jp/packages/3.8/bioc/vignettes/ggtree/inst/doc/treeManipulation.html>

Ea Zankari, Rosa Allesøe, Katrine G Joensen, Lina M Cavaco, Ole Lund, Frank M Aarestrup, PointFinder: a novel web tool for WGS-based detection of antimicrobial resistance associated with chromosomal point mutations in bacterial pathogens, Journal of Antimicrobial Chemotherapy, Volume 72, Issue 10, October 2017, Pages 2764--2768, <https://doi.org/10.1093/jac/dkx217>
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/phylogenetic_trees.Rmd-->


# Graphiques interactifs {#interactive_plots}  

La visualisation de données est de plus en plus demandée d'être interactive par le public. Il est donc de plus en plus nécessaire de créer des graphiques interactifs. Il existe plusieurs méthodes pour les concevoir, mais les deux methodes plus utilisées sont **plotly** et **shiny**. 

Dans cette page, nous allons nous concentrer sur la conversion d'un graphique de type `ggplot()` en un graphique interactif avec **plotly**. Vous pouvez en savoir plus sur **shiny** dans la page [Dashboards with Shiny]. Il est important de noter que les graphiques interactifs ne sont utilisables que dans les documents R markdown au format HTML, pas dans les documents PDF ou Word.

Ci-dessous, nous présentons un épicurve de base qui a été transformé pour être interactif en utilisant l'intégration de **ggplot2** et **plotly** (survolez votre souris sur le graphique, faites un zoom in ou cliquez sur les éléments de la légende). 

```{r plotly_demo, out.width=c('75%'), out.height=c('500px'), echo=F, warning=F, message=F}
pacman::p_load(plotly, rio, here, ggplot2, dplyr, lubridate, aweek)
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

## ces boutons ne sont pas nécessaires 
plotly_buttons_remove <- list('zoom2d','pan2d','lasso2d', 'select2d','zoomIn2d',
                              'zoomOut2d','autoScale2d','hoverClosestCartesian',
                              'toggleSpikelines','hoverCompareCartesian')

p <- linelist %>% 
  mutate(outcome = if_else(is.na(outcome), "Unknown", outcome),
         date_earliest = if_else(is.na(date_infection), date_onset, date_infection),
         week_earliest = floor_date(date_earliest, unit = "week",week_start = 1))%>% 
  count(week_earliest, outcome) %>% 
  ggplot()+
  geom_col(aes(week_earliest, n, fill = outcome))+
  xlab("Semaine de l'infection/de l'apparition") + ylab("Cas par semaine")+
  theme_minimal()

p %>% 
  ggplotly() %>% 
  partial_bundle() %>% 
  config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove)

```

<!-- ======================================================= -->
## Préparation {  }

### Importation des packages {.unnumbered} 

Ces lignes de code importe les packages necessaire pour l'analyse. Dans ce guide, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le package si nécessaire *puis* l'importe pour l'utiliser. Vous pouvez également charger les packages installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les packages en R. 


```{r}
pacman::p_load(
  rio,       # Importation/exportation
  here,      # chemins de fichiers
  lubridate, # Travailler avec les dates
  plotly,    # Graphiques interactifs
  scales,    # les pourcentages rapides
  tidyverse  # gestion et visualisation des données 
  ) 
```

### Commencez avec un `ggplot()` {.unnumbered}  

Dans cette page, nous supposons que vous commencez avec un graphique `ggplot()` que vous voulez rendre interactif. Nous allons construire plusieurs de ces graphiques dans cette page, en utilisant le case `linelist` utilisé dans la plupart des pages de ce manuel. 


### Importation des données {.unnumbered}

Pour commencer, nous allons importer une base de données appelée linelist_cleaned contenant les cas d'une épidémie d'Ebola simulée. Pour suivre, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Importez les données avec la fonction `import()` du package **rio** (cette fonction supporte de nombreux types de fichiers comme .xlsx, .csv, .rds - voir la page [Import and Export] pour plus de détails). 

```{r, echo=F}
# importer la liste de lignes dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# importer le cas de la liste de lignes
linelist <- import("linelist_cleaned.rds")
```

Les 50 premières lignes de la liste des lignes sont affichées ci-dessous.

```{r, message=FALSE, echo=F}
# afficher les données linelist sous forme de tableau
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```






  
<!-- ======================================================= -->
## Tracer avec `ggplotly()` { }

La fonction `ggplotly()` du package **plotly** permet de facilement rendre un `ggplot()` interactif. Il suffit de sauvegarder votre `ggplot()` et de le passer à la fonction `ggplotly()`.   


Ci-dessous, nous traçons une simple courbe représentant la proportion de cas décédés au cours d'une semaine donnée :  

Nous commençons par créer un tableau résumé de chaque semaine épidémiologique, et le pourcentage de cas avec un bilan connu qui sont décédés.  

```{r}
weekly_deaths <- linelist %>%
  group_by(epiweek = floor_date(date_onset, "week")) %>%  # créer et regrouper les données par la colonne epiweek
  summarise(                                              # créer un nouveau tableau de données résumé  :
    n_known_outcome = sum(!is.na(outcome), na.rm=T),      # nombre de cas par groupe dont le statut est connu
    n_death  = sum(outcome == "Death", na.rm=T),          # nombre de cas par groupe qui sont décédés
    pct_death = 100*(n_death / n_known_outcome)           # pourcentage des cas de statut connu qui sont décédés 
  )
```
Here is the first 50 rows of the `weekly_deaths` dataset.  

```{r message=FALSE, echo=F}
DT::datatable(head(weekly_deaths, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```
Ensuite, nous créons le graphique avec **ggplot2**, en utilisant `geom_line()`.  

```{r, warning=F, message=F}
deaths_plot <- ggplot(data = weekly_deaths)+            # commencer par les données hebdomadaires des décès 
  geom_line(mapping = aes(x = epiweek, y = pct_death))  # faire la ligne  

deaths_plot   # imprimer
```


Nous pouvons rendre ce graphique interactif en le passant simplement à `ggplotly()`, comme ci-dessous. Survolez votre souris sur la ligne pour afficher les valeurs x et y. Vous pouvez zoomer sur le tracé, et le déplacer. Vous pouvez également voir des icônes en haut à droite du graphe. Dans cet ordre, elles vous permettent de :  

* Télécharger la vue actuelle sous forme d'image PNG  
* Zoomer avec une boîte de sélection  
* Faire un panoramique, ou déplacer le graphe en cliquant et en faisant rouler le graphe 
* Faire un zoom in ou out, ou revenir au zoom par défaut.  
* Rétablir les axes par défaut  
* Activation/désactivation des "lignes de pointes", qui sont les lignes pointillées partant du point interactif et s'étendant vers les axes x et y.  
* Ajustements pour que les données s'affichent lorsque vous ne survolez pas la ligne.


```{r}
deaths_plot %>% plotly::ggplotly()
```

Les données groupées fonctionnent également avec `ggplotly()`. Ci-dessous, un épicurve hebdomadaire est fait, groupé par outcome. Les barreaux empilés sont interactifs. Cliquez sur les différents éléments de la légende (ils apparaîtront/disparaîtront).  


```{r plot_show, eval=F}
# Faire une courbe épidémique avec incidence2 pacakge
p <- incidence2::incidence(
  linelist,
  date_index = date_onset,
  interval = "weeks",
  groups = outcome) %>% plot(fill = outcome)
```

```{r, echo=T, eval=F}
# Faire le graphique interactif 
p %>% plotly::ggplotly()
```
  
```{r, warning = F, message = F, , out.width=c('95%'), out.height=c('500px'), echo=FALSE}
p %>% 
  ggplotly() %>% 
  partial_bundle() 
```
  
<!-- ======================================================= -->
## Modifications {  }

### Taille du fichier {.unnumbered}  

Quand vous exportez dans un HTML généré par R Markdown (comme ce livre!), vous voulez que le graphe ait une taille de données aussi réduite que possible (sans effets secondaires négatifs dans la plupart des cas). Pour cela, il suffit de passer le graphique interactif à `partial_bundle()`, également de **plotly**. 

```{r plot_tidyshow, eval=F}
p <- p %>% 
  plotly::ggplotly() %>%
  plotly::partial_bundle()
```

### Boutons {.unnumbered}  

Certains des boutons sur un plotly standard sont superflus et peuvent être distrayants, vous pouvez donc les supprimer. Vous pouvez le faire simplement en passant la sortie dans `config()` de **plotly** et en spécifiant les boutons à enlever. Dans l'exemple ci-dessous, nous spécifions en avance les noms des boutons à supprimer, et les transmettons à l'argument `modeBarButtonsToRemove = `. Nous définissons également `displaylogo = FALSE` pour supprimer le logo plotly. 

```{r plot_tidyshow2, eval=F}
## ces boutons sont distrayants et nous voulons les enlever
plotly_buttons_remove <- list('zoom2d','pan2d','lasso2d', 'select2d','zoomIn2d',
                              'zoomOut2d','autoScale2d','hoverClosestCartesian',
                              'toggleSpikelines','hoverCompareCartesian')

p <- p %>%          # re-définir le graphique interactif sans ces boutons 
  plotly::config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove)
```



<!-- ======================================================= -->
## Heat tiles {  }

Vous pouvez rendre presque tous les graphiques `ggplot()` interactifs, y compris les heat tiles. Dans la page sur les [Graphiques thermiques](#heatmaps), vous pouvez lire comment créer le graphique ci-dessous, qui affiche la proportion de jours par semaine pendant lesquels certains facilités ont rapporté des données à leur province. 

Voici le code, bien que nous ne le décriions pas en détails ici.  

```{r  message=F, warning=F}
# importer les données
facility_count_data <- rio::import(here::here("data", "malaria_facility_count_data.rds"))

# regrouper les données en semaines pour le Spring district 
agg_weeks <- facility_count_data %>% 
  filter(District == "Spring",
         data_date < as.Date("2020-08-01")) %>% 
  mutate(week = aweek::date2week(
    data_date,
    start_date = "Monday",
    floor_day = TRUE,
    factor = TRUE)) %>% 
  group_by(location_name, week, .drop = F) %>%
  summarise(
    n_days          = 7,
    n_reports       = n(),
    malaria_tot     = sum(malaria_tot, na.rm = T),
    n_days_reported = length(unique(data_date)),
    p_days_reported = round(100*(n_days_reported / n_days))) %>% 
  ungroup(location_name, week) %>% 
  right_join(tidyr::expand(., week, location_name)) %>% 
  mutate(week = aweek::week2date(week))

# Créer le graphique
metrics_plot <- ggplot(agg_weeks,
       aes(x = week,
           y = location_name,
           fill = p_days_reported))+
  geom_tile(colour="white")+
  scale_fill_gradient(low = "orange", high = "darkgreen", na.value = "grey80")+
  scale_x_date(expand = c(0,0),
               date_breaks = "2 weeks",
               date_labels = "%d\n%b")+
  theme_minimal()+ 
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text  = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1,"cm"),
    legend.key.width  = grid::unit(0.6,"cm"),
    axis.text.x = element_text(size=12),
    axis.text.y = element_text(vjust=0.2),
    axis.ticks = element_line(size=0.4),
    axis.title = element_text(size=12, face="bold"),
    plot.title = element_text(hjust=0,size=14,face="bold"),
    plot.caption = element_text(hjust = 0, face = "italic")
    )+
  labs(x = "Semaine",
       y = "Nom de l'établissement",
       fill = "Rapport de\nperformance (%)",
       title = "Pourcentage de jours par semaine où l'établissement a déclaré des données",
       subtitle = " Les établissements de santé de district, Avril-Mai 2019 ",
       caption = "Semaines de 7 jours commençant le lundi.")

metrics_plot # imprimer
```

Ci-dessous, nous le rendons interactif et le modifions pour les boutons simples et la taille du fichier.  

```{r,  out.width=c('95%'), out.height=c('500px')}
metrics_plot %>% 
  plotly::ggplotly() %>% 
  plotly::partial_bundle() %>% 
  plotly::config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove)
```

<!-- ## Maps {.unnumbered}   -->

<!-- You can also make `ggplot()` GIS maps interactive, although it makes a bit more care.  -->

<!-- THIS SECTION IS UNDER CONSTRUCTION  -->

<!-- Although **plotly** works well with `ggplot2::geom_sf` in RStudio, when you try to include its outputs in R Markdown HTML files (like this book), it doesn't work well.   -->

<!-- So instead you can use {**plotly**}'s own mapping tools which can be tricky but are easy when you know how. Read on...   -->

<!-- We're going to use Covid-19 incidence across African countries for this example. The data used can be found on the [World Health Organisation website](https://covid19.who.int/table).   -->

<!-- You'll also need a new type of file, a GeoJSON, which is sort of similar to a shp file for those familiar with GIS. For this book, we used one from [here](https://geojson-maps.ash.ms).   -->

<!-- GeoJSON files are stored in R as complex lists and you'll need to maipulate them a little. -->

<!-- ```{r, echo=T,} -->
<!-- ## You need two new packages: {rjson} and {purrr} -->
<!-- pacman::p_load(plotly, rjson, purrr) -->

<!-- ## This is a simplified version of the WHO data -->
<!-- df <- rio::import(here::here("data", "gis", "covid_incidence.csv")) -->

<!-- ## Load your geojson file -->
<!-- geoJSON <- rjson::fromJSON(file=here::here("data", "gis", "africa_countries.geo.json")) -->

<!-- ## Here are some of the properties for each element of the object -->
<!-- head(geoJSON$features[[1]]$properties) -->

<!-- ``` -->


<!-- This is the tricky part. For {**plotly**} to match your incidence data to GeoJSON, the countries in the geoJSON need an id in a specific place in the list of lists. For this we need to build a basic function: -->
<!-- ```{r} -->
<!-- ## The property column we need to choose here is "sovereignt" as it is the names for each country -->
<!-- give_id <- function(x){ -->

<!--   x$id <- x$properties$sovereignt  ## Take sovereignt from properties and set it as the id -->

<!--   return(x) -->
<!-- } -->

<!-- ## Use {purrr} to apply this function to every element of the features list of the geoJSON object -->
<!-- geoJSON$features <- purrr::map(.x = geoJSON$features, give_id) -->
<!-- ``` -->

<!-- <!-- ======================================================= --> -->
<!-- ### Maps - plot {  } -->

<!-- UNDER CONSTRUCTION -->

<!-- ```{r, echo=FALSE, eval=FALSE, out.width=c('95%'), out.height=c('500px'),warning=F} -->
<!-- plotly::plot_ly() %>%  -->
<!--   plotly::add_trace(                    #The main plot mapping functionn -->
<!--     type="choropleth", -->
<!--     geojson=geoJSON, -->
<!--     locations=df$Name,          #The column with the names (must match id) -->
<!--     z=df$Cumulative_incidence,  #The column with the incidence values -->
<!--     zmin=0, -->
<!--     zmax=57008, -->
<!--     colorscale="Viridis", -->
<!--     marker=list(line=list(width=0)) -->
<!--   ) %>% -->
<!--   colorbar(title = "Cases per million") %>% -->
<!--   layout(title = "Covid-19 cumulative incidence", -->
<!--                  geo = list(scope = 'africa')) %>%  -->
<!--   config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove) -->
<!-- ``` -->

<!-- ======================================================= -->
## Ressources {  }

Plotly n'est pas seulement conçu pour R, mais fonctionne aussi très bien avec Python (et en fait avec tous les langages de Data science puisqu'il est construit en JavaScript). Vous pouvez en savoir plus à ce sujet sur le site [plotly website](https://plotly.com/r/)


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/interactive_plots.Rmd-->

# (PART) Reports and dashboards {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_reports_dashboards.Rmd-->

# Production de rapports avec R Markdown {#rmarkdown}

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/rmarkdown_overview.png"))
```

R Markdown est un outil largement utilisé pour créer des résultats automatisés, reproductibles et prêts à être partagés, tels que des rapports. Il peut générer des sorties statiques ou interactives, aux formats Word, PDF, HTML, Powerpoint et autres.

Un script R Markdown associe le code R et le texte de sorte que le script *devient votre document de sortie*. Vous pouvez créer un document formaté complet, y compris un texte narratif (qui peut être dynamique pour changer en fonction de vos données), des tableaux, des figures, des puces/chiffres, des bibliographies, etc.

Ces documents peuvent être produits pour être mis à jour régulièrement (par exemple, des rapports de surveillance quotidiens) et/ou être exécutés sur des sous-ensembles de données (par exemple, des rapports pour chaque compétence).

D'autres pages de ce manuel traitent de ce sujet :

-   La page [Organisation des rapports de routine](#reportfactory) montre comment organiser la production de vos rapports avec des dossiers horodatés générés automatiquement.\
-   La page [Tableaux de bord avec R Markdown](#dashboards) explique comment formater un rapport R Markdown en tant que tableau de bord.

Il convient de noter que le projet [R4Epis](https://r4epis.netlify.app/) a développé des modèles de scripts R Markdown pour les épidémies et les scénarios d'enquête les plus courants rencontrés sur les sites des projets MSF.

<!-- ======================================================= -->


## Préparation

**Contexte du R Markdown**

Pour expliquer certains des concepts et des "packages" impliqués :

- **Markdown** est un "langage" qui vous permet d'écrire un document en texte brut, qui peut être converti en HTML et autres formats. Il n'est pas spécifique à R. Les fichiers écrits en Markdown ont une extension '.md'.
- **R Markdown** : est une variante de markdown qui _est spécifique à R_ - il vous permet d'écrire un document en utilisant markdown pour produire du texte *et pour incorporer du code R et afficher leurs sorties*. Les fichiers R Markdown ont une extension '.Rmd'.
- **rmarkdown - le "package"** : Il est utilisé par R pour convertir le fichier .Rmd en la sortie souhaitée. Il se concentre sur la conversion de la syntaxe markdown (texte), nous avons donc également besoin de...
- **knitr** : Ce "package" R lira les morceaux de code, les exécutera et les " tricotera" dans le document. C'est ainsi que les tableaux et les graphiques sont inclus à côté du texte.
- **Pandoc** : Enfin, pandoc convertit le résultat en Word/PDF/Powerpoint, etc. Il s'agit d'un logiciel distinct de R mais qui est installé automatiquement avec RStudio.

En résumé, le processus qui se déroule *en arrière-plan* (vous n'avez pas besoin de connaître toutes ces étapes !) consiste à transmettre le fichier .Rmd à **knitr**, qui exécute les morceaux de code R et crée un nouveau fichier .md (markdown) comprenant le code R et son résultat rendu. Le fichier .md est ensuite traité par pandoc pour créer le produit fini : un document Microsoft Word, un fichier HTML, un document Powerpoint, un PDF, etc.


```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/0_rmd.png"))
```

(source: <https://rmarkdown.rstudio.com/authoring_quick_tour.html>):

**Installation**

Pour créer une sortie R Markdown, vous devez avoir installé les éléments suivants :

- Le package **rmarkdown** (**knitr** sera également installé automatiquement).
- Pandoc, qui doit être installé avec RStudio. Si vous n'utilisez pas RStudio, vous pouvez télécharger Pandoc [ici](http://pandoc.org).
- Si vous souhaitez générer une sortie PDF (un peu plus délicat), vous devrez installer LaTeX. Pour les utilisateurs de R Markdown qui n'ont pas installé LaTeX auparavant, nous vous recommandons d'installer [TinyTeX](https://yihui.name/tinytex/). Vous pouvez utiliser les commandes suivantes:

```{r, eval=F}
pacman::p_load(tinytex)     # installer le package tinytex 
tinytex::install_tinytex()  # commande R pour installer TinyTeX 
```

<!-- ======================================================= -->

## Démarrage

### Installer le package R rmarkdown {.unnumbered}

Installez le "package" R **rmarkdown**.  Dans ce manuel, nous mettons l'accent sur la fonction `p_load()` du "package" **pacman**, qui installe le (ou une liste de) "package (s)" que si nécessaire (uniquement si le package n'est pas déjà installé) *et* le charge pour l'utiliser . On peut également charger des "packages" avec `library()` à partir de R **base**. Voir la page sur [R - les bases](#rbasics) pour plus d'informations sur les packages R.


```{r, eval=F}
pacman::p_load(rmarkdown)
```


### Créer un nouveau fichier Rmd {.unnumbered}

Dans RStudio, ouvrez un nouveau fichier R markdown, en commençant par 'File', puis 'New file' et enfin 'R markdown...'.

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/1_gettingstarted.png"))
```

R Studio vous donnera quelques options de sortie parmi lesquelles choisir. Dans l'exemple ci-dessous, nous sélectionnons "HTML" car nous voulons créer un document HTML. Le titre et les noms des auteurs ne sont pas importants. Si le type de document de sortie que vous voulez n'est pas l'un de ceux-là, ne vous inquiétez pas - vous pouvez choisir n'importe lequel et le changer dans le script plus tard.

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/1_gettingstartedB.png"))
```

Cela ouvrira un nouveau script .Rmd.

### Important à savoir {.unnumbered}

**Le répertoire de travail**

Le répertoire de travail d'un fichier markdown est l'endroit où le fichier Rmd lui-même est enregistré. Par exemple, si le projet R se trouve dans `~/Documents/projetX` et que le fichier Rmd lui-même se trouve dans un sous-dossier `~/Documents/projetX/markdownfiles/markdown.Rmd`, le code `read.csv("data.csv")` dans le markdown cherchera un fichier csv dans le dossier `markdownfiles`, et non dans le dossier racine du projet où les scripts dans les projets chercheraient normalement automatiquement.


Pour faire référence à des fichiers ailleurs, vous devrez soit utiliser le chemin complet du fichier, soit utiliser le package **here**. Le package **here** définit le répertoire de travail comme étant le dossier racine du projet R et est expliqué en détail dans les pages [Projets R](#r_projects) et [Importer et exporter des données](#import_export) de ce manuel. Par exemple, pour importer un fichier appelé "data.csv" depuis le dossier `projectX`, le code serait `import(here("data.csv"))`.

Notez que l'utilisation de `setwd()` dans les scripts R Markdown n'est pas recommandée -- elle ne s'applique qu'au morceau de code dans lequel elle est écrite.

**Travailler sur un disque plutôt que sur votre ordinateur**

Parce que R Markdown peut rencontrer des problèmes avec pandoc lorsqu'il est exécuté sur un serveur de stockage partagé, il est recommandé que votre dossier soit sur votre machine locale, par exemple dans un projet dans "Mes Documents". Si vous utilisez Git (fortement recommandé !), cela vous sera familier. Pour plus de détails, consultez les pages du manuel intitulées [R sur les lecteurs réseau](#network_drives) et [Erreurs fréquentes](#errors).

<!-- ======================================================= -->

## Les composantes du R Markdown

Un document R Markdown peut être édité dans RStudio tout comme un script R standard. Lorsque vous démarrez un nouveau script R Markdown, RStudio essaie d'être utile en affichant un modèle qui explique les différentes sections d'un script R Markdown.

Ce qui suit est ce qui apparaît lorsque vous démarrez un nouveau script Rmd destiné à produire une sortie HTML (comme dans la section précédente).

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/2_defaultRMD.png"))
```

Comme vous pouvez le constater, un fichier Rmd comporte trois éléments de base : YAML, le texte Markdown et les morceaux de code R.

Ces éléments vont *créer et devenir votre document de sortie*. Voir le diagramme ci-dessous :


```{r out.width = "100%", out.height="150%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/rmarkdown_translation.png"))
```


### Métadonnées YAML {.unnumbered}

Appelées "métadonnées YAML" ou simplement "YAML", elles se trouvent en haut du document R Markdown. Cette section du script indique à votre fichier Rmd le type de sortie à produire, les préférences de formatage et d'autres métadonnées telles que le titre du document, l'auteur et la date. Il existe d'autres utilisations qui ne sont pas mentionnées ici (mais auxquelles il est fait référence dans la section ["Produire une sortie"](#production)).   Notez que l'indentation est importante ; les tabulations ne sont pas acceptées, mais les espaces le sont.


Cette section doit commencer par une ligne contenant seulement trois tirets `---` et doit se terminer par une ligne contenant seulement trois tirets `---`. Les paramètres YAML se présentent sous forme de paires `key:value`. L'emplacement des deux points dans YAML est important : les paires `key:value` sont séparées par des deux points (et non par des signes égaux !).

Le fichier YAML doit commencer par les métadonnées du document. L'ordre de ces paramètres YAML primaires (non indentés) n'a pas d'importance. Par exemple :

``` {.yaml}
title: "Mon document"
author: "Moi"
date: "`r Sys.Date()`"
```

Vous pouvez utiliser du code R dans des valeurs YAML en l'écrivant en tant que code en ligne (précédé de `r` dans les crochets arrière) mais aussi entre guillemets (voir l'exemple ci-dessus pour `date:`).


Dans l'image ci-dessus, parce que nous avons cliqué que notre sortie par défaut serait un fichier html, nous pouvons voir que le YAML dit `output: html_document`. Cependant, nous pouvons aussi changer cela pour dire `powerpoint_presentation` ou `word_document` ou même `pdf_document`.

### Texte {.unnumbered}

Il s'agit de la narration de votre document, y compris les titres et les en-têtes. Il est écrit dans le langage "markdown", qui est utilisé dans de nombreux logiciels différents.

Vous trouverez ci-dessous les principales façons d'écrire ce texte. Vous trouverez une documentation plus complète sur l'antisèche R Markdown sur le [site Web de RStudio](https://rstudio.com/resources/cheatsheets/).


#### Nouvelles lignes {.unnumbered}

Dans le format R Markdown, pour aller à une nouvelle ligne, saisissez *deux espaces* à la fin de la ligne précédente, puis appuyez sur Entrée/Retour.  

#### Police {.unnumbered}

Entourez votre texte normal de ces caractères pour modifier la façon dont il apparaît dans le fichier de sortie.

- Caractères de soulignement (`_text_`) ou astérisque simple (`*text*`) pour _italiciser_.
- Double astérisque (`**text**`) pour mettre **le texte en gras**.
- Des "quotes" inversés (````text````) pour afficher le texte sous forme de code.

L'apparence réelle de la police peut être définie en utilisant des modèles spécifiques (spécifiés dans les métadonnées YAML ; voir l'exemple des onglets).

#### Couleur {.unnumbered}

Il n'existe pas de mécanisme simple pour modifier la couleur du texte dans R Markdown. Une solution de contournement, *si votre fichier de sortie est un fichier HTML*, consiste à ajouter une ligne HTML dans le texte Markdown. Le code HTML ci-dessous imprimera une ligne de texte en rouge gras.


``` {.md}
<span style="color: red;">**_DANGER:_** Ceci est un avertissement.</span>  
```

[**_DANGER:_** Ceci est un avertissement.]{style="color: red;"}


#### Titres et en-têtes {.unnumbered}

Un symbole de hachage dans une partie de texte d'un script R Markdown crée un titre. C'est différent d'un morceau de code R dans le script, dans lequel un symbole de hachage est un mécanisme pour commenter/annoter/désactiver, comme dans un script R normal.

Différents niveaux de titre sont établis avec différents nombres de symboles de hachage au début d'une nouvelle ligne. Un symbole de hachage est un titre ou une rubrique primaire. Deux symboles de hachage correspondent à un sous-titre (deuxième niveau). Les titres de troisième et quatrième niveaux peuvent être établis avec un nombre croissant de symboles de hachage.

``` {.md}
# Titre (Titre 1)

## Sous-titre (Titre 2)

### Sous-sous-titre (Titre 3)
```

#### Puces et numérotation {.unnumbered}

Utilisez des astérisques (`*`) pour créer une liste de puces. Terminez la phrase précédente, saisissez deux espaces, tapez sur Entrée/Retour *deux fois*, puis commencez vos puces. Insérez un espace entre l'astérisque et le texte de votre puce. Après chaque puce, saisissez deux espaces, puis appuyez sur la touche Entrée/Retour. Les sous-puces fonctionnent de la même manière, mais sont en retrait. Les numérotations fonctionnent de la même manière, mais au lieu d'un astérisque, écrivez 1), 2), etc. Voici à quoi pourrait ressembler le texte de votre script R Markdown.

``` {.md}

Voici mes puces (il y a deux espaces après ce deux-points):    


* Puce 1 (suivi de deux espaces et Entrée/Retour) 
* Puce 2 (suivi de deux espaces et Entrée/Retour) 
  * Sous-puce 1 (suivi de deux espaces et Entrée/Retour)  
  * Sous-puce 2 (suivi de deux espaces et Entrée/Retour)  
  
```

#### Commenter du texte {.unnumbered}

Vous pouvez "commenter" du texte R Markdown de la même manière que vous pouvez utiliser le "\#" pour commenter une ligne de code R dans un chunk R. Il suffit de mettre le texte en surbrillance et d'appuyer sur Ctrl+Shift+c (Cmd+Shift+c pour Mac). Le texte sera entouré de flèches et deviendra vert. Il n'apparaîtra pas dans votre résultat.


```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/rmarkdown_hide_text.png"))
```

### Morceaux de code {.unnumbered}

Les sections du script qui sont dédiées à l'exécution du code R sont appelées "chunks". C'est là que vous pouvez charger des "packages", importer des données et effectuer la gestion et la visualisation des données. Il peut y avoir de nombreux "chunks" de code (mettez  en autant qu'il en faut pour un code plus lisible et comprehensible), ils peuvent donc vous aider à organiser votre code R en parties, éventuellement entrecoupées de texte. **Remarque** : ces "chunks" auront une couleur de fond légèrement différente de celle de la partie narrative du document.


Chaque chunk s'ouvre sur une ligne qui commence par trois "quotes" inversés et des crochets qui contiennent les paramètres du chunk (`{ }`). Le chunk se termine par trois autres "quotes" inversés.

Vous pouvez créer un nouveau chunk en le tapant vous-même, en utilisant le raccourci clavier "Ctrl + Alt + i" (ou Cmd + Shift + r sur Mac), ou en cliquant sur l'icône verte "insérer un nouveau chunk de code" en haut de votre éditeur de script.

Quelques remarques sur le contenu des accolades `{ }`:

* Ils commencent par 'r' pour indiquer que le nom du langage dans le chunk est R.
* Après le r, vous pouvez éventuellement écrire un "nom" de chunk -- ceux-ci ne sont pas nécessaires mais peuvent vous aider à organiser votre travail. Notez que si vous nommez vos morceaux, vous devez TOUJOURS utiliser des noms uniques, sinon R se plaindra lorsque vous essaierez de compiler  le rendu.
- Les accolades peuvent également inclure d'autres options, écrites comme `tag=value`, telles que :
- `eval = FALSE` pour ne pas exécuter le code R
- `echo = FALSE` pour ne pas imprimer le code source R du chunk dans le document de sortie.
- `warning = FALSE` pour ne pas afficher les avertissements générés par le code R
- `message = FALSE` pour ne pas imprimer les messages produits par le code R.
- `include =` soit TRUE/FALSE si l'on veut inclure les sorties du chunk (par exemple les graphiques) dans le document.
- `out.width =` et `out.height =` - à fournir dans le style `out.width = "75%"`
- `fig.align = "center"` ajuste l'alignement d'une figure sur la page.
- `fig.show='hold'` si votre chunk imprime plusieurs figures et que vous souhaitez qu'elles soient affichées les unes à côté des autres (à associer à `out.width = c("33%", "67%")`. Vous pouvez également définir `fig.show='asis'` pour les afficher en dessous du code qui les génère, `'hide'` pour les cacher, ou `'animate'` pour concaténer plusieurs d'entre elles dans une animation.`
- Un en-tête de chunk doit être écrit en *une ligne*.
- Essayez d'éviter les points, les caractères de soulignement et les espaces. Utilisez des tirets ( - ) à la place si vous avez besoin d'un séparateur.


Lisez plus en détail les options **knitr** [ici](https://yihui.org/knitr/options/).

Certaines des options ci-dessus peuvent être configurées par clique-bouton en utilisant les boutons de réglage en haut à droite du chunk. Ici, vous pouvez spécifier quelles parties du chunk vous voulez que le document rendu inclue, à savoir le code, les sorties et les avertissements. Cela se traduira par des préférences écrites entre les crochets, par exemple `echo=FALSE` si vous spécifiez que vous voulez seulement afficher le rendu et non le code qui le produit 'Show output only'.

```{r out.width = "80%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/3_chunk.png"))
```

Il y a aussi deux flèches en haut à droite de chaque chunk, qui sont utiles pour exécuter du code dans un chunk, ou tout le code des chunks précédents. Survolez-les pour voir ce qu'elles font.

Pour que les options globales soient appliquées à tous les chunks du script, vous pouvez les configurer dans le tout premier chunk de code R du script. Par exemple, pour que seules les sorties soient affichées pour chaque chunk de code et non le code lui-même, vous pouvez inclure cette commande dans le chunk de code R :

```{r, eval=F}
knitr::opts_chunk$set(echo = FALSE) 
```

#### Inclure du code R dans la partie Texte du Markdown {.unnumbered}

Vous pouvez également inclure un minimum  de code R  dans le corps du texte de votre document Markdown en utilisant les "quotes" inversés. Dans les "quotes" inversés, commencez le code par "r" et un espace, afin que RStudio sache qu'il doit évaluer le code en tant que code R. Voir l'exemple ci-dessous.

L'exemple ci-dessous montre plusieurs niveaux de titres, des puces, et utilise du code R pour la date actuelle (`Sys.Date()`) pour l'évaluer en une date imprimée.

```{r out.width = "80%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/2_text.png"))
```

L'exemple ci-dessus est simple (affichage de la date actuelle), mais en utilisant la même syntaxe, vous pouvez afficher des valeurs produites par un code R plus complexe (par exemple, pour calculer le min, la médiane, le max d'une colonne). Vous pouvez également intégrer des objets R ou des valeurs qui ont été créés dans des morceaux de code R plus tôt dans le script.

Par exemple, le script ci-dessous calcule la proportion de cas âgés de moins de 18 ans, en utilisant les fonctions **tidyverse**, et crée les objets `less18`, `total`, et `less18prop`. Cette valeur dynamique est insérée dans le texte suivant. Nous voyons à quoi cela ressemble lorsqu'il est rendu dans un document Word.

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/3_chunktext.png"))
```

### Images {.unnumbered}

Vous pouvez inclure des images dans votre document R Markdown en utilisant l'une des méthodes suivantes:

```{r, eval=F}
![]("path/to/image.png")  
```

Si la première méthode ne marche pas, esssayez d'utiliser:  `knitr::include_graphics()`

```{r, eval=F}
knitr::include_graphics("path/to/image.png")
```

<span style="color: black;"> Rappelez-vous, votre chemin de fichier pourrait être écrit en utilisant le package. **here**</span>

```{r, eval=F}
knitr::include_graphics(here::here("path", "to", "image.png"))
```

### Tables {.unnumbered}

Créez un tableau en utilisant des traits d'union ( - ) et des barres ( | ). Le nombre de traits d'union avant/entre les barres permet de déterminer le nombre d'espaces dans la cellule avant que le texte ne commence à se positionner.


``` {.md}
Column 1 |Column  2 |Column 3
---------|----------|--------
Cell A   |Cell B    |Cell C
Cell D   |Cell E    |Cell F
```

Le code ci-dessus produit le tableau ci-dessous :

| Column 1 | Column 2 | Column 3 |
|----------|----------|----------|
| Cell A   | Cell B   | Cell C   |
| Cell D   | Cell E   | Cell F   |

### Sections à onglets {.unnumbered}

Pour les sorties HTML, on peut organiser les sections en "onglets". Il suffit d'ajouter `.tabset` dans les accolades `{ }` qui sont ouvertes juste *après le titre* de la section. Tous les sous-titres situés sous ce titre (jusqu'à un autre titre de même niveau) apparaîtront sous forme d'onglets sur lesquels l'utilisateur pourra cliquer. En savoir plus [ici](https://bookdown.org/yihui/rmarkdown-cookbook/html-tabs.html)

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/tabbed_script.png"))
knitr::include_graphics(here::here("images", "markdown/tabbed_view.gif"))

```

Vous pouvez ajouter une option supplémentaire `.tabset-pills` après `.tabset` pour donner aux onglets eux-mêmes un aspect plus esthétique avec un fond en noir. 






<!-- ======================================================= -->

## Structure du fichier R Markdown

Il existe plusieurs façons de structurer votre fichier R Markdown et les scripts R associés. Chacune présente des avantages et des inconvénients :


* R Markdown autonome - tout ce qui est nécessaire pour le rapport est importé ou créé dans le même fichier R Markdown.

    * Faire appel à (sourcer) d'autres fichiers - Vous pouvez exécuter des scripts R externes avec la commande `source()` et utiliser leurs sorties dans le Rmd\.
    * Scripts dépendant ou dérivé ("child script") - un mécanisme alternatif pour la commande `source()`

* Utiliser un "runfile" - Exécuter des commandes dans un script R *avant* de rendre le Markdown R.

### Rmd autonome {.unnumbered}


Pour un rapport relativement simple, on peut choisir d'organiser notre script R Markdown de manière à ce qu'il soit "autonome" et n'implique pas de scripts externes.

Tout ce dont on a besoin pour exécuter le script R Markdown est importé ou créé dans le fichier Rmd, y compris tous les morceaux de code et le chargement des "packages". Cette approche "autonome" est appropriée lorsqu'on n'a pas besoin de faire beaucoup de traitement de données (par exemple, elle apporte un fichier de données propre ou semi-propre) et que le rendu du R Markdown ne prendra pas trop de temps.

Dans ce scénario, une organisation logique du script R Markdown pourrait être la suivante :

1) Définir les options globales de **knitr**.
2) Chargement des "packages"
3) Importer les données\
4) Traiter les données
5) Produire des résultats (tableaux, graphiques, etc.)
6) Sauvegarder les résultats, le cas échéant (.csv, .png, etc.)

#### Faire appel à (sourcer) d'autres fichiers {.unnumbered}

Une variante de l'approche "autonome" consiste à faire en sorte que les morceaux de code R Markdown "sourcent" (exécutent) d'autres scripts R. Cela peut rendre votre script R Markdown moins encombré, plus simple et plus facile à organiser. Cela peut rendre votre script R Markdown moins encombré, plus simple et plus facile à organiser. Elle peut également être utile si vous souhaitez afficher les chiffres finaux au début du rapport. Dans cette approche, le script R Markdown final combine simplement les sorties prétraitées dans un document.

Une façon de le faire est de fournir les scripts R (chemin et nom de fichier avec extension) à la commande **base** R `source()`.

```{r, eval=F}
source("your-script.R", local = knitr::knit_global())
# ou sys.source("your-script.R", envir = knitr::knit_global())
```


Notez que lorsque vous utilisez `source()` *dans* le R Markdown, les fichiers externes seront toujours exécutés *pendant le rendu de votre fichier Rmd*. Par conséquent, chaque script est exécuté à chaque fois que vous rendez le rapport. Ainsi, le fait d'avoir ces commandes `source()` *dans* le R Markdown n'accélère pas votre temps d'exécution, et ne vous aide pas beaucoup à débloquer, puisque les erreurs produites seront toujours affichées lors de l'exécution du R Markdown.

Une alternative est d'utiliser l'option `child =` **knitr**.


Vous devez être conscient des différents *environnements* de R. Les objets créés dans un environnement ne seront pas nécessairement disponibles dans l'environnement utilisé par le Markdown R.


### Runfile {.unnumbered}


Par exemple, vous pouvez charger les "packages", charger et nettoyer les données, et même créer les graphiques d'intérêt avant `render()`. Ces étapes peuvent se produire dans le script R, ou dans d'autres scripts qui sont "sourcés". Tant que ces commandes se produisent dans la même session RStudio et que les objets sont enregistrés dans l'environnement, les objets peuvent ensuite être appelés dans le contenu Rmd. Ensuite, le R markdown lui-même ne sera utilisé que pour l'étape finale - pour produire la sortie avec tous les objets prétraités. Il est beaucoup plus facile de débloquer si quelque chose dans le code ne va pas.


Cette approche implique l'utilisation du script R qui contient la ou les commandes `render()` pour pré-traiter les objets qui alimentent le balisage R.


Cette approche est utile pour les raisons suivantes :

* Des messages d'erreur plus informatifs - ces messages seront générés par le script R, et non par le Markdown R. Les erreurs du Markdown R ont tendance à vous indiquer que vous n'avez pas besoin de les corriger. Les erreurs du Markdown R ont tendance à vous indiquer quel "chunk" a un problème, mais ne vous diront pas quelle ligne.
* Le cas échéant, vous pouvez exécuter des étapes de traitement longues avant la commande `render()` - elles ne seront exécutées qu'une seule fois.

Dans l'exemple ci-dessous, nous avons un script R séparé dans lequel nous pré-traitons un objet `data` dans l'environnement R, puis nous rendons le fichier "create_output.Rmd" en utilisant `render()`.


```{r, eval=F}
data <- import("datafile.csv") %>%       # Charger les données et les sauvegarder dans l'environnement
  select(age, hospital, weight)          # Sélectionner les colonnes d'interet

rmarkdown::render(input = "create_output.Rmd")   # Creer le fichier Rmd 
```

### Structure du dossier {.unnumbered}


Le flux de travail concerne également la structure globale des dossiers, par exemple un dossier "output" pour les documents et figures créés, et des dossiers "data" ou "inputs" pour les données nettoyées. Nous n'entrerons pas dans les détails ici, mais consultez la page [Organisation des rapports de routine](#reportfactory).

## Produire le document {#production}

Vous pouvez produire le document de la manière suivante :

- Manuellement en appuyant sur le bouton "Knit" en haut de l'éditeur de script RStudio (rapide et facile)\.
- Exécuter la commande `render()` (exécutée en dehors du script R Markdown)


### Option 1: Bouton "Knit" {.unnumbered}

Une fois le fichier Rmd ouvert, appuyez sur l'icône/bouton "Knit" en haut du fichier.

R Studio affichera la progression dans un onglet "R Markdown" près de votre console R. Le document s'ouvrira automatiquement une fois terminé.

Le document sera enregistré dans le même dossier que votre script R markdown, et avec le même nom de fichier (à l'exception de l'extension). Ce n'est évidemment pas idéal pour le contrôle de version (il sera écrasé à chaque fois que vous cliquerez pour produire le fichier Rmd, à moins d'être déplacé manuellement), car vous devrez peut-être renommer le fichier vous-même (par exemple, ajouter une date).

C'est le bouton de raccourci de RStudio pour la fonction `render()` de **rmarkdown**. Cette approche n'est compatible qu'avec un fihcier R markdown autonome, où tous les composants nécessaires existent ou proviennent du fichier.


```{r out.width = "90%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/4_progress.png"))
```

### Option 2: Commande `render()` {.unnumbered}

Une autre façon de produire votre sortie R Markdown est d'exécuter la fonction `render()` (du "package" **rmarkdown**). Vous devez exécuter cette commande *en dehors* du script R Markdown - donc soit dans un script R séparé (souvent appelé "fichier d'exécution"), soit comme une commande autonome dans la Console R.

```{r, eval=F}
rmarkdown::render(input = "my_report.Rmd")
```


Comme avec "knit", les paramètres par défaut enregistreront la sortie Rmd dans le même dossier que le script Rmd, avec le même nom de fichier (à part l'extension de fichier). Par exemple, "mon_rapport.Rmd", une fois exécuté, créera "mon_rapport.docx" si vous décider de sortir le fichier vers un document Word. Cependant, en utilisant `render()` vous avez la possibilité d'utiliser des paramètres différents. `render()` peut accepter des arguments tels que :


* `output_format =` C'est le format de sortie vers lequel convertir (par exemple, `"html_document"`, `"pdf_document"`, `"word_document"`, ou `"all"`). Vous pouvez également le spécifier dans le YAML à l'intérieur du script R Markdown.
* `output_file =` C'est le nom du fichier de sortie (et le chemin du fichier). Il peut être créé par des fonctions R telles que `here()` ou `str_glue()`, comme illustré ci-dessous.
* `output_dir =` C'est un répertoire de sortie (dossier) pour enregistrer le fichier. Cela vous permet de choisir un autre répertoire que celui dans lequel le fichier Rmd est enregistré.
* `output_options =` Vous pouvez fournir une liste d'options qui remplaceront celles du script YAML (par exemple )
* `output_yaml =` Vous pouvez fournir le chemin d'accès à un fichier .yml qui contient des spécifications YAML.
* `params =` Voir la section sur les paramètres ci-dessous.
* Voir la liste complète [ici](https://pkgs.rstudio.com/rmarkdown/reference/render.html)


Par exemple, pour améliorer le contrôle de version, la commande suivante enregistre le fichier de sortie dans un sous-dossier "outputs", avec la date du jour dans le nom du fichier. Pour créer le nom du fichier, la fonction `str_glue()` du paquet **stringr** est utilisée pour 'coller' ensemble des chaînes statiques (écrites en clair) avec du code R dynamique (écrit entre crochets). Par exemple, si nous sommes le 10 avril 2021, le nom du fichier ci-dessous sera "Report_2021-04-10.docx". Voir la page sur [Caractères et chaînes de caractères](#character_strings) pour plus de détails sur `str_glue()`.

```{r, eval=F}
rmarkdown::render(
  input = "create_output.Rmd",
  output_file = stringr::str_glue("outputs/Report_{Sys.Date()}.docx")) 
```



Au fur et à mesure de l'exécution du fichier Rmarkdown, la console RStudio vous montrera la progression du rendu jusqu'à 100%, et un message final pour indiquer que l'exécution est achevée.


### Option 3 : package **reportfactory** {.unnumbered}

Le "package" R **reportfactory** offre une méthode alternative d'organisation et de compilation de rapports R Markdown *adapté aux cas où vous exécutez des rapports régulièrement (par exemple, quotidiennement, hebdomadairement...).* Il facilite la compilation de plusieurs fichiers R Markdown et l'organisation de leurs sorties. Essentiellement, il fournit une "usine" à partir de laquelle vous pouvez exécuter les rapports R Markdown, obtenir des dossiers automatiquement horodatés pour les fichiers de sortie, et avoir un contrôle de version "léger".

Pour en savoir plus sur ce flux de travail, consultez la page [Organisation des rapports de routine](#reportfactory).

<!-- ======================================================= -->


## Rapports paramétrés

Vous pouvez utiliser le paramétrage pour rendre un rapport dynamique, de sorte qu'il puisse être exécuté avec des paramètres spécifiques (par exemple, une date ou un lieu spécifique ou avec certaines options d'exécution). Nous nous concentrons ci-dessous sur les principes de base, mais il existe d'autres [détails en ligne](https://bookdown.org/yihui/rmarkdown/parameterized-reports.html) sur les rapports paramétrés.

En utilisant la liste linéaire des cas Ebola comme exemple, disons que nous voulons exécuter un rapport de surveillance standard pour chaque hôpital chaque jour. Nous montrons comment on peut le faire en utilisant des paramètres.

*Important: les rapports dynamiques sont également possibles sans la structure formelle des paramètres (sans `params:`), en utilisant de simples objets R dans un script R adjacent. Ceci est expliqué à la fin de cette section.*

### Définition des paramètres {.unnumbered}

Vous avez plusieurs options pour spécifier les valeurs des paramètres pour votre sortie R Markdown.

#### Option 1 : Définir les paramètres dans YAML {.unnumbered}

Editez le YAML pour inclure une option `params:`, avec des déclarations indentées pour chaque paramètre que vous voulez définir. Dans cet exemple, nous créons les paramètres `date` et `hôpital`, pour lesquels nous spécifions des valeurs. Ces valeurs sont susceptibles de changer à chaque fois que le rapport est exécuté. Si vous utilisez le bouton "Knit" pour produire le résultat, les paramètres auront ces valeurs par défaut. De même, si vous utilisez `render()`, les paramètres auront ces valeurs par défaut, sauf indication contraire dans la commande `render()`.


``` {.yaml}
---
title: Surveillance report
output: html_document
params:
 date: 2021-04-10
 hospital: Central Hospital
---
```

En arrière-plan, ces valeurs de paramètres sont contenues dans une liste en lecture seule appelée `params`. Ainsi, vous pouvez insérer les valeurs des paramètres dans le code R comme vous le feriez pour un autre objet/valeur R dans votre environnement. Tapez simplement `params$` suivi du nom du paramètre. Par exemple `params$hospital` pour représenter le nom de l'hôpital ("Central Hospital" par défaut).


Notez que les paramètres peuvent également contenir les valeurs `vrai` ou `faux`, et donc ceux-ci peuvent être inclus dans vos options **knitr** pour un "chunk" R. Par exemple, vous pouvez définir `{r, eval=params$run}` au lieu de `{r, eval=FALSE}`, et maintenant si le chunk s'exécute ou non dépend de la valeur d'un paramètre `run:`.

Notez que pour les paramètres qui sont des dates, ils seront entrés comme une chaîne. Donc, pour que `params$date` soit interprété dans le code R, il faudra probablement l'envelopper avec `as.Date()` ou une fonction similaire pour le convertir en classe Date.

#### Option 2 : Définir les paramètres dans `render()` {.unnumbered}

Comme mentionné plus haut, une alternative à l'appui sur le bouton "Knit" pour produire la sortie est d'exécuter la fonction `render()` à partir d'un script séparé. Dans ce dernier cas, vous pouvez spécifier les paramètres à utiliser dans ce rendu à l'argument `params =` de `render()`.

Notez que toutes les valeurs de paramètres fournies ici vont *écraser* leurs valeurs par défaut si elles sont écrites dans le YAML. Nous écrivons les valeurs entre guillemets car dans ce cas, elles doivent être définies comme des valeurs de type chaîne de caractères.


La commande ci-dessous rend "surveillance_report.Rmd", spécifie un nom de fichier de sortie dynamique et un dossier, et fournit une `list()` de deux paramètres et leurs valeurs à l'argument `params = `.

```{r, eval=F}
rmarkdown::render(
  input = "surveillance_report.Rmd",  
  output_file = stringr::str_glue("outputs/Report_{Sys.Date()}.docx"),
  params = list(date = "2021-04-10", hospital  = "Central Hospital"))
```

#### Option 3 : Définir les paramètres à l'aide d'une interface utilisateur graphique {.unnumbered}

Pour une sensation plus interactive, vous pouvez également utiliser l'interface utilisateur graphique (GUI) pour sélectionner manuellement les valeurs des paramètres. Pour ce faire, nous pouvons cliquer sur le menu déroulant à côté du bouton "Knit" et choisir "Knit with parameters".

Une fenêtre pop-up apparaît alors pour vous permettre de saisir les valeurs des paramètres établis dans le YAML du document.

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/5_parametersGUI.png"))
```

Vous pouvez réaliser la même chose avec une commande `render()` en spécifiant `params = "ask"`, comme démontré ci-dessous.

```{r, eval=F}
rmarkdown::render(
  input = "surveillance_report.Rmd",  
  output_file = stringr::str_glue("outputs/Report_{Sys.Date()}.docx"),
  params = “ask”)
```

Toutefois, la saisie de valeurs dans cette fenêtre "pop-up" est sujette à des erreurs et à des fautes d'orthographe. Vous préférerez peut-être ajouter des restrictions aux valeurs qui peuvent être saisies dans les menus déroulants. Vous pouvez le faire en ajoutant dans le YAML plusieurs spécifications pour chaque entrée `params: ` .

* `label:` est le titre de ce menu déroulant particulier.
* `value:` est la valeur par défaut (de départ)\
* `input:` est défini sur `select` pour le menu déroulant\
* `choices:` Donne les valeurs éligibles dans le menu déroulant


Ci-dessous, ces spécifications sont écrites pour le paramètre `hôpital`.


``` {.yaml}
---
title: Surveillance report
output: html_document
params:
 date: 2021-04-10
 hospital: 
  label: “Town:”
  value: Central Hospital
  input: select
  choices: [Central Hospital, Military Hospital, Port Hospital, St. Mark's Maternity Hospital (SMMH)]
---
```

Lors de l'exécution du fichier (via le bouton "tricot avec des paramètres" ou par `render()`), la fenêtre pop-up aura des options déroulantes à sélectionner.

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/5_parametersGUIB.png"))
```

### Exemple paramétré {.unnumbered}

Le code suivant crée des paramètres pour `date` et '`hôpital`, qui sont utilisés dans le R Markdown comme `params$date` et `params$hospital`, respectivement.

Dans le rapport qui en résulte, vous pouvez voir comment les données sont filtrées sur l'hôpital spécifique, et le titre du graphique fait référence à l'hôpital et à la date corrects. Nous utilisons ici le fichier "linelist_cleaned.rds", mais il serait particulièrement approprié que la linelist elle-même comporte également un horodatage pour s'aligner sur la date paramétrée.

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/6_Rmdexample.png"))
```

Lancer l'excecution produit la sortie finale avec la police et la mise en page par défaut.  

```{r out.width = "80%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/6_RmdexampleB.png"))
```

### Paramétrisation sans `params` {.unnumbered}

Si vous exécutez un fichier R Markdown avec `render()` à partir d'un script séparé, vous pouvez en fait avoir le résultat du paramétrage sans utiliser la fonctionnalité `params:`.

Par exemple, dans le *script R* qui contient la commande `render()`, vous pouvez simplement définir `hôpital` et `date` comme deux objets R (valeurs) avant la commande `render()`. Dans le R Markdown, vous n'auriez pas besoin d'avoir une section `params:` dans le YAML, et nous ferions référence à l'objet `date` plutôt qu'à `params$date` et à `hôpital` plutôt qu'à `params$hospital`.

```{r, eval=F}
# Il s'agit d'un script R distinct du fichier R Markdown.

# définir les objets R
hospital <- "Central Hospital"
date <- "2021-04-10"

# Exécuter le fichier R markdown
rmarkdown::render(input = "create_output.Rmd") 
```

Suivre cette approche signifie que vous ne pouvez pas "Exécuter avec des paramètres", utiliser l'interface graphique ou inclure des options d'exécution dans les paramètres. Cependant, elle permet de simplifier le code, ce qui peut être avantageux.

<!-- ======================================================= -->

## Rapports en boucle


Nous pouvons vouloir exécuter un rapport plusieurs fois, en faisant varier les paramètres d'entrée, afin de produire un rapport pour chaque juridiction/unité. Cela peut être fait en utilisant des outils *d'itération*, qui sont expliqués en détail dans la page [Itération, boucles et listes](#iteration). Les options comprennent le paquet **purrr**, ou l'utilisation d'une *boucle for* comme expliqué ci-dessous.

Ci-dessous, nous utilisons une simple *boucle for* pour générer un rapport de surveillance pour tous les hôpitaux d'intérêt. Ceci est fait avec une seule commande (au lieu de changer manuellement le paramètre de l'hôpital un par un). La commande permettant de rendre les rapports doit exister dans un script séparé *sauf* le rapport Rmd. Ce script contiendra également des objets définis à parcourir en boucle - la date du jour, et un vecteur de noms d'hôpitaux à parcourir en boucle.

```{r, eval=F}
hospitals <- c("Central Hospital",
                "Military Hospital", 
                "Port Hospital",
                "St. Mark's Maternity Hospital (SMMH)") 
```

Nous introduisons ensuite ces valeurs une par une dans la commande `render()` en utilisant une boucle, qui exécute la commande une fois pour chaque valeur du vecteur `hospitals`. La lettre `i` représente la position de l'index (1 à 4) de l'hôpital actuellement utilisé dans cette itération, tel que `hospital_list[1]` serait "Central Hospital". Cette information est fournie à deux endroits dans la commande `render()` :

1) Au nom du fichier, de sorte que le nom du fichier de la première itération, s'il est produit le 10 avril 2021, sera "Report_Central Hospital_2021-04-10.docx", enregistré dans le sous-dossier "output" du répertoire de travail.
2) Pour `params =` de sorte que le Rmd utilise le nom de l'hôpital en interne chaque fois que la valeur `params$hospital` est appelée (par exemple pour filtrer l'ensemble de données sur l'hôpital particulier uniquement). Dans cet exemple, quatre fichiers seront créés - un pour chaque hôpital.

```{r, eval=F}
for(i in 1:length(hospitals)){
  rmarkdown::render(
    input = "surveillance_report.Rmd",
    output_file = str_glue("output/Report_{hospitals[i]}_{Sys.Date()}.docx"),
    params = list(hospital  = hospitals[i]))
}       
```

<!-- In the scenario where you are f not using this strict form of parameterisation but saving objects to the environment, as discussed at the end of the parameterisation section, the render function would look like this: -->

<!-- ```md -->

<!-- for(i in 1:length(hospital_list)){ -->

<!-- rmarkdown::render("surveillance_report.Rmd", -->

<!--                   output_file = paste0("output/Report_", hospital_list[i], refdate, ".docx") -->

<!-- }        -->

<!-- ``` -->

<!-- The text within the markdown would then need to refer to `hospital_list[i]` and `refdate`.  -->

<!-- ======================================================= -->

## Canevas (Modèles de document)

En utilisant un canevas de document (exemple type) qui contient le formatage souhaité, vous pouvez ajuster l'esthétique de la sortie Rmd. Vous pouvez par exemple créer un fichier MS Word ou Powerpoint qui contient des pages/diapositives avec les dimensions, les filigranes, les fonds et les polices de caractères souhaités.

### Documents Word {.unnumbered}

Pour créer un canevas, commencez un nouveau document Word (ou utilisez une sortie existante avec un formatage qui vous convient), et modifiez les polices en définissant les Styles. Dans Style,les titres 1, 2 et 3 font référence aux différents niveaux d'en-tête markdown (respectivement `# Header 1`, `## Header 2` et `## Header 3`). Cliquez avec le bouton droit de la souris sur le style et cliquez sur "modifier" pour changer le formatage de la police ainsi que le paragraphe (par exemple, vous pouvez introduire des sauts de page avant certains styles, ce qui peut faciliter l'espacement). D'autres aspects du document Word, tels que les marges, la taille de la page, les en-têtes, etc., peuvent être modifiés comme un document Word habituel dans lequel vous travaillez directement.

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/7_template.png"))
```

### Documents Powerpoint {.unnumbered}

Comme ci-dessus, créez un nouveau jeu de diapositives ou utilisez un fichier Powerpoint existant avec le formatage souhaité. Pour une édition plus poussée, cliquez sur 'View' et 'Slide Master'. À partir de là, vous pouvez modifier l'apparence de la diapositive "de base" en modifiant le formatage du texte dans les zones de texte, ainsi que les dimensions de l'arrière-plan/de la page pour l'ensemble de la page.

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/8_ppttemplate.png"))
```

Malheureusement, l'édition des fichiers Powerpoint est un peu moins souple :


- Un en-tête de premier niveau (`# Header 1`) deviendra automatiquement le titre d'une nouvelle diapositive,
- Un texte `## Header 2` n'apparaîtra pas comme un sous-titre mais comme un texte dans la zone de texte principale de la diapositive (à moins que vous ne trouviez un moyen de manoeuvrer la slide de base).
- Les graphiques et les tableaux générés seront automatiquement placés dans de nouvelles diapositives. Vous devrez les combiner, par exemple avec la fonction **patchwork** pour combiner les ggplots, afin qu'ils apparaissent sur la même page. Consultez cet [article de blog](https://mattherman.info/blog/ppt-patchwork/) sur l'utilisation du paquet **patchwork** pour placer plusieurs images sur une seule diapositive.

Voir le [paquet **officer**](https://davidgohel.github.io/officer/) pour un outil permettant de travailler plus en profondeur avec les présentations Powerpoint.

### Intégration des canevas (modèle de document) dans le YAML {.unnumbered}

Une fois qu'un canevas est préparé, le détail de celui-ci peut être ajouté dans le YAML du fichier  Rmd sous la ligne "output" et sous l'endroit où le type de document est spécifié (qui va sur une ligne séparée elle-même). Notons que `reference_doc` peut être utilisé pour les modèles de diapositives Powerpoint.

Il est plus facile de sauvegarder le canevas dans le même dossier que celui où se trouve le fichier Rmd (comme dans l'exemple ci-dessous), ou dans un sous-dossier.



``` {.yaml}
---
title: Surveillance report
output: 
 word_document:
  reference_docx: "template.docx"
params:
 date: 2021-04-10
 hospital: Central Hospital
template:
 
---
```

### Formatage des fichiers HTML {.unnumbered}


Les fichiers HTML n'utilisent pas de modèles, mais les styles peuvent être configurés dans le YAML. Les HTML sont des documents interactifs, et sont particulièrement flexibles. Nous couvrons ici quelques options de base.

- Table des matières : On peut ajouter une table des matières avec `toc: true` ci-dessous, et aussi spécifier qu'elle reste visible ("flottante") quand on la fait défiler, avec `toc_float: true`.

- Thèmes : Nous pouvons nous référer à certains thèmes pré-faits, qui proviennent d'une bibliothèque de thèmes Bootswatch. Dans l'exemple ci-dessous, nous utilisons cerulean. D'autres options incluent : journal, flatly, darkly, readable, spacelab, united, cosmo, lumen, paper, sandstone, simplex, et yeti.

- Mise en évidence : Cette configuration modifie l'aspect du texte mis en évidence (par exemple, le code dans les morceaux qui sont affichés). Les styles pris en charge sont default, tango, pygments, kate, monochrome, espresso, zenburn, haddock, breezedark et textmate.

Voici un exemple de la manière d'intégrer les options ci-dessus dans le YAML.



``` {.yaml}
---
title: "HTML example"
output:
  html_document:
    toc: true
    toc_float: true
    theme: cerulean
    highlight: kate
    
---
```

Vous trouverez ci-dessous deux exemples de sorties HTML comportant toutes deux des tables des matières flottantes, mais avec des thèmes et des styles de mise en évidence différents :

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/9_html.png"))
```

## Contenu dynamique

Dans une sortie HTML, le contenu de votre rapport peut être dynamique. Voici quelques exemples :


### Tableaux {.unnumbered}

Dans un rapport HTML, vous pouvez imprimer des tableaux de données de telle sorte que le contenu soit dynamique, avec des filtres et des barres de défilement. Il existe plusieurs "packages" qui offrent cette possibilité.


Pour ce faire, avec le "package" **DT**, tel qu'il est utilisé dans ce manuel, vous pouvez insérer un morceau de code comme celui-ci :

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/10_dynamictable.png"))
```

La fonction `datatable()` affichera le tableau de données fourni comme un tableau dynamique pour le lecteur. Vous pouvez définir `rownames = FALSE` pour simplifier le côté gauche de la table. `filter = "top"` fournit un filtre sur chaque colonne. Dans l'argument `option()`, fournissez une liste d'autres spécifications. Nous en incluons deux ci-dessous : `pageLength = 5` fixe le nombre de lignes qui apparaissent à 5 (les lignes restantes peuvent être visualisées en cliquant sur les flèches), et `scrollX=TRUE` active une barre de défilement en bas du tableau (pour les colonnes qui s'étendent trop à droite).


Si votre jeu de données est très grand, pensez à n'afficher que les X premières lignes en enveloppant le nom du jeu de données dans `head()`.

### Les widgets HTML {.unnumbered}

Les [widgets HTML pour R](http://www.htmlwidgets.org/) sont une classe spéciale de "packages" R qui permettent une interactivité accrue en utilisant des bibliothèques JavaScript. Vous pouvez les intégrer dans des sorties HTML R Markdown.

Voici quelques exemples courants de ces widgets :

- Plotly (utilisé dans cette page du manuel et dans la page [Graphiques interactifs](#interactive_plots)).
- visNetwork (utilisé dans la page [Chaînes de transmission](#transmission_chains) de ce manuel)
- Leaflet (utilisé dans la page [Bases des GIS](#gis) de ce manuel)
- dygraphs (utile pour afficher de manière interactive des données de séries chronologiques)\
- DT (`datatable()`) (utilisé pour afficher des tableaux dynamiques avec filtre, tri, etc.)

La fonction `ggplotly()` de **plotly** est particulièrement facile à utiliser. Voir la page [Graphiques interactifs](#interactive_plots).

## Ressources

De plus amples informations sont disponibles sur le site:

* <https://bookdown.org/yihui/rmarkdown/>
- <https://rmarkdown.rstudio.com/articles_intro.html>

Une bonne explication de markdown vs knitr vs Rmarkdown se trouve ici: <https://stackoverflow.com/questions/40563479/relationship-between-r-markdown-knitr-pandoc-and-bookdown>


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/rmarkdown.Rmd-->


# Organisation des rapports de routine {#reportfactory}

Cette page couvre le "package" **reportfactory**, qui est un *complément à la page sur [Production de rapports avec R Markdown](#rmarkdown) *. 

Ce package est adapté aux cas où on exécute des rapports régulièrement (par exemple, quotidiennement, hebdomadairement...). Il facilite la compilation de plusieurs fichiers R Markdown et l'organisation de leurs fichiers de sortie. Essentiellement, il fournit une "fabrique de rapports" ("factory") qui est une machine d'exécution à partir de laquelle vous pouvez exécuter les rapports R Markdown, obtenir des dossiers automatiquement horodatés pour les fichiers de sortie, et avoir un contrôle de version "léger".

**reportfactory** est l'un des "packages"  développés par RECON (R Epidemics Consortium). Voici leur [site Web](https://www.repidemicsconsortium.org/) et [Github](https://github.com/reconverse).  


## Préparation

### Charger les packages {.unnumbered}  

Depuis RStudio, installez la dernière version du package **reportfactory** depuis Github.  

Vous pouvez le faire via le "package" **pacman** avec `p_load_current_gh()` qui forcera l'installation de la dernière version depuis Github. Tapez la chaîne de caractères "reconverse/reportfactory", qui spécifie l'organisation Github (reconverse) et le répertoire (reportfactory). Vous pouvez également utiliser `install_github()` du "package" **remotes**, comme alternative.


```{r, eval=FALSE}
# Installer et charger la dernière version du package depuis Github
pacman::p_load_current_gh("reconverse/reportfactory")
#remotes::install_github("reconverse/reportfactory") # alternative
```

## Nouvelle "factory"  

Pour créer une nouvelle "factory", exécutez la fonction `new_factory()`. Ceci créera un nouveau dossier de projet R autonome. Par défaut :  

* "factory" sera ajoutée à votre répertoire de travail.
* Le nom du projet R de la "factory" sera appelé "new_factory.Rproj".  
* Votre session RStudio s'installera dans ce projet R.  

```{r, eval=F}
# Ceci créera la "factory" dans le répertoire de travail
new_factory()
```

En regardant à l'intérieur de la "factory", vous pouvez voir que des sous-dossiers et certains fichiers ont été créés automatiquement.  

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_new2.png"))
```

* Le dossier *report_sources* contient vos scripts R Markdown, qui génèrent vos rapports.  
* Le dossier *outputs* contient les fichiers de sortie du rapport (par exemple, HTML, Word, PDF, etc.).  
* Le dossier *scripts* peut être utilisé pour stocker d'autres scripts R (par exemple, ceux qui proviennent de vos scripts Rmd).  
* Le dossier *data* peut être utilisé pour contenir vos données à partir desquelles vous travvaillez (les sous-dossiers "raw" et "clean" sont inclus).  
* Un fichier *.here*, afin que vous puissiez utiliser le package **here** pour faire appel aux fichiers dans les sous-dossiers selon leur relation avec le dossier à la racine (voir la page [Projets R](#r_projects) pour plus de détails).  
* Un fichier *gitignore* a été créé au cas où vous lieriez ce projet R à un répertoire Github (voir [Contrôle de version et collaboration avec Git et Github](#collaboration)).  
* Un fichier README vide, si vous utilisez un dépôt Github.  


<span style="color: orange;">**_CAUTION:_** selon les paramètres de votre ordinateur, des fichiers tels que ".here" peuvent exister mais restés cachés.</span>  

Parmi les paramètres par défaut, en voici quelques-uns que vous pourriez vouloir ajuster dans la commande `new_factory()` :  

* `factory = ` - Fournit un nom pour le dossier de la "factory" (par défaut "new_factory")  
* `path = ` - Désigne un chemin de fichier pour la nouvelle "factory" (par défaut le répertoire de travail)  
* `report_sources = ` - Donne un autre nom au sous-dossier qui contient les scripts R Markdown (par défaut, "report_sources")  
* `outputs = ` Fournit un nom alternatif pour le dossier qui contient les fichiers de sortie du rapport (par défaut "outputs").  

Voir `?new_factory` pour une liste complète des arguments.  


Lorsque vous créez la nouvelle "factory", votre session R est transférée vers le nouveau projet R, vous devez donc charger à nouveau le "package" **reportfactory**.


```{r, eval=FALSE}
pacman::p_load(reportfactory)
```

Maintenant vous pouvez lancer la commande `factory_overview()` pour voir la structure interne (tous les dossiers et fichiers) de la "factory".

```{r, eval=F}
factory_overview()            #  afficher l'aperçu de la factory dans la console
```

L'"arbre" suivant des dossiers et fichiers de la "factory" est affiché dans la console R. Notez que dans le dossier "data", il y a des sous-dossiers pour les données "raw" et "clean", et des exemples de données CSV. Il y a aussi "example_report.Rmd" dans le dossier "report_sources".      

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_overview.png"))
```


## Créer un rapport  

À partir du projet R de la fabrique, créez un rapport R Markdown comme vous le feriez normalement, et enregistrez-le dans le dossier "report_sources". Consultez la page [Production de rapports avec R Markdown](#rmarkdown) pour obtenir des instructions. À titre d'exemple, nous avons ajouté les éléments suivants à la fabrique :  

* Un nouveau script R markdown intitulé "daily_sitrep.Rmd", enregistré dans le dossier "report_sources".  
* Les données du rapport ("linelist_cleaned.rds"), enregistrées dans le sous-dossier "clean" du dossier "data".  

Nous pouvons voir en utilisant `factory_overview()` notre R Markdown dans le dossier "report_sources" et le fichier de données dans le dossier "clean" data (en surbrillance) :  

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_overview2.png"))
```

Voici une capture d'écran du début du fihier R Markdown "daily_sitrep.Rmd". Vous pouvez voir que le format de sortie est défini comme étant HTML, via l'en-tête YAML `output: html_document`.  

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_new_rmd.png"))
```

Dans ce script simple, il y a des commandes pour :  

* Charger les "packages" nécessaires  
* Importer les données de la liste linéaire en utilisant un chemin de fichier du "package" **here** (pour en savoir plus, consultez la page [Importer et exporter des données](#import_export)).

```{r, eval=F}
linelist <- import(here("data", "clean", "linelist_cleaned.rds"))
```

* Imprimer un tableau récapitulatif des cas, et exportez-le avec `export()` comme un fichier .csv.  
* Imprimer une courbe épidemiologique, et l'exporter avec `ggsave()` comme un fichier .png.  


Vous pouvez examiner la liste des rapports R Markdown dans le dossier "report_sources" avec cette commande :  

```{r, eval=F}
list_reports()
```


## Compiler  

Dans une  "factory", "compiler" un rapport R Markdown signifie que le script .Rmd sera exécuté et que la sortie sera produite (comme spécifié dans le script YAML, par exemple en HTML, Word, PDF, etc).  

*La fabrique créera automatiquement un dossier daté et horodaté pour les sorties dans le dossier "outputs"*.  

Le rapport lui-même et tous les fichiers exportés produits par le script (par exemple, csv, png, xlsx) seront enregistrés dans ce dossier. En outre, le script Rmd lui-même sera enregistré dans ce dossier, de sorte que vous ayez une trace de cette version du script.  

Cela contraste avec le comportement normal d'un R Markdown exécuté en indépendant, qui enregistre les sorties à l'emplacement du script Rmd. Ce comportement par défaut peut donner lieu à des dossiers encombrés et désordonnés. La "factory" vise à améliorer l'organisation lorsque l'on doit exécuter des rapports fréquemment.  

### Compiler par nom {.unnumbered}  

Vous pouvez compiler un rapport spécifique en exécutant `compile_reports()` et en fournissant le nom du script Rmd (sans extension .Rmd) à `reports = `. Pour simplifier, vous pouvez sauter le `reports = ` et juste écrire le nom R Markdown entre guillemets, comme ci-dessous. 

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_compile1.png"))
```


Cette commande compile uniquement le rapport "daily_sitrep.Rmd", en sauvegardant le rapport HTML, le tableau .csv et les exportations des courbes épi .png dans un sous-dossier daté et horodaté spécifique au rapport, dans le dossier "outputs".  

Notez que si vous choisissez de fournir l'extension .Rmd, vous devez saisir correctement l'extension telle qu'elle est enregistrée dans le nom du fichier (.rmd vs. .Rmd).  

Notez également que lorsque vous compilez, vous pouvez voir plusieurs fichiers apparaître temporairement dans le dossier "report_sources" - mais ils disparaîtront rapidement lorsqu'ils seront transférés dans le bon dossier "outputs". 

### Compiler par numéro {.unnumbered}

Vous pouvez également spécifier le script Rmd à compiler en fournissant un nombre ou un vecteur de nombres à `reports = `. Les nombres doivent correspondre à l'ordre dans lequel les rapports apparaissent lorsque vous exécutez `list_reports()`.   

```{r, eval=F}
# Compilez les deuxième et quatrième Rmds dans le dossier "report_sources".
compile_reports(reports = c(2, 4))
```



### Compiler tous les rapports {.unnumbered}

Vous pouvez compiler *tous* les rapports R Markdown dans le dossier "report_sources" en mettant l'argument `reports = ` à TRUE.  

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_compile_all.png"))
```


### Compilation à partir du sous-dossier {.unnumbered}  

Vous pouvez ajouter des sous-dossiers au dossier "report_sources". Pour exécuter un rapport R Markdown à partir d'un sous-dossier, il suffit de fournir le nom du dossier à `subfolder = `. Voici un exemple de code pour compiler un rapport Rmd qui se trouve dans un sous-dossier de "report_sources".  

```{r, eval=F}
compile_reports(
     reports = "summary_for_partners.Rmd",
     subfolder = "for_partners")
```

Vous pouvez compiler tous les rapports Rmd dans un sous-dossier en fournissant le nom du sous-dossier à `reports = `, avec un slash à la fin, comme ci-dessous.    

```{r, eval=F}
compile_reports(reports = "for_partners/")
```


### Paramétrisation {.unnumbered}

Comme indiqué dans la page sur [Production de rapports avec R Markdown](#rmarkdown), vous pouvez exécuter des rapports avec des paramètres spécifiques. Vous pouvez passer ces paramètres comme une liste à `compile_reports()` via l'argument `params = `. Par exemple, dans ce rapport fictif, trois paramètres sont fournis aux rapports R Markdown.  

```{r, eval=F}
compile_reports(
  reports = "daily_sitrep.Rmd",
  params = list(most_recent_data = TRUE,
                region = "NORTHERN",
                rates_denominator = 10000),
  subfolder = "regional"
)
```


### En utilisant un "run-file" {.unnumbered}  

Si vous avez plusieurs rapports à exécuter, pensez à créer un script R qui contient toutes les commandes `compile_reports()`. Un utilisateur peut simplement exécuter toutes les commandes de ce script R et tous les rapports seront compilés. Vous pouvez enregistrer ce "fichier d'exécution" dans le dossier "scripts".


## Fichiers de sortie  

Après avoir compilé les rapports plusieurs fois, le dossier "outputs" pourrait ressembler à ceci (certains éléments surlignés pour plus de clarté) :  


```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_overview_all.png"))
```


* Dans "outputs", des sous-dossiers ont été créés pour chaque rapport Rmd.  
* Dans ces dossiers, d'autres sous-dossiers ont été créés pour chaque compilation unique.  
  * Ces dossiers sont datés et horodatés ("2021-04-23_T11-07-36" signifie 23 avril 2021 à 11:07:36).  
  * Vous pouvez modifier le format de l'horodatage. Voir `?compile_reports`.
* Dans chaque dossier de compilation par date/heure, la sortie du rapport est stockée (par exemple HTML, PDF, Word) avec le script Rmd (contrôle de version !) et tout autre fichier exporté (par exemple table.csv, epidemic_curve.png).  

Voici une vue de l'intérieur d'un des dossiers horodatés, pour le rapport "daily_sitrep". Le chemin du fichier est surligné en jaune pour plus de clarté.  

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_compile_folder.png"))
```


Enfin, vous trouverez ci-dessous une capture d'écran de la sortie du rapport HTML. 

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_html.png"))
```

Vous pouvez utiliser `list_outputs()` pour consulter une liste des fichiers de sortie.  




## Divers  

### Exécution {.unnumbered} 

Vous pouvez toujours "exécuter" un de vos rapports R Markdown en cliquant sur le bouton "Knit", si vous le souhaitez. Si vous faites cela, comme par défaut, les sorties apparaîtront dans le dossier où le Rmd est enregistré - le dossier "report_sources". Dans les versions précédentes de **reportfactory**, avoir des fichiers non-Rmd dans "report_sources" empêchait la compilation, mais ce n'est plus le cas. Vous pouvez exécuter `compile_reports()` et aucune erreur ne se produira.  

### Scripts {.unnumbered}  

Nous vous encourageons à utiliser le dossier "scripts" pour stocker les "fichiers d'exécution" ou les scripts .R qui proviennent de vos scripts .Rmd. Consultez la page [Production de rapports avec R Markdown](#rmarkdown) pour obtenir des conseils sur la manière de structurer votre code dans plusieurs fichiers.  

### Extras {.unnumbered} 

* Avec **reportfactory**, vous pouvez utiliser la fonction `list_deps()` pour lister tous les "packages" requis pour tous les rapports dans l'ensemble de la fabrique.  

* Il y a un "package" de support utilisé en de développement appelé **rfextras** qui offre plus de fonctions d'aide pour vous assister dans la construction des rapports, comme :  
  * `load_scripts()` - source/charge tous les scripts .R dans un dossier donné (le dossier "scripts" par défaut).  
  * `find_latest()` - trouve la dernière version d'un fichier (par exemple, le dernier jeu de données).




<!-- ======================================================= -->
## Ressources {  }

Voir la [page Github](https://github.com/reconverse/reportfactory) du "package" **reportfactory** 

Voir la [page Github](https://github.com/reconhub/rfextras) du package **rfextras**  

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/reportfactory.Rmd-->


# Tableaux de bord avec R Markdown {#dashboards}

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_output.png"))
```

Cette page couvre l'utilisation base du paquet **flexdashboard**. Ce paquet vous permet de formater facilement la sortie R Markdown comme un tableau de bord avec des panneaux et des pages. Le contenu du tableau de bord peut être du texte, des figures/tableaux statiques ou des graphiques interactifs.  

Avantages de **flexdashboard** :  

* Il nécessite un codage R non standard minimal. Avec très peu de pratique, vous pouvez rapidement créer un tableau de bord.  
* Le tableau de bord peut généralement être envoyé par e-mail à des collègues sous forme de fichier HTML autonome, aucun serveur n'est nécessaire.  
* Vous pouvez combiner **flexdashboard** avec **shiny**, **ggplotly** et d'autres *"widgets html"* pour ajouter de l'interactivité.  

Inconvénients de **flexdashboard** :  

* Moins de personnalisation par rapport à l'utilisation de **shiny** seul pour créer un tableau de bord.  


Des tutoriels assez complets sur l'utilisation de **flexdashboard** qui ont informé cette page se trouvent dans la section Ressources (fin de la page). Nous décrivons ci-dessous les fonctionnalités de base et donnons un exemple de construction d'un tableau de bord pour explorer une épidémie, en utilisant les données du cas `linelist`.  


## Préparation

### Charger les paquets {.unnumbered}  

Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* le charge pour l'utiliser. Vous pouvez aussi charger les paquets installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les paquets R.  

```{r}
pacman::p_load(
  rio, # import/export de données     
  here, # localisation des fichiers
  tidyverse, # gestion et visualisation des données
  flexdashboard, # versions tableaux de bord des rapports R Markdown
  shiny, # figures interactives
  plotly # figures interactives
)
```

### Importer des données {.unnumbered}  

Nous importons le jeu de données des cas d'une épidémie d'Ebola simulée. Si vous voulez suivre, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>cliquez pour télécharger la liste de lignes "propre"</a> (en tant que fichier .rds). Importez des données avec la fonction `import()` du paquet **rio** (elle gère de nombreux types de fichiers comme .xlsx, .csv, .rds - voir la page [Importation et exportation](#import_export) pour plus de détails). 

```{r, echo=F}
# Importez la liste de diffusion dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# Importez la liste de cas
linelist <- import("linelist_cleaned.rds")
```

Les 50 premières lignes de la linelist sont affichées ci-dessous.

```{r, message=FALSE, echo=F}
# affiche les données de la liste de diffusion sous forme de tableau
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


## Créer un nouveau R Markdown  

Après avoir installé le package, créez un nouveau fichier R Markdown en cliquant sur *Fichier > Nouveau fichier > R Markdown*. 

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_new1.png"))
```


Dans la fenêtre qui s'ouvre, sélectionnez "From Template" et choisissez le modèle "Flex Dashboard". Vous serez ensuite invité à nommer le document. Dans l'exemple de cette page, nous allons nommer notre R Markdown "outbreak_dashboard.Rmd".  
  

```{r out.width = "100%", out.height="75%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_new2.png"))
```




## Le script  

Le script est un script R Markdown, et a donc les mêmes composants et la même organisation que les scripts décrits dans la page sur les [Rapports avec R Markdown](#rmarkdown). Nous allons brièvement les revoir et souligner les différences avec les autres formats de sortie R Markdown.  

### YAML {.unnumbered}

En haut du script se trouve l'en-tête "YAML". Il doit commencer par trois tirets `---` et doit se terminer par trois tirets `---`. Les paramètres YAML sont présentés par paires `key: value`. **L'indentation et le placement des deux points dans YAML sont importants**: les paires `key: value` sont séparées par des deux points (pas par des signes égaux !). 

Le fichier YAML doit commencer par les métadonnées du document. L'ordre de ces paramètres YAML primaires (non indentés) n'a pas d'importance. Par exemple :  

```{r, eval=F}
tite: "Mon document"
author: "Moi"
date: "`r Sys.Date()`"
```

Vous pouvez utiliser du code R dans des valeurs YAML en le mettant comme du code en ligne (précédé de `r` entre guillemets) mais aussi entre guillemets (voir ci-dessus pour Date).  

Un paramètre YAML obligatoire est `output: `, qui spécifie le type de fichier à produire (par exemple, `html_document`, `pdf_document`, `word_document`, ou `powerpoint_presentation`). Pour **flexdashboard**, la valeur de ce paramètre est un peu confuse - elle doit être définie comme `output:flexdashboard::flex_dashboard`. Notez les deux-points simples et doubles, et le trait de soulignement. Ce paramètre de sortie YAML est souvent suivi par *un deux-points supplémentaire* et des sous-paramètres indentés (voir les paramètres `orientation:` et `vertical_layout:` ci-dessous).  

```{r, eval=F}
title: "Mon tableau de bord"
author: "Moi"
date: "`r Sys.Date()`"
output:
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: scroll
```

Comme indiqué ci-dessus, des indentations (2 espaces) sont utilisées pour les sous-paramètres. Dans ce cas, n'oubliez pas de mettre un deux-points supplémentaire après le primaire, comme `key:value:`.  

Le cas échéant, les valeurs logiques doivent être données dans YAML en minuscules (`true`, `false`, `null`). Si un deux-points fait partie de votre valeur (par exemple, dans le titre), mettez la valeur entre guillemets. Voir les exemples dans les sections ci-dessous.  



### Morceaux de code {.unnumbered}   

Un script R Markdown peut contenir plusieurs "chunks" de code - il s'agit de zones du script dans lesquelles vous pouvez écrire du code R sur plusieurs lignes et qui fonctionnent comme des mini-scripts R.  

Les morceaux de code sont créés à l'aide de trois crochets arrière et de parenthèses avec un "r" minuscule à l'intérieur. Le chunk est fermé par trois crochets arrière. Vous pouvez créer un nouveau chunk en le tapant vous-même, en utilisant le raccourci clavier "Ctrl + Alt + i" (ou Cmd + Shift + r sur Mac), ou en cliquant sur l'icône verte "insérer un nouveau chunk de code" en haut de votre éditeur de script. De nombreux exemples sont donnés ci-dessous.  


### Texte narratif {.unnumbered}  

En dehors d'un "chunk" de code R, vous pouvez écrire un texte narratif. Comme décrit dans la page [Rapports avec R Markdown](#rmarkdown), vous pouvez mettre du texte en italique en l'entourant d'un astérisque (*texte italique*), ou en gras en l'entourant de deux astérisques (**texte gras**). Rappelez-vous que les puces et les schémas de numérotation sont sensibles aux nouvelles lignes, à l'indentation et au fait de terminer une ligne par deux espaces.  

Vous pouvez également insérer du code R en ligne dans du texte, comme décrit à la page [Rapports avec R Markdown](#rmarkdown), en entourant le code de barres obliques inversées et en commençant la commande par "r" : `r 1+1` (voir l'exemple avec la date ci-dessus).  



### Titres {.unnumbered}  

Différents niveaux de titres sont établis avec différents nombres de symboles de hachage, comme décrit dans la page [Rapports avec R Markdown](#rmarkdown).  

Dans **flexdashboard**, un titre primaire (#) crée une "page" du tableau de bord. Les titres de deuxième niveau (##) créent une colonne ou une ligne en fonction de votre paramètre `orientation:` (voir les détails ci-dessous). Les titres de troisième niveau (###) créent des panneaux pour les graphiques, les tableaux, le texte, etc.   

```md
# Titre de premier niveau (page)

## En-tête de deuxième niveau (ligne ou colonne)  

### En-tête de troisième niveau (panneau pour le graphique, le tableau, etc.)
```





## Attributs de section  

Comme dans un markdown R normal, vous pouvez spécifier des attributs à appliquer aux parties de votre tableau de bord en incluant des options `key=value` après un titre, entre des accolades `{ }`. Par exemple, dans un rapport HTML R Markdown typique, vous pouvez organiser les sous-titres en onglets avec `## Ma rubrique {.tabset}`.  

Notez que ces attributs sont écrits après un *titre* dans une partie texte du script. Ils sont différents des options **knitr** insérées en haut des morceaux de code R, telles que `out.height = `.  

Les attributs de section spécifiques à **flexdashboard** comprennent :  

* `{data-orientation=}` Défini à `rows` ou `columns`. Si votre tableau de bord comporte plusieurs pages, ajoutez cet attribut à chaque page pour indiquer l'orientation (expliqué plus en détail dans [la section de mise en page](#layout)).  
* `{data-width=}` et `{data-height=}` définissent la taille relative des graphiques, colonnes, lignes disposés dans la même dimension (horizontale ou verticale). Les tailles absolues sont ajustées pour remplir au mieux l'espace sur n'importe quel dispositif d'affichage grâce au moteur [flexbox](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Using_CSS_flexible_boxes).  
     * La hauteur des graphiques dépend également de la définition du paramètre YAML `vertical_layout: fill` ou `vertical_layout: scroll`. S'il est défini sur scroll, la hauteur des figures reflétera l'option traditionnelle `fig.height = ` dans le chunk de code R.  
     * Voir la documentation complète sur la taille sur le [flexdashboard website](https://rmarkdown.rstudio.com/flexdashboard/using.html#sizing)  
* `{.hidden}` Utilisez cette option pour exclure une page spécifique de la barre de navigation.  
* `{data-navbar=}` Utilisez ceci dans un titre de niveau page pour l'imbriquer dans un menu déroulant de la barre de navigation. Indiquez le nom (entre guillemets) du menu déroulant. Voir l'exemple ci-dessous.  


## Mise en page {#layout}  

Ajustez la mise en page de votre tableau de bord de la manière suivante :  

* Ajoutez des pages, des colonnes/lignes et des graphiques avec des titres R Markdown (par exemple, #, ## ou ###).  
* Ajustez l'`orientation:` de paramètre YAML  à `rangees` ou `colonnes`.  
* Spécifiez si la mise en page remplit le navigateur ou permet le défilement.  
* Ajouter des onglets à un titre de section particulier  


### Pages {.unnumbered}  

Les titres de premier niveau (#) dans le R Markdown représentent les "pages" du tableau de bord. Par défaut, les pages apparaissent dans une barre de navigation en haut du tableau de bord.  

```{r, out.height = c('100%'), out.width = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_pages_top_script.png"))
```


```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_pages_top_view.png"))
```



Vous pouvez regrouper des pages en un "menu" dans la barre de navigation supérieure en ajoutant l'attribut `{data-navmenu=}` au titre de la page. Attention, n'incluez pas d'espaces autour du signe égal, sinon cela ne fonctionnera pas !  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_navmenu_script.png"))
```


Voici ce que produit le script :  


```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_navmenu_view.png"))
```

Vous pouvez également convertir une page ou une colonne en une "barre latérale" sur le côté gauche du tableau de bord en ajoutant l'attribut `{.sidebar}`. Elle peut contenir du texte (visible de n'importe quelle page) ou, si vous avez intégré l'interactivité **shiny**, elle peut être utile pour contenir des commandes d'entrée utilisateur telles que des curseurs ou des menus déroulants.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_sidebar_script.png"))
```

Voici ce que produit le script :  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_sidebar_view.png"))
```




### Orientation {.unnumbered}  

Définissez le paramètre YAML `orientation:` pour indiquer comment vos titres Markdown de second niveau (##) doivent être interprétés - comme `orientation: colonnes` ou `orientation: lignes`. 

Les titres de second niveau (##) seront interprétés comme de nouvelles colonnes ou lignes en fonction de ce paramètre `orientation`.  

Si vous définissez `orientation: colonnes`, les titres de second niveau créeront de nouvelles colonnes dans le tableau de bord. Le tableau de bord ci-dessous comporte une page, contenant deux colonnes, avec un total de trois panneaux. Vous pouvez ajuster la largeur relative des colonnes avec `{data-width=}` comme indiqué ci-dessous.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_columns_script.png"))
```

Voici ce que produit le script :  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_columns_view.png"))
```

Si vous définissez `orientation: lignes`, les en-têtes de second niveau créeront de nouvelles lignes au lieu de colonnes. Voici le même script que ci-dessus, mais avec `orientation: lignes` pour que les en-têtes de second niveau produisent des lignes au lieu de colonnes. Vous pouvez ajuster la *hauteur* relative des lignes avec `{data-height=}` comme indiqué ci-dessous.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_rows_script.png"))
```

Voici ce que produit le script :  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_rows_view.png"))
```

Si votre tableau de bord comporte plusieurs pages, vous pouvez désigner l'orientation pour chaque page spécifique en ajoutant l'attribut `{data-orientation=}` à l'en-tête de chaque page (spécifiez soit `lignes` soit `colonnes` sans les guillemets).  

### Onglets {.unnumbered} 

Vous pouvez diviser le contenu en onglets avec l'attribut `{.tabset}`, comme dans les autres sorties HTML R Markdown.  

Il suffit d'ajouter cet attribut après le titre souhaité. Les sous-titres sous ce titre seront affichés sous forme d'onglets. Par exemple, dans l'exemple de script ci-dessous, la colonne 2 à droite (##) est modifiée de manière à ce que les volets de la courbe épidémique et du tableau (###) soient affichés sous forme d'onglets.  

Vous pouvez faire de même avec les lignes si votre orientation est celle des lignes.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_tabs_script.png"))
```

Voici ce que produit le script :  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_tabs_view.png"))
```


## Ajout de contenu  

Commençons à construire un tableau de bord. Notre tableau de bord simple aura 1 page, 2 colonnes, et 4 panneaux. Nous allons construire les panneaux pièce par pièce pour la démonstration.  

Vous pouvez facilement inclure des sorties R standard telles que du texte, des ggplots et des tableaux (voir la page [Tableaux pour la présentation](#tables_presentation)). Il suffit de les coder dans un chunk de code R comme vous le feriez pour tout autre script R Markdown.  

Remarque : vous pouvez télécharger le script Rmd terminé et la sortie du tableau de bord HTML - voir la page [Télécharger le manuel et les données](#download_book_data).  


### Texte {.unnumbered}  

Vous pouvez saisir du texte Markdown et inclure du code *en ligne* comme pour toute autre sortie R Markdown. Voir la page [Rapports avec R Markdown](#rmarkdown) pour plus de détails. 

Dans ce tableau de bord, nous incluons un panneau de texte récapitulatif qui comprend un texte dynamique indiquant la dernière date d'hospitalisation et le nombre de cas signalés dans l'épidémie. 

### Tableaux {.unnumbered}  

Vous pouvez inclure des morceaux de code R qui impriment des sorties telles que des tableaux. Mais la sortie sera plus belle et s'adaptera mieux à la taille de la fenêtre si vous utilisez la fonction `kable()` de **knitr** pour afficher vos tableaux. Les fonctions **flextable** peuvent produire des tableaux qui sont raccourcis / coupés.  

Par exemple, ci-dessous, nous faisons passer la fonction `linelist()` par une commande `count()` pour produire un tableau récapitulatif des cas par hôpital. Finalement, le tableau est envoyé à `knitr::kable()` et le résultat a une barre de défilement sur la droite. Vous pouvez en savoir plus sur la personnalisation de votre tableau avec `kable()` et **kableExtra** [ici](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html).  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_tables_script.png"))
```

Voici ce que produit le script :  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_tables_view.png"))
```


Si vous voulez afficher un tableau dynamique qui permet à l'utilisateur de filtrer, trier et/ou cliquer sur les "pages" du cadre de données, utilisez le package **DT** et sa fonction `datatable()`, comme dans le code ci-dessous.  

Dans l'exemple de code ci-dessous, le cadre de données `linelist` est imprimé. Vous pouvez définir `rownames = FALSE` pour conserver l'espace horizontal, et `filter = "top"` pour avoir les filtres en haut de chaque colonne. Une liste d'autres spécifications peut être fournie à `options = `. Ci-dessous, nous avons défini `pageLength = ` pour que 5 lignes apparaissent et `scrollX = ` pour que l'utilisateur puisse utiliser une barre de défilement en bas pour faire défiler horizontalement. L'argument `class = 'white-space: nowrap'` garantit que chaque ligne ne comporte qu'une seule ligne (et non plusieurs). Vous trouverez d'autres arguments et valeurs possibles [ici](https://rstudio.github.io/DT/?_ga=2.2810736.1321860763.1619286819-369061888.1601594705) ou en entrant `?datatable`.

```{r, eval=F}
DT::datatable(linelist, 
              rownames = FALSE, 
              options = liste(pageLength = 5, scrollX = TRUE), 
              class = 'white-space: nowrap' )
```

### Tracés {.unnumbered}  

Vous pouvez imprimer les graphiques dans un tableau de bord comme vous le feriez dans un script R. Dans notre exemple, nous utilisons le paquet **incidence2** pour créer une "courbe épidémique" par groupe d'âge avec deux commandes simples (voir la page [Courbes épidémiques](#epicurves)). Cependant, vous pourriez utiliser `ggplot()` et imprimer un graphique de la même manière.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_plots_script.png"))
```

Voici ce que produit le script :  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_plots_view.png"))
```


### Graphiques interactifs {.unnumbered}  

Vous pouvez également passer un ggplot standard ou un autre objet de tracé à `ggplotly()` du paquet **plotly** (voir la page [Graphiques interactifs](#interactive_plots)). Cela rendra votre graphique interactif, permettra au lecteur de "zoomer" et affichera en surimpression la valeur de chaque point de données (dans ce scénario, le nombre de cas par semaine et le groupe d'âge dans la courbe).  

```{r, eval=F}
age_outbreak <- incidence(linelist, date_onset, "week", groups = age_cat)
plot(age_outbreak, fill = age_cat, col_pal = muted, title = "") %>% 
  plotly::ggplotly()
```

Voici à quoi cela ressemble dans le tableau de bord (gif). Cette fonctionnalité interactive fonctionnera même si vous envoyez le tableau de bord par courriel sous forme de fichier statique (pas en ligne sur un serveur).  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_ggplotly.gif"))
```

### Widgets HTML {.unnumbered}

Les [widgets HTML pour R](http://www.htmlwidgets.org/) sont une classe spéciale de paquets R qui augmentent l'interactivité en utilisant des bibliothèques JavaScript. Vous pouvez les intégrer dans les sorties Markdown de R (comme un flexdashboard) et dans les tableaux de bord Shiny.  

Voici quelques exemples courants de ces widgets :  

* Plotly (utilisé dans cette page du manuel et dans la page [Graphiques interactifs](#interactive_plots)).
* visNetwork (utilisé dans la page [Chaînes de transmission](#transmission_chains) de ce manuel)  
* Leaflet (utilisé dans la page [bases de GIS](#gis) de ce manuel)  
* digraphs (utile pour montrer de manière interactive des séries de données temporelles)  
* DT (`datatable()`) (utilisé pour afficher des tableaux dynamiques avec des filtres, des tris, etc.)  

Ci-dessous, nous démontrons l'ajout d'une chaîne de transmission d'épidémie qui utilise visNetwork au tableau de bord. Le script ne montre que le nouveau code ajouté à la section "Column 2" du script R Markdown. Vous pouvez trouver le code dans la page [Chaînes de transmission](#transmission_chains) de ce manuel.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_chain_script.png"))
```

Voici ce que produit le script :  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_chain.gif"))
```



## Organisation du code

Vous pouvez choisir d'avoir tout le code dans le script R Markdown **flexdashboard**. Alternativement, pour avoir un script de tableau de bord plus propre et concis, vous pouvez choisir de faire appel à du code/figures qui sont hébergés ou créés dans des scripts R externes. Ceci est décrit plus en détail dans la page [Rapports avec R Markdown](#rmarkdown). 


## Shiny  

L'intégration du paquet R **shiny** peut rendre vos tableaux de bord encore plus réactifs aux entrées de l'utilisateur. Par exemple, vous pouvez demander à l'utilisateur de sélectionner une juridiction ou une plage de dates, et faire réagir les panneaux à son choix (par exemple, filtrer les données affichées). Pour intégrer la réactivité **shiny** dans **flexdashboard**, il vous suffit d'apporter quelques modifications à votre script Markdown R **flexdashboard**.  

Vous pouvez également utiliser **shiny** pour produire des applications ou des tableaux de bord *sans* flexdashboard. La page du manuel sur [les tableaux de bord avec Shiny](#shiny) donne un aperçu de cette approche, y compris des conseils sur la syntaxe **shiny**, la structure des fichiers d'application et les options de partage et de publication (y compris les options de serveur libre). Ces conseils syntaxiques et généraux s'appliquent également au contexte **flexdashboard**.  

L'intégration de **shiny** dans **flexdashboard** constitue cependant un changement fondamental pour votre flexdashboard. Il ne produira plus une sortie HTML que vous pouvez envoyer par courriel et que tout le monde peut ouvrir et visualiser. Il s'agira plutôt d'une "application". Le bouton "Knit" en haut du script sera remplacé par une icône "Run document", qui ouvrira une instance du tableau de bord interactif localement sur votre ordinateur.  

Le partage de votre tableau de bord nécessitera maintenant que vous.. :  

* Envoyer le script Rmd au spectateur, qu'il l'ouvre dans R sur son ordinateur et qu'il exécute l'application, ou bien...  
* L'application/le tableau de bord est hébergé sur un serveur accessible à l'observateur.  

L'intégration de **shiny** présente donc des avantages, mais aussi des complications. Si le partage facile par email est une priorité et que vous n'avez pas besoin des capacités réactives de **shiny**, considérez l'interactivité réduite offerte par `ggplotly()` comme démontré ci-dessus.    

Nous donnons ci-dessous un exemple très simple utilisant le même "outbreak_dashboard.Rmd" que ci-dessus. Une documentation complète sur l'intégration de Shiny dans **flexdashboard** est disponible en ligne [ici](https://rmarkdown.rstudio.com/flexdashboard/shiny.html).  



### Paramètres {.unnumbered}  

Activez **shiny** dans un **flexdashboard** en ajoutant le paramètre YAML `runtime: shiny` au même niveau d'indentation que `output: `, comme ci-dessous :  

```md
---
title: "Tableau de bord d'épidémie (Démo Shiny)".
output: 
  flexdashboard::flex_dashboard :
    orientation: columns
    vertical_layout : fill
runtime: shiny
---

```

Il est également pratique d'activer une "barre latérale" pour contenir les widgets de saisie shiny qui collecteront les informations de l'utilisateur. Comme expliqué ci-dessus, créez une colonne et indiquez l'option `{.sidebar}` pour créer une barre latérale sur le côté gauche. Vous pouvez ajouter du texte et des morceaux de R contenant les commandes `input` **shiny** dans cette colonne.  

Si votre application/ tableau de bord est hébergé sur un serveur et peut avoir plusieurs utilisateurs simultanés, nommez le premier morceau de code R comme `global`. Incluez les commandes pour importer/charger vos données dans ce chunk. Ce chunk au nom spécial est traité différemment, et les données qui y sont importées ne le sont qu'une fois (et non en continu) et sont disponibles pour tous les utilisateurs. Cela améliore la vitesse de démarrage de l'application.  

### Exemple travaillé {.unnumbered}  

Ici, nous adaptons le script flexdashboard "outbreak_dashboard.Rmd" pour inclure **shiny**. Nous allons ajouter la possibilité pour l'utilisateur de sélectionner un hôpital dans un menu déroulant, et de faire en sorte que la courbe épidémique ne reflète que les cas de cet hôpital, avec un titre de graphique dynamique. Nous faisons ce qui suit :  

* Ajouter `runtime: shiny` à la YAML.  
* Re-nommer le chunk de configuration comme `global`.  
* Créer une barre latérale contenant :  
  * Du code pour créer un vecteur de noms d'hôpitaux uniques  
  * Une commande `selectInput()` (menu déroulant **shiny**) avec le choix des noms d'hôpitaux. La sélection est sauvegardée sous le nom de `hospital_choice`, qui peut être référencé dans le code suivant comme `input$hospital_choice`.  
* Le code de la courbe d'épidémie (colonne 2) est enveloppé dans `renderPlot({ })`, incluant :  
  * Un filtre sur l'ensemble de données qui restreint la colonne `hospital` à la valeur actuelle de `input$hospital_choice`.  
  * Un titre dynamique du tracé qui incorpore `input$hospital_choice`.  
  
Notez que tout code faisant référence à une valeur `input$` doit se trouver dans une fonction `render({})` (pour être réactif).  

Voici le haut du script, incluant YAML, le chunk global, et la barre latérale :  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_shiny_script1.png"))
```
  
Voici la colonne 2, avec le tracé de l'épicurve réactive :  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_shiny_script2.png"))
```

Et voici le tableau de bord :  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_shiny_view.gif"))
```




### Autres exemples {.unnumbered}  

Pour lire un exemple de tableau de bord Shiny-**flexdashboard** lié à la santé et utilisant l'interactivité **shiny** et le widget de cartographie **leaflet**, consultez ce chapitre du livre en ligne [Geospatial Health Data : Modeling and Visualization with R-INLA and Shiny](https://www.paulamoraga.com/book-geospatial/sec-dashboardswithshiny.html).  




## Partage  

Les tableaux de bord qui ne contiennent pas d'éléments Shiny produisent un fichier HTML (.html), qui peut être envoyé par courriel (si la taille le permet). Ceci est utile, car vous pouvez envoyer le rapport du "tableau de bord" sans avoir à configurer un serveur pour l'héberger en tant que site web.  

Si vous avez intégré **shiny**, vous ne pourrez pas envoyer une sortie par e-mail, mais vous pouvez envoyer le script lui-même à un utilisateur R, ou héberger le tableau de bord sur un serveur comme expliqué ci-dessus.  


## Ressources  

Les excellents tutoriels qui ont informé cette page se trouvent ci-dessous. Si vous les consultez, vous pourrez probablement créer votre propre tableau de bord en moins d'une heure.  

https://bookdown.org/yihui/rmarkdown/dashboards.html

https://rmarkdown.rstudio.com/flexdashboard/

https://rmarkdown.rstudio.com/flexdashboard/using.html

https://rmarkdown.rstudio.com/flexdashboard/examples.html
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/flexdashboard.Rmd-->


# Tableaux de bord avec Shiny {#shiny}  


Les tableaux de bord sont souvent une excellente méthode de partager les résultats d'analyses avec d'autres personnes. Produire un tableau de bord avec **shiny** nécessite une connaissance relativement avancée du langage R, mais vous offrez la personnalisation des tableaux et des possibilités incroyables. 

<!-- L'un des plus grands inconvénients de `R` est sa facilité d'utilisation pour les personnes qui sont nouvelles ou qui n'ont aucune expérience des langages de programmation. Bien que ces compétences soient très précieuses, la plupart des gens trouveront que cela représente un obstacle au partage des analyses, en particulier dans les environnements multidisciplinaires. Maintenir une installation `R` demande un certain travail, et tout le monde ne sera pas à l'aise pour exécuter du code partagé, même s'il est bien documenté et facile à lire. Ceci est *surtout* vrai lorsque les utilisateurs doivent modifier les paramètres du code!  -->

<!-- Les tableaux de bord basés sur R sont également avantageux en ce qu'ils centralisent la façon dont le code est exécuté - lorsque le même code est exécuté sur différentes machines, souvent les gens devront faire face à différents chemins de fichiers, différentes versions de R, et différentes installations de packages. Pour cette raison, les tableaux de bord sont un excellent moyen de partager du code avec d'autres personnes de manière facile ! -->

Il est recommandé qu'une personne apprenant les tableaux de bord avec **shiny** ait une bonne connaissance de la transformation et de la visualisation des données, et soit à l'aise pour déboguer du code et écrire des fonctions. Le travail avec les tableaux de bord n'est pas intuitif au début et est parfois difficile à comprendre, mais il s'agit d'une excellente compétence à apprendre qui devient beaucoup plus facile avec la pratique !

Cette page donne un bref aperçu de la façon de créer des tableaux de bord avec **shiny** et ses extensions. 
Pour une méthode alternative de création de tableaux de bord qui est plus rapide, plus facile, mais peut-être moins personnalisable, voir la page sur **flextable** ([Rapports avec R Markdown](#rmarkdown)).  



## Préparation  



### Chargement des packages {.unnumbered}  


Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le package si nécessaire *et* le charge pour l'utiliser. Vous pouvez aussi charger les packages installés avec `library()` de **base** R. Voir la page sur [bases de R](#rbasics) pour plus d'informations sur les packages R.  

Nous commençons par installer le package R **shiny** :  




```{r, eval = FALSE}
pacman::p_load("shiny")
```



### Importer des données {.unnumbered}  

Si vous souhaitez suivre cette page, consultez la section du [manuel pour le téléchargement des données](#download_book_data). Il y a des liens pour télécharger les scripts R et les fichiers de données qui produisent l'application Shiny finale.  

Si vous essayez de reconstruire l'application à l'aide de ces fichiers, veuillez tenir compte de la structure des dossiers du projet R qui est créée au cours de la démonstration (par exemple, des dossiers pour "data" et pour "funcs").  





<!-- ======================================================= -->
## La structure d'une application shiny { }

### Structures de fichiers de base {.unnumbered}  


Pour comprendre `shiny`, nous devons d'abord comprendre comment fonctionne la structure des fichiers d'une application ! Nous devrions créer un tout nouveau répertoire avant de commencer. Cela peut être rendu plus facile en choisissant _New project_ dans _Rstudio_, et en choisissant _Shiny Web Application_. Cela créera la structure de base d'une application shiny pour vous.

En ouvrant ce projet, vous remarquerez qu'il y a déjà un fichier `.R` appelé _app.R_. Il est *essentiel* que nous ayons une des deux structures de fichiers de base :

1. Un seul fichier appelé _app.R_, *ou*
2. Deux fichiers, l'un appelé _ui.R_ et l'autre _server.R_ 

Dans cette page, nous utiliserons la première approche qui consiste à avoir un seul fichier appelé *app.R*. Voici un exemple de script :  




```{r, eval = FALSE}
# exemple de un script app.R

library(shiny)

ui <- fluidPage(

    # titre de l'application 
    titlePanel("My app"),

    # Sidebar avec le widget slider input 
    sidebarLayout(
        sidebarPanel(
            sliderInput("input_1")
        ),

        # afficher le graphique
        mainPanel(
           plotOutput("my_plot")
        )
    )
)


# Définir la logique du serveur requise pour dessiner un histogramme

server <- function(input, output) {
     
     plot_1 <- reactive({
          plot_func(param = input_1)
     })
     
    output$my_plot <- renderPlot({
       plot_1()
    })
}


# Executer application 
shinyApp(ui = ui, server = server)


```


Si vous ouvrez ce fichier, vous remarquerez que deux objets sont définis, un appelé `ui` et l'autre appelé `server`. Ces objets *doivent* être définis dans *toutes* les applications shiny et sont essentiels à la structure de l'application elle-même ! En fait, la seule différence entre les deux structures de fichiers décrites ci-dessus est que dans la structure 1, `ui` et `server` sont définis dans un seul fichier, alors que dans la structure 2, ils sont définis dans des fichiers séparés. Note : nous pouvons aussi (et nous devrions si nous avons une application plus grande) avoir d'autres fichiers .R dans notre structure que nous pouvons sourcer avec `source()` dans notre application.



### Le serveur et l'interface utilisateur {.unnumbered}


Nous devons maintenant comprendre ce que les objets `server` et `ui` font réellement. *Pour faire simple, ce sont deux objets qui interagissent l'un avec l'autre à chaque fois que l'utilisateur interagit avec l'application shiny.*

L'élément UI d'une shiny app est, à la base, le code R qui crée une interface HTML. Cela signifie que tout ce qui est *affiché* dans l'interface utilisateur d'une application. Cela inclut généralement :

* Les "widgets", par exemples les menus déroulants, cases à cocher, curseurs, etc. avec lesquels l'utilisateur peut interagir.
* Graphiques, tableaux, etc; toutes sorties générées par le code R.
* Les aspects de navigation d'une application, par exemples les onglets, volets, etc. 
* Texte générique, liens hypertextes, etc.
* Éléments HTML et CSS (abordés plus tard)

La chose la plus importante à comprendre au sujet de l'interface utilisateur est qu'elle *reçoit des entrées* de l'utilisateur et *affiche des sorties* du serveur. Aucun code *actif* n'est exécuté dans l'interface utilisateur *à tout moment*. Tous les changements vus dans l'interface utilisateur sont transmis au serveur (plus ou moins). Nous devons donc effectuer tous nos tracés, téléchargements, etc. dans le serveur.

Le serveur de l'application shiny est l'endroit où tout le code est exécuté une fois que l'application démarre. La façon dont cela fonctionne est un peu complique. Le serveur va effectivement _réagir_ à l'interface utilisateur et exécuter des bout de code en réponse. Si les choses changent dans le serveur, elles seront transmises à l'interface utilisateur, où les changements seront visibles. Il est important de noter que le code dans le serveur sera exécuté *non consécutivement* (ou il est préférable d'y penser de cette façon). En gros, chaque fois qu'une entrée de l'interface utilisateur affecte un bout de code dans le serveur, celui-ci s'exécutera automatiquement, et la sortie sera produite et affichée.

Tout cela semble probablement très abstrait pour l'instant, nous allons donc travailler avec quelques exemples pour avoir une idée claire de la façon dont cela fonctionne réellement. 


### Avant de commencer à construire une application {.unnumbered}

Avant de commencer à construire une application, il est extrêmement utile de savoir *ce que* vous voulez construire. Puisque votre interface utilisateur sera écrite en code, vous ne pouvez pas vraiment visualiser ce que vous construisez, sauf si vous visez quelque chose de spécifique. Pour cette raison, il est extrêmement utile de regarder de nombreux exemples d'applications shinys pour avoir une idée de ce que vous pouvez faire. Encore mieux, si vous pouvez regarder le code source derrière ces applications ! Voici quelques bonnes ressources pour cela :

* La [galerie d'applications Rstudio](https://shiny.rstudio.com/gallery/)  

Une fois que vous avez une idée de ce qui est possible, il est également utile de dessiner ce à quoi vous voulez que votre application ressemble. Vous pouvez faire un dessin soit sur du papier ou dans un logiciel de dessin (PowerPoint, MS paint, etc.). Il est utile de commencer simplement pour votre première application ! Il n'y a certainment pas d'honte à utiliser le code d'une belle application que vous trouvez en ligne comme modèle pour votre travail. C'est beaucoup plus facile que construire quelque chose à partir de zéro !



## Construire une interface utilisateur 


Lorsque nous construisons notre application, il est plus facile de travailler d'abord sur l'interface utilisateur afin de voir ce que nous faisons, et de ne pas risquer que l'application échoue à cause d'erreurs de serveur. Comme mentionné précédemment, il est souvent bon d'utiliser un modèle pour travailler sur l'interface utilisateur. Il y a un certain nombre de modèles standards qui peuvent être utilisés avec shiny et qui sont disponibles dans le package de base shiny, mais il est intéressant de noter qu'il y a aussi un certain nombre d'extensions du package comme `shinydashboard`. Nous allons utiliser un exemple de la base shiny pour commencer. 

Une interface utilisateur shiny est généralement définie comme une série de fonctions imbriquées, dans l'ordre suivant:

1. Une fonction définissant la mise en page générale (la plus basique est `fluidPage()`, mais d'autres sont disponibles)
2. Des panneaux à l'intérieur de la mise en page tels que:
     - une barre latérale (`sidebarPanel()`)
     - un panneau "principal" (`mainPanel()`)
     - un onglet (`tabPanel()`)
     - une "colonne" générique (`column()`)
3. Widgets et sorties : ils peuvent conférer des entrées au serveur (widgets) ou des sorties du serveur (outputs)
     - Les widgets sont généralement appelés `xxxInput()`, par exemple `selectInput()`.
     - Les sorties sont généralement appelées `xxxOutput()`, par exemple `plotOutput()`.

Il est important de préciser que ces éléments ne peuvent pas être visualisés facilement de manière abstraite. Il est donc préférable de regarder un exemple ! Considérons la création d'une application de base qui visualise nos données de denombrement des equipemnts de lutte contre le paludisme par district. Ces données comportent un grand nombre de paramètres différents, et il serait formidable que l'utilisateur final puisse appliquer des filtres pour voir les données par groupe d'âge/district comme il l'entend ! Nous pouvons utiliser une mise en page shiny très simple pour commencer, la mise en page de la barre latérale. Il s'agit d'une mise en page où les widgets sont placés dans une barre latérale sur la gauche, et le graphique est placé sur la droite.

Elaborons notre application: nous pouvons commencer par un sélecteur qui nous permet de choisir le district où nous voulons visualiser les données, et un autre qui nous permet de visualiser le groupe d'âge qui nous intéresse. Nous allons utiliser ces filtres pour afficher une épicurve qui reflète ces paramètres. Pour cela, nous avons besoin de:

1. Deux menus déroulants qui nous permettent de choisir le district que nous voulons et le groupe d'âge qui nous intéresse. 
2. Une zone où nous pouvons montrer notre épicurve obtenue.

Cela pourrait ressembler à quelque chose comme ceci:



```{r, eval = FALSE}

library(shiny)

ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         # sélectionneur pour le district
         selectInput(
              inputId = "select_district",
              label = "Select district",
              choices = c(
                   "All",
                   "Spring",
                   "Bolo",
                   "Dingo",
                   "Barnard"
              ),
              selected = "All",
              multiple = TRUE
         ),
         # sélecteur pour le groupe d'âge
         selectInput(
              inputId = "select_agegroup",
              label = "Sélectionnez le groupe d'âge",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         )

    ),

    mainPanel(
      # epicurve va ici
      plotOutput("malaria_epicurve")
    )
    
  )
)


```



Lorsque app.R est exécuté avec le code d'interface utilisateur ci-dessus (sans code actif dans la partie `server` de app.R) le mise en page apparaît comme ceci. Notez qu'il n'y aura pas de tracé s'il n'y a pas de serveur pour conduire à ce resultat, mais nos entrées fonctionnent !

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "shiny", "simple_UI_view.png"))
```


C'est une bonne occasion de discuter les fonctionnement des widgets. Notez que chaque widget accepte un `inputId`, un `label`, et une série d'autres options qui sont spécifiques au type de widget. Ce `inputId` est extrêmement important ! Ce sont les IDs qui sont utilisés pour passer les informations de l'IU au serveur. Pour cette raison, ils *doivent être uniques*. Vous devriez faire un effort pour les nommer de manière sensée et spécifique à ce avec quoi ils interagissent dans le cas de grandes applications.

Vous devez lire attentivement la documentation pour obtenir tous les détails sur ce que font chacun de ces widgets. Les widgets transmettront des types de données spécifiques au serveur en fonction du type de widget, et cela doit être bien compris. Par exemple, `selectInput()` transmettra un type de caractère au serveur :

- Si nous sélectionnons _Spring_ pour le premier widget ici, il transmettra l'objet caractère `"Spring"` au serveur. 
- Si nous sélectionnons deux éléments dans le menu déroulant, ils seront transmis sous forme de vecteur de caractères (par exemple, `c("Spring", "Bolo")`).

D'autres widgets transmettront différents types d'objets au serveur ! Par exemple

- `numericInput()` passera un objet de type numérique au serveur.
- `checkboxInput()` transmettra un objet de type logique au serveur (`TRUE` ou `FALSE`).

Il est également intéressant de noter le *vecteur nommé* que nous avons utilisé pour les données d'âge ici. Pour de nombreux widgets, l'utilisation d'un vecteur nommé comme choix affichera les *noms* du vecteur comme choix d'affichage, mais passera la *valeur* sélectionnée du vecteur au serveur. Par exemple, ici quelqu'un peut sélectionner "15+" dans le menu déroulant, et l'interface utilisateur transmettra `"malaria_rdt_15"` au serveur - qui se trouve être le nom de la colonne qui nous intéresse !


Il y a beaucoup de widgets que vous pouvez utiliser dans votre application. Les widgets vous permettent également de télécharger des fichiers dans votre application, et de télécharger des sorties. Il existe également d'excellentes examples de shiny qui vous donnent accès à plus de widgets que le shiny de base. Le package **shinyWidgets** en est un excellent exemple. Pour voir quelques exemples, vous pouvez consulter les liens suivants :

- [galerie de widgets de la base shiny](https://shiny.rstudio.com/gallery/widget-gallery.html)
- [galerie de widgets shinyWidgets](https://github.com/dreamRs/shinyWidgets)




## Chargement des données dans notre application

L'étape suivante dans le développement de notre application consiste à mettre en place le serveur et à le faire fonctionner. Pour ce faire, nous devons charger des données dans notre application, et déterminer tous les calculs que nous allons effectuer. Une application shiny n'est pas facile à déboguer, car on ne sait pas toujours d'où viennent les erreurs. Il est donc idéal de faire fonctionner tout le code de traitement et de visualisation des données avant de commencer à créer le serveur lui-même.

Ainsi, étant donné que nous voulons créer une application qui affiche des courbes épi qui changent en fonction de l'entrée de l'utilisateur, nous devons réfléchir au code dont nous aurions besoin pour l'exécuter dans un script R normal. Nous devrons :

1. Charger nos packages
2. Charger nos données
3. Transformer nos données
4. Développer une _fonction_ pour visualiser nos données en fonction des entrées de l'utilisateur.

Cette liste est assez simple, et ne devrait pas être trop difficile à réaliser. Il est maintenant important de réfléchir aux parties de ce processus qui doivent *être faites une seule fois* et à celles qui doivent *être exécutées en réponse aux entrées de l'utilisateur*. En effet, les applications shiny exécutent généralement du code avant de s'exécuter, ce qui n'est fait qu'une seule fois. La performance de notre application sera améliorée si une grande partie de notre code peut être déplacée dans cette section. Pour cet exemple, nous n'avons besoin de charger nos données/packages et d'effectuer des transformations de base qu'une seule fois, nous pouvons donc placer ce code *hors du serveur*. Cela signifie que la seule chose dont nous aurons besoin dans le serveur est le code pour visualiser nos données. Développons d'abord tous ces composants dans un script. Cependant, puisque nous visualisons nos données à l'aide d'une fonction, nous pouvons également placer le code _pour la fonction_ en dehors du serveur afin que notre fonction soit dans l'environnement lorsque l'application s'exécute !

Commençons par charger nos données. Puisque nous travaillons avec un nouveau projet, et que nous voulons le rendre propre, nous pouvons créer un nouveau répertoire appelé data, et y ajouter nos données sur le paludisme. Nous pouvons exécuter ce code ci-dessous dans un script de test que nous supprimerons éventuellement lorsque nous aurons nettoyé la structure de notre application.

```{r, echo = TRUE}
pacman::p_load("tidyverse", "lubridate")

# read data
malaria_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) %>% 
  as_tibble()

print(malaria_data)


```



Il sera plus facile de travailler avec ces données si nous utilisons des données ordonnées standards, nous devons donc également les transformer en un format de données plus long, où le groupe d'âge est une colonne, et les cas une autre colonne. Nous pouvons le faire facilement en utilisant ce que nous avons appris dans la page [Pivoter les données](#pivoting_data).  


```{r, echo = TRUE}

malaria_data <- malaria_data %>%
  select(-newid) %>%
  pivot_longer(cols = starts_with("malaria_"), names_to = "age_group", values_to = "cases_reported")

print(malaria_data)

```


Et avec cela, nous avons fini de préparer nos données ! Cela raye les points 1, 2 et 3 de notre liste de choses à développer pour notre "script R de test". La dernière tâche, et la plus difficile, sera de construire une fonction pour produire une épicurve basée sur des paramètres définis par l'utilisateur. Comme nous l'avons mentionné précédemment, il est *hautement recommandé* à toute personne apprenant shiny de regarder d'abord la section sur la programmation fonctionnelle ([Écrire les fonctions](#writing_functions)) pour comprendre comment cela fonctionne !

Lorsque nous définissons notre fonction, il peut être difficile de penser aux paramètres que nous voulons inclure. Dans le cadre de la programmation fonctionnelle avec shiny, chaque paramètre pertinent est généralement associé à un widget, ce qui facilite la réflexion ! Par exemple, dans notre application actuelle, nous voulons être en mesure de filtrer par district, et avoir un widget pour cela, donc nous pouvons ajouter un paramètre de district pour refléter cela. Nous n'avons *pas* de fonctionnalité d'application pour filtrer par établissement (pour l'instant), donc nous n'avons pas besoin de l'ajouter comme paramètre. Commençons par créer une fonction avec trois paramètres :

1. L'ensemble de données de base
2. Le district de choix
3. Le groupe d'âge choisi

```{r}

plot_epicurve <- function(data, district = "All", agegroup = "malaria_tot") {
  
  if (!("All" %in% district)) {
    data <- data %>%
      filter(District %in% district)
    
    plot_title_district <- stringr::str_glue("{paste0(district, collapse = ', ')} districts")
    
  } else {
    
    plot_title_district <- "all districts"
    
  }
  
  # s'il n'y a pas de données restantes, retourne NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  data <- data %>%
    filter(age_group == agegroup)
  
  
  # s'il n'y a pas de données restantes, retourne NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  if (agegroup == "malaria_tot") {
      agegroup_title <- "All ages"
  } else {
    agegroup_title <- stringr::str_glue("{str_remove(agegroup, 'malaria_rdt')} years")
  }
  
  
  ggplot(data, aes(x = data_date, y = cases_reported)) +
    geom_col(width = 1, fill = "darkred") +
    theme_minimal() +
    labs(
      x = "date",
      y = "number of cases",
      title = stringr::str_glue("Malaria cases - {plot_title_district}"),
      subtitle = agegroup_title
    )
  
  
  
}

```



Nous n'entrerons pas dans les détails de cette fonction, car elle est relativement simple dans son fonctionnement. Une chose à noter cependant, c'est que nous gérons les erreurs en retournant `NULL` alors qu'elle devrait entrainer une erreur. En effet, si le serveur shiny produit un objet `NULL` au lieu d'un objet plot, rien ne sera affiché dans l'interface utilisateur ! C'est important, car sinon les erreurs vont souvent provoquer l'arrêt du fonctionnement de votre application.  

Une autre chose à noter est l'utilisation de l'opérateur `%in%` lors de l'évaluation de l'entrée `district`. Comme mentionné ci-dessus, cela pourrait arriver comme un vecteur de caractères avec plusieurs valeurs, donc l'utilisation de `%in%` est plus flexible que disons, `==`.  

Testons notre fonction !

```{r, echo = TRUE, warning = FALSE}

plot_epicurve(malaria_data, district = "Bolo", agegroup = "malaria_rdt_0-4")

```


Maintenant que notre fonction fonctionne, nous devons comprendre comment tout cela va s'intégrer dans notre shiny application. Nous avons déjà mentionné le concept de _startup code_, mais voyons comment l'intégrer dans la structure de notre application. Il y a deux façons de le faire !

1. Placer ce code dans votre fichier _app.R_ au début du script (au-dessus de l'interface utilisateur), ou bien  
2. Créer un nouveau fichier dans le répertoire de votre application appelé _global.R_, et placer le code de démarrage dans ce fichier.

Il convient de noter à ce stade qu'il est généralement plus facile, en particulier pour les applications plus importantes, d'utiliser la deuxième structure de fichiers, car elle vous permet de séparer votre structure de fichiers d'une manière simple. Développons maintenant complètement ce script global.R. Voici à quoi il pourrait ressembler :


```{r, eval = F}
# global.R script

pacman::p_load("tidyverse", "lubridate", "shiny")


# lire les données
malaria_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) %>% 
  as_tibble()

# données nettoyées et  pivotées en longueur

malaria_data <- malaria_data %>%
  select(-newid) %>%
  pivot_longer(cols = starts_with("malaria_"), names_to = "age_group", values_to = "cases_reported")


# definir la fonction pour la representation graphique
plot_epicurve <- function(data, district = "All", agegroup = "malaria_tot") {
  
  # creer le titre du graphe
  if (!("All" %in% district)) {            
    data <- data %>%
      filter(District %in% district)
    
    plot_title_district <- stringr::str_glue("{paste0(district, collapse = ', ')} districts")
    
  } else {
    
    plot_title_district <- "all districts"
    
  }
  
  # s'il n'y a pas de données restantes, retourne NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  # filtrer par groupe d'âge
  data <- data %>%
    filter(age_group == agegroup)
  
  
  # s'il n'y a pas de données restantes, retourne NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  if (agegroup == "malaria_tot") {
      agegroup_title <- "All ages"
  } else {
    agegroup_title <- stringr::str_glue("{str_remove(agegroup, 'malaria_rdt')} years")
  }
  
  
  ggplot(data, aes(x = data_date, y = cases_reported)) +
    geom_col(width = 1, fill = "darkred") +
    theme_minimal() +
    labs(
      x = "date",
      y = "number of cases",
      title = stringr::str_glue("Malaria cases - {plot_title_district}"),
      subtitle = agegroup_title
    )
  
  
  
}



```



Facile ! Une des grandes caractéristiques de shiny est qu'il comprend à quoi servent les fichiers nommés _app.R_, _server.R_, _ui.R_ et _global.R_, il n'y a donc pas besoin de les connecter entre eux par un quelconque code. Ainsi, il suffit d'avoir ce code dans _global.R_ dans le répertoire pour qu'il s'exécute avant que nous démarrions notre application.  

Nous devons également noter que l'organisation de notre application serait améliorée si nous déplacions la fonction de traçage dans son propre fichier - cela sera particulièrement utile lorsque les applications deviendront plus grandes. Pour ce faire, nous pourrions créer un autre répertoire appelé _funcs_, et y placer cette fonction dans un fichier appelé _plot_epicurve.R_. Nous pourrions ensuite lire cette fonction via la commande suivante dans _global.R_.

```{r, eval = F}

source(here("funcs", "plot_epicurve.R"), local = TRUE)

```


Notez que vous devriez *toujours* spécifier `local = TRUE` dans les applications shiny, car cela affectera le sourcing quand/si l'application est publiée sur un serveur. 

## Développer un serveur d'applications

Maintenant que nous avons la plupart de notre code, il nous reste à développer notre serveur. Il s'agit de la dernière pièce de notre application, et c'est probablement la plus difficile à comprendre. Le serveur est une grande fonction R, mais il est utile de le considérer comme une série de petites fonctions, ou de tâches que l'application peut exécuter. Il est important de comprendre que ces fonctions ne sont pas exécutées dans un ordre linéaire. Il existe un ordre, mais il n'est pas nécessaire de le comprendre lorsqu'on débute avec Shiny. À un niveau très basique, ces tâches ou fonctions s'activent lorsqu'un changement dans les entrées de l'utilisateur les affecte, *à moins que le développeur ne les ait configurées pour qu'elles se comportent différemment*. Encore une fois, tout cela est assez abstrait, mais passons d'abord en revue les trois types _d'objets_ shiny de base

1. Les sources réactives - c'est un autre terme pour les entrées de l'utilisateur. Le serveur shiny a accès aux sorties de l'interface utilisateur par le biais des widgets que nous avons programmés. Chaque fois que les valeurs de ces derniers sont modifiées, elles sont transmises au serveur.

2. Conducteurs réactifs - ce sont des objets qui existent *seulement* à l'intérieur du shiny server. Nous n'en avons pas vraiment besoin pour les applications simples, mais ils produisent des objets qui ne peuvent être vus qu'à l'intérieur du serveur, et utilisés dans d'autres opérations. Ils dépendent généralement de sources réactives.

3. Les points de terminaison - ce sont les sorties qui sont transmises du serveur à l'interface utilisateur. Dans notre exemple, il s'agit de la courbe épi que nous produisons. 

Avec ceci en tête, construisons notre serveur étape par étape. Nous montrons à nouveau le code de l'interface utilisateur à titre de référence :

```{r, eval = FALSE}

ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         # sélectionneur pour le district
         selectInput(
              inputId = "select_district",
              label = "Sélectionnez le district",
              choices = c(
                   "All",
                   "Spring",
                   "Bolo",
                   "Dingo",
                   "Barnard"
              ),
              selected = "All",
              multiple = TRUE
         ),
         # sélectionnez le groupe d'âge
         selectInput(
              inputId = "select_agegroup",
              label = "Sélectionnez le groupe d'âge",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         )

    ),

    mainPanel(
      # epicurve va ici
      plotOutput("malaria_epicurve")
    )
    
  )
)


```

De ce code UI nous avons :


- Deux entrées :
  - Sélecteur de district (avec un inputId de `select_district`)
  - Un sélecteur de groupe d'âge (avec un inputId de `select_agegroup`)
- Une sortie :
  - L'épicurve (avec un outputId de `malaria_epicurve`)

Comme indiqué précédemment, ces noms uniques que nous avons attribués à nos entrées et sorties sont cruciaux. Ils *doivent être uniques* et sont utilisés pour transmettre des informations entre l'interface utilisateur et le serveur. Dans notre serveur, nous accédons à nos entrées via la syntaxe `input$inputID` et les sorties sont transmises à l'interface utilisateur via la syntaxe `output$output_name` Voyons un exemple, car encore une fois, c'est difficile à comprendre autrement !

```{r, eval = FALSE}

server <- function(input, output, session) {
  
  output$malaria_epicurve <- renderPlot(
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  )
  
}


```



Le serveur pour une application simple comme celle-ci est en fait assez simple ! Vous remarquerez que le serveur est une fonction avec trois paramètres - `input`, `output`, et `session` - ce n'est pas très important à comprendre pour le moment, mais il est important de s'en tenir à cette configuration ! Dans notre serveur, nous n'avons qu'une seule tâche - elle rend un graphique basé sur la fonction que nous avons créée plus tôt, et les entrées du serveur. Remarquez que les noms des objets d'entrée et de sortie correspondent exactement à ceux de l'interface utilisateur.

Pour comprendre les bases de la façon dont le serveur réagit aux entrées de l'utilisateur, vous devez noter que la sortie saura (grâce au package sous-jacent) quand les entrées changent, et réexécutera cette fonction pour créer un graphique à chaque fois qu'elles changent. Notez que nous utilisons également la fonction `renderPlot()` ici - c'est l'une des fonctions d'une famille de classes spécifiques qui passent ces objets à une sortie ui. Il existe un certain nombre de fonctions qui se comportent de manière similaire, mais vous devez vous assurer que la fonction utilisée correspond à la classe de l'objet que vous transmettez à l'interface utilisateur ! Par exemple :

- `renderText()` - envoie du texte à l'interface utilisateur
- `renderDataTable` - envoie une table interactive à l'interface utilisateur.

Rappelez-vous que ces *fonctions* doivent également correspondre à la fonction de sortie utilisée dans l'interface utilisateur - ainsi, `renderPlot()` est associé à `plotOutput()`, et `renderText()` est associé à `textOutput()`. 

Nous avons enfin créé une application fonctionnelle ! Nous pouvons l'exécuter en appuyant sur le bouton Run App en haut à droite de la fenêtre du script dans Rstudio. Notez que vous pouvez choisir de lancer votre application dans votre navigateur par défaut (plutôt que dans Rstudio), ce qui reflétera plus fidèlement ce à quoi l'application ressemblera pour les autres utilisateurs.  


```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "shiny", "app_simple_view.gif"))
```



Il est amusant de noter que dans la console R, l'application est "à l'écoute" ! On parle de réactivité !  

```{r, echo=F}
knitr::include_graphics(here::here("images", "shiny", "listening.png"))
```



<!-- À FAIRE: *AJOUTER QUELQUE CHOSE LORS DU TÉLÉCHARGEMENT D'UN FICHIER ZIP DE L'APPLICATION?*  -->



## Ajout de fonctionnalités supplémentaires

À ce stade, nous avons enfin une application qui fonctionne, mais nous avons très peu de fonctionnalités. De plus, nous n'avons pas encore touché à la surface de ce que shiny peut faire, il y a donc encore beaucoup à apprendre ! Continuons à développer notre application existante en ajoutant quelques fonctionnalités supplémentaires. Voici quelques éléments qu'il serait bon d'ajouter : 

1. Un texte explicatif 
2. Un bouton de téléchargement pour notre parcelle - cela permettrait à l'utilisateur d'obtenir une version de haute qualité de l'image qu'il génère dans l'application.
3. Un sélecteur pour des  equipements particuliers
4. Une autre page de tableau de bord - elle pourrait afficher un tableau de nos données.

Cela fait beaucoup de choses à ajouter, mais nous pouvons l'utiliser pour en apprendre davantage sur un tas de prouesses différentes en cours. Il y a tant à apprendre sur shiny (il peut être *très* avancé, mais il est à espérer qu'une fois que les utilisateurs ont une meilleure idée de la façon de l'utiliser, ils peuvent devenir plus à l'aise en utilisant des sources d'apprentissage externes aussi).



### Ajouter du texte statique {.unnumbered}  


Parlons d'abord de l'ajout de texte statique à notre application shiny. L'ajout de texte à notre application est extrêmement facile, une fois que vous en avez une connaissance de base. Puisque le texte statique ne change pas dans l'application shiny (si vous voulez qu'il change, vous pouvez utiliser les fonctions de *rendu de texte* dans le serveur ! Nous n'allons pas entrer dans les détails, mais vous pouvez ajouter un certain nombre d'éléments différents à votre interface utilisateur (et même des éléments personnalisés) en interfaçant R avec *HTML* et *css*.

HTML et css sont des langages qui sont explicitement impliqués dans la conception de l'interface utilisateur. Nous n'avons pas besoin de trop les comprendre, mais *HTML* crée des objets dans l'IU (comme une boîte de texte, ou un tableau), et *css* est généralement utilisé pour changer le style et l'esthétique de ces objets. Shiny a accès à un large éventail de _balises HTML_ - celles-ci sont présentes pour les objets qui se comportent d'une manière spécifique, comme les en-têtes, les paragraphes de texte, les sauts de ligne, les tableaux, etc. Nous pouvons utiliser certains de ces exemples comme ceci :

- `h1()` - il s'agit d'une balise *header*, qui rendra le texte inclus automatiquement plus grand, et changera les valeurs par défaut en ce qui concerne la police, la couleur, etc (selon le thème général de votre application). Vous pouvez accéder à des sous-titres _plus petits et plus petits_ avec `h2()` jusqu'à `h6()` également. L'utilisation ressemble à :
  * `h1("mon en-tête - section 1")`

- `p()` - il s'agit d'une balise *paragraphe*, qui rendra le texte inclus similaire à un texte dans un corps de texte. Ce texte sera automatiquement enveloppé, et sera d'une taille relativement petite (les pieds de page pourraient être plus petits par exemple.) Pensez-y comme le corps de texte d'un document Word. L'utilisation ressemble à :  

  * `p("Ceci est un corps de texte plus large où j'explique la fonction de mon application")`
  
- `tags$b()` et `tags$i()` - elles sont utilisées pour créer des `tags$b()` en gras et des `tags$i()` en italique avec le texte inclus !

- `tags$ul()`, `tags$ol()` et `tags$li()` - ce sont des balises utilisées pour créer des *listes*. Elles sont toutes utilisées dans la syntaxe ci-dessous, et permettent à l'utilisateur de créer une liste ordonnée (`tags$ol()`, c'est-à-dire numérotée) ou non ordonnée (`tags$ul()`, c'est-à-dire à puces). `tags$li()` est utilisé pour désigner les éléments de la liste, quel que soit le type de liste utilisé. par exemple :

```{r, eval = F}

tags$ol(
  
  tags$li("Item 1"),
  
  tags$li("Item 2"),
  
  tags$li("Item 3")
  
)

```


- `br()` et `hr()` - ces balises créent respectivement des *sauts de ligne* et des *lignes horizontales* (avec un saut de ligne). Utilisez-les pour séparer les sections de votre application et de votre texte ! Il n'est pas nécessaire de passer des éléments à ces balises (les parenthèses peuvent rester vides).


- `div()` - c'est une balise *générique* qui peut *contenir n'importe quoi*, et peut être *nommée n'importe comment*. Une fois que vous aurez progressé dans la conception de l'IU, vous pourrez les utiliser pour compartimenter votre IU, donner des styles spécifiques à certaines sections et créer des interactions entre le serveur et les éléments de l'IU. Nous n'entrerons pas dans les détails, mais il vaut la peine de les connaître !

Notez que chacun de ces objets peut être accédé par `tags$...` ou pour certains, juste la fonction. Ce sont effectivement des synonymes, mais il peut être utile d'utiliser le style `tags$...` si vous préférez être plus explicite et ne pas écraser les fonctions accidentellement. Ceci n'est en aucun cas une liste exhaustive des balises disponibles. Il existe une liste complète de toutes les balises disponibles dans shiny [ici](https://shiny.rstudio.com/articles/tag-glossary.html) et encore plus peuvent être utilisées en insérant du HTML directement dans votre interface !


Si vous vous sentez confiant, vous pouvez également ajouter des *éléments de style css* à vos balises HTML avec l'argument `style` dans chacune d'entre elles. Nous n'allons pas entrer dans les détails de ce fonctionnement, mais une astuce pour tester les changements esthétiques d'une interface utilisateur est d'utiliser le mode inspecteur HTML dans chrome (de votre shiny application que vous exécutez dans le navigateur), et de modifier le style des objets vous-même !

Ajoutons du texte à notre application

```{r, eval = F}

ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         h4("Options"),
         # sélectionneur pour le district
         selectInput(
              inputId = "select_district",
              label = "Select district",
              choices = c(
                   "All",
                   "Spring",
                   "Bolo",
                   "Dingo",
                   "Barnard"
              ),
              selected = "All",
              multiple = TRUE
         ),
         # sélecteur pour le groupe d'âge
         selectInput(
              inputId = "select_agegroup",
              label = "Select age group",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         ),
    ),

    mainPanel(
      # epicurve va ici
      plotOutput("malaria_epicurve"),
      br(),
      hr(),
      p("Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:"),
    tags$ul(
      tags$li(tags$b("location_name"), " - the facility that the data were collected at"),
      tags$li(tags$b("data_date"), " - the date the data were collected at"),
      tags$li(tags$b("submitted_daate"), " - the date the data were submitted at"),
      tags$li(tags$b("Province"), " - the province the data were collected at (all 'North' for this dataset)"),
      tags$li(tags$b("District"), " - the district the data were collected at"),
      tags$li(tags$b("age_group"), " - the age group the data were collected for (0-5, 5-14, 15+, and all ages)"),
      tags$li(tags$b("cases_reported"), " - the number of cases reported for the facility/age group on the given date")
    )
    
  )
)
)



```

```{r, echo=F}
knitr::include_graphics(here::here("images", "shiny", "app_text_view.png"))
```


### Ajouter un lien {.unnumbered}


Pour ajouter un lien à un site Web, utilisez `tags$a()` avec le lien et le texte à afficher comme indiqué ci-dessous. Pour avoir un paragraphe autonome, mettez-le dans `p()`. Pour que seuls quelques mots d'une phrase soient liés, divisez la phrase en plusieurs parties et utilisez `tags$a()` pour la partie hyperliée. Pour que le lien s'ouvre dans une *nouvelle* fenêtre du navigateur, ajoutez `target = "_blank"` comme argument.  

```{r, eval=F}
tags$a(href = "www.epiRhandbook.com", "Visit our website!")
```




### Ajout d'un bouton de téléchargement {.unnumbered}

Passons à la deuxième des trois fonctions. Un bouton de téléchargement est une chose assez courante à ajouter à une application et est assez facile à réaliser. Nous devons ajouter un autre Widget à notre interface, et nous devons ajouter une autre sortie à notre serveur pour l'attacher. Nous pouvons également introduire des *conducteurs réactifs* dans cet exemple !


Mettons d'abord à jour notre interface utilisateur - c'est facile car shiny est livré avec un widget appelé `downloadButton()` - donnons-lui un inputId et un label.

```{r, eval = FALSE}

ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         # sélectionneur pour le district
         selectInput(
              inputId = "select_district",
              label = "Select district",
              choices = c(
                   "All",
                   "Spring",
                   "Bolo",
                   "Dingo",
                   "Barnard"
              ),
              selected = "All",
              multiple = FALSE
         ),
         # sélecteur pour le groupe d'âge
         selectInput(
              inputId = "select_agegroup",
              label = "Select age group",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         ),
         # ligne horizontale
         hr(),
         downloadButton(
           outputId = "download_epicurve",
           label = "Download plot"
         )

    ),

    mainPanel(
      # epicurve va ici
      plotOutput("malaria_epicurve"),
      br(),
      hr(),
      p("Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:"),
      tags$ul(
        tags$li(tags$b("location_name"), " - the facility that the data were collected at"),
        tags$li(tags$b("data_date"), " - the date the data were collected at"),
        tags$li(tags$b("submitted_daate"), " - the date the data were submitted at"),
        tags$li(tags$b("Province"), " - the province the data were collected at (all 'North' for this dataset)"),
        tags$li(tags$b("District"), " - the district the data were collected at"),
        tags$li(tags$b("age_group"), " - the age group the data were collected for (0-5, 5-14, 15+, and all ages)"),
        tags$li(tags$b("cases_reported"), " - the number of cases reported for the facility/age group on the given date")
      )
      
    )
    
  )
)


```
 

Notez que nous avons également ajouté une balise `hr()` - celle-ci ajoute une ligne horizontale séparant nos widgets de contrôle de nos widgets de téléchargement. C'est une autre des balises HTML dont nous avons parlé précédemment.

Maintenant que notre interface utilisateur est prête, nous devons ajouter le composant serveur. Les téléchargements sont effectués dans le serveur avec la fonction `downloadHandler()`. Comme pour notre plot, nous devons l'attacher à une sortie qui a le même inputId que le bouton de téléchargement. Cette fonction prend deux arguments - `filename` et `content` - ce sont tous deux des fonctions. Comme vous pouvez le deviner, `filename` est utilisé pour spécifier le nom du fichier à télécharger, et `content` est utilisé pour spécifier ce qui doit être téléchargé. `content` contient une fonction que vous utiliserez pour sauvegarder des données localement - donc si vous téléchargez un fichier csv, vous pourrez utiliser `rio::export()`. Comme nous téléchargeons un graphique, nous utiliserons `ggplot2::ggsave()`. Voyons comment nous allons programmer ceci (nous ne l'ajouterons pas encore au serveur). 

```{r, eval = FALSE}

server <- function(input, output, session) {
  
  output$malaria_epicurve <- renderPlot(
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  )
  
  output$download_epicurve <- downloadHandler(
    filename = function() {
      stringr::str_glue("malaria_epicurve_{input$select_district}.png")
    },
    
    content = function(file) {
      ggsave(file, 
             plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup),
             width = 8, height = 5, dpi = 300)
    }
    
  )
  
}


```



Notez que la fonction `content` prend toujours un argument `file`, que nous mettons là où le nom du fichier de sortie est spécifié. Vous pouvez également remarquer que nous répétons du code ici - nous utilisons notre fonction `plot_epicurve()` deux fois dans ce serveur, une fois pour le téléchargement et une fois pour l'image affichée dans l'application. Bien que cela n'affecte pas massivement les performances, cela signifie que le code pour générer ce tracé devra être exécuté lorsque l'utilisateur change les widgets spécifiant le district et le groupe d'âge, *et* à nouveau lorsque vous voulez télécharger le tracé. Dans les grandes applications, les décisions sous-optimales comme celle-ci ralentiront de plus en plus les choses, il est donc bon d'apprendre comment rendre notre application plus efficace dans ce sens. Ce qui serait plus logique, c'est d'avoir un moyen d'exécuter le code epicurve lorsque les districts/groupes d'âge sont modifiés, *et de laisser ce code être utilisé par* les fonctions renderPlot() et downloadHandler(). C'est là que les conducteurs réactifs entrent en jeu ! 

Les conducteurs réactifs sont des objets qui sont créés dans le serveur shiny de manière *réactive*, mais qui ne sont pas édités - ils peuvent simplement être utilisés par d'autres parties du serveur. Il existe un certain nombre de types différents de *conducteurs réactifs*, mais nous allons passer en revue les deux principaux.

1.`reactive()` - c'est le conducteur réactif le plus basique - il réagira à chaque fois que les entrées utilisées à l'intérieur changeront (comme nos widgets de district/groupe d'âge).  
2. `eventReactive()` - ce conducteur réactif fonctionne de la même manière que `reactive()`, sauf que l'utilisateur peut spécifier les entrées qui le font réexécuter. Ceci est utile si votre conducteur réactif prend beaucoup de temps à traiter, mais ceci sera expliqué plus tard.  

Regardons les deux exemples :

```{r, eval = FALSE}

malaria_plot_r <- reactive({
  
  plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  
})


# ne s'exécute que lorsque le sélecteur de district change !
malaria_plot_er <- eventReactive(input$select_district, {
  
  plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  
})



```


Lorsque nous utilisons la configuration `eventReactive()`, nous pouvons spécifier quelles entrées provoquent l'exécution de ce morceau de code - ce n'est pas très utile pour nous pour le moment, donc nous pouvons le laisser pour l'instant. Notez que vous pouvez inclure plusieurs entrées avec `c()`.

Voyons comment nous pouvons intégrer cela dans notre code serveur :


```{r, eval = FALSE}

server <- function(input, output, session) {
  
  malaria_plot <- reactive({
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  })
  
  
  
  output$malaria_epicurve <- renderPlot(
    malaria_plot()
  )
  
  output$download_epicurve <- downloadHandler(
    
    filename = function() {
      stringr::str_glue("malaria_epicurve_{input$select_district}.png")
    },
    
    content = function(file) {
      ggsave(file, 
             malaria_plot(),
             width = 8, height = 5, dpi = 300)
    }
    
  )
  
}


```


Vous pouvez voir que nous faisons simplement appel à la sortie de notre réactif que nous avons défini dans nos fonctions de téléchargement et de rendu de tracé. Une chose à noter qui fait souvent trébucher les gens est que vous devez utiliser les sorties des réactifs comme s'il s'agissait de fonctions - vous devez donc *ajouter des parenthèses vides à la fin de celles-ci* (par exemple, `malaria_plot()` est correct, et `malaria_plot` ne l'est pas). Maintenant que nous avons ajouté cette solution, notre application est un peu plus ordonnée, plus rapide et plus facile à modifier puisque tout le code qui exécute la fonction epicurve se trouve à un seul endroit.


```{r, echo=F}
knitr::include_graphics(here::here("images", "shiny", "download_button_view.png"))
```



### Ajout d'un sélecteur d'equipements {.unnumbered}  

Passons à la fonctionnalité suivante : un sélecteur d'equipements spécifiques. Nous allons implémenter un autre paramètre dans notre fonction afin de pouvoir le passer comme argument dans notre code. Voyons d'abord ce qu'il en est - il fonctionne sur les mêmes principes que les autres paramètres que nous avons mis en place. Mettons à jour et testons notre fonction.


```{r, echo = TRUE}

plot_epicurve <- function(data, district = "All", agegroup = "malaria_tot", facility = "All") {
  
  if (!("All" %in% district)) {
    data <- data %>%
      filter(District %in% district)
    
    plot_title_district <- stringr::str_glue("{paste0(district, collapse = ', ')} districts")
    
  } else {
    
    plot_title_district <- "all districts"
    
  }
  

  # si il n ya pas de données restantes, retourner NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  data <- data %>%
    filter(age_group == agegroup)
  
  

  # si il n ya pas de données restantes, retourner NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  if (agegroup == "malaria_tot") {
      agegroup_title <- "All ages"
  } else {
    agegroup_title <- stringr::str_glue("{str_remove(agegroup, 'malaria_rdt')} years")
  }
  
    if (!("All" %in% facility)) {
    data <- data %>%
      filter(location_name == facility)
    
    plot_title_facility <- facility
    
  } else {
    
    plot_title_facility <- "all facilities"
    
  }
  
  # s'il n'y a pas de données restantes, retourne NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }

  
  
  ggplot(data, aes(x = data_date, y = cases_reported)) +
    geom_col(width = 1, fill = "darkred") +
    theme_minimal() +
    labs(
      x = "date",
      y = "number of cases",
      title = stringr::str_glue("Malaria cases - {plot_title_district}; {plot_title_facility}"),
      subtitle = agegroup_title
    )
  
  
  
}
```

Testons ca:  

```{r, warning=F, message=F}

plot_epicurve(malaria_data, district = "Spring", agegroup = "malaria_rdt_0-4", facility = "Facility 1")

```



Avec tous les equipements présentes dans nos données, il n'est pas très clair quels equipements correspondent à quels districts - et l'utilisateur final ne le saura pas non plus. Cela pourrait rendre l'utilisation de l'application assez peu intuitive. Pour cette raison, nous devrions faire en sorte que les options des equipements dans l'interface utilisateur changent dynamiquement lorsque l'utilisateur change de district - de sorte que l'une filtre l'autre ! Puisque nous utilisons un grand nombre de variables dans les options, nous pourrions également vouloir générer certaines de nos options pour l'interface utilisateur dans notre fichier _global.R_ _à partir des données_. Par exemple, nous pouvons ajouter ce morceau de code à _global.R_ après avoir lu nos données :


```{r, , message =  FALSE}

all_districts <- c("All", unique(malaria_data$District))

# base de données des noms de lieux par district
facility_list <- malaria_data %>%
  group_by(location_name, District) %>%
  summarise() %>% 
  ungroup()

```

Let's look at them:  

```{r}
all_districts
```


```{r}
facility_list
```



Nous pouvons passer ces nouvelles variables à l'interface utilisateur sans aucun problème, puisqu'elles sont globalement visibles à la fois par le serveur et l'interface utilisateur ! Mettons à jour notre interface utilisateur :


```{r, eval = FALSE}


ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         # selecteur pour district
         selectInput(
              inputId = "select_district",
              label = "Select dsitrict",
              choices = all_districts,
              selected = "All",
              multiple = FALSE
         ),
         # selecteur pour age group
         selectInput(
              inputId = "select_agegroup",
              label = "Select age group",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         ),
         # selecteur poiur facility
         selectInput(
           inputId = "select_facility",
           label = "Select Facility",
           choices = c("All", facility_list$location_name),
           selected = "All"
         ),
         
         # ligne horizontale
         hr(),
         downloadButton(
           outputId = "download_epicurve",
           label = "Download plot"
         )

    ),

    mainPanel(
      # epicurve va ici
      plotOutput("malaria_epicurve"),
      br(),
      hr(),
      p("Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:"),
      tags$ul(
        tags$li(tags$b("location_name"), " - the facility that the data were collected at"),
        tags$li(tags$b("data_date"), " - the date the data were collected at"),
        tags$li(tags$b("submitted_daate"), " - the date the data were submitted at"),
        tags$li(tags$b("Province"), " - the province the data were collected at (all 'North' for this dataset)"),
        tags$li(tags$b("District"), " - the district the data were collected at"),
        tags$li(tags$b("age_group"), " - the age group the data were collected for (0-5, 5-14, 15+, and all ages)"),
        tags$li(tags$b("cases_reported"), " - the number of cases reported for the facility/age group on the given date")
      )
      
    )
    
  )
)


```



Remarquez comment nous passons maintenant des variables pour nos choix au lieu de les coder en dur dans l'interface utilisateur ! Cela pourrait également rendre notre code plus compact ! Enfin, nous devrons mettre à jour le serveur. Il sera facile de mettre à jour notre fonction pour incorporer notre nouvelle entrée (nous devons juste la passer comme argument à notre nouveau paramètre), mais nous devons nous rappeler que nous voulons aussi que l'interface utilisateur soit mise à jour dynamiquement lorsque l'utilisateur change le district sélectionné. Il est important de comprendre ici que nous *pouvons modifier les paramètres et le comportement des widgets* pendant l'exécution de l'application, mais que cela doit être fait *dans le serveur*. Nous devons comprendre une nouvelle façon d'envoyer des données au serveur pour apprendre à le faire.

Les fonctions dont nous avons besoin pour comprendre comment faire cela sont connues sous le nom de fonctions *observatrices*, et sont similaires aux fonctions *réactives* dans leur comportement. Elles présentent toutefois une différence essentielle :

- Les fonctions réactives n'affectent pas directement les sorties et produisent des objets qui peuvent être vus à d'autres endroits du serveur.
- Les fonctions d'observation *peuvent* affecter les sorties du serveur, mais le font via des effets secondaires d'autres fonctions. (Elles peuvent aussi faire d'autres choses, mais c'est leur principale fonction en pratique).

Comme pour les fonctions réactives, il existe deux types de fonctions d'observation, qui sont divisées par la même logique que les fonctions réactives :

1. `observe()` - cette fonction s'exécute à chaque fois que les entrées qu'elle contient changent.
2. `observeEvent()` - cette fonction s'exécute lorsqu'une entrée *spécifiée par l'utilisateur* change.

Nous devons également comprendre les fonctions fournies par Shiny qui mettent à jour les widgets. Elles sont assez simples à exécuter - elles prennent d'abord l'objet `session` de la fonction serveur (il n'est pas nécessaire de le comprendre pour l'instant), puis le `inputId` de la fonction à modifier. Nous passons ensuite de nouvelles versions de tous les paramètres qui sont déjà pris par `selectInput()` - ceux-ci seront automatiquement mis à jour dans le widget. 

Regardons un exemple isolé de la façon dont nous pourrions utiliser ceci dans notre serveur. Lorsque l'utilisateur change de district, nous voulons filtrer notre tableau d'installations par district, et mettre à jour les choix pour *seulement refléter ceux qui sont disponibles dans ce district* (et une option pour toutes les installations).

```{r, eval = FALSE}

observe({
  
  if (input$select_district == "All") {
    new_choices <- facility_list$location_name
  } else {
    new_choices <- facility_list %>%
      filter(District == input$select_district) %>%
      pull(location_name)
  }
  
  new_choices <- c("All", new_choices)
  
  updateSelectInput(session, inputId = "select_facility",
                    choices = new_choices)
  
})


```


Et voilà ! nous pouvons l'ajouter dans notre serveur, et ce comportement fonctionnera désormais. Voici à quoi devrait ressembler notre nouveau serveur :

```{r, eval = FALSE}
server <- function(input, output, session) {
  
  malaria_plot <- reactive({
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup, facility = input$select_facility)
  })
  
  
  
  observe({
    
    if (input$select_district == "All") {
      new_choices <- facility_list$location_name
    } else {
      new_choices <- facility_list %>%
        filter(District == input$select_district) %>%
        pull(location_name)
    }
    
    new_choices <- c("All", new_choices)
    
    updateSelectInput(session, inputId = "select_facility",
                      choices = new_choices)
    
  })
  
  
  output$malaria_epicurve <- renderPlot(
    malaria_plot()
  )
  
  output$download_epicurve <- downloadHandler(
    
    filename = function() {
      stringr::str_glue("malaria_epicurve_{input$select_district}.png")
    },
    
    content = function(file) {
      ggsave(file, 
             malaria_plot(),
             width = 8, height = 5, dpi = 300)
    }
    
  )
  
  
  
}

```


```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "shiny", "app_menu_view.gif"))
```







### Ajout d'un autre onglet avec une table {.unnumbered}


Nous allons maintenant passer au dernier composant que nous voulons ajouter à notre application. Nous voulons séparer notre interface utilisateur en deux onglets, dont l'un comportera un tableau interactif où l'utilisateur pourra voir les données avec lesquelles il réalise la courbe épidémique. Pour ce faire, nous pouvons utiliser les éléments d'interface intégrés qui sont fournis avec Shiny pour les onglets. À un niveau de base, nous pouvons enfermer la plupart de notre panneau principal dans cette structure générale :

```{r, eval = FALSE}


# ... le reste de l'ui

mainPanel(
  
  tabsetPanel(
    type = "tabs",
    tabPanel(
      "Epidemic Curves",
      ...
    ),
    tabPanel(
      "Data",
      ...
    )
  )
)


```


Appliquons cela à notre interface utilisateur. Nous voudrons également utiliser le package **DT** ici - c'est un excellent package pour créer des tableaux interactifs à partir de données préexistantes. Nous pouvons voir qu'il est utilisé pour `DT::datatableOutput()` dans cet exemple.

```{r, echo = FALSE}
library(DT)
```

```{r, eval = FALSE}
ui <- fluidPage(
     
     titlePanel("Malaria facility visualisation app"),
     
     sidebarLayout(
          
          sidebarPanel(
               # sélectionneur pour le district
               selectInput(
                    inputId = "select_district",
                    label = "Select district",
                    choices = all_districts,
                    selected = "All",
                    multiple = FALSE
               ),
               # selector for age group
               selectInput(
                    inputId = "select_agegroup",
                    label = "Select age group",
                    choices = c(
                         "All ages" = "malaria_tot",
                         "0-4 yrs" = "malaria_rdt_0-4",
                         "5-14 yrs" = "malaria_rdt_5-14",
                         "15+ yrs" = "malaria_rdt_15"
                    ), 
                    selected = "All",
                    multiple = FALSE
               ),
               # selector for facility
               selectInput(
                    inputId = "select_facility",
                    label = "Select Facility",
                    choices = c("All", facility_list$location_name),
                    selected = "All"
               ),
               
               # horizontal line
               hr(),
               downloadButton(
                    outputId = "download_epicurve",
                    label = "Download plot"
               )
               
          ),
          
          mainPanel(
               tabsetPanel(
                    type = "tabs",
                    tabPanel(
                         "Epidemic Curves",
                         plotOutput("malaria_epicurve")
                    ),
                    tabPanel(
                         "Data",
                         DT::dataTableOutput("raw_data")
                    )
               ),
               br(),
               hr(),
               p("Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:"),
               tags$ul(
                    tags$li(tags$b("location_name"), " - the facility that the data were collected at"),
                    tags$li(tags$b("data_date"), " - the date the data were collected at"),
                    tags$li(tags$b("submitted_daate"), " - the date the data were submitted at"),
                    tags$li(tags$b("Province"), " - the province the data were collected at (all 'North' for this dataset)"),
                    tags$li(tags$b("District"), " - the district the data were collected at"),
                    tags$li(tags$b("age_group"), " - the age group the data were collected for (0-5, 5-14, 15+, and all ages)"),
                    tags$li(tags$b("cases_reported"), " - the number of cases reported for the facility/age group on the given date")
               )
               
               
          )
     )
)


```



Maintenant notre application est organisée en onglets ! Faisons également les modifications nécessaires sur le serveur. Puisque nous n'avons pas besoin de manipuler notre jeu de données avant de le rendre, c'est en fait très simple - nous rendons simplement le jeu de données malaria_data via DT::renderDT() à l'interface utilisateur !


```{r, eval = FALSE}
server <- function(input, output, session) {
  
  malaria_plot <- reactive({
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup, facility = input$select_facility)
  })
  
  
  
  observe({
    
    if (input$select_district == "All") {
      new_choices <- facility_list$location_name
    } else {
      new_choices <- facility_list %>%
        filter(District == input$select_district) %>%
        pull(location_name)
    }
    
    new_choices <- c("All", new_choices)
    
    updateSelectInput(session, inputId = "select_facility",
                      choices = new_choices)
    
  })
  
  
  output$malaria_epicurve <- renderPlot(
    malaria_plot()
  )
  
  output$download_epicurve <- downloadHandler(
    
    filename = function() {
      stringr::str_glue("malaria_epicurve_{input$select_district}.png")
    },
    
    content = function(file) {
      ggsave(file, 
             malaria_plot(),
             width = 8, height = 5, dpi = 300)
    }
    
  )
  
  # rendre le tableau de données à ui
  output$raw_data <- DT::renderDT(
    malaria_data
  )
  
  
}


```


```{r, out.width=c('100%', '100%'), fig.show='hold', echo = F, fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "shiny", "app_table_view.gif"))
```


## Partager les applications shiny


Maintenant que vous avez développé votre application, vous voulez probablement la partager avec d'autres - c'est le principal avantage de shiny après tout ! Nous pouvons le faire en partageant le code directement, ou nous pouvons le publier sur un serveur. Si nous partageons le code, d'autres personnes pourront voir ce que vous avez fait et s'en inspirer, mais cela annulera l'un des principaux avantages de shiny - *il peut éliminer le besoin pour les utilisateurs finaux de maintenir une installation R*. Pour cette raison, si vous partagez votre application avec des utilisateurs qui ne sont pas à l'aise avec R, il est beaucoup plus facile de partager une application qui a été publiée sur un serveur. 

Si vous préférez partager le code, vous pouvez créer un fichier .zip de l'application ou, mieux encore, *publier votre application sur github et ajouter des collaborateurs*.

Cependant, si nous publions l'application en ligne, nous devons faire un peu plus de travail. En fin de compte, nous voulons que votre application soit accessible via une URL Web afin que d'autres puissent y accéder rapidement et facilement. Malheureusement, pour publier votre application sur un serveur, vous devez avoir accès à un serveur sur lequel la publier ! Il existe un certain nombre d'options d'hébergement à cet égard :

- _shinyapps.io_ : c'est l'endroit le plus facile pour publier des applications shinys, car il nécessite le moins de travail de configuration possible et offre des licences gratuites, mais limitées.

- _RStudio Connect_ : il s'agit d'une version beaucoup plus puissante d'un serveur R, qui peut effectuer de nombreuses opérations, y compris la publication de shiny apps. Elle est cependant plus difficile à utiliser et moins recommandée pour les utilisateurs débutants.

Pour les besoins de ce document, nous utiliserons _shinyapps.io_, car il est plus facile pour les premiers utilisateurs. Vous pouvez créer un compte gratuit ici pour commencer - il y a également différents plans de prix pour les licesnses de serveur si nécessaire. Plus vous prévoyez d'avoir d'utilisateurs, plus votre plan tarifaire devra être cher, alors tenez-en compte. Si vous cherchez à créer quelque chose à l'usage d'un petit groupe d'individus, une licence gratuite peut convenir parfaitement, mais une application destinée au public peut nécessiter plus de licences.

Tout d'abord, nous devons nous assurer que notre application est adaptée à la publication sur un serveur. Dans votre application, vous devez redémarrer votre session R et vous assurer qu'elle fonctionne sans exécuter de code supplémentaire. C'est important, car une application qui nécessite le chargement de packages ou la lecture de données non définis dans le code de votre application ne fonctionnera pas sur un serveur. Notez également que vous ne pouvez pas avoir de chemins de fichiers *explicites* dans votre application - ceux-ci seront invalides dans le paramétrage du serveur - l'utilisation du package `here` résout très bien ce problème. Enfin, si vous lisez des données à partir d'une source qui nécessite une authentification de l'utilisateur, comme les serveurs de votre organisation, cela ne fonctionnera généralement pas sur un serveur. Vous devrez vous mettre en relation avec votre service informatique pour savoir comment mettre le serveur shiny  sur la whitelist.

*Création d'un compte*

Une fois que vous avez votre compte, vous pouvez naviguer vers la page des jetons sous _Accounts_. Ici, vous devriez ajouter un nouveau jeton - il sera utilisé pour déployer votre application. 

A partir de là, vous devez noter que l'url de votre compte reflétera le nom de votre application - donc si votre application s'appelle _mon_app_, l'url sera ajouté comme _xxx.io/mon_app/_. Choisissez judicieusement le nom de votre application ! Maintenant que vous êtes prêt, cliquez sur deploy - si vous réussissez, votre application sera lancée sur l'url que vous avez choisi !

*Quelque chose sur la création d'applications dans des documents ?*

## Lecture complémentaire

Jusqu'à présent, nous avons couvert beaucoup d'aspects de shiny, et nous avons à peine effleuré la surface de ce qui est offert pour shiny. Bien que ce guide serve d'introduction, il y a beaucoup plus à apprendre pour comprendre pleinement shiny. Vous devriez commencer à créer des applications et ajouter progressivement de plus en plus de fonctionnalités.


## packages d'extension recommandés

Ce qui suit représente une sélection d'extensions de haute qualité pour shiny qui peuvent vous aider à obtenir beaucoup plus de shiny. Sans ordre particulier :

- **shinyWidgets** - ce package vous donne beaucoup plus de widgets qui peuvent être utilisés dans votre application. Lancez `shinyWidgets::shinyWidgetsGallery()` pour voir une sélection des widgets disponibles avec ce package. Voir des exemples [ici](https://github.com/dreamRs/shinyWidgets)  

- **shinyjs** - c'est un excellent package qui donne à l'utilisateur la possibilité d'étendre considérablement l'utilité de shiny via une série de javascript. Les applications de ce package vont de très simples à très avancées, mais vous voudrez peut-être l'utiliser d'abord pour manipuler l'interface utilisateur de manière simple, comme cacher/afficher des éléments, ou activer/désactiver des boutons. En savoir plus [ici](https://deanattali.com/shinyjs/basic)

- **shinydashboard** - ce package étend massivement l'interface utilisateur disponible qui peut être utilisée dans shiny, en particulier en permettant à l'utilisateur de créer un tableau de bord complexe avec une variété de mises en page complexes. Voir plus [ici](https://rstudio.github.io/shinydashboard/)

- **shinydashboardPlus** - Obtenez encore plus de fonctionnalités du framework **shinydashboard** ! En savoir plus [ici](https://rinterface.github.io/shinydashboardPlus/articles/shinydashboardPlus.html)

- **shinythemes** - changez le thème css par défaut de votre application shiny avec une large gamme de modèles prédéfinis ! En savoir plus [ici](https://rstudio.github.io/shinythemes/)


Il existe également un certain nombre de packages qui peuvent être utilisés pour créer des sorties interactives compatibles avec shiny. 

- **DT** est semi-incorporé dans base-shiny, mais fournit un grand ensemble de fonctions pour créer des tableaux interactifs.

- **plotly** est un package pour créer des graphiques interactifs que l'utilisateur peut manipuler dans l'application. Vous pouvez également convertir vos graphiques en versions interactives via `plotly::ggplotly()` ! Comme alternatives, **dygraphs** et **highcharter** sont également excellents.


## Ressources recommandées




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/shiny_basics.Rmd-->

# (PART) Miscellaneous {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_misc.Rmd-->


# Fonctions d'écriture {#writing_functions}


<!-- ======================================================= -->
## Préparation { }


### Load packages {-}

Ce morceau de code montre le chargement des paquets nécessaires aux analyses. Dans ce manuel, nous mettons l'accent sur `p_load()` de **pacman**, qui installe le paquet si nécessaire *et* le charge pour l'utiliser. Vous pouvez aussi charger les paquets installés avec `library()` de **base** R. Voir la page sur [R basics] pour plus d'informations sur les paquets R.  

```{r, echo=F, warning=F, message=F}
pacman::p_load(
  rio, # Importation de fichiers
  here, # Localisateur de fichiers
  skimr, # obtenir une vue d'ensemble des données
  tidyverse, # gestion des données + graphiques ggplot2, 
  gtsummary, # statistiques et tests sommaires
  janitor, # ajout de totaux et de pourcentages aux tableaux
  scales, # conversion facile des proportions en pourcentages  
  flextable, # convertir les tableaux en HTML
  purrr, # facilite la programmation fonctionnelle
  readr, #pour lire les fichiers csv
  highcharter #pour créer un objet highchart et dessiner un graphique particulier

  )
```

### Importer des données {-}

Nous importons le jeu de données des cas d'une épidémie d'Ebola simulée. Si vous souhaitez télécharger les données pour les suivre pas à pas, consultez les instructions de la page [Télécharger le manuel et les données](#download_book_data). Le jeu de données est importé à l'aide de la fonction `import()` du paquet **rio**. Voir la page [Importer et exporter des données](#import_export) pour les différentes manières d'importer des données.

Nous utiliserons également dans la dernière partie de cette page des données sur la grippe H7N9 de 2013.

```{r, echo=F}
# Importez les linelists dans R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

flu_china <- rio::import(here::here("data", "case_linelists", "fluH7N9_China_2013.csv"))

```


## Fonctions  

Les fonctions sont utiles en programmation car elles permettent de rendre les codes plus faciles à comprendre, plus courts et moins sujets aux erreurs (à condition qu'il n'y ait pas d'erreurs dans la fonction elle-même).

Si vous êtes arrivé jusqu'à ce manuel, cela signifie que vous avez rencontré d'innombrables fonctions, car en R, chaque opération est un appel de fonction.
`+, for, if, [, $, { â¦`. Par exemple, `x + y` est la même chose que `'+'(x, y)`.

R est l'un des langages qui offre le plus de possibilités de travailler avec des fonctions et qui donne suffisamment d'outils à l'utilisateur pour les écrire facilement. Nous ne devrions pas penser aux fonctions comme étant fixées au sommet ou à la fin de la chaîne de programmation, R offre la possibilité de les utiliser comme s'il s'agissait de vecteurs et même de les utiliser à l'intérieur d'autres fonctions, listes...

Il existe de nombreuses ressources très avancées sur la programmation fonctionnelle et nous ne donnerons ici qu'un aperçu pour vous aider à démarrer avec la programmation fonctionnelle avec de courts exemples pratiques. Nous vous encourageons ensuite à visiter les liens sur les références pour en savoir plus.





## Pourquoi utiliser une fonction ? 

Avant de répondre à cette question, il est important de noter que vous avez déjà eu des conseils pour écrire vos toutes premières fonctions R dans la page sur [l'itération, les boucles et les listes] de ce manuel. En fait, l'utilisation de "if/else" et de boucles est souvent au cour de bon nombre de nos fonctions car elles permettent d'élargir l'application de notre code en autorisant des conditions multiples ou d'itérer des codes pour des tâches répétitives.

- Je répète plusieurs fois le même bloc de code pour l'appliquer à une variable ou à des données différentes ?

- Si je m'en débarrasse, cela raccourcira-t-il considérablement mon code global et le rendra-t-il plus rapide ?

- Est-il possible que le code que j'ai écrit soit réutilisé mais avec une valeur différente à plusieurs endroits du code ?

Si la réponse à l'une des questions précédentes est "OUI", alors vous avez probablement besoin d'écrire une fonction

## Comment R construit-il les fonctions ?

Les fonctions dans R ont trois composants principaux :

- le `formals()` qui est la liste d'arguments qui contrôle la façon dont nous pouvons appeler la fonction.

- le `body()` qui est le code à l'intérieur de la fonction, c'est-à-dire entre les parenthèses ou à la suite des parenthèses, selon la façon dont on l'écrit.

et,

- l' `environnement()` qui aide à localiser les variables de la fonction et détermine comment la fonction trouve sa valeur.
 
Une fois que vous avez créé votre fonction, vous pouvez vérifier chacun de ces composants en appelant la fonction associée.
 

## Syntaxe et structure de base

- Une fonction devra être nommée correctement afin que son travail soit facilement compréhensible dès que l'on lit son nom. En fait, c'est déjà le cas avec la majorité de l'architecture R de base. Des fonctions comme `mean()`, `print()`, `summary()` ont des noms qui sont très simples. 

- Une fonction a besoin d'arguments, comme les données sur lesquelles elle travaille et d'autres objets qui peuvent être des valeurs statiques, entre autres options.  

- Et enfin, une fonction donnera une sortie basée sur sa tâche principale et les arguments qui lui ont été donnés. Habituellement, nous utilisons les fonctions intégrées telles que `print()`, `return()`... pour produire la sortie. La sortie peut être une valeur logique, un nombre, un caractère, un cadre de données... en bref, tout type d'objet R.

En gros, c'est la composition d'une fonction :

```{r, eval=FALSE}

nom_fonction <- function(argument_1, argument_2, argument_3){
  
           function_task
  
           return(output)
}


```

Nous pouvons créer notre première fonction qui sera appelée `contain_covid19()``. 

```{r}

contain_covid19 <- function(barrier_gest, wear_mask, get_vaccine){
  
                            if(barrier_gest == "yes" & wear_mask == "yes" & get_vaccine == "yes" ) 
       
                            return("success")
  
  else("please make sure all are yes, this pandemic has to end!")
}


```

Nous pouvons ensuite vérifier les composants de notre fonction nouvellement créée.

```{r}

formals(contain_covid19)
body(contain_covid19)
environment(contain_covid19)

```


Maintenant, nous allons tester notre fonction. Pour appeler notre fonction écrite, vous l'utilisez comme vous utilisez toutes les fonctions R, c'est-à-dire en écrivant le nom de la fonction et en ajoutant les arguments requis.

```{r}

contain_covid19(barrier_gest = "yes", wear_mask = "yes", get_vaccine = "yes")

```

Par précaution, nous pouvons réécrire le nom de chaque argument. Mais sans les préciser, le code devrait fonctionner puisque R a en mémoire le positionnement de chaque argument. Ainsi, tant que vous mettez les valeurs des arguments dans le bon ordre, vous pouvez éviter d'écrire les noms des arguments lors de l'appel des fonctions.

```{r}

contain_covid19("yes", "yes", "yes")

```

Voyons ensuite ce qui se passe si l'une des valeurs est `"no"` ou **pas** `"yes"`.

```{r}

contain_covid19(barrier_gest = "yes", wear_mask = "yes", get_vaccine = "no")
```

Si nous fournissons un argument qui n'est pas reconnu, nous obtenons une erreur: 

```{r, eval=F}
contain_covid19(barrier_gest = "sometimes", wear_mask = "yes", get_vaccine = "no")
```

`Erreur dans contain_covid19(barrier_gest = "sometimes", wear_mask = "yes", : 
  Impossible de trouver la fonction "contain_covid19"``.


<span style="color : black ;">**_NOTE:_** Certaines fonctions (la plupart du temps très courtes et simples) peuvent ne pas avoir besoin de nom et peuvent être utilisées directement sur une ligne de code ou à l'intérieur d'une autre fonction pour effectuer une tâche rapide. Elles sont appelées **fonctions anonymes** .</span>

Par exemple ci-dessous est une première fonction anonyme qui ne garde que les variables de caractères le jeu de données.

```{r, eval=F}
linelist %>% 
  dplyr::slice_head(n=10) %>% #équivalent à la fonction "head" de base de R et qui renvoie les n premières observations de l'ensemble de données.
  select(function(x) is.character(x)) 
```
  
```{r, echo=F}
linelist %>% 
  dplyr::slice_head(n=10) %>% #équivalent de la fonction "head" de base de R et qui retourne les n premières observations de l'ensemble de données
  select(function(x) is.character(x)) %>%  
DT::datatable(rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```


Ensuite, une autre fonction qui sélectionne une observation sur deux de notre ensemble de données (cela peut être utile lorsque nous avons des données longitudinales avec de nombreux enregistrements par patient, par exemple après avoir été classés par date ou par visite).
Dans ce cas, la fonction à écrire en dehors de dplyr serait `function (x) (x%%2 == 0)` pour s'appliquer au vecteur contenant tous les numéros de ligne.


```{r, eval=F}
linelist %>%   
   slice_head(n=20) %>% 
   tibble::rownames_to_column() %>% # ajoute les indices de chaque obs comme rownames pour voir clairement la sélection finale
   filter(row_number() %%2 == 0)
```

```{r, echo=F}
linelist %>%   
   slice_head(n=20) %>% 
   tibble::rownames_to_column() %>% # ajoute les indices de chaque obs comme rownames pour voir clairement la sélection finale
   filter(row_number() %%2 == 0) %>% 
DT::datatable(rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )

```


Un code R de base possible pour la même tâche serait le suivant :

```{r, eval = F}

linelist_firstobs <- head(linelist, 20)

linelist_firstobs[base::Filter(function(x) (x%%2 == 0), seq(nrow(linelist_firstobs))),]
```

```{r, echo=F}

linelist_firstobs <- head(linelist, 20)

linelist_firstobs[base::Filter(function(x) (x%%2 == 0), seq(nrow(linelist_firstobs))),] %>% 
DT::datatable(rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )

```


<span style="color : orange ;">**_CAUTION:_** S'il est vrai que l'utilisation de fonctions peut nous aider dans notre code, il peut néanmoins être long d'écrire certaines fonctions ou d'en corriger une si elle n'a pas été pensée en profondeur, écrite de manière adéquate et qu'elle renvoie des erreurs en conséquence. C'est pour cette raison qu'il est souvent recommandé d'écrire d'abord le code R, de s'assurer qu'il fait ce que nous voulons qu'il fasse, puis de le transformer en une fonction avec ses trois composants principaux tels que listés ci-dessus. </span>

## Exemples  

### Retourner les tableaux de proportion pour plusieurs colonnes {.unnumbered}  

Oui, nous avons déjà de belles fonctions dans de nombreux paquets permettant de résumer des informations d'une manière très simple et agréable. Mais nous allons tout de même essayer de créer nos propres fonctions, lors de nos premiers pas dans l'écriture de fonctions.

Dans cet exemple, nous voulons montrer comment l'écriture d'une simple fonction vous évitera de copier-coller le même code plusieurs fois.

```{r}

proptab_multiple <- function(my_data, var_to_tab){
  
  #imprimez le nom de chaque variable d'intérêt avant de faire la tabulation
  print(var_to_tab)

  with(my_data,
       rbind( #liez les résultats des deux fonctions suivantes par ligne
        #tabuler la variable d'intérêt: ne donne que des nombres
          table(my_data[[var_to_tab]], useNA = "no"),
          #calculer les proportions pour chaque variable d'intérêt et arrondir la valeur à 2 décimales
         round(prop.table(table(my_data[[var_to_tab]]))*100,2)
         )
       )
}


proptab_multiple(linelist, "gender")

proptab_multiple(linelist, "age_cat")

proptab_multiple(linelist, "outcome")


```

<span style="color : darkgreen ;">**_TIP:_** Comme indiqué ci-dessus, il est très important de commenter vos fonctions comme vous le feriez pour la programmation générale. Gardez à l'esprit que le but d'une fonction est de rendre un code facile à lire, plus court et plus efficace. Alors on devrait être capable de comprendre ce que fait la fonction juste en lisant son nom et avoir plus de détails en lisant les commentaires.</span>


Une deuxième option est d'utiliser cette fonction dans une autre via une boucle pour faire le processus en une fois :

```{r}


for(var_to_tab in c("gender", "age_cat", "outcome")){
  
  print(proptab_multiple(linelist, var_to_tab))
  
}

```

Une manière plus simple serait d'utiliser la base R "appliquer" au lieu d'une "boucle for" comme exprimé ci-dessous :

```{r, include=FALSE, eval=FALSE}

base::lapply(linelist[,c("gender", "age_cat", "outcome")], table)

```


<span style="color : darkgreen ;">**_TIP:_** R est souvent défini comme un langage de programmation fonctionnel et presque chaque fois que vous exécutez une ligne de code, vous utilisez certaines fonctions intégrées. Une bonne habitude pour être plus à l'aise avec l'écriture de fonctions est d'avoir souvent un regard interne sur la façon dont les fonctions de base que vous utilisez quotidiennement sont construites. Le raccourci pour le faire est de sélectionner le nom de la fonction puis de cliquer sur `Ctrl+F2` ou `fn+F2` ou `Cmd+F2` (selon votre ordinateur) .</span>

### Utilisation de **purrr** : écrire des fonctions qui peuvent être appliquées de manière itérative.

### Modifier la classe de plusieurs colonnes dans un ensemble de données {.unnumbered}  

Disons que de nombreuses variables de caractère dans les données originales `linelist` doivent être changées en "factor" pour des raisons d'analyse et de traçage. Au lieu de répéter l'étape plusieurs fois, nous pouvons juste utiliser `lapply()` pour faire la transformation de toutes les variables concernées sur une seule ligne de code.


<span style="color : orange ;">**_CAUTION:_** `lapply()` renvoie une liste, donc son utilisation peut nécessiter une modification supplémentaire en dernière étape.</span>


```{r, include=FALSE}

linelist_factor1 <- linelist %>%
      lapply(
          function(x) if(is.character(x)) as.factor(x) else x) %>%
      as.data.frame() %>% 
      glimpse()

```


La même étape peut être effectuée en utilisant la fonction `map_if()` du paquet **purrr**.

```{r}

linelist_factor2 <- linelist %>%
  purrr::map_if(is.character, as.factor)

linelist_factor2 %>%
        glimpse()

```


### Produire itérativement des graphiques pour différents niveaux d'une variable {.unnumbered}

Nous allons produire ici un graphique circulaire pour examiner la distribution des résultats des patients en Chine pendant l'épidémie de H7N9 pour chaque province. Au lieu de répéter le code pour chacun d'entre eux, nous allons simplement appliquer une fonction que nous allons créer.

```{r}

#Préciser les options pour l'utilisation de highchart
options(highcharter.theme =  highcharter::hc_theme_smpl(tooltip = list(valueDecimals = 2)))


#créer une fonction appelée "chart_outcome_province" qui prend comme argument l'ensemble de données et le nom de la province pour laquelle on veut tracer la distribution du résultat.

chart_outcome_province <- function(data_used, prov){
  
  tab_prov <- data_used %>% 
    filter(province == prov,
           !is.na(outcome))%>% 
    group_by(outcome) %>% 
    count() %>%
    adorn_totals(where = "row") %>% 
    adorn_percentages(denominator = "col", )%>%
    mutate(
        perc_outcome= round(n*100,2))
  
  
  tab_prov %>%
    filter(outcome != "Total") %>% 
  highcharter::hchart(
    "pie", hcaes(x = outcome, y = perc_outcome),
    name = paste0("Distribution du résultat en :", prov)
    )
  
}

chart_outcome_province(flu_china, "Shanghai")
chart_outcome_province(flu_china, "Zhejiang")
chart_outcome_province(flu_china, "Jiangsu")


```



### Produire itérativement des tableaux pour différents niveaux d'une variable {.unnumbered}

Ici, nous allons créer trois indicateurs à résumer dans un tableau et nous voudrions produire ce tableau pour chacune des provinces. Nos indicateurs sont le délai entre l'apparition et l'hospitalisation, le pourcentage de guérison et l'âge médian des cas.

```{r}


indic_1 <- flu_china %>% 
  group_by(province) %>% 
  mutate(
    date_hosp= strptime(date_of_hospitalisation, format = "%m/%d/%Y"),
    date_ons= strptime(date_of_onset, format = "%m/%d/%Y"), 
    delay_onset_hosp= as.numeric(date_hosp - date_ons)/86400,
    mean_delay_onset_hosp = round(mean(delay_onset_hosp, na.rm=TRUE ), 0)) %>%
  select(province, mean_delay_onset_hosp) %>% 
  distinct()
     

indic_2 <- flu_china %>% 
            filter(!is.na(outcome)) %>% 
            group_by(province, outcome) %>% 
            count() %>%
            pivot_wider(names_from = outcome, values_from = n) %>% 
    adorn_totals(where = "col") %>% 
    mutate(
        perc_recovery= round((Recover/Total)*100,2))%>% 
  select(province, perc_recovery)
    
    
    
indic_3 <- flu_china %>% 
            group_by(province) %>% 
            mutate(
                    median_age_cases = median(as.numeric(age), na.rm = TRUE)
            ) %>% 
  select(province, median_age_cases) %>% 
  distinct()

#Joindre les trois ensembles de données d'indicateurs

table_indic_all <- indic_1 %>% 
  dplyr::left_join(indic_2, by = "province") %>% 
        left_join(indic_3, by = "province")


#Imprimez les indicateurs dans un tableau mobile


print_indic_prov <- function(table_used, prov){
  
  #d'abord transformer un peu le dataframe pour faciliter l'impression.
  indic_prov <- table_used %>%
    filter(province==prov) %>%
    pivot_longer(names_to = "Indicateurs", cols = 2:4) %>% 
   mutate( indic_label = factor(Indicateurs,
   levels= c("mean_delay_onset_hosp", "perc_recovery", "median_age_cases"),
   labels=c("Délai moyen d'apparition hôpital", "Pourcentage de récupération", "Âge médian des cas"))
   ) %>% 
    ungroup(province) %>% 
    select(indic_label, value)
  

    tab_print <- flextable(indic_prov) %>%
    theme_vanilla() %>% 
    flextable::fontsize(part = "body", size = 10) 
    
    
     tab_print <- tab_print %>% 
                  autofit() %>%
                  set_header_labels( 
                indic_label= "Indicateurs", value= "Estimation") %>%
    flextable::bg( bg = "darkblue", part = "header") %>%
    flextable::bold(part = "header") %>%
    flextable::color(color = "white", part = "header") %>% 
    add_header_lines(values = paste0("Indicateurs pour la province de : ", prov)) %>% 
bold(part = "header")
 
 tab_print <- set_formatter_type(tab_print,
   fmt_double = "%.2f",
   na_str = "-")

tab_print 
    
}




print_indic_prov(table_indic_all, "Shanghai")
print_indic_prov(table_indic_all, "Jiangsu")


```


## Conseils et meilleures pratiques pour des fonctions bien rodées

La programmation fonctionnelle a pour but d'alléger le code et d'en faciliter la lecture. Elle devrait produire le contraire. Les conseils ci-dessous vous aideront à avoir un code propre et facile à lire. 


### Nommage et syntaxe {.unnumbered}

- Evitez d'utiliser des caractères qui auraient pu être facilement pris par d'autres fonctions déjà existantes dans votre environnement.

- Il est recommandé que le nom de la fonction soit court et facile à comprendre pour un autre lecteur.

- Il est préférable d'utiliser des verbes pour le nom de la fonction et des noms pour les noms des arguments.


### Noms de colonnes et évaluation ordonnée {.unnumbered}  

Si vous voulez savoir comment référencer les *noms de colonnes* qui sont fournis à votre code en tant qu'arguments, lisez ce [guide de programmation tidyverse](https://dplyr.tidyverse.org/articles/programming.html). Parmi les sujets abordés figurent *l'évaluation tidée* et l'utilisation de l'accolade double `{{ }}`.

Par exemple, voici un squelette de code rapide tiré du tutoriel de la page mentionnée juste au-dessus :  

```{r, eval=F}

var_summary <- function(data, var) {
  data %>%
    summarise(n = n(), min = min({{ var }}), max = max({{ var }})))
}
mtcars %>% 
  group_by(cyl) %>% 
  var_summary(mpg)

```


### Test et gestion des erreurs {.unnumbered}

Plus la tâche d'une fonction est compliquée, plus la possibilité d'erreurs est élevée. Il est donc parfois nécessaire d'ajouter une vérification dans la fonction pour aider à comprendre rapidement d'où vient l'erreur et trouver un moyen de la corriger.

- Il peut être plus que recommandé d'introduire une vérification de l'absence d'un argument en utilisant `missing(argument)`. Cette simple vérification peut retourner la valeur "VRAI" ou "FAUX".

```{r , error=TRUE}

contain_covid19_missing <- function(barrier_gest, wear_mask, get_vaccine){
  
  if (missing(barrier_gest)) (print("please provide arg1"))
  if (missing(wear_mask)) print("please provide arg2")
  if (missing(get_vaccine)) print("please provide arg3")


  if (!barrier_gest == "yes" | wear_mask == "yes" | get_vaccine == "yes" ) 
       
       return ("you can do better")
  
  else("please make sure all are yes, this pandemic has to end!")
}


contain_covid19_missing(get_vaccine = "yes")

```


- Utilisez `stop()` pour les erreurs plus faciles à détecter.

```{r, error=TRUE}

contain_covid19_stop <- function(barrier_gest, wear_mask, get_vaccine){
  
  if(!is.character(barrier_gest)) (stop("arg1 should be a character, please enter the value with `yes`, `no` or `sometimes`"))
  
  if(barrier_gest == "yes" & wear_mask == "yes" & get_vaccine == "yes" ) 
       
       return ("success")
  
  else("please make sure all are yes, this pandemic has to end!")
}


contain_covid19_stop(barrier_gest=1, wear_mask="yes", get_vaccine = "no")

```

- Comme nous le voyons lorsque nous exécutons la plupart des fonctions intégrées, des messages et des avertissements peuvent apparaître dans certaines conditions. Nous pouvons les intégrer dans nos fonctions écrites en utilisant les fonctions `message()` et `warning()`.

- Nous pouvons également gérer les erreurs en utilisant la fonction `safely()` qui prend une fonction en argument et l'exécute de manière sûre. En fait, la fonction s'exécutera sans s'arrêter si elle rencontre une erreur. `safely()` retourne en sortie une **liste** avec deux objets qui sont les résultats et l'erreur qu'elle a "sautée".

Nous pouvons vérifier en exécutant d'abord la fonction `mean()`, puis en l'exécutant avec `safely()`.


```{r, warning=FALSE}

map(linelist, mean)
```


```{r, warning=FALSE}
safe_mean <- safely(mean)
linelist %>% 
  map(safe_mean)

```


Comme dit précédemment, bien commenter nos codes est déjà un bon moyen d'avoir de la documentation dans notre travail.  


<!-- ======================================================= -->
## Ressources


[Lien vers R pour la science des données](https://r4ds.had.co.nz/functions.html)   

[Cheatsheet advance R programming](https://www.rstudio.com/wp-content/uploads/2016/02/advancedR.pdf)

[Cheatsheet purr Package](https://purrr.tidyverse.org/)

[Video-ACM talk by Hadley Wickham : Les joies de la programmation fonctionnelle (comment fonctionne map_dbl)](https://youtube.videoken.com/embed/bzUmK0Y07ck)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/writing_functions.Rmd-->

# Interactions avec les répertoires {#directories}  

Dans cette page, nous couvrons les scénarios courants où vous créez, interagissez avec, enregistrez et importez avec des répertoires (dossiers).  


## Préparation  

### Paquet **fs** {.unnumbered}  

Le paquet **fs** est un paquet **tidyverse** qui facilite les interactions avec les répertoires, en améliorant certaines des fonctions **base** de R. Dans les sections ci-dessous, nous utiliserons souvent des fonctions de **fs**.  

```{r}
pacman::p_load(
  fs, # interactions fichiers/répertoires
  rio, # importation/exportation
  here, # chemins d'accès relatifs aux fichiers
  tidyverse) # gestion et visualisation des données
```


### Imprimer le répertoire comme un arbre de dendrogramme {.unnumbered}  

Utilisez la fonction `dir_tree()` de **fs**.  

Fournissez le chemin d'accès au dossier dans `path = ` et décidez si vous voulez afficher un seul niveau (`recurse = FALSE`) ou tous les fichiers de tous les sous-niveaux (`recurse = TRUE`). Ci-dessous, nous utilisons `here()` comme raccourci pour le projet R et spécifions son sous-dossier "data", qui contient toutes les données utilisées pour ce manuel R. Nous le paramétrons pour afficher tous les fichiers contenus dans "data" et ses sous-dossiers (par exemple "cache", "modèles épidémiques", "population", "shp" et "weather").  


```{r}
fs::dir_tree(path = here("data"), recurse = TRUE)
```


## Lister les fichiers d'un répertoire  

Pour lister uniquement les noms de fichiers d'un répertoire, vous pouvez utiliser `dir()` à partir de **base** R. Par exemple, cette commande liste les noms des fichiers contenus dans le sous-dossier "population" du dossier "data" d'un projet R. Le chemin de fichier relatif est fourni en utilisant `here()`. (dont vous trouverez plus d'informations sur la page [Importer et exporter des données](#import_export) ).  

```{r}
# noms de fichiers
dir(here("data", "gis", "population"))
```

Pour lister les chemins complets des fichiers du répertoire, vous pouvez utiliser `dir_ls()` de **fs**. Une alternative R **base** est `list.files()`.  

```{r}
# chemins d'accès aux fichiers
dir_ls(here("data", "gis", "population"))
```

Pour obtenir toutes les informations sur les métadonnées de chaque fichier d'un répertoire (par exemple le chemin, la date de modification, etc.), vous pouvez utiliser `dir_info()` de **fs**.  

Cela peut être particulièrement utile si vous voulez extraire la date de dernière modification du fichier, par exemple si vous voulez importer la version la plus récente d'un fichier. Pour un exemple de ceci, voir la page [Importer et exporter des données](#import_export).     

```{r, eval=F}
# informations sur le fichier
dir_info(here("data", "gis", "population"))
```

Voici le cadre de données renvoyé. Faites défiler vers la droite pour voir toutes les colonnes.  

```{r, echo=F}
DT::datatable(dir_info(here("data", "gis", "population")), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

## Informations sur les fichiers  

Pour extraire des informations de métadonnées sur un fichier spécifique, vous pouvez utiliser `file_info()`de **fs** (ou `file.info()`de **base** R).  

```{r, eval=F}
file_info(here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, echo=F}
DT::datatable(file_info(here("data", "case_linelists", "linelist_cleaned.rds")), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Ici, nous utilisons le `$` pour indexer le résultat et retourner uniquement la valeur `modification_time`.  

```{r}
file_info(here("data", "case_linelists", "linelist_cleaned.rds"))$modification_time
```




## Vérifier s'il existe  

### Objets R {.unnumbered}  

Vous pouvez utiliser `exists()` de **base** R pour vérifier si un objet R existe *dans* R (fournir le nom de l'objet entre guillemets).  

```{r}
exists("linelist")
```

Notez que certains paquets R **base** utilisent des noms d'objets génériques comme "data" en coulisse, qui apparaîtront comme VRAIS à moins que `inherit = FALSE` soit spécifié. C'est une des raisons pour ne pas nommer votre jeu de données "data".  

```{r}
exists("data")
exists("data", inherit = FALSE)
```

Si vous écrivez une fonction, vous devriez utiliser `missing()` de **base** R pour vérifier si un argument est présent ou non, au lieu de `exists()`.  



### Répertoires {.unnumbered}  

Pour vérifier si un répertoire existe, fournissez le chemin du fichier (et son nom) à `is_dir()` de **fs**. Faites défiler vers la droite pour voir que `TRUE` est imprimé.    

```{r}
is_dir(here("data"))
```

Une alternative est `file.exists()` de **base** R.  


### Les fichiers {.unnumbered}  

Pour vérifier si un fichier spécifique existe, utilisez `is_file()` de **fs**. Faites défiler vers la droite pour voir que `TRUE` est imprimé.  

```{r}
is_file(here("data", "case_linelists", "linelist_cleaned.rds"))
```

Une alternative **base** R est `file.exists()`.  



## Créer  

### Répertoires {.unnumbered}  

Pour créer un nouveau répertoire (dossier), vous pouvez utiliser `dir_create()` de **fs**. Si le répertoire existe déjà, il ne sera pas écrasé et aucune erreur ne sera retournée. 

```{r, eval=F}
dir_create(here("data", "test"))
```

Une alternative est `dir.create()` de **base** R, qui affichera une erreur si le répertoire existe déjà. En revanche, `dir_create()` dans ce scénario sera silencieux.  

### Fichiers {.unnumbered}  

Vous pouvez créer un fichier (vide) avec `file_create()` à partir de **fs**. Si le fichier existe déjà, il ne sera pas écrasé ou modifié.  

```{r, eval=F}
file_create(here("data", "test.rds"))
```

Une alternative R **base** est `file.create()`. Mais si le fichier existe déjà, cette option le tronquera. Si vous utilisez `file_create()`, le fichier sera laissé inchangé.  


### Créer si n'existe pas {.unnumbered}  

EN COURS DE CONSTRUCTION  


### Supprimer

### Objets R {.unnumbered}  

Utilisez `rm()` de **base** R pour supprimer un objet R.  

### Répertoires {.nonnuméroté}  

Utilisez `dir_delete()` de **fs**. 


### Fichiers {.nonnumérotés}  

Vous pouvez supprimer des fichiers avec `file_delete()` de **fs**.  



### Exécuter d'autres fichiers  

### `source()` {.unnumbered}  

Pour exécuter un script R à partir d'un autre script R, vous pouvez utiliser la commande `source()` (de **base** R).

```{r, eval=F}
source(here("scripts", "cleaning_scripts", "clean_testing_data.R"))
```

Cela revient à afficher le script R ci-dessus et à cliquer sur le bouton "Source" en haut à droite du script. Ceci exécutera le script mais le fera silencieusement (pas de sortie sur la console R) sauf si cela est spécifiquement prévu. Voir la page [Graphiques interactifs](#interactive_plots) pour des exemples d'utilisation de `source()` pour interagir avec un utilisateur via la console R en mode question-réponse.  

```{r, fig.align = 'center', out.height = '300%', echo=F}
knitr::include_graphics(here::here("images", "source_button.png"))
```


### `render()` {.unnumbered}  

`render()` est une variation de `source()` le plus souvent utilisée pour les scripts R markdown. Vous fournissez le `input = ` qui est le fichier R markdown, et aussi le `output_format = ` (typiquement soit "html_document", "pdf_document", "word_document", ""). 

Voir la page sur les [Production de rapports avec R Markdown](#rmarkdown) pour plus de détails. Consultez également la documentation de `render()` [ici](https://rmarkdown.rstudio.com/docs/reference/render.html) ou en entrant `?render`.  



### Exécuter des fichiers dans un répertoire {.unnumbered}

Vous pouvez créer une *boucle for* et l'utiliser pour `source()` chaque fichier d'un répertoire, tel qu'identifié avec `dir()`. 

```{r, eval=F}
for(script in dir(here("scripts"), pattern = ".R$")) { # pour chaque nom de script dans le dossier "scripts" du projet R (avec l'extension .R)
  source(here("scripts", script))                        # source le fichier avec le nom correspondant qui existe dans le dossier scripts
}
```

Si vous ne voulez exécuter que certains scripts, vous pouvez les identifier par leur nom comme ceci :  

```{r, eval=F}

scripts_to_run <- c(
     "epicurves.R",
     "demographic_tables.R",
     "survival_curves.R"
)

for(script in scripts_to_run) {
  source(here("scripts", script))
}

```



Voici une [comparaison](https://cran.r-project.org/web/packages/fs/vignettes/function-comparisons.html) des fonctions R **fs** et **base**.  

### Importer des fichiers dans un répertoire {.unnumbered}

Voir la page [Importer et exporter des données](#import_export) pour importer et exporter des fichiers individuels.  

Consultez également la page [Importer et exporter des données](#import_export) pour connaître les méthodes permettant d'importer automatiquement le fichier le plus récent, en fonction d'une date figurant dans le nom du fichier *ou* en examinant les métadonnées du fichier.  

Voir la page [Itération, boucles et listes](#iteration) pour un exemple avec le paquet **purrr** démontrant :  

* La division d'un cadre de données et son enregistrement dans plusieurs fichiers CSV.  
* Division d'un cadre de données et enregistrement de chaque partie comme une feuille séparée dans un classeur Excel.  
* Importer plusieurs fichiers CSV et les combiner en un seul cadre de données.  
* Importer un classeur Excel avec plusieurs feuilles et les combiner dans un cadre de données.  




## **base** R  

Voir ci-dessous les fonctions `list.files()` et `dir()`, qui effectuent la même opération de listage des fichiers dans un répertoire spécifié. Vous pouvez spécifier `ignore.case =` ou un motif spécifique à rechercher. 

```{r, eval=F}
list.files(path = ici("data"))

list.files(path = ici("data"), pattern = ".csv")
# dir(path = ici("data"), pattern = ".csv")

list.files(path = ici("data"), pattern = "evd", ignore.case = TRUE)

```

Si un fichier est actuellement "ouvert", il s'affiche dans votre dossier avec un tilde devant, comme "~$hospital_linelists.xlsx".  


<!-- ======================================================= -->
## Ressources { }

https://cran.r-project.org/web/packages/fs/vignettes/function-comparisons.html
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/directories.Rmd-->


# Contrôle de version et collaboration avec Git et Github {#collaboration}

Ce chapitre présente un aperçu de l'utilisation de Git pour collaborer avec d'autres personnes. Des tutoriels plus complets peuvent être en bas de page dans la section Ressources.

## Qu'est-ce que Git ?

Git est un logiciel de **contrôle de version** qui permet de suivre les modifications dans un dossier. Il peut être utilisé comme l'option "track change" dans Word, LibreOffice ou Google docs, mais pour tous les types de fichiers. C'est l'une des options les plus puissantes et les plus utilisées pour le contrôle de version.

**Pourquoi n'en ai-je jamais entendu parler?**
Les personnes ayant une formation de développeur
développeurs apprennent couramment à utiliser un logiciel de contrôle de version (Git,
Mercurial, Subversion ou autres), peu d'entre nous issus de disciplines
disciplines quantitatives sont enseignés ces compétences. Par conséquent, la plupart des épidémiologistes n'en
Par conséquent, la plupart des épidémiologistes n'en ont jamais entendu parler au cours de leurs études, et doivent l'apprendre à la volée.

**Attendez, j'ai entendu parler de Github, c'est la même chose?**
Pas exactement, mais vous les utilisez souvent ensemble, et nous vous montrerons comment faire. En bref :

- **Git** est le système de contrôle de version, un logiciel. Vous pouvez l'utiliser
    localement sur votre ordinateur ou pour synchroniser un dossier avec un
    hôte **site web**. Par défaut, on utilise un terminal pour donner à Git
    en ligne de commande.

- Vous pouvez utiliser un **client/interface Git** pour éviter la ligne de commande et
    et effectuer les mêmes actions (au moins pour les actions simples et super courantes).
    courantes).

- Si vous souhaitez stocker votre dossier dans un **site web hôte** pour
    collaborer avec d'autres, vous pouvez créer un compte sur Github,
    Gitlab, Bitbucket ou autres.

Ainsi, vous pouvez utiliser le client/interface **Github Desktop**, qui utilise
**Git** en arrière-plan pour gérer vos fichiers, à la fois localement sur votre
ordinateur, et à distance sur un serveur **Github**.

## Pourquoi utiliser le combo Git et Github ?

L'utilisation de **Git** facilite :

1) L'archivage des versions documentées avec des modifications incrémentielles de sorte que vous
    afin que vous puissiez facilement revenir en arrière à n'importe quel état antérieur.
2) Avoir des *branches* parallèles, c'est-à-dire des versions de développement/"de travail" avec
    des moyens structurés pour intégrer les changements après révision

Cela peut être fait localement sur votre ordinateur, même si vous ne collaborez pas avec d'autres personnes.
avec d'autres personnes. Avez-vous déjà :

- regretté d'avoir supprimé une section du code, pour réaliser deux mois plus tard que mois plus tard que vous en aviez réellement besoin ?


- revenir sur un projet qui avait été mis en pause et tenter de se rappeler si vous aviez fait cette modification délicate dans l'un des éléments du projet. de vous rappeler si vous aviez fait cette modification délicate dans l'un des modèles ?

- aviez un fichier *model_1.R* et un autre fichier *model_1\_test.R* et un fichier
    *model_1\_not_working.R* pour faire des essais ?

- avait un fichier *report.Rmd*, un fichier *report_full.Rmd*, un fichier
    *report_true_final.Rmd*, un fichier *report_final_20210304.Rmd*, un fichier
    *report_final_20210402.Rmd* et maudit vos compétences en archivage ?

Git vous aidera dans tout cela, et vaut la peine d'être appris pour cette seule raison.


Cependant, il devient encore plus puissant lorsqu'il est utilisé avec un référentiel en ligne
tel que Github pour soutenir des **projets collaboratifs**. Cela facilite :

- la collaboration : d'autres personnes peuvent examiner, commenter, et accepter/refuser des modifications

- Le partage de votre code, de vos données et de vos résultats, et l'invitation à faire des commentaires
    du public (ou en privé, avec votre équipe)

et évite :

- "Oups, j'ai oublié d'envoyer la dernière version et maintenant vous devez
    refaire deux jours de travail sur ce nouveau fichier".

- Mina, Henry et Oumar ont tous travaillé en même temps sur un script et doivent
    doivent fusionner manuellement leurs modifications

- Deux personnes tentent de modifier le même fichier sur Dropbox et Sharepoint 
    et cela crée une erreur de synchronisation.

### Cela semble compliqué, je ne suis pas un programmeur {-}

Cela peut l'être. Les exemples d'utilisations avancées peuvent être assez effrayants. Cependant, un peu comme R, ou même Excel, vous n'avez pas besoin de devenir un expert pour profiter des avantages de l'outil. L'apprentissage d'un *petit nombre de fonctions et de notions* vous permet de suivre vos modifications, de synchroniser vos fichiers sur un référentiel en ligne et de collaborer avec vos collègues en très peu de temps. 

En raison de la courbe d'apprentissage, le contexte d'urgence n'est peut-être pas le meilleur moment pour apprendre ces outils. Mais l'apprentissage peut se faire par étapes. Une fois que vous aurez acquis quelques notions, votre flux de travail peut être assez efficace et rapide. Si vous ne travaillez pas sur un projet où la collaboration avec des personnes peut-etre Git n'est pas une nécessité, mais **c'est en fait un bon moment pour devenir l'utiliser** en solo avant de vous lancer dans la collaboration.

## Installation

### Installer Git {.unnumbered}

*Git* est le moteur qui se cache derrière les coulisses de votre ordinateur, qui suit
les changements, les branches (versions), les fusions et les retours en arrière. Vous pouvez d'abord
installer *Git* de le lien ici: <https://git-scm.com/downloads>.

### Installer une interface (facultatif mais recommandé) {.unnumbered}

Git possède son propre langage de commandes, qui peuvent être tapées dans un terminal de ligne de commande. Cependant, il existe de nombreux clients/interfaces et en tant que non-développeur, dans votre utilisation quotidienne, vous aurez rarement _besoin_ d'interagir directement avec Git. 
L'interface Git fournit généralement des outils de visualisation agréables pour les modifications de fichiers ou les branches. 

De nombreuses options existent, sur tous les systèmes d'exploitation, de plus simples aux plus complexes. Parmi les bonnes options pour les débutants, citons le volet Git de RStudio et le [Github Desktop](https://desktop.github.com/), que nous présenterons dans ce chapitre. Les options intermédiaires (plus puissantes, mais plus complexes) comprennent Source Tree, Gitkracken, Smart Git et d'autres programmes.

Trouvez un explication rapide sur les [clients Git](-%09https:/happygitwithr.com/git-client.html#git-client).

*Remarque : comme les interfaces utilisent toutes Git en interne, vous pouvez en essayer plusieurs, passer de l'une à l'autre en fonction de vos besoins.*

Comme indiqué ci-dessous, vous aurez occasionnellement avoir besoin d'écrire des commandes Git dans un terminal tel que le volet terminal de RStudio (un onglet adjacent à la Console R) ou le terminal Git Bash.


### Compte Github {.unnumbered}

Créez un compte gratuit sur [github.com](github.com).

Il se peut que l'on vous propose de configurer une authentification à deux facteurs avec une application sur votre téléphone. Pour en savoir plus, consultez le document Github [help documents](https://docs.github.com/en/github/authenticating-to-github/securing-your-account-with-two-factor-authentication-2fa).

Si vous utilisez Github Desktop, vous pouvez entrer vos informations d'identification Gitub après l'installation en suivant ces [étapes](https://docs.github.com/en/desktop/installing-and-configuring-github-desktop/authenticating-to-github). Si vous ne le faites pas, les informations d'identification vous seront demandées ultérieurement lorsque vous tenterez de cloner un projet à partir de Github.


## Vocabulaire, concepts et fonctions de base

Comme lors de l'apprentissage de R, il y a un peu de vocabulaire à retenir pour comprendre Git. Voici les [bases pour vous aider à démarrer](https://www.freecodecamp.org/news/an-introduction-to-git-for-absolute-beginners-86fa1d32ff71/)/ [tutoriel interactif](learngitbranching.js.org). Dans les prochaines sections, nous montrerons comment utiliser les interfaces. 

### Référentiel {.unnumbered}

Un *référentiel* Git ("*repo*") est un dossier qui contient tous les sous-dossiers et fichiers de votre projet (données, code, images, etc.) et l'historique de leurs révisions. Lorsque vous commencez à suivre les changements dans le dépôt, Git créera un dossier caché qui contient toutes les informations de suivi. Un référentiel Git typique est votre dossier *R Project* (voir la page du manuel sur les [Projets R](#r_projects)).

Nous allons montrer comment créer (*initialiser*) un dépôt Gitub, Github Desktop ou Rstudio dans les sections suivantes.

### Commits {.unnumbered}

Un *commit* est un **snapshot** du projet à un moment donné. Lorsque vous apportez un changement au projet, vous faites un nouveau commit pour suivre les modifications (le delta) apportées à vos fichiers. Par exemple, vous avez peut-être édité quelques lignes de code et mis à jour une jeu de données associé. Une fois que vos modifications sont enregistrées, vous pouvez regrouper ces changements en un seul "commit".

Chaque commit a un ID unique (un *hash*). À des fins de contrôle de version, vous pouvez revenir en arrière dans votre projet en vous basant sur les commits, pars les garder relativement petits et cohérents. Vous joindrez également une brève description des modifications appelée "message de validation".

*Modifications organisées* ? Mettre en scène les changements, c'est les ajouter à la *zone de mise en scène* en préparation pour le prochain commit. L'idée est que vous pouvez finement décider quels changements inclure dans un commit. Par exemple, si vous avez travaillé sur la spécification d'un modèle dans un
un autre script, il serait judicieux d'avoir deux commits différents (ce serait plus facile au cas où vous voudriez annuler les changements sur la figure mais pas sur le modèle).


### Branches {.unnumbered}

Une branche représente une *ligne indépendante* de changements dans votre repo, une version parallèle et alternative des fichiers de votre projet. 


Les branches sont utiles pour tester les modifications avant qu'elles soient incorporées dans la branche *principale*, qui est généralement la version principale/finale/"live" de votre projet. Lorsque vous avez fini d'expérimenter sur une branche, vous pouvez apporter les changements dans votre branche *principle*, en la *fusionnant*, ou la supprimer, si les changements n'ont pas été couronnés de succès.

*Note : vous n'avez pas besoin de collaborer avec d'autres personnes pour utiliser les branches, ni d'avoir un référentiel en ligne distant.*



### Dépôts locaux et distants {.unnumbered}

*Cloner* consiste à créer une copie d'un dépôt Git à un autre endroit.

Par exemple, vous pouvez *cloner* un dépôt en ligne _de_ Github localement sur votre ordinateur, ou commencer par un dépôt local et le cloner en ligne en ligne _sur_ Github.

Lorsque vous avez cloné un référentiel, les fichiers du projet existent à deux endroits :

- le référentiel *LOCAL* sur votre ordinateur physique. C'est là que vous apportez les modifications réelles aux fichiers/codes.

- le référentiel *ROMPU*, en ligne : les versions de vos fichiers de projet dans le dépôt Github (ou sur tout autre hébergeur).

Pour synchroniser ces dépôts, nous allons utiliser d'autres fonctions. En effet , contrairement à Sharepoint, Dropbox ou autre logiciel de synchronisation, Git ne met pas automatiquement à jour votre dépôt local en fonction de ce qui est en ligne,ou vice-versa. C'est vous qui choisissez quand et comment synchroniser.

- `git fetch` télécharge les nouvelles modifications depuis le dépôt distant mais ne modifie pas votre dépôt local. Pensez-y comme une vérification de l'état du dépôt distant.

- `git pull` télécharge les nouvelles modifications depuis les dépôts distants et met à jour votre dépôt local.

- Lorsque vous avez fait un ou plusieurs commits localement, vous pouvez utiliser `git push` pour faire les commits vers le dépôt distant. Ceci envoie vos modifications sur Github afin que d'autres personnes puissent les voir et les tirer s'ils le souhaitent.


## Démarrer : créer un nouveau dépôt

Il y a plusieurs façons de créer de nouveaux dépôts. Vous pouvez le faire à partir de la console, de Github, ou d'une interface.

Deux approches générales sont possibles :

- Créer un nouveau projet R à partir d'un dépôt Github existant ou nouveau. (*préféré pour les débutants*), ou bien
- Créer un dépôt Github pour un projet R existant.


### Fichiers de démarrage {.unnumbered}

Lorsque vous créez un nouveau référentiel, vous pouvez éventuellement créer  tous les fichiers ci-dessous, ou vous pouvez les ajouter à votre référentiel à un stade ultérieur. Ils se trouvent généralement dans le dossier "racine" du référentiel.

- Un fichier *README* est un fichier que quelqu'un peut lire pour comprendre pourquoi votre projet existe et ce qu'il doit savoir pour l'utiliser. Il sera vide au début, mais vous pouvez le compléter plus tard.

- Un fichier *.gitignore* est un fichier texte dont chaque ligne contient des dossiers ou fichiers que Git devrait ignorer (ne pas suivre les modifications). Lisez plus sur ce sujet et voir des exemples [ici](https://www.freecodecamp.org/news/gitignore-what-is-it-and-how-to-add-to-repo/).

- Vous pouvez choisir une *licence* pour votre travail, afin que les autres personnes sachent sous quelles conditions elles peuvent utiliser ou reproduire votre travail. Pour de plus amples informations, consultez la page [Creative Commons licenses](https://creativecommons.org/licenses/).

### Créer un nouveau référentiel dans Github {.unnumbered}

Pour créer un nouveau dépôt, connectez-vous à Github et cherchez le bouton vert. Ce dépôt vide peut être cloné localement sur votre ordinateur (voir la section suivante).

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_new.png"))
```

Vous devez choisir si vous voulez que votre dépôt soit **public** (visible par tout le monde sur Internet) ou **privé** (seulement visible pour ceux qui ont la permission a acceder le dépôt). Cela a des implications importantes si vos données sont sensibles. Si votre référentiel est privé, vous rencontrerez certains quotas dans des circonstances particulières, par exemple si vous utilisez les *actions* de Github pour exécuter automatiquement votre code dans le nuage.
 
### Clone à partir d'un dépôt Github {.unnumbered}

Vous pouvez *cloner* un référentiel Github existant pour créer un nouveau projet R local sur votre ordinateur.

Le dépôt Github peut être un dépôt qui existe déjà et contient du contenu, ou un dépôt vide que vous venez de créer. Dans ce dernier cas, vous créez essentiellement le repo Github et le projet local R en même temps (voir les instructions ci-dessus).

_Remarque_ : si vous n'avez pas de droits de contribution sur un dépôt Github, il est possible de d'abord "*forker*" (creer une copie) le dépôt vers votre profil, et ensuite de procéder aux autres actions. La bifurcation est expliquée à la fin de ce chapitre, mais nous vous recommandons de lire les autres sections en premier. 

Etape 1 : Naviguez dans Github jusqu'au dépôt, cliquez sur le bouton vert "**Code**". et copier **l'URL HTTPS clone** (voir image ci-dessous)

```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_clone.png"))
```

L'étape suivante peut être effectuée dans n'importe quelle interface. Nous allons illustrer avec Rstudio et le bureau Github.

#### Dans Rstudio {.unnumbered}

Dans RStudio, démarrez un nouveau projet R en cliquant sur *Fichier \> Nouveau projet \> Contrôle de version \> Git*.

- Lorsque vous êtes invité à saisir l'"URL du dépôt", collez l'URL HTTPS de Github.
- Attribuez au projet R un nom court et informatif.
- Choisissez l'endroit où le nouveau projet R sera enregistré localement.
- Cochez "Ouvrir dans une nouvelle session" et cliquez sur "Créer un projet".


Vous êtes maintenant dans un nouveau projet RStudio local qui est un clone du dépôt Github. Ce projet local et le dépôt Github sont maintenant liés.

#### Dans le bureau de Github {.unnumbered}

- Cliquez sur *Fichier \> Cloner un référentiel*.

- Sélectionnez l'onglet URL

- Collez l'URL HTTPS de Github dans la première case.

- Sélectionnez le dossier dans lequel vous voulez avoir votre dépôt local.

- Cliquez sur "CLONE".

```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_clone_desktop.png"))
```

### Nouveau repo Github à partir d'un projet R existant {.unnumbered}

Un autre scénario de configuration est que vous avez un projet R existant avec du contenu, et que vous voulez créer un dépôt Github pour celui-ci.

1) Créez un nouveau dépôt Github vide pour le projet (cf. instructions ci-dessus).
2) Clonez ce dépôt localement (voir les instructions HTTPS ci-dessus).
3) Copiez tout le contenu de votre projet R (codes, données, etc.) dans ce nouveau dépôt local vide (utilisez le copier-coller, par exemple).
4) Ouvrez votre nouveau projet dans RStudio, et allez dans le volet Git. Les nouveaux fichiers devraient s'enregistrer comme des modifications de fichiers, désormais suivies par Git. Par conséquent, vous pouvez  regrouper ces modifications sous forme de *commit* et les *pousser* (**push**) vers Github. Une fois *poussé* (*pushed*), le dépôt sur Github reflétera tous les fichiers.
    
Voir la section sur le flux de travail Github ci-dessous pour plus de détails sur ce processus.

### A quoi cela ressemble-t-il maintenant ? {.unnumbered}

#### Dans RStudio {-}

Une fois que vous avez cloné un dépôt Github vers un nouveau projet R, vous voyez maintenant dans RStudio un onglet "Git". Cet onglet apparaît dans le même volet de RStudio que votre environnement R : 

```{r echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Git_console.png"))
```

Veuillez noter les boutons encerclés dans l'image ci-dessus, puisqu'ils seront référencés plus tard (de gauche à droite) :

- Bouton pour *commettre* (*commit*) les modifications du fichier sauvegardé dans la branche locale (cela ouvrira une nouvelle fenêtre)
- Flèche bleue pour *tirer* (*pull*, mettre à jour votre version locale de la branche avec avec les changements effectués sur la version distante/Github de cette branche)
- Flèche verte pour *pousser* (*push*, envoyer tous les commits/modifications de votre version version locale de la branche vers la version distante/Github de cette branche)
- L'onglet Git dans RStudio
- Bouton pour créer une NOUVELLE branche en utilisant comme base la branche locale affichée. *Vous voulez presque toujours créer une branche à partir de la branche principale (après la première extraction).*
- La branche dans laquelle vous travaillez actuellement
- Les modifications que vous avez apportées au code ou à d'autres fichiers apparaissent ci-dessous

#### dans Github Desktop {-}

Github Desktop est une application indépendante qui vous permet de gérer tous vos dépôts. Lorsque vous l'ouvrez, l'interface vous permet de choisir le dépôt sur lequel vous souhaitez travailler, puis d'effectuer les actions Git à partir de là.

```{r echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_interface.png"))
```


### Flux de travail Git + Github

### Aperçu du processus {.unnumbered}

Une fois que vous avez terminé la configuration (décrite ci-dessus), vous aurez un Github qui est connecté (*cloné*) à un projet R local. La branche *principale* (créée par défaut) est la version dite "live" de *tous* les fichiers. Lorsque vous voulez faire des modifications, il est bon de créer une pratique de créer une *nouvelle branche* à partir de la branche *principale* (comme "créer une copie"). Il s'agit d'un flux de travail typique de Git, car la création d'une branche est facile et rapide.


Un flux de travail typique est le suivant :

1.  Assurez-vous que votre dépôt local est à jour, mettez-le à jour si ce n'est pas

2.  Allez sur la branche sur laquelle vous travailliez précédemment, ou créez une nouvelle branche 

3.  Travaillez sur les fichiers localement sur votre ordinateur, faites un ou plusieurs commits à cette branche

4.  Mettre à jour la version distante de la branche avec vos modifications (push)

5.  Lorsque vous êtes satisfait de votre branche, vous pouvez fusionner la version en ligne de la branche de travail avec la branche "principale" afin de créer une nouvelle branche.

Les autres membres de l'équipe peuvent faire la même chose avec leurs propres branches, ou peut contribuer des commits dans votre branche de travail aussi. 

Nous détaillons ci-dessous le processus, étape par étape.

Voici un schéma que nous avons développé - il se présente sous la forme d'un tableau à double sens, ce qui devrait aider les épidémiologistes à comprendre.

```{r echo=F, out.height='150%', out.width='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_table.png"))
```

Voici [un autre diagramme](https://build5nines.com/introduction-to-git-version-control-workflow/).

*Note : jusqu'à récemment, le terme de branche "master" était utilisé, mais on parle maintenant de branche "principale" ("master").

```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "GitHub-Flow.png"))
```

Image [source](https://build5nines.com/introduction-to-git-version-control-workflow/)

## Créer une nouvelle branche

Lorsque vous sélectionnez une branche sur laquelle travailler, **Git réinitialise votre répertoire de travail comme il était la dernière fois que vous étiez sur cette branche**.

### Dans le volet Git de Rstudio {.unnumbered}

Assurez-vous que vous êtes dans la branche "principale", puis cliquez sur l'icône violette pour créer une nouvelle branche (voir image ci-dessus).

- Vous serez invité à nommer votre branche avec un nom descriptif en un mot (vous pouvez utiliser des caractères de soulignement si nécessaire).
- Vous verrez que, localement, vous êtes toujours dans le même projet R, mais que vous ne travaillez plus sur la branche "principale".
- Une fois créée, la nouvelle branche apparaîtra également sur le site Github comme une branche.
    
Vous pouvez visualiser les branches dans le volet Git de Rstudio après avoir cliqué sur "Historique" ("History").

```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_rstudio_branchs.png"))
```


### Dans le bureau de Github {.unnumbered}

Le processus est très similaire, il vous est demandé de donner un nom à votre branche.Ensuite, il vous sera demandé de "Publier votre branche sur Github" pour que la nouvelle branche apparaisse également dans le dépôt distant.


```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_new_branch.png"))
```

### Dans la console {.unnumbered}

Ce qui se passe en réalité dans les coulisses est que vous créez une nouvelle branche avec `git branch`, puis vous allez dans la branche avec `git checkout` ( _i.e._ dites à Git que vos prochains commits se feront là).  Dans votre dépôt git :

```{bash, eval = FALSE}
git branch my-new-branch # Créez la nouvelle branche
git checkout my-new-branch # Aller sur la branche
git checkout -b ma-nouvelle-branche # Les deux à la fois (raccourci)
```


Pour plus d'informations sur l'utilisation de la console, voir la section sur les Commandes Git à la fin.

## Valider les changements

Vous pouvez maintenant modifier le code, ajouter de nouveaux fichiers, mettre à jour les ensembles de données, etc.


Chacune de vos modifications est suivie, *une fois que le fichier respectif est sauvegardé*. Les fichiers modifiés apparaîtront dans l'onglet Git de RStudio, dans Github Desktop, ou en utilisant la commande `git status` dans le terminal (voir ci-dessous).

Chaque fois que vous effectuez des modifications substantielles (par exemple, l'ajout ou la mise à jour d'une section de code), faites une pause et *committez* ces changements. Pensez à un commit comme un "partie" de changements liés à un objectif commun. Vous pouvez toujours continuer à réviser un fichier après y avoir apporté des modifications.

*Conseil sur les commits* : en général, il est préférable de faire de petits commits, qui peuvent être facilement annulées si un problème survient, pour commiter ensemble des modifications liées à un objectif commun. Pour y parvenir, vous trouverez que *vous devez commiter souvent*. Au début, vous allez probablement oublier de commiter souvent, mais ensuite l'habitude s'installe.

### Dans Rstudio {.unnumbered}

L'exemple ci-dessous montre que, depuis le dernier commit, le script R Markdown "collaboration.Rmd" a été modifié, et plusieurs images PNG ont été ajoutées.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_tracking2.png"))
```

Vous vous demandez peut-être ce que représentent les carrés jaunes, bleus, verts et rouges à côté de les noms de fichiers. Voici un instantané de la [feuille de route RStudio cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf) qui explique leur signification. Notez que les changements avec des " ?" jaunes peuvent toujours être mises en scène, validées et poussées.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_tracking.png"))
```

- Cliquez sur le bouton "Commit" dans l'onglet Git, ce qui ouvrira une nouvelle fenêtre (voir ci-dessous).

- Cliquez sur le nom d'un fichier dans le cadre supérieur gauche.

- Passez en revue les modifications que vous avez apportées à ce fichier (surlignées en vert ou en rouge ci-dessous).

- "Mettez en scène" le fichier, ce qui inclura ces modifications dans le commit. Faites en cochant la case à côté du nom du fichier. Alternativement, vous pouvez mettre en surbrillance plusieurs noms de fichiers, puis cliquer sur "Stage".

- Rédigez un message de validation court mais descriptif (obligatoire).

- Appuyez sur le bouton "Commit". Une boîte de dialogue apparaîtra, indiquant le succès ou un message d'erreur.


Vous pouvez maintenant effectuer d'autres modifications et d'autres livraisons, autant de fois que vous le souhaitez.

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_commit.png"))
```

### Dans le bureau de Github {.unnumbered}

Vous pouvez voir la liste des fichiers qui ont été modifiés sur la gauche. Si vous sélectionnez un fichier texte, vous verrez un résumé des modifications qui ont été apportées dans le volet de droite (cette vue ne fonctionnera pas sur des fichiers plus complexes comme les .docs ou les .xlsx).

Pour mettre en scène les changements, il suffit de cocher la petite case à côté des noms de fichiers. Lorsque vous avez sélectionné les fichiers que vous voulez ajouter à cette livraison, donnez un nom à la livraison, une description, puis cliquez sur le bouton **commit**.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_commit.png"))
```

### Dans la console {.unnumbered}

Les deux fonctions utilisées dans les coulisses sont `git add` pour sélectionner/établir et `git commit` pour effectuer la livraison.

```{bash, eval = FALSE}
git status # voir les changements 

git add new_pages/collaboration.Rmd # sélectionner les fichiers à livrer (= mettre en scène les changements)

git commit -m "Describe commit from Github Desktop" # livrer les changements avec un message

git log # affiche les informations sur les commits passés
```


### Modifier un commit précédent {.unnumbered}

Que se passe-t-il si vous commettez des changements, continuez à travailler et réalisez que vous avez fait des changements qui devraient "appartenir" au commit précédent (à votre avis)?  N'ayez crainte ! Vous pouvez ajouter ces changements à votre validation précédente.

Dans Rstudio, cela devrait être assez évident puisqu'il y a une case "Amend previous commit" sur la même ligne que le bouton COMMIT. 

Pour une raison peu claire, la fonctionnalité n'a pas été implémentée en tant que telle dans Github Desktop, mais il existe une
moyen de la contourner. Si vous avez validé **mais pas encore poussé** vos changements,  un bouton "UNDO" apparaît juste en dessous du bouton COMMIT. Cliquez dessus et il annulera votre validation (mais conservera vos fichiers indexés et votre message de validation).  Sauvegardez vos changements, ajoutez de nouveaux fichiers à la livraison si nécessaire et livrez à nouveau.

Dans la console :  

```{bash, eval = FALSE}
git add [YOUR FILES] # Ajoute vos nouvelles modifications

git commit --amend # Modifie le commit précédent

git commit --amend -m "An updated commit message" # Modifie la livraison précédente ET met à jour le message de livraison.
```


_Note : réfléchissez avant de modifier des commits qui sont déjà publics et partagés avec vos collaborateurs_.


## Tirez et poussez les modifications vers Github

"D'abord TIREZ, ensuite POUSSER"

C'est une bonne pratique de *aller chercher* et de *tirez* avant de commencer à travailler sur votre projet, afin de mettre à jour la version de la branche sur votre ordinateur local avec toutes les modifications qui ont été apportées dans la version distante/Github.

TIREZ souvent. N'hésitez pas. *Tirez toujours avant de pousser*.

Lorsque vos modifications sont effectuées et validées et que vous êtes satisfait de l'état de votre projet, vous pouvez *pousser* vos commits vers la version distante/Github de votre branche.


Rincez et répétez pendant que vous travaillez sur le référentiel.

**Note:** il est beaucoup plus facile de revenir sur des modifications qui ont été commises mais pas poussées (c'est-à-dire qu'elles sont toujours en cours de traitement) que de revenir sur des changements qui ont été poussés vers le dépôt distant (et peut-être déjà extraites par quelqu'un d'autre). 


#### Dans Rstudio {.unnumbered}

*TIREZ* - Cliquez d'abord sur l'icône "Tirez" (flèche vers le bas) qui récupère et tire en même temps.

*POUSSER* - Cliquez sur l'icône verte "Tirez" (flèche vers le haut). Il peut vous être demandé
d'entrer votre nom d'utilisateur et votre mot de passe Github.La première fois, vous devrez peut-être entrer deux lignes de commande Git dans le *Terminal*:

- **git config --global utilisateur.email " [you\@example.com](mailto:you@example.com){.email} "** (votre adresse électronique Github
    ), et
- **git config --global user.name "Votre nom d'utilisateur Github "**

Pour en savoir plus sur la façon de saisir ces commandes, consultez la section ci-dessous sur les commandes Git.

***TIP:*** On vous demande trop souvent de fournir votre mot de passe ? Consultez les chapitres 10 & 11 de ce [tutoriel](https://happygitwithr.com/credential-caching.html#credential-caching) pour se connecter à un référentiel en utilisant une clé SSH (plus compliqué).  


#### Dans le bureau de Github {.unnumbered}

Cliquez sur le bouton "Récupérer l'origine" pour vérifier s'il y a de nouveaux commits sur le dépôt distant.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_fetch_button.png"))
```

Si Git trouve de nouveaux commits sur le dépôt distant, le bouton se se transformera en bouton "Pull". Comme le même bouton est utilisé pour pousser et tirer, vous ne pouvez pas pousser vos modifications si vous ne tirez pas auparavant.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_pull_button.png"))
```

Vous pouvez aller dans l'onglet "History" (près de l'onglet "Changes") pour voir toutes les commits (les vôtres et ceux des autres). C'est un bon moyen de savoir ce que vos collaborateurs ont fait. Vous pouvez lire le message du commit, la description s'il y en a une, et comparer le code des deux fichiers en utilisant le volet *diff*.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_history.png"))
```

Une fois que toutes les modifications distantes ont été tirées, et qu'au moins une modification locale a été validée, vous pouvez pousser en cliquant sur le même bouton.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_push_button.png"))
```

#### Console {.unnumbered}

Sans surprise, les commandes sont *chercher* (fetch), *tirez* (pull) et *pousser* (push).

```{bash, eval = FALSE}
git fetch # Y a-t-il de nouveaux commits dans le répertoire distant ?
git pull # Apporte les commits distants dans votre branche locale
git push # Pousse les commits locaux de cette branche vers la branche distante
```


### Je veux tirer mais j'ai du travail local {.unnumbered}

Cela peut arriver parfois : vous avez effectué des modifications sur votre dépôt local, mais le dépôt distant a des commits que vous n'avez pas tirés. 


Git refusera de les extraire car cela pourrait écraser vos modifications. Il existe plusieurs stratégies pour conserver vos modifications,  bien décrites dans [Happy Git with R](https://happygitwithr.com/pull-tricky.html), parmi lesquelles les deux principales sont :

- livrer vos modifications, récupérer les modifications distantes, les tirer, résoudre les conflits si nécessaire (voir la section ci-dessous), et pousser le tout en ligne
- `stash` vos changements, ce qui les met en quelque sorte de côté, les tirer, les déstocker (restauration), puis commit, résolution des conflits, et push. 

Si les fichiers concernés par les modifications distantes et les fichiers concernés par vos modifications locales ne se chevauchent pas, Git peut résoudre les conflits automatiquement.

Dans Github Desktop, cela peut être fait avec des boutons. Pour mettre en cache, allez dans _Branch > Stash all changes_.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_stash.png"))
```



## Fusionner la branche dans Main 

Si vous avez fini de faire des changements, vous pouvez commencer le processus de fusionner ces changements dans la branche principale. En fonction de votre situation, cela peut être rapide, ou vous pouvez avoir des étapes délibérées de révision et d'approbation impliquant des coéquipiers.

### Localement dans Github Desktop {.unnumbered}

On peut fusionner des branches localement en utilisant Github Desktop. Tout d'abord, allez dans (checkout) la branche qui sera le destinataire des commits, en d'autres termes, la branche que vous voulez mettre à jour. Ensuite, allez dans le menu *Branche \> Fusionner en branche actuelle* et cliquez. Une boîte vous permet de sélectionner la branche à partir de laquelle vous souhaitez importer.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_merge.png"))
```

### Dans la console {.unnumbered}

Revenez d'abord à la branche qui sera le destination des changements. C'est généralement *master*, mais cela peut être une autre branche si vous le souhaite. Fusionnez votre branche de travail dans master.

```{bash, eval = FALSE}
git checkout master # Retournez à master (ou à la branche que vous voulez déplacer)
git merge this_fancy_new_branch
```

[Cette page](https://git-scm.com/book/en/v2/Git-Branching-Basic-Branching-and-Merging) montre un exemple plus avancé de branchement et explique un peu ce qui se passe en coulisses.

### Dans Github : soumettre des demandes de pull (tirer) {.unnumbered}


S'il est tout à fait possible de fusionner deux branches localement, ou sans en informer qui que ce soit, une fusion peut être discutée ou étudiée par plusieurs personnes avant d'être intégrée à la branche master. Pour aider à ce processus, Github propose des fonctionnalités de discussion autour de la fusion : la **pull request**.

Une pull request (une "PR") est une demande de fusion d'une branche dans une autre (en d'autres termes, une demande pour que _votre branche de travail soit intégrée à la branche "principale"_). Une pull request implique généralement plusieurs commits. Une pull request commence généralement une conversation et un processus de révision avant qu'elle soit acceptée et que la branche soit fusionnée. Par exemple,  vous pouvez lire les discussions sur les demandes de téléchargement sur le site [dplyr's github](https://github.com/tidyverse/dplyr/pulls).


Vous pouvez soumettre une demande de modification (PR) directement à partir du site Web (comme illustré ci-dessous) ou à partir de Github.

- Accéder au dépôt Github (en ligne)
- Affichez l'onglet "demande de retrait" et cliquez sur le bouton "New pull request".
- Choisissez dans le menu déroulant de fusionner votre branche avec la branche principale.
- Rédigez un commentaire détaillé de la Pull Request et cliquez sur "Create Pull Request".

Dans l'image ci-dessous, la branche "forests" a été sélectionnée pour être fusionnée dans "main" :

```{r echo=F, out.width = '100%', out.height='150%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_pull_request2.png"))
```

Maintenant vous devriez être capable de voir la pull request (image d'exemple ci-dessous) :

- Passez en revue l'onglet "Fichiers modifiés" pour voir comment la branche "principale" changerait si la branche était fusionnée.
- Sur la droite, vous pouvez demander une révision aux membres de votre équipe en marquant leur identifiant Github. Si vous le souhaitez, vous pouvez définir les paramètres du référentiel pour qu'une révision approuvée soit nécessaire afin de fusionner avec la branche master.
- Une fois la demande de retrait est approuvée, un bouton permettant de "Merge pull request" devient actif. Cliquez dessus.
- Une fois terminé, supprimez votre branche comme expliqué ci-dessous.

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_pull_request.png"))
```

### Résolution des conflits {.unnumbered}

Lorsque deux personnes modifient la ou les mêmes lignes au même moment, un conflit de fusion se produit. En effet, Git refuse de prendre une décision quant à version à conserver, mais il vous aide à trouver où se trouve le conflit. **NE PANIQUEZ PAS**. La plupart du temps, il est assez simple à résoudre.

Par exemple, sur Github :

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_conflict2.png"))
```


Après que la fusion ait soulevé un conflit, ouvrez le fichier dans votre éditeur préféré. Le conflit sera indiqué par une série de caractères :

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_conflict3.png"))
```

Le texte entre *\<\<\<\<\<\<\< HEAD* et *=======* provient de votre dépôt local, et celui entre *=======* et *\>\>\>\>\>\>\>* de l'autre branche (qui peut être origin, main ou toute autre branche de votre choix).

Vous devez décider quelle version du code vous préférez (ou même écrire une troisième, en incluant les changements des deux côtés si cela est pertinent), effacer le reste et retirer toutes les marques que Git a ajoutées *((\<\<\<\<\<\<\< HEAD, =======, \>\>\>\>\>\>\> origin/master/your_branch_name*). 

Ensuite, sauvegardez le fichier, mettez-le en scène et commitez-le : c'est le commit qui rend la version fusionnée "officielle". N'oubliez pas de pousser (push) ensuite.

Plus vous et vos collaborateurs tirent et poussent souvent, plus les conflits seront réduits.


*Remarque : Si vous vous sentez à l'aise avec la console, il existe des outils plus [avancés de fusionner options](https://git-scm.com/book/en/v2/Git-Tools-Advanced-Merging) (par exemple, ignorer les espaces, donner la priorité à un collaborateur, etc.)

### Supprimer votre branche {.unnumbered}

Une fois qu'une branche a été fusionnée dans master et n'est plus nécessaire, vous pouvez la supprimer.

#### Github + Rstudio

Allez sur le dépôt sur Github et cliquez sur le bouton pour afficher toutes les branches (à côté de la liste déroulante pour les branches). Trouvez maintenant votre branche et cliquez sur l'icône de la corbeille à côté d'elle. Lisez plus de détails sur la suppression d'une branche [ici](https://docs.github.com/en/free-pro-team@latest/github/collaborating-with-issues-and-pull-requests/creating-and-deleting-branches-within-your-repository#deleting-a-branch).

Assurez-vous de supprimer également la branche localement sur votre ordinateur. Cela ne se fera pas automatiquement.

- Dans RStudio, assurez-vous que vous êtes dans la branche Main.
- Passez à la saisie de commandes Git dans le "Terminal" de RStudio (l'onglet adjacent à la console R), et tapez : **git branch -d nom_branche**, où "nom_branche" est le nom de la branche à supprimer.
- Rafraîchissez votre onglet Git et la branche devrait avoir disparu.


#### Dans Github

Vérifiez simplement la branche que vous voulez supprimer ce retrouve maintenant dans le menu *Branche \> Supprimer*.


### Forking {.unnumbered}

Vous pouvez bifurquer d'un projet si vous souhaitez contribuer mais que vous n'avez pas les droits pour le faire, ou si vous voulez simplement le modifier pour votre usage personnel. Une courte description de la bifurcation se trouve [ici](https://guides.github.com/activities/forking/).

Sur Github, cliquez sur le bouton "Fork" :  

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_fork_1.png"))
```

Ceci clonera le dépôt original, mais dans votre propre profil. Donc maintenant,  il y a deux versions du dépôt **sur Github** : l'original, que vous ne pouvez pas modifier, et la version clonée dans votre profil.

Ensuite, vous pouvez procéder au clonage de votre version du dépôt en ligne en local sur votre ordinateur, en utilisant l'une des méthodes décrites dans les sections précédentes. Ensuite, vous pouvez créer une nouvelle branche, faire des changements, les livrer et les pousser _vers votre dépôt distant_.

Une fois que vous êtes satisfait du résultat, vous pouvez créer une Pull Request à partir de Github ou de Github Desktop pour entamer la conversation avec les propriétaires/mainteneurs du dépôt d'origine.


**Et si vous avez besoin de commits plus récents du dépôt officiel?**

Imaginez que quelqu'un apporte une modification critical au dépôt officiel, que vous voulez inclure dans votre version clonée. Il est possible de synchroniser votre fork avec le dépôt officiel. Cela implique l'utilisation du terminal, mais ce n'est pas trop compliqué. Vous devez surtout vous rappeler que :

- _upstream_ = le dépôt officiel, celui que vous n'avez pas pu modifier
- _origin_ = votre version du dépôt sur votre profil Github


Vous pouvez lire [ce tutoriel](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/syncing-a-fork) ou suivre les instructions ci-dessous : 


Tout d'abord, tapez dans votre terminal Git (à l'intérieur de votre repo) :  

```{bash, eval = FALSE}
git remote -v
```
 
Si vous n'avez pas encore configuré le référentiel amont, vous devriez voir deux lignes, commençant par _origin_. Elles montrent le dépôt distant  vers lequel `chercher` et `pousser` pointent. Rappelez-vous, _origin_ est le surnom conventionnel pour votre propre version du dépôt sur Github. Par exemple :  

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_fork_2.png"))
```

Maintenant, ajoutez un nouveau dépôt distant :  

```{bash, eval = FALSE}
git remote add upstream https://github.com/epirhandbook/Epi_R_handbook.git
```
 
Ici, l'adresse est l'adresse que Github génère lorsque vous clonez un dépôt (voir la section sur le clonage). Vous aurez maintenant quatre pointeurs distants :

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_fork_3.png"))
```

Maintenant que la configuration est faite, chaque fois que vous voulez obtenir les changements de le dépôt original (_upstream_), il suffit d'aller (_checkout_) dans la branche la branche que vous voulez mettre à jour et taper :

```{bash, eval = FALSE}
git fetch upstream # Obtenir les nouveaux commits du dépôt distant
git checkout la_branche_que_vous_voulez_mettre_à_jour
git merge upstream/the_branch_you_want_to_update # Fusionne la branche amont dans votre branche.
git push # Mettez à jour votre propre version du dépôt distant.
```

S'il y a des conflits, vous devrez les résoudre, comme expliqué dans la section Résoudre les conflits. 


**Résumé** : forker est un clonage, mais du côté du serveur Github. Le reste des actions sont des actions typiques du flux de travail de collaboration (cloner, pousser, tirer, commettre, fusionner, soumettre des demandes de tirage...).

_Remarque : si la bifurcation est un concept et non une commande Git, elle existe aussi sur d'autres hôtes Web, comme [Bitbucket](https://www.atlassian.com/git/tutorials/comparing-workflows/forking-workflow)._


```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_fork_4.png"))
```


## Ce que nous avons appris

Vous avez appris à :  

- paramétrer Git pour suivre les modifications dans vos dossiers,  
- connecter votre référentiel local à un référentiel en ligne distant,
- livrer les changements,  
- synchroniser vos dépôts local et distant.  

Tout cela devrait vous permettre de démarrer et de répondre à la plupart de vos besoins en tant qu'épidémiologistes. Nous n'avons généralement pas un usage aussi avancé que les développeurs. 

Cependant, sachez que si vous voulez (ou devez) aller plus loin, Git offre plus de puissance pour simplifier l'historique des commits, le retour en arrière d'un ou plusieurs commits, le cherry-pick des commits, etc. Tout cela peut sembler de la pure magie, mais maintenant que vous avez les bases, il est plus facile de s'appuyer dessus.


Notez que si le volet Git de Rstudio et Github Desktop sont bons pour les débutants / l'utilisation quotidienne dans notre travail, ils n'offrent pas une interface pour certaines des fonctions intermédiaires/avancées de Git. Certaines interfaces plus complètes vous permettent d'en faire plus en pointant et en cliquant (généralement au prix d'une mise en page plus complexe). 

Rappelez-vous que puisque vous pouvez utiliser n'importe quel outil à n'importe quel moment pour suivre votre dépôt, vous pouvez très facilement installer une interface pour l'essayer parfois, ou pour effectuer occasionnellement une tâche complexe moins courante, tout en préférant une interface simplifiée le reste du temps (par exemple en utilisant Github Desktop la plupart du temps, et passer à SourceTree ou Gitbash pour certaines tâches spécifiques).


### Commandes Git {#git}


### Apprentissage recommandé {.unnumbered}

Pour apprendre les commandes Git à l'aide d'un tutoriel interactif, voir [ce site web](https://learngitbranching.js.org/).

### Où entrer les commandes {.unnumbered}

Vous entrez les commandes dans un shell Git.

*Option 1* Vous pouvez ouvrir un nouveau Terminal dans RStudio. Cet onglet se trouve à côté de la Console R. Si vous ne parvenez pas à y taper du texte, cliquez sur le menu déroulant sous "Terminal" et sélectionnez "Nouveau terminal". Tapez les dans l'espace clignotant situé devant le symbole du dollar "\$".

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_terminal.png"))
```

*Option 2* Vous pouvez également ouvrir un *shell* (un terminal pour entrer des commandes) en cliquant sur l'icône bleue "gears" dans l'onglet Git (près de l'environnement RStudio). Sélectionnez "Shell" dans le menu déroulant. Une nouvelle fenêtre s'ouvre dans laquelle vous pouvez saisir les commandes après le signe du dollar "\$".

*Option 3* Cliquez avec le bouton droit de la souris pour ouvrir "Git Bash here", qui ouvrira le même type de terminal, ou ouvrez *Git Bash* depuis votre liste d'applications. [Plus d'informations pour les débutants sur Git Bash](https://happygitwithr.com/shell.html), comment le trouver et quelques commandes bash dont vous aurez besoin.

### Exemples de commandes {.unnumbered}

Nous présentons ci-dessous quelques commandes git courantes. Lorsque vous les utilisez, gardez à l'esprit quelle branche est active (check-out), car cela changera l'action !

Dans les commandes ci-dessous, <name> représente un nom de branche et <commit_hash> représente l'ID de hachage d'un commit spécifique. <num> représente un nombre. Ne tapez pas les symboles \< ou \>>.

| Commande Git | Action |
|--------------------------|--------------------------------------------------------------------------|
| `git branch <name>` | Créer une nouvelle branche avec le nom <name> |
| `git checkout <name>` | Bascule de la branche actuelle vers <name>`.
| `git checkout -b <name>` | Raccourci pour créer une nouvelle branche *et* y basculer |
| `git status` | Voir les modifications non suivies |
| `git add <file>` | Mettre en scène un fichier |
| `git commit -m <message> `Comptabilise les changements actuellement mis en scène dans la branche actuelle avec le message |
| `git fetch` | Récupérer les commits du dépôt distant |
| `git pull` | Extraire les commits du dépôt distant dans la branche actuelle |
| `git push` | Pousse les commits locaux vers le répertoire distant |
| `git switch` | Une alternative à `git checkout` qui est en train d'être introduite progressivement dans Git |
| `git merge <name>` | Fusionner la branche <name> dans la branche courante |
| `git rebase <name>` | Ajoute les commits de la branche courante à la branche <name> |



<!-- ======================================================= -->

## Ressources

Une grande partie de cette page a été informée de [ce site "Happy Git with R" site web](https://happygitwithr.com/) par Jenny Bryan. Il y a une section très utile qui vous aide à résoudre les erreurs courantes liées à Git et à R.

Le [Guide de documentation et de démarrage de Github.com guide](https://docs.github.com/en/github).

La fiche technique de RStudio ["IDE" cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf) qui comprend des conseils sur Git avec RStudio.

<https://ohi-science.org/news/github-going-back-in-time>

**Les commandes Git pour les débutants**

Un [tutoriel didacticiel](learngitbranching.js.org) pour apprendre les commandes Git.

<https://www.freecodecamp.org/news/an-introduction-to-git-for-absolute-beginners-86fa1d32ff71/> : bon pour apprendre les bases absolues pour suivre les changements dans un dossier sur votre propre ordinateur.

De beaux schémas pour comprendre les branches : <https://speakerdeck.com/alicebartlett/git-for-humans>


**Tutoriels couvrant les sujets de base et plus avancés**.

<https://tutorialzine.com/2016/06/learn-git-in-30-minutes>

<https://dzone.com/articles/git-tutorial-commands-and-operations-in-git>
<https://swcarpentry.github.io/git-novice/> (cours court)
<https://rsjakob.gitbooks.io/git/content/chapter1.html>

Le [livre Pro Git](https://git-scm.com/book/en/v2) est considéré comme une référence officielle. Bien que certains chapitres soient corrects, il est généralement un peu _technical_. C'est probablement une bonne ressource une fois que vous avez utilisé Git un peu et quand vous voulez apprendre un peu plus précisément ce qui se passe et comment aller plus loin avec Git. 
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/collaboration.Rmd-->

# Erreurs fréquentes{#errors}

Cette page contient une liste d'erreurs les plus fréquentes et propose des solutions pour les résoudre.   
 
## Interprétation des messages d'erreurs

Les messages d'erreurs en R peuvent parfois être compliqués, c'est la raison pour laquelle Google sera votre partenaire. Recherchez le message d'erreur avec "R" et cherchez des messages récents dans [StackExchange.com](StackExchange.com), [stackoverflow.com](stackoverflow.com), [community.rstudio.com](community.rstudio.com), twitter(\#rstats) et d'autres forums utilisés par les programmeurs pour poser des questions et obtenir des réponses. Essayez de trouver des messages récents qui ont résolu des problèmes similaires.

Si, après nombreuses recherches, vous ne trouvez pas de réponse à votre problème, envisagez créer un *exemple reproductible* ("reprex") et poser la question vous-même. Consultez la page [obtenir de l'aide](#help) pour des conseils sur comment créer et publier un exemple reproductible sur les forums.

## Erreurs fréquentes

Nous énumérons ci-dessous quelques erreurs courantes et les explications/solutions possibles. Certaines d'entre elles sont tirées de l'analyse de Noam Ross, qui a analysé les messages de forum les plus courants sur Stack Overflow concernant les messages d'erreur en R (voir l'analyse [ici](https://github.com/noamross/zero-dependency-problems/blob/master/misc/stack-overflow-common-r-errors.md)).

### Erreurs de typographie {.unnumbered}

    Error: unexpected symbol in:
    "  geom_histogram(stat = "identity")+
      tidyquant::geom_ma(n=7, size = 2, color = "red" lty"

Si vous voyez `unexpected symbol`, vérifiez qu'il ne manque pas de virgules

### Erreurs de packages {.unnumbered}

    could not find function "x"...

Ceci signifie probablement que vous avez mal saisi le nom de la fonction, ou bien vous avez oublié d'installer ou de lancer un package.

    Error in select(data, var) : unused argument (var)

Vous pensez que vous utilisez `dplyr::select()` mais la fonction `select()` a été masquée par `MASS::select()` - spécifiez `dplyr::` ou réordonnez le chargement de votre package pour que dplyr soit après tous les autres.

D'autres erreurs de cache communes proviennent de : `plyr::summarise()` et `stats::filter()`. Considérez l'utilisation du [**conflicted** package](https://www.tidyverse.org/blog/2018/06/conflicted/).

    Error in install.packages : ERROR: failed to lock directory ‘C:\Users\Name\Documents\R\win-library\4.0’ for modifying
    Try removing ‘C:\Users\Name\Documents\R\win-library\4.0/00LOCK’

Si vous obtenez une erreur indiquant que vous devez supprimer un fichier "00LOCK", allez dans votre bibliothèque "R" dans le répertoire de votre ordinateur (par exemple, R/win-library/) et cherchez un dossier nommé "00LOCK". Supprimez-le manuellement et essayez d'installer à nouveau le package. Un processus d'installation précédent a probablement été interrompu, ce qui a mené à cela.

### Erreurs d'objet {.unnumbered}

    No such file or directory:

Si vous voyez une erreur comme celle-ci lorsque vous essayez d'exporter ou d'importer : Vérifiez l'orthographe du fichier et de son chemin d'accès, et si le chemin d'accès contient des barres obliques, assurez-vous qu'il s'agit bien d'une barre oblique en avant `/` et non d'une barre oblique en arrière `\`. Vérifiez également que vous avez utilisé la bonne extension de fichier (par exemple, .csv, .xlsx).

    object 'x' not found 

Ceci signifie que l'objet que vous référencez n'existe pas. Peut-être que le code ci-dessus ne s'est pas correctement exécuté ?

    Error in 'x': subscript out of bounds

Ceci signifie que vous avez essayé d'accéder à quelque chose (un élément d'un vecteur ou d'une liste) qui n'existe pas.

### Erreurs de syntaxe des fonctions {.unnumbered}

    # ran recode without re-stating the x variable in mutate(x = recode(x, OLD = NEW)
    Error: Problem with `mutate()` input `hospital`.
    x argument ".x" is missing, with no default
    i Input `hospital` is `recode(...)`.

L'erreur ci-dessus (`argument .x is missing, with no default`) est fréquente dans `mutate()` si vous fournissez une fonction comme `recode()` ou `replace_na()` où l'on s'attend à ce que vous fournissiez le nom de la colonne comme premier argument. Ceci est facile à oublier.\

### Erreurs de logique {.unnumbered}

    Error in if

Ceci signifie probablement qu'une instruction `if` a été appliquée à quelque chose qui n'était ni VRAI ni FAUX.

### Erreurs de facteur {.unnumbered}

    #Tried to add a value ("Missing") to a factor (with replace_na operating on a factor)
    Problem with `mutate()` input `age_cat`.
    i invalid factor level, NA generated
    i Input `age_cat` is `replace_na(age_cat, "Missing")`.invalid factor level, NA generated

Si vous voyez cette erreur concernant des niveaux de facteur invalides, vous avez probablement une colonne de la classe Factor (qui contient des niveaux prédéfinis) et vous avez essayé d'y ajouter une nouvelle valeur. Convertissez-la en classe Character avant d'ajouter une nouvelle valeur.

### Erreurs graphique {.unnumbered}

`Error: Insufficient values in manual scale. 3 needed but only 2 provided.` ggplot() scale_fill_manual() values = c("orange", "purple") ... insuffisant pour le nombre de niveaux de facteurs ... considérer si NA est maintenant un niveau de facteur...

    Can't add x object

Vous avez probablement un `+` supplémentaire à la fin d'une commande ggplot que vous devez supprimer.

### Erreurs R Markdown {.unnumbered}

Si le message d'erreur est de type `Error in options[[sprintf("fig.%s", i)]]`, vérifiez que vos options knitr en haut de chaque chunk utilisent correctement `out.width =` ou `out.height =` et *pas* `fig.width=` et `fig.height=`.

### Autres {.unnumbered}

Vérifiez si vous avez réorganisé les verbes **dplyr** en pipe et si vous n'avez pas remplacé un pipe au milieu, ou si vous n'avez pas retiré un pipe de la fin après avoir réorganisé.

<!-- ======================================================= -->

## Ressources

Voici un autre article de blog qui recense les [R programming errors faced by beginners](https://www.r-bloggers.com/2016/06/common-r-programming-errors-faced-by-beginners/)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/errors.Rmd-->


# Obtenir de l'aide{#help}

Cette page explique comment obtenir de l'aide en postant un problème sur Github ou en publiant un exemple reproductible ("reprex") sur un forum en ligne.  




## Github issues  

Plusieurs packages et projets en R ont leur code sur le site Github.com. Vous pouvez communiquer directement avec les auteurs sur ce site en postant un "Issue".   

Pour en savoir plus sur comment sauvegarder vos travaux sur Github, consultez la page [Collaboration et Github](#collaboration). 

Sur Github, chaque projet est contenu dans un *repository*. Chaque repository contient du code, des données, des résultats, la documentation d'aide, etc. Il existe également un moyen de communiquer avec les auteurs dénommé "Issues".   

Ci-dessous, la page Github pour le package **incidence2** (utilisé pour créer des courbes épidémiques). Vous pouvez voir l'onglet "Issues" surligné en jaune. Vous pouvez voir qu'il y a 5 issues ouvertes.    

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "errors_Github_issues.png"))
```

Une fois dans l'onglet Issues, vous pouvez voir les questions ou issues ouvertes. Lisez-les pour vous assurer que votre question n'est pas déjà abordée. Vous pouvez ouvrir un nouveau issue en cliquant sur le bouton vert à droite. Vous aurez besoin d'un compte Github pour le faire.  

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "errors_Github_issues2.png"))
```


  
Dans votre question, suivez les instructions ci-dessous pour fournir un exemple minimal et reproductible. Et soyez gentil! La plupart des personnes qui développent des packages et des projets R le font pendant leur temps libre (comme ce manuel!). 


Pour en savoir plus sur la gestion des questions dans votre propre repository Github, consultez la [documentation sur les issues](https://guides.github.com/features/issues/) sur Github.  



## Exemple reproductible  

Fournir un exemple reproductible ("reprex") est essentiel pour obtenir de l'aide lorsque vous postez un message dans un forum ou dans un issue Github. Les gens veulent vous aider, mais vous devez leur donner un exemple avec lequel ils peuvent travailler sur leur propre ordinateur. L'exemple doit :  

* Démontrer le problème que vous avez rencontré  
* Être *minimal*, c'est-à-dire qu'il ne doit contenir que les données et le code nécessaires à la reproduction du problème.  
* Être *reproductible*, c'est-à-dire que tous les objets (par exemple les données) et les requêtes sur les packages (par exemple `library()` ou `p_load()`) sont inclus.

*Aussi, assurez-vous de ne pas poster de données confidentielles avec le reprex!* Vous pouvez créer des tableaux de données exemplaires, ou utiliser l'un des tableaux de données intégrés à R (entrez `data()` pour ouvrir une liste de ces ensembles de données).   



### Le package **reprex** {.unnumbered}  

Le package **reprex** peut vous aider à créer un exemple reproduisible :   

1) **reprex** est installé avec **tidyverse**, donc chargez l'un ou l'autre des packages  

```{r, eval=F}
# installer/charger tidyverse (qui inclut reprex)
pacman::p_load(tidyverse)
```

2) Commencez un script R qui crée votre problème, étape par étape, en commençant par le chargement des packages et des données.  

```{r, eval=F}
# charger les packages
pacman::p_load(
     tidyverse,  # gestion des donnees et visualisation 
     outbreaks)  # exemple de données sur les épidémies

# Liste des cas d'épidémie de grippe
outbreak_raw <- outbreaks::fluH7N9_china_2013 # récupérer les données à partir du package outbreaks 

# nettoyage des données
outbreak <- outbreak_raw %>% 
     mutate(across(contains("date"), as.Date))

# graphique de l'epidémie 

ggplot(data = outbreak)+
     geom_histogram(
          mapping = aes(x = date_of_onset),
          binwidth = 7
     )+
  scale_x_date(
    date_format = "%d %m"
  )

```
*Copiez* tout le code sur votre clipboard, et exécutez la commande suivante :   

```{r, eval=F}
reprex::reprex()
```

Vous verrez une fenêtre HTML apparaître dans la fenêtre viewer de RStudio. Elle contiendra l'ensemble de votre code et tous les messages, les erreurs, ou les résultats de graphique. Ce résultat est également copié sur votre presse-papier, de façon à ce que vous pouvez le poster directement dans un issue Github ou un poste de forum.  

```{r, out.width=c('100%', '100%'), warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "errors_reprex_RStudio1.png"))
```


* Si vous définissez `session_info = TRUE`, le resultat de `sessioninfo::session_info()` avec votre version de R et du package R sera inclus.  
* Vous pouvez spécifier un répertoire de travail avec `wd = `.  
* Vous pouvez en lire plus sur les arguments et les variations possibles dans la [documentation]() ou en saisissant `?reprex`.

Dans l'exemple ci-dessus, la commande `ggplot()` ne s'est pas exécutée car l'argument `date_format =` n'est pas correcte - il devrait être `date_labels = `.  



### Données minimales {.unnumbered}  

Les aidants doivent être en mesure d'utiliser vos données - idéalement, ils doivent pouvoir les créer *avec du code*.  

Pour créer un ensemble de données minimales, considérez anonymer et utiliser seulement un sous-ensemble des observations. 

EN CONSTRUCTION - vous pouvez également utiliser la fonction `dput()` pour créer un ensemble de données minimales. 




## Poster sur un forum  

Lisez beaucoup de messages de forum. Essayez de comprendre quels messages sont bien rédigés et lesquels ne sont pas.  

1) Tout d'abord, décidez si vous devez poser la question. Avez-vous *parcouru* le site web du forum, en essayant divers termes de recherche, pour voir si votre question n'a pas déjà été posée ?  

2) Choisissez un titre informatif pour votre question (pas "Au secours ! ça ne marche pas").  

3) Rédigez votre question :  

* Présentez votre situation et votre problème  
* Liez aux posts de problèmes similaires et expliquez pourquoi ils ne répondent pas à votre question
* Incluez toute information pertinente pour aider quelqu'un qui ne connaît pas le contexte de votre travail.  
* Donnez un exemple minimal reproductible avec les informations de votre session R.  
* Utilisez la propre orthographe, grammaire et ponctuation, et divisez votre question en paragraphes pour faciliter la lecture.  

4) surveillez votre question une fois qu'elle est publiée pour pouvoir répondre à toute demande de clarification. Soyez gentil et aimable - souvent, les personnes qui répondent vous aident volontairement. Si vous avez une question complémentaire, demandez-vous si elle doit faire l'objet d'une question différente.  

5) Marquez la question comme ayant reçu une réponse, *si* vous obtenez une réponse qui répond à la demande *originale*. Cela permet aux autres personnes de reconnaître rapidement la solution.  


Lisez ces articles sur [comment poser une bonne question](https://stackoverflow.com/help/how-to-ask) et le [code de conduite de Stack overflow](https://stackoverflow.com/conduct).  


<!-- ======================================================= -->
## Resources { }


Page Tidyverse sur comment [obtenir de l'aide](https://www.tidyverse.org/help/#:~:text=When%20you%20want%20to%20make,to%20load%20the%20reprex%20package.&text=Enter%20reprex()%20in%20the,preview%20of%20your%20rendered%20reprex.)

Astuces pour [produire un ensemble de données minimal](https://xiangxing98.github.io/R_Learning/R_Reproducible.nb.html#producing-a-minimal-dataset)

Documentation de la [fonction dput](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/dput)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/help.Rmd-->

# Relational databases { }  

THIS PAGE IS UNDER CONSTRUCTION


## Preparation  


### Load packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,           # import/export
  here,          # filepaths
  tidyverse      # data mgmt and vizualization
  )
```

### Import data {.unnumbered}

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```


```{r, eval=F}
# fake import the linelist
linelist <- import("linelist_cleaned.xlsx")
```




## SQL and R


## Connecting to relational databases  




## Examples for writing this page

To include an image, save it into the "images" folder and call it using the command below. The example below shows how to display two images at 50% next to each other. 

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "RStudio_tools_options_1.png"))

knitr::include_graphics(here::here("images", "RStudio_tools_options.png"))
```

To display an interactive data frame for the user, use this command (replace linelist with your data frame - you can also use head() to reduce the number of rows for processing time.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
## Resources {  }



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/relational_databases.Rmd-->


# R sur les lecteurs réseau {#network_drives}  

 


<!-- ======================================================= -->
## Aperçu { }

L'utilisation de R sur des lecteurs partagés du réseau ou de "l'entreprise" peut présenter des défis supplémentaires. Cette page contient des approches, des erreurs courantes et des suggestions de dépannage tirées de notre expérience de travail sur ces questions. Cela inclut des conseils pour les situations particulièrement délicates impliquant R Markdown.  


**Utilisation de R sur des lecteurs réseau : Principes généraux**  

1) Vous devez obtenir un accès administrateur pour votre ordinateur. Configurez RStudio spécifiquement pour qu'il s'exécute en tant qu'administrateur.  
2) Enregistrez les paquets dans une bibliothèque située sur un lecteur portant une lettre (par exemple, "C :") lorsque cela est possible. Utilisez le moins possible une bibliothèque de paquets dont le chemin commence par "\\\".  
3) Le paquet **rmarkdown** ne doit **pas** être dans une bibliothèque de paquets "\\\", car alors il ne peut pas se connecter à TinyTex ou Pandoc.  




## RStudio en tant qu'administrateur  

Lorsque vous cliquez sur l'icône RStudio pour ouvrir RStudio, faites-le avec un clic droit. Selon votre machine, vous verrez peut-être une option "Exécuter en tant qu'administrateur". Sinon, vous verrez peut-être une option permettant de sélectionner Propriétés (une fenêtre devrait alors apparaître avec l'option "Compatibilité", et vous pourrez cocher la case "Exécuter en tant qu'administrateur").  




### Commandes utiles 

Vous trouverez ci-dessous quelques commandes utiles lorsque vous essayez de résoudre des problèmes en utilisant R sur des lecteurs réseau.  

Vous pouvez renvoyer le(s) chemin(s) des bibliothèques de paquets que R utilise. Ils seront listés dans l'ordre que R utilise pour installer/charger/rechercher les paquets. Ainsi, si vous voulez que R utilise une autre bibliothèque par défaut, vous pouvez changer l'ordre de ces chemins (voir ci-dessous).  

```{r, eval=F}
# Recherche de bibliothèques
.libPaths() # Vos chemins de bibliothèques, listés dans l'ordre d'installation/de recherche de R. 
                              # Note : toutes les bibliothèques seront listées, mais pour en installer certaines (par exemple C :), vous devrez peut-être exécuter RStock. 
                              # devrez peut-être exécuter RStudio en tant qu'administrateur (cela n'apparaîtra pas dans le menu déroulant 
                              # menu déroulant des bibliothèques d'installation des paquets) 
```

Vous pouvez vouloir changer l'ordre des bibliothèques de paquets utilisées par R. Par exemple, si R récupère un emplacement de bibliothèque qui commence par "\\\" et un autre qui commence par une lettre, par exemple "D :". Vous pouvez ajuster l'ordre de `.libPaths()` avec le code suivant.  

```{r, eval=F}
# Changer l'ordre des bibliothèques
# Ceci peut affecter la priorité de R à trouver un paquet. Par exemple, vous pouvez vouloir que votre bibliothèque C : soit listée en premier.
myPaths <- .libPaths() # obtenir les chemins d'accès
myPaths <- c(myPaths[2], myPaths[1]) # les commuter
.libPaths(myPaths) # les réaffecter
```

Si vous avez des difficultés à connecter R Markdown à Pandoc, commencez par ce code pour savoir où RStudio pense que votre installation Pandoc se trouve.

```{r, eval=F}
# Trouver Pandoc
Sys.getenv("RSTUDIO_PANDOC") # Trouver où RStudio pense que votre installation Pandoc se trouve
```

Si vous voulez voir à partir de quelle bibliothèque un paquet est chargé, essayez le code suivant :  

```{r, eval=F}
# Trouver un paquetage
# donne le premier emplacement du paquet (notez l'ordre de vos bibliothèques)
find.package("rmarkdown", lib.loc = NULL, quiet = FALSE, verbose = getOption("verbose")) 
```



<!-- ======================================================= -->
## Dépannage des erreurs courantes { }



**"Failed to compile...tex in rmarkdown "**  
* Vérifiez l'installation de TinyTex, ou installez TinyTex à l'emplacement C :. Voir la page [R - les bases](#rbasics)pour savoir comment installer TinyTex.  

```{r, eval=F}
# vérifiez/installez tinytex, à l'emplacement C :.
tinytex::install_tinytex()
tinytex:::is_tinytex() # devrait retourner VRAI (notez les trois deux points)
```


**Les routines Internet ne peuvent pas être chargées**.  

Par exemple, `Erreur dans tools::startDynamicHelp() : les routines internet ne peuvent pas être chargées`.  

* Essayez de sélectionner la version 32 bits de RStudio via Tools/Global Options.  
  * Note : si la version 32 bits n'apparaît pas dans le menu, assurez-vous que vous n'utilisez pas RStudio v1.2.  
* Alternativement, essayez de désinstaller R et de le réinstaller avec une version différente (32 au lieu de 64).


**C : la bibliothèque n'apparaît pas comme une option lorsque j'essaie d'installer les paquets manuellement**.

* Lancez RStudio en tant qu'administrateur, cette option apparaîtra alors.  
* Pour configurer RStudio pour qu'il soit toujours exécuté en tant qu'administrateur (avantageux lorsque vous utilisez un projet R où vous ne cliquez pas sur l'icône RStudio pour l'ouvrir)... cliquez avec le bouton droit de la souris sur l'icône Rstudio. 

L'image ci-dessous montre comment vous pouvez sélectionner manuellement la bibliothèque dans laquelle installer un paquet. Cette fenêtre apparaît lorsque vous ouvrez le volet Packages RStudio et cliquez sur "Installer". 

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "network_install.png"))
```

**Pandoc 1 erreur**  

Si vous obtenez "pandoc error 1" lorsque vous tricotez des scripts R Markdowns sur des lecteurs réseau :  

* Sur plusieurs emplacements de bibliothèque, faites en sorte que celui qui a un lecteur avec une lettre soit listé en premier (voir les codes ci-dessus).  
* La solution ci-dessus a fonctionné lors du tricotage sur le lecteur local, mais lors d'une connexion Internet en réseau.  
* Voir plus de conseils ici : https://ciser.cornell.edu/rmarkdown-knit-to-html-word-pdf/  

**Erreur Pandoc 83**  

L'erreur ressemblera à quelque chose comme ceci : `Impossible de trouver le fichier...rmarkdown...lua...`. Cela signifie qu'il n'a pas pu trouver ce fichier.  

Voir https://stackoverflow.com/questions/58830927/rmarkdown-unable-to-locate-lua-filter-when-knitting-to-word  

Possibilités :  

1) Le paquet Rmarkdown n'est pas installé  
2) Le paquet Rmarkdown n'est pas trouvable  
3) Un problème de droits d'administration.  

Il est possible que R ne soit pas capable de trouver le fichier du paquet **rmarkdown**, donc vérifiez dans quelle bibliothèque se trouve le paquet **rmarkdown** (voir le code ci-dessus). Si le paquet est installé dans une bibliothèque inaccessible (par exemple, commençant par "\\\"), pensez à le déplacer manuellement vers C : ou une autre bibliothèque de lecteur nommée. Soyez conscient que le paquet **rmarkdown** doit pouvoir se connecter à l'installation de TinyTex, et ne peut donc pas être installé dans une bibliothèque sur un lecteur réseau.


**Erreur Pandoc 61**  

Par exemple : `Erreur : la conversion du document pandoc a échoué avec l'erreur 61` ou `Impossible d'aller chercher...`.  

* Essayez d'exécuter RStudio en tant qu'administrateur (cliquez avec le bouton droit de la souris sur l'icône, sélectionnez exécuter en tant qu'administrateur, voir les instructions ci-dessus).  
* Voir également si le paquet spécifique qui n'a pas pu être atteint peut être déplacé vers la bibliothèque C :.

**Erreur LaTex (voir ci-dessous)**.

Une erreur du type : ` ! Paquet pdftex.def Erreur : File 'cict_qm2_2020-06-29_files/figure-latex/unnamed-chunk-5-1.png' non trouvé : utilisation de la fonction brouillon.` ou `Erreur : LaTeX n'a pas réussi à compiler file_name.tex.`.  

* Voir https://yihui.org/tinytex/r/#debugging pour des conseils de débogage.  
* Voir file_name.log pour plus d'informations.


**Erreur Pandoc 127**  

Cela peut être un problème de RAM (espace). Redémarrez votre session R et réessayez. 


**Mappage de lecteurs réseau**

Le mappage d'un lecteur réseau peut être risqué. Consultez votre service informatique avant d'essayer.  

Un conseil emprunté à cette [discussion du forum](https://stackoverflow.com/questions/48161177/r-markdown-openbinaryfile-does-not-exist-no-such-file-or-directory/55616529?noredirect=1#comment97966859_55616529) : 

Comment ouvrir un fichier "via un lecteur réseau mappé" ?  

* Tout d'abord, vous devez connaître l'emplacement réseau auquel vous essayez d'accéder.  
* Ensuite, dans le gestionnaire de fichiers de Windows, vous devez cliquer avec le bouton droit de la souris sur "Ce PC" dans le volet de droite, et sélectionner "Mapper un lecteur réseau".  
* Passez par la boîte de dialogue pour définir l'emplacement réseau de tout à l'heure comme un lecteur de lettres.  
* Maintenant, vous avez deux façons d'accéder au fichier que vous ouvrez. L'utilisation du chemin d'accès par lettre du lecteur devrait fonctionner.  


**Erreur dans install.packages()**  

Si vous obtenez une erreur qui inclut la mention d'un répertoire "verrouillé", par exemple : `Erreur dans install.packages : ERROR : échec du verrouillage du répertoire...``

Regardez dans votre bibliothèque de paquets et vous verrez un répertoire dont le nom commence par "00LOCK". Essayez les astuces suivantes :  

* Supprimez manuellement le répertoire du dossier "00LOCK" de votre bibliothèque de paquets. Essayez d'installer à nouveau le paquetage.  
* Vous pouvez aussi essayer la commande `pacman::p_unlock()` (vous pouvez aussi mettre cette commande dans le Rprofile pour qu'elle s'exécute à chaque fois que le projet s'ouvre). Ensuite, essayez à nouveau d'installer le paquet. Cela peut prendre plusieurs essais.  
* Essayez d'exécuter RStudio en mode Administrateur, et essayez d'installer les paquets un par un.  
* Si tout le reste échoue, installez le paquet dans une autre bibliothèque ou un autre dossier (par exemple Temp), puis copiez manuellement le dossier du paquet dans la bibliothèque souhaitée.  

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/network_drives.Rmd-->


# Tableau de données {#data_table}  
     
Le manuel se concentre sur les fonctions verbales **dplyr** et l'opérateur pipe **magrittr** `%>%` comme méthode pour nettoyer et regrouper les données, mais le paquet **data.table** offre une méthode alternative que vous pourriez rencontrer dans votre carrière R.  


<!-- ======================================================= -->
## Intro aux tableaux de données { }

Une table de données est une structure de données bidimensionnelle comme un cadre de données qui permet d'effectuer des opérations de regroupement complexes. La syntaxe data.table est structurée de manière à ce que les opérations puissent être effectuées sur les lignes, les colonnes et les groupes. 

La structure est **DT[i, j, by]**, séparée par 3 parties : les arguments **i, j** et **by**. L'argument **i** permet de sous-dimensionner les lignes requises, l'argument **j** permet d'opérer sur les colonnes et l'argument **by** permet d'opérer sur les colonnes par groupes.
  
Cette page abordera les sujets suivants :  

* Importation de données et utilisation de `fread()` et `fwrite()`.
* Sélection et filtrage des lignes en utilisant l'argument **i**.
* Utilisation des fonctions d'aide `%like%`, `%chin%`, `%between%`. 
* Sélection et calcul sur les colonnes à l'aide de l'argument **j**.
* Calculer par groupes avec l'argument **by**
* Ajouter et mettre à jour des données dans des tableaux de données en utilisant `:=`


<!-- ======================================================= -->
## Load packages and import data { }


## Chargement des paquets et importation des données { }

### Charger des paquets {.unnumbered}  

En utilisant la fonction `p_load()` de **pacman**, nous chargeons (et installons si nécessaire) les paquets nécessaires à cette analyse.
     
     
     
     
```{r}
pacman::p_load(
  rio, # pour importer les données
  data.table, # pour regrouper et nettoyer les données
  tidyverse, # permet d'utiliser la fonction pipe (%>%) dans ce chapitre
  here 
  ) 
```



### Importer les données {.unnumbered}

Cette page va explorer certaines des fonctions principales de **data.table** en utilisant la liste de cas référencée tout au long du manuel.

Nous importons le jeu de données des cas d'une épidémie d'Ebola simulée. Si vous souhaitez télécharger les données pour les suivre pas à pas, consultez les instructions de la page [Donwload book and data]. L'ensemble de données est importé à l'aide de la fonction `import()` du paquet **rio**. Voir la page [Import and export] pour les différentes manières d'importer des données. A partir de là, nous utilisons `data.table()` pour convertir le cadre de données en un tableau de données.

```{r}
linelist <- rio::import(here("data", "linelist_cleaned.xlsx")) %>% data.table()
```

La fonction `fread()` est utilisée pour importer directement des fichiers délimités réguliers, tels que les fichiers .csv, vers un format de table de données. Cette fonction, et sa contrepartie, `fwrite()`, utilisée pour écrire les tables de données comme des fichiers délimités réguliers, sont des options très rapides et efficaces en termes de calcul pour les grandes bases de données.


Les 20 premières lignes de `linelist` :  

```{r message=FALSE, echo=F, eval=FALSE}
DT::datatable(head(linelist,20), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Les commandes de base de R, telles que `dim()`, utilisées pour les cadres de données, peuvent également être utilisées pour les tableaux de données.

```{r}
dim(linelist) #donne le nombre de lignes et de colonnes du tableau de données
```



<!-- ======================================================= -->
## L'argument i: sélection et filtrage des lignes{ }
     
En rappelant la structure **DT[i, j, by]**, nous pouvons filtrer les lignes en utilisant soit des numéros de ligne, soit des expressions logiques. L'argument i est le premier ; par conséquent, la syntaxe **DT[i]** ou **DT[i,]** peut être utilisée. 

Le premier exemple récupère les 5 premières lignes de la table de données, le deuxième exemple sous-entend que les cas sont âgés de 18 ans ou plus, et le troisième exemple sous-entend que les cas âgés de 18 ans ou plus mais non diagnostiqués à l'hôpital central :


```{r, eval=F}
linelist[1:5] #renvoie la 1ère à la 5ème ligne
linelist[age >= 18] #sous-entend les cas égaux ou supérieurs à 18 ans
linelist[age >= 18 & hospital != "Central Hospital"] #subset les cas égaux ou supérieurs à 18 ans mais non diagnostiqués à Central Hospital

```

L'utilisation de .N dans l'argument i représente le nombre total de lignes dans la table de données. Cela peut être utilisé pour effectuer un sous-ensemble sur le nombre de lignes : 

```{r, eval=F}
linelist[.N] #renvoie la dernière ligne
linelist[15 :.N] #renvoie la 15ème à la dernière ligne
```


### Utilisation de fonctions d'aide pour le filtrage {.unnumbered}  

Le tableau de données utilise des fonctions d'aide qui facilitent le sous-ensemble des lignes. La fonction `%like%` est utilisée pour faire correspondre un motif dans une colonne, `%chin%` est utilisée pour faire correspondre un caractère spécifique, et la fonction d'aide `%between%` est utilisée pour faire correspondre des colonnes numériques dans une plage prédéfinie.

Dans les exemples suivants, nous :
* filtrons les lignes où la variable hospital contient "Hospital".
* filtrons les lignes où le résultat est "Recover" ou "Death".
* filtrons les lignes dans la tranche d'âge 40-60 ans

```{r, eval=F}
linelist[hospital %like% "Hospital"] #filtre les lignes où la variable hospital contient "Hospital"
linelist[outcome %chin% c("Recover", "Death")] #filtre les lignes où l'issue est "Recover" ou "Death".
linelist[age %between% c(40, 60)] #filtre les lignes dans la tranche d'âge 40-60

#%between% doit prendre un vecteur de longueur 2, tandis que %chin% peut prendre des vecteurs de longueur >= 1

```

## L'argument j: sélection et calcul sur les colonnes{ }

En utilisant la structure **DT[i, j, by]**, nous pouvons sélectionner des colonnes en utilisant des nombres ou des noms. L'argument **j** est le second ; on utilise donc la syntaxe **DT[, j]**. Pour faciliter les calculs sur l'argument **j**, la colonne est enveloppée en utilisant soit `list()` soit `.()`. 


### Sélection de colonnes {.unnumbered} 

Le premier exemple récupère les première, troisième et cinquième colonnes de la table de données, le deuxième exemple sélectionne toutes les colonnes à l'exception des colonnes taille, poids et sexe. Le troisième exemple utilise la terminaison `.()` pour sélectionner les colonnes **identification du cas** et **résultat**.


```{r, eval=F}
linelist[ , c(1,3,5)]
linelist[ , -c("gender", "age", "wt_kg", "ht_cm")]
linelist[ , list(case_id, outcome)] #linelist[ , .(case_id, outcome)] fonctionne tout aussi bien

```

### Calcul sur les colonnes {.unnumbered} 

En combinant les arguments **i** et **j**, il est possible de filtrer les lignes et de calculer sur les colonnes. L'utilisation de **.N** dans l'argument **j** représente également le nombre total de lignes dans le tableau de données et peut être utile pour retourner le nombre de lignes après le filtrage des lignes.

Dans les exemples suivants, nous :
* Comptons le nombre de cas qui sont restés plus de 7 jours à l'hôpital.
* Calculer l'âge moyen des cas qui sont décédés à l'hôpital militaire.
* Calculer l'écart-type, la médiane et l'âge moyen des cas qui se sont rétablis à l'hôpital central.

```{r}
linelist[days_onset_hosp > 7 , .N]
linelist[hospital %like% "Military" & outcome %chin% "Death", .(mean(age, na.rm = T))] #na.rm = T supprime les valeurs N/A
linelist[hospital == "Central Hospital" & outcome == "Recover", 
                 .(mean_age = mean(age, na.rm = T),
                   median_age = median(age, na.rm = T),
                   sd_age = sd(age, na.rm = T))] #cette syntaxe n'utilise pas les fonctions d'aide mais fonctionne tout aussi bien

```

N'oubliez pas que l'utilisation de la terminaison .() dans l'argument j facilite le calcul, renvoie un tableau de données et permet de nommer les colonnes.

## L'argument by : calcul par groupes{ }

L'argument **by** est le troisième argument de la structure **DT[i, j, by]**. L'argument **by** accepte à la fois un vecteur de caractères et la syntaxe `list()` ou `.()`. L'utilisation de la syntaxe `.()` dans l'argument **by** permet de renommer les colonnes à la volée.

Dans les exemples suivants, nous :	
* regroupons le nombre de cas par hôpital
* dans les cas de 18 ans ou plus, calculer la taille et le poids moyens des cas selon le sexe et selon qu'ils sont guéris ou décédés
* dans les admissions qui ont duré plus de 7 jours, compter le nombre de cas selon le mois d'admission et l'hôpital où ils ont été admis.


```{r}
linelist[, .N, .(hospital)] #le nombre de cas par hôpital
linelist[age > 18, .(mean_wt = mean(wt_kg, na.rm = T),
                             mean_ht = mean(ht_cm, na.rm = T)), .(gender, outcome)] #NAs représentent les catégories pour lesquelles les données sont manquantes
linelist[days_onset_hosp > 7, .N, .(month = month(date_hospitalisation), hospital)]

```

Data.table permet également d'enchaîner les expressions comme suit :

```{r}

linelist[, .N, .(hospital)][order(-N)][1:3] #La première sélectionne tous les cas par hôpital, la deuxième ordonne les cas par ordre décroissant, la troisième sous-ensemble les 3 hôpitaux ayant le plus grand nombre de cas.


```

Dans ces exemples, nous partons du principe qu'une ligne du tableau de données correspond à un nouveau cas, et nous pouvons donc utiliser la fonction **.N** pour représenter le nombre de lignes du tableau de données. Une autre fonction utile pour représenter le nombre de cas uniques est `uniqueN()`, qui retourne le nombre de valeurs uniques dans une entrée donnée. Ceci est illustré ici :

```{r}

linelist[, .(uniqueN(gender))] #souvenez-vous que .() dans l'argument j renvoie un tableau de données

```

La réponse est 3, car les valeurs uniques de la colonne sexe sont m, f et N/A. Comparez avec la fonction R de base `unique()`, qui renvoie toutes les valeurs uniques dans une entrée donnée :

```{r}

linelist[, .(unique(gender))]
```

Pour trouver le nombre de cas uniques dans un mois donné, nous écririons ce qui suit :

```{r}

linelist[, .(uniqueN(case_id)), .(month = month(date_hospitalisation))]

```

## Ajout et mise à jour des tables de données { }

L'opérateur `:=` est utilisé pour ajouter ou mettre à jour des données dans une table de données. L'ajout de colonnes à votre table de données peut se faire de la manière suivante :

```{r}

linelist[, adult := age >= 18] #ajoute une colonne
linelist[, c("child", "wt_lbs") := .(age < 18, wt_kg*2.204)] #pour ajouter plusieurs colonnes, il faut utiliser la syntaxe c("") et list() ou .()
linelist[, `:=` (bmi_in_range = (bmi > 16 & bmi < 40),
                         no_infector_source_data = is.na(infector) | is.na(source))] #Cette méthode utilise := comme un opérateur fonctionnel `:=`.
linelist[, adult := NULL] #supprime la colonne

```


Des agrégations plus complexes dépassent le cadre de ce chapitre d'introduction, mais l'idée est de fournir une alternative populaire et viable à **dplyr** pour regrouper et nettoyer les données. Le package **data.table** est un excellent package qui permet d'obtenir un code soigné et lisible.


## Ressources { }

Voici quelques ressources utiles pour plus d'informations :
* https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html
* https://github.com/Rdatatable/data.table
* https://s3.amazonaws.com/assets.datacamp.com/img/blog/data+table+cheat+sheet.pdf
* https://www.machinelearningplus.com/data-manipulation/datatable-in-r-complete-guide/
* https://www.datacamp.com/community/tutorials/data-table-r-tutorial

Vous pouvez exécuter n'importe quelle fonction de synthèse sur des données groupées ; voir la Cheat Sheet ici pour plus d'informations :
https://s3.amazonaws.com/assets.datacamp.com/blog_assets/datatable_Cheat_Sheet_R.pdf

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/data_table.Rmd-->


# Advanced RStudio { }  

THIS PAGE IS UNDER CONSTRUCTION



## Find in Files 

This is an advanced search function that allows you to "Find and replace" terms across many scripts at one time.  

## Keyboard shortcuts  

https://www.dataquest.io/blog/rstudio-tips-tricks-shortcuts/


## Connections  

Query SQL  



## Customize appearance  


## Package management with **renv**  
The renv package is replacing the Packrat package that RStudio used to maintain.


## View function source code  


## Environments  


## RStudio Connect and Cloud  


## Using Python with RStudio  



## Github  

See the page on [Collaboration with Github] for tips on how to use RStudio with Github.  




<!-- ======================================================= -->
## Resources {  }



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/rstudio_advanced.Rmd-->

